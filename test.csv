title,link,text
Founding Fathers of the United States,https://en.wikipedia.org/wiki/Founding_Fathers_of_the_United_States,"
 The Founding Fathers of the United States, often simply referred to as the Founding Fathers or the Founders, were a group of late-18th-century American revolutionary leaders who united the Thirteen Colonies, oversaw the War of Independence from Great Britain, established the United States of America, and crafted a framework of government for the new nation. 
 The Founding Fathers include those who signed the United States Declaration of Independence, the Articles of Confederation, the United States Constitution and other founding documents; it can also be applied to certain military personnel who fought in the American Revolution.  The single person most identified as ""Father"" of the United States is George Washington, a general in the American Revolution and the 1st President of the United States.  In 1973, historian Richard B. Morris identified seven figures as key founders, based on what he called the ""triple tests"" of leadership, longevity, and statesmanship: John Adams, Benjamin Franklin, Alexander Hamilton, John Jay, Thomas Jefferson, James Madison, and Washington.[2]
 Most of the Founding Fathers were of English ancestry, though many had family roots extending across various regions of the British Isles, including Scotland, Wales, and Ireland. Additionally, some traced their lineage back to the early Dutch settlers of New York (New Netherland) during the colonial era, while others were descendants of French Huguenots who settled in the colonies, escaping religious persecution in France.[3][4][5]
 Historian Richard Morris' selection of seven key founders was widely accepted through the 20th century.[7][8] John Adams, Thomas Jefferson, and Benjamin Franklin were members of the Committee of Five that were charged by the Second Continental Congress with drafting the Declaration of Independence. Franklin, Adams, and John Jay negotiated the 1783 Treaty of Paris, which established American independence and brought an end to the American Revolutionary War.[9] The constitutions drafted by Jay and Adams for their respective states of New York (1777) and Massachusetts (1780) proved influential in the language used in developing the U.S. Constitution.[10][11][12] The Federalist Papers, which advocated the ratification of the Constitution, were written by Alexander Hamilton, James Madison, and Jay. George Washington was commander-in-chief of the Continental Army and later president of the Constitutional Convention.[13][14] 
 Each of these men held additional important roles in the early government of the United States. Washington, Adams, Jefferson, and Madison served as the first four presidents; Adams and Jefferson were the nation's first two vice presidents;[15] Jay was the nation's first chief justice;[16] Hamilton was the first secretary of the treasury;[17] Jefferson was the first secretary of state;[18][19] and Franklin was America's most senior diplomat from the start of the Revolutionary War through its conclusion with the signing of the Treaty of Paris in 1783.[20]
 The list of Founding Fathers is often expanded to include the signers of the Declaration of Independence and individuals who later approved the U.S. Constitution.[2] Some scholars regard all delegates to the Constitutional Convention as Founding Fathers whether they approved the Constitution or not.[21][22] In addition, some historians include signers of the Articles of Confederation, which was adopted in 1781 as the nation's first constitution.[23]
 Historians have come to recognize others as founders, such as Revolutionary War military leaders as well as participants in developments leading up to the war, including prominent writers, orators, and other men and women who contributed to cause.[8][24][25][26] Since the 19th century, the Founding Fathers have shifted from the concept of them as demigods who created the modern nation-state, to take into account their inability to address issues such as slavery and the debt owed after the American Revolutionary War.[27][28] Scholars emphasize that the Founding Fathers' accomplishments and shortcomings be viewed within the context of their time.[29]
 The phrase ""Founding Fathers"" was first used by U.S. senator Warren G. Harding in his keynote speech at the Republican National Convention of 1916.[30] Harding later repeated the phrase at his March 4, 1921, presidential inauguration.[31] While U.S. presidents used the terms ""founders"" and ""fathers"" in their speeches throughout much of the early 20th century, it was another 60 years before Harding's phrase would be used again during the presidential inaugural ceremonies. Ronald Reagan referred to ""Founding Fathers"" at both his first inauguration on January 20, 1981, and his second on January 20, 1985.[32][33]
 In 1811, responding to praise for his generation, John Adams wrote to a younger Josiah Quincy III, ""I ought not to object to your Reverence for your Fathers as you call them ... but to tell you a very great secret ... I have no reason to believe We were better than you are.""[34] He also wrote, ""Don't call me, ... Father ... [or] Founder ... These titles belong to no man, but to the American people in general.""[35]
 In Thomas Jefferson's second presidential inaugural address in 1805, he referred to those who first came to the New World as ""forefathers"".[36] At his 1825 inauguration, John Quincy Adams called the U.S. Constitution ""the work of our forefathers"" and expressed his gratitude to ""founders of the Union"".[37] In July of the following year, this second President Adams, in an executive order upon the deaths of his father, John, and Jefferson, who both died on the same day, paid tribute to them as ""Fathers"" and ""Founders of the Republic"".[38] These terms were used in the U.S. throughout the 19th century, from the inaugurations of Martin Van Buren and James Polk in 1837 and 1845, to Abraham Lincoln's Cooper Union speech in 1860 and his Gettysburg Address in 1863, and up to William McKinley's first inauguration in 1897.[39][40][41][42]
 At a 1902 celebration of Washington's Birthday in Brooklyn, James M. Beck, a constitutional lawyer and later a U.S. congressman, delivered an address, ""Founders of the Republic"", in which he connected the concepts of founders and fathers, saying: ""It is well for us to remember certain human aspects of the founders of the republic. Let me first refer to the fact that these fathers of the republic were for the most part young men.""[25]
 The National Archives has identified three founding documents as the ""Charters of Freedom"": Declaration of Independence, United States Constitution, and Bill of Rights. According to the Archives, these documents ""have secured the rights of the American people for nearly two and a half centuries and are considered instrumental to the founding and philosophy of the United States.""[43] In addition, as the nation's first constitution, the Articles of Confederation and Perpetual Union is also a founding document.[44][45] As a result, signers of three key documents are generally considered to be Founding Fathers of the United States: Declaration of Independence (DI),[21] Articles of Confederation (AC),[23] and U.S. Constitution (USC).[22] The following table provides a list of these signers, some of whom signed more than one document.
 The 55 delegates who attended the Constitutional Convention are referred to as framers. Of these, the 16 listed below did not sign the document.[46] Three refused, while the remainder left early, either in protest of the proceedings or for personal reasons.[47][48] Nevertheless, some sources regard all framers as founders, including those who did not sign:[22][49]
 (*) Randolph, Mason, and Gerry were the only three present at the Constitution's adoption who refused to sign.
 In addition to the signers and Framers of the founding documents and one of the seven notable leaders previously mentioned—John Jay—the following are regarded as founders based on their contributions to the creation and early development of the new nation:
 Historians have come to recognize the roles women played in the nation's early development, using the term ""Founding Mothers"".[84][85] Among the women honored in this respect are:
 The following men and women are also recognized for the notable contributions they made during the founding era:
 In the mid-1760s, Parliament began levying taxes on the colonies to finance Britain's debts from the French and Indian War, a decade-long conflict that ended in 1763.[128][129] Opposition to Stamp Act and Townshend Acts united the colonies in a common cause.[130] While the Stamp Act was withdrawn, taxes on tea remained under the Townshend Acts and took on a new form in 1773 with Parliament's adoption of the Tea Act. The new tea tax, along with stricter customs enforcement, was not well-received across the colonies, particularly in Massachusetts.[131]
 On December 16, 1773, 150 colonists disguised as Mohawk Indians boarded ships in Boston and dumped 342 chests of tea into the city's harbor, a protest that came to be known as the Boston Tea Party.[132][133] Orchestrated by Samuel Adams and the Boston Committee of Correspondence, the protest was viewed as treasonous by British authorities.[134] In response, Parliament passed the Coercive or Intolerable Acts, a series of punitive laws that closed Boston's port and placed the colony under direct control of the British government. These measures stirred unrest throughout the colonies, which felt Parliament had overreached its authority and was posing a threat to the self-rule that had existed in the Americas since the 1600s.[131]
 Intent on responding to the Acts, twelve of the Thirteen Colonies agreed to send delegates to meet in Philadelphia as the First Continental Congress, with Georgia declining because it needed British military support in its conflict with native tribes.[135] The concept of an American union had been entertained long before 1774, but always embraced the idea that it would be subject to the authority of the British Empire. By 1774, however, letters published in colonial newspapers, mostly by anonymous writers, began asserting the need for a ""Congress"" to represent all Americans, one that would have equal status with British authority.[136]
 The Continental Congress was convened to deal with a series of pressing issues the colonies were facing with Britain. Its delegates were men considered to be the most intelligent and thoughtful among the colonialists. In the wake of the Intolerable Acts, at the hands of an unyielding British King and Parliament, the colonies were forced to choose between either totally submitting to arbitrary Parliamentary authority or resorting to unified armed resistance.[137][138] The new Congress functioned as the directing body in declaring a great war and was sanctioned only by reason of the guidance it provided during the armed struggle. Its authority remained ill-defined, and few of its delegates realized that events would soon lead them to decide policies that ultimately established a ""new power among the nations"". In the process the Congress performed many experiments in government before an adequate Constitution evolved.[139]
 The First Continental Congress convened at Philadelphia's Carpenter's Hall on September 5, 1774.[140] The Congress, which had no legal authority to raise taxes or call on colonial militias, consisted of 56 delegates, including George Washington of Virginia; John Adams and Samuel Adams of Massachusetts; John Jay of New York; John Dickinson of Pennsylvania; and Roger Sherman of Connecticut. Peyton Randolph of Virginia was unanimously elected its first president.[81][141]
 The Congress came close to disbanding in its first few days over the issue of representation, with smaller colonies desiring equality with the larger ones. While Patrick Henry, from the largest colony, Virginia, disagreed, he stressed the greater importance of uniting the colonies: ""The distinctions between Virginians, Pennsylvanians, New Yorkers, and New Englanders are no more. I am not a Virginian, but an American!"".[142] The delegates then began with a discussion of the Suffolk Resolves, which had just been approved at a town meeting in Milton, Massachusetts.[143] Joseph Warren, chairman of the Resolves drafting committee, had dispatched Paul Revere to deliver signed copies to the Congress in Philadelphia.[144][145][134] The Resolves called for the ouster of British officials, a trade embargo of British goods, and the formation of a militia throughout the colonies.[143] Despite the radical nature of the resolves, on September 17 the Congress passed them in their entirety in exchange for assurances that Massachusetts' colonists would do nothing to provoke war.[146][147]
 The delegates then approved a series of measures, including a Petition to the King in an appeal for peace and a Declaration and Resolves which introduced the ideas of natural law and natural rights, foreshadowing some of the principles found in the Declaration of Independence and Bill of Rights.[148] The declaration asserted the rights of colonists and outlined Parliament's abuses of power. Proposed by Richard Henry Lee, it also included a trade boycott known as the Continental Association.[149] The Association, a crucial step toward unification, empowered committees of correspondence throughout the colonies to enforce the boycott. The Declaration and its boycott directly challenged Parliament's right to govern in the Americas, bolstering the view of King George III and his administration under Lord North that the colonies were in a state of rebellion.[150]
 Lord Dartmouth, the secretary of state for the colonies who had been sympathetic to the Americans, condemned the newly established Congress for what he considered its illegal formation and actions.[151][152] In tandem with the Intolerable Acts, British Army commander-in-chief Lieutenant General Thomas Gage was installed as governor of Massachusetts. In January 1775, Gage's superior, Lord Dartmouth, ordered the general to arrest those responsible for the Tea Party and to seize the munitions that had been stockpiled by militia forces outside of Boston. The letter took several months to reach Gage, who acted immediately by sending out 700 army regulars. During their march to Lexington and Concord on the morning of April 19, 1775, the British troops encountered militia forces, who had been warned the night before by Paul Revere and another messenger on horseback, William Dawes. Even though it is unknown who fired the first shot, the Revolutionary War began.[153]
 On May 10, 1775, less than three weeks after the Battles at Lexington and Concord, the Second Continental Congress convened in the Pennsylvania State House. The gathering essentially reconstituted the First Congress with many of the same delegates in attendance.[154] Among the new arrivals were Benjamin Franklin of Pennsylvania, John Hancock of Massachusetts, and in June, Thomas Jefferson of Virginia. Hancock was elected president two weeks into the session when Peyton Randolph was recalled to Virginia to preside over the House of Burgesses as speaker, and Jefferson was named to replace him in the Virginia delegation.[155] After adopting the rules of debate from the previous year and reinforcing its emphasis on secrecy,[156][157] the Congress turned to its foremost concern, the defense of the colonies.[158]
 The provincial assembly in Massachusetts, which had declared the colony's governorship vacant, reached out to the Congress for direction on two matters: whether the assembly could assume the powers of civil government and whether the Congress would take over the army being formed in Boston.[159] In answer to the first question, on June 9 the colony's leaders were directed to choose a council to govern within the spirit of the colony's charter.[160][161] As for the second, Congress spent several days discussing plans for guiding the forces of all thirteen colonies. Finally, on June 14 Congress approved provisioning the New England militias, agreed to send ten companies of riflemen from other colonies as reinforcements, and appointed a committee to draft rules for governing the military, thus establishing the Continental Army. The next day, Samuel and John Adams nominated Washington as commander-in-chief, a motion that was unanimously approved.[162][163] Two days later, on June 17, the militias clashed with British forces at Bunker Hill, a victory for Britain but a costly one.[164]
 The Congress's actions came despite the divide between conservatives who still hoped for reconciliation with England and at the other end of the spectrum, those who favored independence.[165] To satisfy the former, Congress adopted the Olive Branch Petition on July 5, an appeal for peace to King George III written by John Dickinson. Then, the following day, it approved the Declaration of the Causes and Necessity of Taking Up Arms, a resolution justifying military action.[162] The declaration, intended for Washington to read to the troops upon his arrival in Massachusetts, was drafted by Jefferson but edited by Dickinson who thought its language too strong.[166][167] When the Olive Branch Petition arrived in London in September, the king refused to look at it.[168] By then, he had already issued a proclamation declaring the American colonies in rebellion.[169]
 Under the auspices of the Second Continental Congress and its Committee of Five,[170] Thomas Jefferson drafted the Declaration of Independence. It was presented to the Congress by the Committee on June 28,[171] and after much debate and editing of the document, on July 2, 1776,[172][173] Congress passed the Lee Resolution, which declared the United Colonies independent from Great Britain. Two days later, on July 4, the Declaration of Independence was adopted.[174] The name ""United States of America"", which first appeared in the Declaration, was formally approved by the Congress on September 9, 1776.[175]
 In an effort to get this important document promptly into the public realm John Hancock, president of the Second Continental Congress, commissioned John Dunlap, editor and printer of the Pennsylvania Packet, to print 200 broadside copies of the Declaration, which came to be known as the Dunlap broadsides. Printing commenced the day after the Declaration was adopted. They were distributed throughout the 13 colonies/states with copies sent to General Washington and his troops at New York with a directive that it be read aloud. Copies were also sent to Britain and other points in Europe.[176][177][171]
 While the colonists were fighting the British to gain independence their newly formed government, with its Articles of Confederation, were put to the test, revealing the shortcomings and weaknesses of America's first Constitution. During this time Washington became convinced that a strong federal government was urgently needed, as the individual states were not meeting the organizational and supply demands of the war on their own individual accord.[178][179] Key precipitating events included the Boston Tea Party in 1773, Paul Revere's Ride in 1775, and the Battles of Lexington and Concord in 1775.[180] George Washington's crossing of the Delaware River was a major American victory over Hessian forces at the Battle of Trenton and greatly boosted American morale.[181] The Battle of Saratoga and the Siege of Yorktown, which primarily ended the fighting between American and British, were also pivotal events during the war. The 1783 Treaty of Paris marked the official end of the war.[182]
 After the war, Washington was instrumental in organizing the effort to create a ""national militia"" made up of individual state units, and under the direction of the federal government. He also endorsed the creation of a military academy to train artillery offices and engineers. Not wanting to leave the country disarmed and vulnerable so soon after the war, Washington favored a peacetime army of 2,600 men. He also favored the creation of a navy that could repel any European intruders. He approached Henry Knox, who accompanied Washington during most of his campaigns, with the prospect of becoming the future Secretary of War.[183]
 After Washington's final victory at the surrender at Yorktown on October 19, 1781, more than a year passed before official negotiations for peace commenced. The Treaty of Paris was drafted in November 1782, and negotiations began in April 1783. The completed treaty was signed on September 3. Benjamin Franklin, John Adams, John Jay and Henry Laurens represented the United States,[184] while David Hartley, a member of Parliament, and Richard Oswald, a prominent and influential Scottish businessman, represented Great Britain.[185][186]
 Franklin, who had a long-established rapport with the French and was almost entirely responsible for securing an alliance with them a few months after the start of the war, was greeted with high honors from the French council, while the others received due accommodations but were generally considered to be amateur negotiators.[187] Communications between Britain and France were largely effected through Franklin and Lord Shelburne who was on good terms with Franklin.[188] Franklin, Adams and Jay understood the concerns of the French at this uncertain juncture and, using that to their advantage, in the final sessions of negotiations convinced both the French and the British that American independence was in their best interests.[189]
 Under the Articles of Confederation, the Congress of the Confederation had no power to collect taxes, regulate commerce, pay the national debt, conduct diplomatic relations, or effectively manage the western territories.[190][191][192] Key leaders – George Washington, Thomas Jefferson, Alexander Hamilton, James Madison, and others – began fearing for the young nation's fate.[193] As the Articles' weaknesses became more and more apparent, the idea of creating a strong central government gained support, leading to the call for a convention to amend the Articles.[194][195]
 The Constitutional Convention met in the Pennsylvania State House from May 14 through September 17, 1787.[196] The 55 delegates in attendance represented a cross-section of 18th-century American leadership. The vast majority were well-educated and prosperous, and all were prominent in their respective states with over 70 percent (40 delegates) serving in the Congress when the convention was proposed.[197][192]
 Many delegates were late to arrive, and after eleven days' delay, a quorum was finally present on May 25 to elect Washington, the nation's most trusted figure, as convention president.[198][199] Four days later, on May 29, the convention adopted a rule of secrecy, a controversial decision but a common practice that allowed delegates to speak freely.[200][201][202]
 Immediately following the secrecy vote, Virginia governor Edmund Randolph introduced the Virginia Plan, fifteen resolutions written by Madison and his colleagues proposing a government of three branches: a single executive, a bicameral (two-house) legislature, and a judiciary.[203][204][205] The lower house was to be elected by the people, with seats apportioned by state population. The upper house would be chosen by the lower house from delegates nominated by state legislatures. The executive, who would have veto power over legislation, would be elected by the Congress, which could overrule state laws.[206][207] While the plan exceeded the convention's objective of merely amending the Articles, most delegates were willing to abandon their original mandate in favor of crafting a new form of government.[208][195]
 Discussions of the Virginia resolutions continued into mid-June, when William Paterson of New Jersey presented an alternative proposal.[209] The New Jersey Plan retained most of the Articles' provisions, including a one-house legislature and equal power for the states. One of the plan's innovations was a ""plural"" executive branch, but its primary concession was to allow the national government to regulate trade and commerce.[210][211][212] Meeting as a committee of the whole, the delegates discussed the two proposals beginning with the question of whether there should be a single or three-fold executive and then whether to grant the executive veto power.[213] After agreeing on a single executive who could veto legislation, the delegates turned to an even more contentious issue, legislative representation.[214] Larger states favored proportional representation based on population, while smaller states wanted each state to have the same number of legislators.[215][216][217]
 By mid-July, the debates between the large-state and small-state factions had reached an impasse.[218] With the convention on the verge of collapse, Roger Sherman of Connecticut introduced what became known as the Connecticut (or Great) Compromise.[219][220][221] Sherman's proposal called for a House of Representatives elected proportionally and a Senate where all states would have the same number of seats. On July 16, the compromise was approved by the narrowest of margins, 5 states to 4.[222][223]
 The proceedings left most delegates with reservations.[224][225] Several went home early in protest, believing the convention was overstepping its authority.[226][227][228] Others were concerned about the lack of a Bill of Rights safeguarding individual liberties.[229][230] Even Madison, the Constitution's chief architect, was dissatisfied, particularly over equal representation in the Senate and the failure to grant Congress the power to veto state legislation.[231] Misgivings aside, a final draft was approved overwhelmingly on September 17, with 11 states in favor and New York unable to vote since it had only one delegate remaining, Hamilton.[224] Rhode Island, which was in a dispute over the state's paper currency, had refused to send anyone to the convention.[232][233] Of the 42 delegates present, only three refused to sign: Randolph and George Mason, both of Virginia, and Elbridge Gerry of Massachusetts.[234][225]
 The U. S. Constitution faced one more hurdle: approval by the legislatures in at least nine of the 13 states.[235] Within three days of the signing, the draft was submitted to the Congress of the Confederation, which forwarded the document to the states for ratification.[236] In November, Pennsylvania's legislature convened the first of the conventions. Before it could vote, Delaware became the first state to ratify, approving the Constitution on December 7 by a 30–0 margin.[237] Pennsylvania followed suit five days later, splitting its vote 46–23.[238] Despite unanimous votes in New Jersey and Georgia, several key states appeared to be leaning against ratification because of the omission of a Bill of Rights, particularly Virginia where the opposition was led by Mason and Patrick Henry, who had refused to participate in the convention claiming he ""smelt a rat"".[239][240][241] Rather than risk everything, the Federalists relented, promising that if the Constitution was adopted, amendments would be added to secure people's rights.[242]
 Over the next year, the string of ratifications continued. Finally, on June 21, 1788, New Hampshire became the ninth state to ratify, making the Constitution the law of the land.[243][244] Virginia followed suit four days later, and New York did the same in late July.[239] After North Carolina's assent in November, another year-and-a-half would pass before the 13th state would weigh in.[245] Facing trade sanctions and the possibility of being forced out of the union, Rhode Island approved the Constitution on May 29, 1790, by a begrudging 34–32 vote.[246][245]
 The Constitution officially took effect on March 4, 1789 (235 years ago) (1789-03-04), when the House and Senate met for their first sessions. On April 30, Washington was sworn in as the nation's first president.[247][248][249] Ten amendments, known collectively as the United States Bill of Rights, were ratified on December 15, 1791.[250] Because the delegates were sworn to secrecy, Madison's notes on the ratification were not published until after his death in 1836.[251]
 The Constitution, as drafted, was sharply criticized by the Anti-Federalists, a group that contended the document failed to safeguard individual liberties from the federal government. Leading Anti-Federalists included Patrick Henry and Richard Henry Lee, both from Virginia, and Samuel Adams of Massachusetts. Delegates at the Constitutional Convention who shared their views were Virginians George Mason and Edmund Randolph and Massachusetts representative Elbridge Gerry, the three delegates who refused to sign the final document.[252] Henry, who derived his hatred of a central governing authority from his Scottish ancestry, did all in his power to defeat the Constitution, opposing Madison every step of the way.[253]
 The criticisms are what led to the amendments proposed under the Bill of Rights. Madison, the bill's principal author, was originally opposed to the amendments, but was influenced by the 1776 Virginia Declaration of Rights, primarily written by Mason, and the Declaration of Independence, by Thomas Jefferson.[254] Jefferson, while in France, shared Henry's and Mason's fears about a strong central government, especially the president's power, but because of his friendship with Madison and the pending Bill of Rights, he quieted his concerns.[255] Alexander Hamilton, however, was opposed to a Bill of Rights believing the amendments not only unnecessary but dangerous:
 Madison had no way of knowing the debate between Virginia's two legislative houses would delay the adoption of the amendments for more than two years.[257] The final draft, referred to the states by the federal Congress on September 25, 1789,[258] was not ratified by Virginia's Senate until December 15, 1791.[257]
The Bill of Rights drew its authority from the consent of the people and held that,
 Madison came to be recognized as the founding era's foremost proponent of religious liberty, free speech, and freedom of the press.[260]
 The first five U.S. presidents are regarded as Founding Fathers for their active participation in the American Revolution: Washington, John Adams, Jefferson, Madison, and Monroe. Each of them served as a delegate to the Continental Congress.[261]
 The Founding Fathers represented the upper echelon of political leadership in the British colonies during the latter half of the 18th century.[262][263] All were leaders in their communities and respective colonies who were willing to assume responsibility for public affairs.[264]
 Of the signers of the Declaration of Independence, Articles of Confederation, and U.S. Constitution, nearly all were native born and of British heritage, including Scots, Irish, and Welsh.[265][266] Nearly half were lawyers, while the remainder were primarily businessmen and planter-farmers.[267][268][269] The average age of the founders was 43.[270] Benjamin Franklin, born in 1706, was the oldest, while only a few were born after 1750 and thus were in their 20s.[271][272][273]
 The following sections discuss these and other demographic topics in greater detail. For the most part, the information is confined to signers/delegates associated with the Declaration of Independence, Articles of Confederation, and Constitution.
 All of the Founding Fathers had extensive political experience at the national and state levels.[274][275] As just one example, the signers of the Declaration of Independence and Articles of Confederation were members of Second Continental Congress, while four-fifths of the delegates at the Constitutional Convention had served in the Congress either during or prior to the convention. The remaining fifth attending the convention were recognized as leaders in the state assemblies that appointed them.
 Following are brief profiles of the political backgrounds of some of the more notable founders:
 More than a third of the Founding Fathers attended or graduated from colleges in the American colonies, while additional founders attended college abroad, primarily in England and Scotland. All other founders either were home schooled, received tutoring, completed apprenticeships, or were self-educated.
 Following is a listing of founders who graduated from six of the nine colleges established in the Americas during the Colonial Era. A few founders, such as Alexander Hamilton[288] and James Monroe,[289] attended college (Columbia and William & Mary, respectively) but did not graduate. The other three colonial colleges, all founded in the 1760s, included Brown University (College of Rhode Island), Dartmouth College, and Rutgers University (Queen's College).
 Following are founders who graduated from institutions in Britain:
 All of the founders were white, and two-thirds (36 out of 55) were natives of the American Colonies, while nineteen were born in other parts of the British Empire.
 While the Founding Fathers were engaged in a broad range of occupations, most had careers in three professions: about half the founders were lawyers, a sixth were planters/farmers, another sixth were merchants/businessmen, and the others were spread across miscellaneous professions.
 Of the 55 delegates to the Constitutional Convention in 1787, 28 were Anglicans (Church of England or Episcopalian), 21 were other Protestants, and three were Catholics (Daniel Carroll and Fitzsimons; Charles Carroll was Catholic but was not a Constitution signatory).[385] Among the non-Anglican Protestant delegates to the Constitutional Convention, eight were Presbyterians, seven were Congregationalists, two were Lutherans, two were Dutch Reformed, and two were Methodists.[385] 
 A few prominent Founding Fathers were anti-clerical, notably Jefferson.[386][387] Historian Gregg L. Frazer argues that the leading founders (John Adams, Jefferson, Franklin, Wilson, Morris, Madison, Hamilton, and Washington) were neither Christians nor Deists, but rather supporters of a hybrid ""theistic rationalism"".[388] Many founders deliberately avoided public discussion of their faith. Historian David L. Holmes uses evidence gleaned from letters, government documents, and second-hand accounts to identify their religious beliefs.[50]
 Four U.S. founders are minted on American currency—Benjamin Franklin, Alexander Hamilton, Thomas Jefferson, and George Washington; Washington and Jefferson both appear on three different denominations.
 
According to David Sehat, in modern politics:[389]  Independence Day (colloquially called the Fourth of July) is a United States national holiday celebrated yearly on July 4 to commemorate the signing of the Declaration of Independence and the founding of the nation. Washington's Birthday is also observed as a national federal holiday, and on April 13 Jefferson's Birthday honors the US founder and president.
 The Founding Fathers were portrayed in the Tony Award–winning 1969 musical 1776, which depicted the debates over and eventual adoption of the Declaration of Independence. The stage production was adapted into the 1972 film of the same name. The 1989 film A More Perfect Union, which was filmed on location in Independence Hall, depicts the events of the Constitutional Convention. The writing and passing of the founding documents are depicted in the 1997 documentary miniseries Liberty!, and the passage of the Declaration of Independence is portrayed in the second episode of the 2008 miniseries John Adams and the third episode of the 2015 miniseries Sons of Liberty. The Founders also feature in the 1986 miniseries George Washington II: The Forging of a Nation, the 2002–2003 animated television series Liberty's Kids, the 2020 miniseries Washington, and in many other films and television portrayals.[citation needed]
 Several Founding Fathers, Hamilton, Washington, Jefferson, and Madison—were reimagined in Hamilton, a 2015 musical inspired by Ron Chernow's 2004 biography Alexander Hamilton, with music, lyrics and book by Lin-Manuel Miranda. The musical won eleven Tony Awards and a Pulitzer Prize for Drama.[390]
 Several major professional sports teams in the Northeastern United States are named for themes based on the founders:
 Religious persecution had existed for centuries around the world and it existed in colonial America.[391] Founders such as Thomas Jefferson, James Madison, Patrick Henry, and George Mason first established a measure of religious freedom in Virginia in 1776 with the Virginia Declaration of Rights, which became a model for religious liberty for the nation.[392] Prior to this, Baptists, Presbyterians, and Lutherans had for a decade petitioned against the Church of England's efforts to suppress religious liberties in Virginia.
 Jefferson left the Continental Congress to return to Virginia to join the fight for religious freedom, which proved difficult since many members of the Virginia legislature belonged to the established Church of England. While Jefferson was not completely successful, he managed to have repealed the various laws that were punitive toward those with different religious beliefs.[392][393][394] Jefferson was the architect for separation of Church and State, which opposed the use of public funds to support any established religion and believed it was unwise to link civil rights to religious doctrine.[395][394]
 The United States Constitution, ratified in 1788, states in Article VI that ""no religious Test shall ever be required as a Qualification to any Office or public Trust under the United States"". Freedom of religion and freedom of speech were further affirmed as the nation's law in the Bill of Rights.[391] The 14th Amendment of 1868 provided all Americans with ""equal protection under the laws"" and thus applied the First Amendment restriction against limiting the free exercise of religion to the states.[396][397]
 Washington, a local leader of the Church of England, was also a strong proponent of religious freedom, He assured Baptists worried that the Constitution might not protect their religious liberties, that, ""... certainly, I would never have placed my signature to it."" Jews also viewed Washington as a champion of freedom and sought his assurances that they would enjoy complete religious freedom. Washington responded by declaring America's revolution in religion stood as an example for the rest of the world.[398]
 The Founding Fathers were not unified on the issue of slavery and continued to accommodate it within the new nation. Some were morally opposed to it and some attempted to end it in several of the colonies, but nationally, slavery remained protected. In her study of Jefferson, a slaveholder of 600 slaves, Annette Gordon-Reed notes ironically, ""Others of the founders held slaves, but no other founder drafted the charter for American freedom"".[399] As well as Jefferson, Washington and many other Founding Fathers were slaveowners; 41 of the 56 signers of the Declaration owned slaves. Some were conflicted by the institution, seeing it as immoral and politically divisive; Washington freed his slaves, in his will. Jay and Hamilton led the successful fight to outlaw the international slave trade in New York, with efforts beginning in 1777.[400][401]
Thomas Jefferson included an anti slavery clause in his original draft of the Declaration of Independence:[402]
 Founders such as Samuel Adams and John Adams were against slavery. Rush wrote a pamphlet in 1773 which criticized the slave trade, and slavery. Rush argued scientifically that Africans are not intellectually or morally inferior, and any apparent evidence to the contrary is only the ""perverted expression"" of slavery, which ""is so foreign to the human mind, that the moral faculties, as well as those of the understanding are debased, and rendered torpid by it."" The Continental Association contained a clause which banned any Patriot involvement in slave trading.[403][404][405][406]
 Franklin, though a key founder of the Pennsylvania Abolition Society,[407] owned slaves whom he manumitted (released). While serving in the Rhode Island Assembly, in 1769 Hopkins introduced one of the earliest anti-slavery laws in the colonies. When Jefferson entered public life as a member of the House of Burgesses, he began as a social reformer by an effort to secure legislation permitting emancipation of slaves.  Jay founded the New York Manumission Society in 1785, for which Hamilton became an officer. They and other members of the Society founded the African Free School in New York, to educate the children of free blacks and slaves. When Jay was governor of New York in 1798, he helped secure and signed into law an abolition law; fully ending forced labor as of 1827. He freed his slaves in 1798. Hamilton opposed slavery, as his experiences left him familiar with it and its effect on slaves and slaveholders,[408] though he did negotiate slave transactions for his wife's family, the Schuylers.[409] Evidence suggests Hamilton may have owned a house slave[citation needed] and after the Jay Treaty was signed, Hamilton advocated that American slaves freed by the British during the war be forcibly returned to their enslavers.[410][411] Henry Laurens, ran the largest slave trading house in North America. In the 1750s alone, his firm, Austin and Laurens, handled sales of more than 8,000 Africans.[412] 
 Slaves and slavery are mentioned indirectly in the 1787 Constitution. For example, Article 1, Section 2, Clause 3 prescribes that ""three-fifths of all other Persons"" are to be counted for the apportionment of seats in the House of Representatives and direct taxes. Additionally, in Article 4, Section 2, Clause 3, slaves are referred to as ""persons held in service or labor"".[407][413] The Founding Fathers made some efforts to contain slavery. Many Northern states had adopted legislation to end, or significantly reduce slavery, during and after the revolution.[413] In 1782, Virginia passed a manumission law that allowed owners to free their slaves by will or deed.[414] As a result, thousands of slaves were manumitted in Virginia.[414] In the Ordinance of 1784, Jefferson proposed to ban slavery in all the western territories, which failed to pass Congress by one vote. Partially following Jefferson's plan, Congress did ban slavery in the Northwest Ordinance, for lands north of the Ohio River. The international slave trade was banned in all states except South Carolina by 1800. In 1807, President Jefferson called for and signed into law a federally enforced ban on the international slave trade, throughout the U.S. and its territories. It became a federal crime to import or export a slave. However, the domestic slave trade was allowed for expansion or for diffusion of slavery into the Louisiana Territory.[413]
 
According to Jeffrey K. Tulis and Nicole Mellow:[415] Scholars such as Eric Foner have expanded the theme.[416][417][418] Black abolitionists played a key role by stressing that freed blacks needed equal rights after slavery was abolished.[419] Biographer David Blight states that Frederick Douglass, ""played a pivotal role in America's Second Founding out of the apocalypse of the Civil War, and he very much wished to see himself as a founder and a defender of the Second American Republic.""[420] Constitutional provision for racial equality for free blacks was enacted by a Republican Congress led by Thaddeus Stevens, Charles Sumner and Lyman Trumbull.[421] The ""second founding"" comprised the 13th, 14th and 15th amendments to the Constitution. All citizens now had federal rights that could be enforced in federal court. In a deep reaction, after 1876 freedmen lost many of these rights and had second class citizenship in the era of lynching and Jim Crow laws. Finally in the 1950s the U.S., Supreme Court started to restore those rights. Under the leadership of Martin Luther King and James Bevel, the Civil Rights movement made the nation aware of the crisis, and under President Lyndon Johnson major civil rights legislation was passed in 1964–65, and 1968.[422]
 Historians who wrote about the American Revolution era and the founding of the United States government now number in the thousands. Their inclusion would go well beyond the scope of this article. Some of the most prominent ones, however, are listed below. While most scholarly works maintain overall objectivity, historian Arthur H. Shaffer notes that many of the early works about the American Revolution often express a national bias, or anti-bias. Shaffer maintains that this bias lends a direct insight into the minds of the founders and their adversaries respectively. He notes that any bias is the product of a national interest and prevailing political mood, and as such cannot be dismissed as having no historic value for the modern historian.[423] Conversely, various modern accounts of history contain anachronisms, modern day ideals and perceptions used in an effort to write about the past and as such can distort the historical account in an effort to placate a modern audience.[424][425]
 Several of the earliest histories of the founding of the United States and its founders were written by Jeremy Belknap, author of his three-volume work, The history of New-Hampshire, published in 1784.[426]
 Articles and books by these and other 20th- and 21st-century historians, combined with the digitization of primary sources such as handwritten letters, continue to contribute to an encyclopedic body of knowledge about the Founding Fathers:
 According to American historian Joseph Ellis, the concept of the Founding Fathers of the U.S. emerged in the 1820s as the last survivors died out. Ellis says the founders, or the fathers comprised an aggregate of semi-sacred figures whose particular accomplishments and singular achievements were decidedly less important than their sheer presence as a powerful but faceless symbol of past greatness. For the generation of national leaders coming of age in the 1820s and 1830s, such as Andrew Jackson, Henry Clay, Daniel Webster, and John C. Calhoun, the founders represented heroic but anonymous abstraction whose long shadow fell across all followers and whose legendary accomplishments defied comparison.[citation needed]
 
"
President of the United States,https://en.wikipedia.org/wiki/President_of_the_United_States,"
 

 The president of the United States (POTUS)[B] is the head of state and head of government of the United States. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.
 The power of the presidency has grown substantially[12] since the first president, George Washington, took office in 1789.[6] While presidential power has ebbed and flowed over time, the presidency has played an increasingly significant role in American political life since the beginning of the 20th century, carrying over into the 21st century with notable expansions during the presidencies of Franklin D. Roosevelt and George W. Bush.[13][14] In modern times, the president is one of the world's most powerful political figures and the leader of the world's only remaining superpower.[15][16][17][18] As the leader of the nation with the largest economy by nominal GDP, the president possesses significant domestic and international hard and soft power. For much of the 20th century, especially during the Cold War, the U.S. president was often called ""the leader of the free world"".[19]
 Article II of the Constitution establishes the executive branch of the federal government and vests executive power in the president. The power includes the execution and enforcement of federal law and the responsibility to appoint federal executive, diplomatic, regulatory, and judicial officers.  Based on constitutional provisions empowering the president to appoint and receive ambassadors and conclude treaties with foreign powers, and on subsequent laws enacted by Congress, the modern presidency has primary responsibility for conducting U.S. foreign policy. The role includes responsibility for directing the world's most expensive military, which has the second-largest nuclear arsenal.
 The president also plays a leading role in federal legislation and domestic policymaking. As part of the system of separation of powers, Article I, Section 7 of the Constitution gives the president the power to sign or veto federal legislation. Since modern presidents are typically viewed as leaders of their political parties, major policymaking is significantly shaped by the outcome of presidential elections, with presidents taking an active role in promoting their policy priorities to members of Congress who are often electorally dependent on the president.[20] In recent decades, presidents have also made increasing use of executive orders, agency regulations, and judicial appointments to shape domestic policy.
 The president is elected indirectly through the Electoral College to a four-year term, along with the vice president. Under the Twenty-second Amendment, ratified in 1951, no person who has been elected to two presidential terms may be elected to a third. In addition, nine vice presidents have become president by virtue of a president's intra-term death or resignation.[C] In all, 45 individuals have served 47 presidencies spanning 60 four-year terms.[D] Donald Trump is the 47th and current president since January 20, 2025.[22][23]
 During the American Revolutionary War, the Thirteen Colonies, represented by the Second Continental Congress in Philadelphia, declared themselves to be independent sovereign states and no longer under British rule.[24] The affirmation was made in the Declaration of Independence, which was written predominantly by Thomas Jefferson and adopted unanimously on July 4, 1776, by the Second Continental Congress. Recognizing the necessity of closely coordinating their efforts against the British,[25] the Continental Congress simultaneously began the process of drafting a constitution that would bind the states together. There were long debates on a number of issues, including representation and voting, and the exact powers to be given the central government.[26] Congress finished work on the Articles of Confederation to establish a perpetual union between the states in November 1777 and sent it to the states for ratification.[24]
 Under the Articles, which took effect on March 1, 1781, the Congress of the Confederation was a central political authority without any legislative power. It could make its own resolutions, determinations, and regulations, but not any laws, and could not impose any taxes or enforce local commercial regulations upon its citizens.[25] This institutional design reflected how Americans believed the deposed British system of Crown and Parliament ought to have functioned with respect to the royal dominion: a superintending body for matters that concerned the entire empire.[25] The states were out from under any monarchy and assigned some formerly royal prerogatives (e.g., making war, receiving ambassadors, etc.) to Congress; the remaining prerogatives were lodged within their own respective state governments. The members of Congress elected a president of the United States in Congress Assembled to preside over its deliberation as a neutral discussion moderator. Unrelated to and quite dissimilar from the later office of president of the United States, it was a largely ceremonial position without much influence.[27]
 In 1783, the Treaty of Paris secured independence for each of the former colonies. With peace at hand, the states each turned toward their own internal affairs.[24] By 1786, Americans found their continental borders besieged and weak and their respective economies in crises as neighboring states agitated trade rivalries with one another. They witnessed their hard currency pouring into foreign markets to pay for imports, their Mediterranean commerce preyed upon by North African pirates, and their foreign-financed Revolutionary War debts unpaid and accruing interest.[24] Civil and political unrest loomed.  Events such as the Newburgh Conspiracy and Shays' Rebellion demonstrated that the Articles of Confederation were not working.
 Following the successful resolution of commercial and fishing disputes between Virginia and Maryland at the Mount Vernon Conference in 1785, Virginia called for a trade conference between all the states, set for September 1786 in Annapolis, Maryland, with an aim toward resolving further-reaching interstate commercial antagonisms. When the convention failed for lack of attendance due to suspicions among most of the other states, Alexander Hamilton of New York led the Annapolis delegates in a call for a convention to offer revisions to the Articles, to be held the next spring in Philadelphia. Prospects for the next convention appeared bleak until James Madison and Edmund Randolph succeeded in securing George Washington's attendance to Philadelphia as a delegate for Virginia.[24][28]
 When the Constitutional Convention convened in May 1787, the 12 state delegations in attendance (Rhode Island did not send delegates) brought with them an accumulated experience over a diverse set of institutional arrangements between legislative and executive branches from within their respective state governments. Most states maintained a weak executive without veto or appointment powers, elected annually by the legislature to a single term only, sharing power with an executive council, and countered by a strong legislature.[24] New York offered the greatest exception, having a strong, unitary governor with veto and appointment power elected to a three-year term, and eligible for reelection to an indefinite number of terms thereafter.[24] It was through the closed-door negotiations at Philadelphia that the presidency framed in the U.S. Constitution emerged.
 As the nation's first president, George Washington established many norms that would come to define the office.[29][30] His decision to retire after two terms helped address fears that the nation would devolve into monarchy and established a precedent that would not be broken until 1940 and would eventually be made permanent by the Twenty-Second Amendment. By the end of his presidency, political parties had developed,[31] with John Adams defeating Thomas Jefferson in 1796, the first truly contested presidential election.[32]  After Jefferson defeated Adams in 1800, he and his fellow Virginians James Madison and James Monroe would each serve two terms, eventually dominating the nation's politics during the Era of Good Feelings until Adams' son John Quincy Adams won election in 1824 after the Democratic-Republican Party split.
 The election of Andrew Jackson in 1828 was a significant milestone, as Jackson was not part of the Virginia and Massachusetts elite that had held the presidency for its first 40 years.[33] Jacksonian democracy sought to strengthen the presidency at the expense of Congress, while broadening public participation as the nation rapidly expanded westward. However, his successor, Martin Van Buren, became unpopular after the Panic of 1837,[34] and the death of William Henry Harrison and subsequent poor relations between John Tyler and Congress led to further weakening of the office.[35] Including Van Buren, in the 24 years between 1837 and 1861, six presidential terms would be filled by eight different men, with none serving two terms.[36] The Senate played an important role during this period, with the Great Triumvirate of Henry Clay, Daniel Webster, and John C. Calhoun playing key roles in shaping national policy in the 1830s and 1840s until debates over slavery began pulling the nation apart in the 1850s.[37][38]
 Abraham Lincoln's leadership during the Civil War has led historians to regard him as one of the nation's greatest presidents.[E] The circumstances of the war and Republican domination of Congress made the office very powerful,[39][40] and Lincoln's re-election in 1864 was the first time a president had been re-elected since Jackson in 1832. After Lincoln's assassination, his successor Andrew Johnson lost all political support[41] and was nearly removed from office,[42] with Congress remaining powerful during the two-term presidency of Civil War general Ulysses S. Grant. After the end of Reconstruction, Grover Cleveland would eventually become the first Democratic president elected since before the war, running in three consecutive elections (1884, 1888, 1892) and winning twice. In 1900, William McKinley became the first incumbent to win re-election since Grant in 1872.
 After McKinley's assassination by Leon Czolgosz in 1901, Theodore Roosevelt became a dominant figure in American politics.[43] Historians believe Roosevelt permanently changed the political system by strengthening the presidency,[44] with some key accomplishments including breaking up trusts, conservationism, labor reforms, making personal character as important as the issues, and hand-picking his successor, William Howard Taft. The following decade, Woodrow Wilson led the nation to victory during World War I, although Wilson's proposal for the League of Nations was rejected by the Senate.[45]  Warren Harding, while popular in office, would see his legacy tarnished by scandals, especially Teapot Dome,[46] and Herbert Hoover quickly became very unpopular after failing to alleviate the Great Depression.[47]
 The ascendancy of Franklin D. Roosevelt in 1933 led further toward what historians now describe as the Imperial presidency.[48] Backed by enormous Democratic majorities in Congress and public support for major change, Roosevelt's New Deal dramatically increased the size and scope of the federal government, including more executive agencies.[49]: 211–12  The traditionally small presidential staff was greatly expanded, with the Executive Office of the President being created in 1939, none of whom require Senate confirmation.[49]: 229–231  Roosevelt's unprecedented re-election to a third and fourth term, the victory of the United States in World War II, and the nation's growing economy all helped established the office as a position of global leadership.[49]: 269  His successors, Harry Truman and Dwight D. Eisenhower, each served two terms as the Cold War led the presidency to be viewed as the ""leader of the free world"",[50] while John F. Kennedy was a youthful and popular leader who benefited from the rise of television in the 1960s.[51][52]
 After Lyndon B. Johnson lost popular support due to the Vietnam War and Richard Nixon's presidency collapsed in the Watergate scandal, Congress enacted a series of reforms intended to reassert itself.[53][54] These included the War Powers Resolution, enacted over Nixon's veto in 1973,[55][56] and the Congressional Budget and Impoundment Control Act of 1974 that sought to strengthen congressional fiscal powers.[57] By 1976, Gerald Ford conceded that ""the historic pendulum"" had swung toward Congress, raising the possibility of a ""disruptive"" erosion of his ability to govern.[58] Ford failed to win election to a full term and his successor, Jimmy Carter, failed to win re-election.  Ronald Reagan, who had been an actor before beginning his political career, used his talent as a communicator to help reshape the American agenda away from New Deal policies toward more conservative ideology.[59][60]
 After the Cold War, the United States became the world's undisputed leading power.[61] Bill Clinton, George W. Bush, and Barack Obama each served two terms as president. Meanwhile, the nation gradually became more politically polarized, leading to the ascendency of increasingly polarized Congressmembers, especially following the 1994 mid-term elections which saw Republicans control the House for the first time in 40 years, and the rise of routine filibusters in the Senate.[62] Recent presidents have thus increasingly focused on executive orders, agency regulations, and judicial appointments to implement major policies, at the expense of legislation and congressional power.[63] Presidential elections in the 21st century have reflected this continuing polarization, with no candidate except Obama in 2008 winning by more than five percent of the popular vote and two, George W. Bush (2000) and Donald Trump (2016), winning in the Electoral College while losing the popular vote.[F] Bush (2004) and Trump (2024) were later re-elected, winning both in the Electoral College and the popular vote.
 The nation's Founding Fathers expected the Congress, which was the first branch of government described in the Constitution, to be the dominant branch of government; however, they did not expect a strong executive department.[64] However, presidential power has shifted over time, which has resulted in claims that the modern presidency has become too powerful,[65][66] unchecked, unbalanced,[67] and ""monarchist"" in nature.[68] In 2008 professor Dana D. Nelson expressed belief that presidents over the previous thirty years worked towards ""undivided presidential control of the executive branch and its agencies"".[69] She criticized proponents of the unitary executive theory for expanding ""the many existing uncheckable executive powers—such as executive orders, decrees, memorandums, proclamations, national security directives and legislative signing statements—that already allow presidents to enact a good deal of foreign and domestic policy without aid, interference or consent from Congress"".[69] Bill Wilson, board member of Americans for Limited Government, opined that the expanded presidency was ""the greatest threat ever to individual freedom and democratic rule"".[70]
 Article I, Section 1 of the Constitution vests all lawmaking power in Congress's hands, and Article 1, Section 6, Clause 2 prevents the president (and all other executive branch officers) from simultaneously being a member of Congress. Nevertheless, the modern presidency exerts significant power over legislation, both due to constitutional provisions and historical developments over time.
 The president's most significant legislative power derives from the Presentment Clause, which gives the president the power to veto any bill passed by Congress. While Congress can override a presidential veto, it requires a two-thirds vote of both houses, which is usually very difficult to achieve except for widely supported bipartisan legislation. The framers of the Constitution feared that Congress would seek to increase its power and enable a ""tyranny of the majority"", so giving the indirectly elected president a veto was viewed as an important check on the legislative power. While George Washington believed the veto should only be used in cases where a bill was unconstitutional, it is now routinely used in cases where presidents have policy disagreements with a bill.  The veto – or threat of a veto – has thus evolved to make the modern presidency a central part of the American legislative process.
 Specifically, under the Presentment Clause, once a bill has been presented by Congress, the president has three options:
 In 1996, Congress attempted to enhance the president's veto power with the Line Item Veto Act. The legislation empowered the president to sign any spending bill into law while simultaneously striking certain spending items within the bill, particularly any new spending, any amount of discretionary spending, or any new limited tax benefit. Congress could then repass that particular item. If the president then vetoed the new legislation, Congress could override the veto by its ordinary means, a two-thirds vote in both houses. In Clinton v. City of New York, 524 U.S. 417 (1998), the U.S. Supreme Court ruled such a legislative alteration of the veto power to be unconstitutional.
 For most of American history, candidates for president have sought election on the basis of a promised legislative agenda. Article II, Section 3, Clause 2 requires the president to recommend such measures to Congress which the president deems ""necessary and expedient"". This is done through the constitutionally-based State of the Union address, which usually outlines the president's legislative proposals for the coming year, and through other formal and informal communications with Congress.
 The president can be involved in crafting legislation by suggesting, requesting, or even insisting that Congress enact laws that the president believes are needed. Additionally, the president can attempt to shape legislation during the legislative process by exerting influence on individual members of Congress.[71] Presidents possess this power because the Constitution is silent about who can write legislation, but the power is limited because only members of Congress can introduce legislation.[72]
 The president or other officials of the executive branch may draft legislation and then ask senators or representatives to introduce these drafts into Congress. Additionally, the president may attempt to have Congress alter proposed legislation by threatening to veto that legislation unless requested changes are made.[73]
 Many laws enacted by Congress do not address every possible detail, and either explicitly or implicitly delegate powers of implementation to an appropriate federal agency. As the head of the executive branch, presidents control a vast array of agencies that can issue regulations with little oversight from Congress.
 In the 20th century, critics charged that too many legislative and budgetary powers that should have belonged to Congress had slid into the hands of presidents. One critic charged that presidents could appoint a ""virtual army of 'czars'—each wholly unaccountable to Congress yet tasked with spearheading major policy efforts for the White House"".[74] Presidents have been criticized for making signing statements when signing congressional legislation about how they understand a bill or plan to execute it.[75] This practice has been criticized by the American Bar Association as unconstitutional.[76] Conservative commentator George Will wrote of an ""increasingly swollen executive branch"" and ""the eclipse of Congress"".[77]
 To allow the government to act quickly in case of a major domestic or international crisis arising when Congress is not in session, the president is empowered by Article II, Section 3 of the Constitution to call a special session of one or both houses of Congress. Since John Adams first did so in 1797, the president has called the full Congress to convene for a special session on 27 occasions. Harry S. Truman was the most recent to do so in July 1948, known as the Turnip Day Session. In addition, prior to ratification of the Twentieth Amendment in 1933, which brought forward the date on which Congress convenes from December to January, newly inaugurated presidents would routinely call the Senate to meet to confirm nominations or ratify treaties. In practice, the power has fallen into disuse in the modern era as Congress now formally remains in session year-round, convening pro forma sessions every three days even when ostensibly in recess. Correspondingly, the president is authorized to adjourn Congress if the House and Senate cannot agree on the time of adjournment; no president has ever had to exercise this power.[78][79]
 The president is head of the executive branch of the federal government and is constitutionally obligated to ""take care that the laws be faithfully executed"".[80] The executive branch has over four million employees, including the military.[81]
 Presidents make political appointments. An incoming president may make up to 4,000 upon taking office, 1,200 of which must be confirmed by the U.S. Senate. Ambassadors, members of the Cabinet, and various officers, are among the positions filled by presidential appointment with Senate confirmation.[82][83]
 The power of a president to fire executive officials has long been a contentious political issue. Generally, a president may remove executive officials at will.[84] However, Congress can curtail and constrain a president's authority to fire commissioners of independent regulatory agencies and certain inferior executive officers by statute.[85]
 To manage the growing federal bureaucracy, presidents have gradually surrounded themselves with many layers of staff, who were eventually organized into the Executive Office of the President of the United States. Within the Executive Office, the president's innermost layer of aides, and their assistants, are located in the White House Office.
 The president also possesses the power to manage operations of the federal government by issuing various types of directives, such as presidential proclamation and executive orders. When the president is lawfully exercising one of the constitutionally conferred presidential responsibilities, the scope of this power is broad.[86] Even so, these directives are subject to judicial review by U.S. federal courts, which can find them to be unconstitutional. Congress can overturn an executive order through legislation.
 Article II, Section 3, Clause 4 requires the president to ""receive Ambassadors"". This clause, known as the Reception Clause, has been interpreted to imply that the president possesses broad power over matters of foreign policy,[87] and to provide support for the president's exclusive authority to grant recognition to a foreign government.[88] The Constitution also empowers the president to appoint United States ambassadors, and to propose and chiefly negotiate agreements between the United States and other countries. Such agreements, upon receiving the advice and consent of the U.S. Senate (by a two-thirds majority vote), become binding with the force of federal law.
 While foreign affairs has always been a significant element of presidential responsibilities, advances in technology since the Constitution's adoption have increased presidential power. Where formerly ambassadors were vested with significant power to independently negotiate on behalf of the United States, presidents now routinely meet directly with leaders of foreign countries.
 
One of the most important of executive powers is the president's role as commander-in-chief of the United States Armed Forces. The power to declare war is constitutionally vested in Congress, but the president has ultimate responsibility for the direction and disposition of the military. The exact degree of authority that the Constitution grants to the president as commander-in-chief has been the subject of much debate throughout history, with Congress at various times granting the president wide authority and at others attempting to restrict that authority.[89] The framers of the Constitution took care to limit the president's powers regarding the military; Alexander Hamilton explained this in Federalist No. 69:.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} In the modern era, pursuant to the War Powers Resolution, Congress must authorize any troop deployments longer than 60 days, although that process relies on triggering mechanisms that have never been employed, rendering it ineffectual.[91] Additionally, Congress provides a check to presidential military power through its control over military spending and regulation. Presidents have historically initiated the process for going to war,[92][93] but critics have charged that there have been several conflicts in which presidents did not get official declarations, including Theodore Roosevelt's military move into Panama in 1903,[92] the Korean War,[92] the Vietnam War,[92] and the invasions of Grenada in 1983[94] and Panama in 1989.[95]
 The amount of military detail handled personally by the president in wartime has varied greatly.[96] George Washington, the first U.S. president, firmly established military subordination under civilian authority. In 1794, Washington used his constitutional powers to assemble 12,000 militia to quell the Whiskey Rebellion, a conflict in Western Pennsylvania involving armed farmers and distillers who refused to pay an excise tax on spirits. According to historian Joseph Ellis, this was the ""first and only time a sitting American president led troops in the field"", though James Madison briefly took control of artillery units in defense of Washington, D.C., during the War of 1812.[97] Abraham Lincoln was deeply involved in overall strategy and in day-to-day operations during the American Civil War, 1861–1865; historians have given Lincoln high praise for his strategic sense and his ability to select and encourage commanders such as Ulysses S. Grant.[98]
 The present-day operational command of the Armed Forces is delegated to the Department of Defense and is normally exercised through the secretary of defense. The chairman of the Joint Chiefs of Staff and the Combatant Commands assist with the operation as outlined in the presidentially approved Unified Command Plan (UCP).[99][100][101]
 The president has the power to nominate federal judges, including members of the United States courts of appeals and the Supreme Court of the United States. However, these nominations require Senate confirmation before they may take office. Securing Senate approval can provide a major obstacle for presidents who wish to orient the federal judiciary toward a particular ideological stance. When nominating judges to U.S. district courts, presidents often respect the long-standing tradition of senatorial courtesy. Presidents may also grant pardons and reprieves. Gerald Ford pardoned Richard Nixon a month after taking office. Presidents often grant pardons shortly before leaving office, like when Bill Clinton pardoned Patty Hearst on his last day in office; this is often controversial.[102][103][104]
 Two doctrines concerning executive power have developed that enable the president to exercise executive power with a degree of autonomy. The first is executive privilege, which allows the president to withhold from disclosure any communications made directly to the president in the performance of executive duties. George Washington first claimed the privilege when Congress requested to see Chief Justice John Jay's notes from an unpopular treaty negotiation with Great Britain. While not enshrined in the Constitution or any other law, Washington's action created the precedent for the privilege. When Nixon tried to use executive privilege as a reason for not turning over subpoenaed evidence to Congress during the Watergate scandal, the Supreme Court ruled in United States v. Nixon, 418 U.S. 683 (1974), that executive privilege did not apply in cases where a president was attempting to avoid criminal prosecution. When Bill Clinton attempted to use executive privilege regarding the Lewinsky scandal, the Supreme Court ruled in Clinton v. Jones, 520 U.S. 681 (1997), that the privilege also could not be used in civil suits. These cases established the legal precedent that executive privilege is valid, although the exact extent of the privilege has yet to be clearly defined. Additionally, federal courts have allowed this privilege to radiate outward and protect other executive branch employees but have weakened that protection for those executive branch communications that do not involve the president.[105]
 The state secrets privilege allows the president and the executive branch to withhold information or documents from discovery in legal proceedings if such release would harm national security. Precedent for the privilege arose early in the 19th century when Thomas Jefferson refused to release military documents in the treason trial of Aaron Burr and again in Totten v. United States 92 U.S. 105 (1876), when the Supreme Court dismissed a case brought by a former Union spy.[106] However, the privilege was not formally recognized by the U.S. Supreme Court until United States v. Reynolds 345 U.S. 1 (1953), where it was held to be a common law evidentiary privilege.[107] Before the September 11 attacks, use of the privilege had been rare, but increasing in frequency.[108] Since 2001, the government has asserted the privilege in more cases and at earlier stages of the litigation, thus in some instances causing dismissal of the suits before reaching the merits of the claims, as in the Ninth Circuit's ruling in Mohamed v. Jeppesen Dataplan, Inc.[107][109][110] Critics of the privilege claim its use has become a tool for the government to cover up illegal or embarrassing government actions.[111][112]
 The degree to which the president personally has absolute immunity from court cases is contested and has been the subject of several Supreme Court decisions. Nixon v. Fitzgerald (1982) dismissed a civil lawsuit against by-then former president Richard Nixon based on his official actions. Clinton v. Jones (1997) decided that a president has no immunity against civil suits for actions taken before becoming president and ruled that a sexual harassment suit could proceed without delay, even against a sitting president. The 2019 Mueller report on Russian interference in the 2016 presidential election detailed evidence of possible obstruction of justice, but investigators declined to refer Donald Trump for prosecution based on a United States Department of Justice policy against indicting an incumbent president. The report noted that impeachment by Congress was available as a remedy. As of October 2019, a case was pending in the federal courts regarding access to personal tax returns in a criminal case brought against Donald Trump by the New York County District Attorney alleging violations of New York state law.[113]
 Memoranda from the Office of Legal Counsel issued in 1973 and 2000 internally prohibit the Department of Justice from prosecuting a president, which some legal scholars have criticized but others have endorsed.[114]
 In defense against federal criminal prosecution for his alleged 2020 election subversion, in January 2024, Trump argued to the DC Circuit Court of Appeals that a president enjoys absolute immunity for criminal acts conducted while in office. The next month, a three-judge panel of the court unanimously ruled against Trump. It was the first time an appeals court had addressed such a presidential immunity matter, since no other sitting or former president had ever been criminally indicted.[115]
 In Trump v. United States, on July 1, 2024, the Supreme Court ruled that presidents were entitled to absolute immunity from exercising core powers enumerated by the Constitution, presumption of immunity for other official acts, and no immunity for unofficial actions. The case was sent back to lower courts to determine which actions in the criminal complaint should be classified as official vs. unofficial.[116] The ruling was the first time the courts granted a president criminal immunity.
 As head of state, the president represents the United States government to its own people and represents the nation to the rest of the world. For example, during a state visit by a foreign head of state, the president typically hosts a State Arrival Ceremony held on the South Lawn, a custom begun by John F. Kennedy in 1961.[117] This is followed by a state dinner given by the president which is held in the State Dining Room later in the evening.[118]
 As a national leader, the president also fulfills many less formal ceremonial duties. For example, William Howard Taft started the tradition of throwing out the ceremonial first pitch in 1910 at Griffith Stadium, Washington, D.C., on the Washington Senators's Opening Day. Every president since Taft, except for Jimmy Carter, threw out at least one ceremonial first ball or pitch for Opening Day, the All-Star Game, or the World Series, usually with much fanfare.[119] Every president since Theodore Roosevelt has served as honorary president of the Boy Scouts of America.[120]
 Other presidential traditions are associated with American holidays. Rutherford B. Hayes began in 1878 the first White House egg rolling for local children.[121] Beginning in 1947, during the Harry S. Truman administration, every Thanksgiving the president is presented with a live domestic turkey during the annual National Thanksgiving Turkey Presentation held at the White House. Since 1989, when the custom of ""pardoning"" the turkey was formalized by George H. W. Bush, the turkey has been taken to a farm where it will live out the rest of its natural life.[122]
 Presidential traditions also involve the president's role as head of government. Many outgoing presidents since James Buchanan traditionally give advice to their successor during the presidential transition.[123] Ronald Reagan and his successors have also left a private message on the desk of the Oval Office on Inauguration Day for the incoming president.[124]
 The modern presidency holds the president as one of the nation's premier celebrities. Some argue that images of the presidency have a tendency to be manipulated by administration public relations officials as well as by presidents themselves. One critic described the presidency as ""propagandized leadership"" which has a ""mesmerizing power surrounding the office"".[125] Administration public relations managers staged carefully crafted photo-ops of smiling presidents with smiling crowds for television cameras.[126] One critic wrote the image of John F. Kennedy was described as carefully framed ""in rich detail"" which ""drew on the power of myth"" regarding the incident of PT 109[127] and wrote that Kennedy understood how to use images to further his presidential ambitions.[128] As a result, some political commentators have opined that American voters have unrealistic expectations of presidents: voters expect a president to ""drive the economy, vanquish enemies, lead the free world, comfort tornado victims, heal the national soul and protect borrowers from hidden credit-card fees"".[129]
 The president is typically considered to be the head of their political party. Since the entire House of Representatives and at least one-third of the Senate is elected simultaneously with the president, candidates from a political party inevitably have their electoral success intertwined with the performance of the party's presidential candidate. The coattail effect, or lack thereof, will also often impact a party's candidates at state and local levels of government as well. However, there are often tensions between a president and others in the party, with presidents who lose significant support from their party's caucus in Congress generally viewed to be weaker and less effective.
 With the rise of the United States as a superpower in the 20th century, and the United States having the world's largest economy into the 21st century, the president is typically viewed as a global leader, and at times the world's most powerful political figure. The position of the United States as the leading member of NATO, and the country's strong relationships with other wealthy or democratic nations like those comprising the European Union, have led to the moniker that the president is the ""leader of the free world"".
 Article II, Section 1, Clause 5 of the Constitution sets three qualifications for holding the presidency. To serve as president, one must:
 A person who meets the above qualifications would, however, still be disqualified from holding the office of president under any of the following conditions:
 The modern presidential campaign begins before the primary elections, which the two major political parties use to clear the field of candidates before their national nominating conventions, where the most successful candidate is made the party's presidential nominee. Typically, the party's presidential candidate chooses a vice presidential nominee, and this choice is rubber-stamped by the convention. The most common previous profession of presidents is lawyer.[137]
 Nominees participate in nationally televised debates, and while the debates are usually restricted to the Democratic and Republican nominees, third party candidates may be invited, such as Ross Perot in the 1992 debates. Nominees campaign across the country to explain their views, convince voters and solicit contributions. Much of the modern electoral process is concerned with winning swing states through frequent visits and mass media advertising drives.
 The president is elected indirectly by the voters of each state and the District of Columbia through the Electoral College, a body of electors formed every four years for the sole purpose of electing the president and vice president to concurrent four-year terms. As prescribed by Article II, Section 1, Clause 2, each state is entitled to a number of electors equal to the size of its total delegation in both houses of Congress. Additionally, the Twenty-third Amendment provides that the District of Columbia is entitled to the number it would have if it were a state, but in no case more than that of the least populous state.[138] Currently, all states and the District of Columbia select their electors based on a popular election.[139] In all but two states, the party whose presidential–vice presidential ticket receives a plurality of popular votes in the state has its entire slate of elector nominees chosen as the state's electors.[140] Maine and Nebraska deviate from this winner-take-all practice, awarding two electors to the statewide winner and one to the winner in each congressional district.[141][142]
 On the first Monday after the second Wednesday in December, about six weeks after the election, the electors convene in their respective state capitals (and in Washington, D.C.) to vote for president and, on a separate ballot, for vice president. They typically vote for the candidates of the party that nominated them. While there is no constitutional mandate or federal law requiring them to do so, the District of Columbia and 32 states have laws requiring that their electors vote for the candidates to whom they are pledged.[143][144] The constitutionality of these laws was upheld in Chiafalo v. Washington (2020).[145] Following the vote, each state then sends a certified record of their electoral votes to Congress. The votes of the electors are opened and counted during a joint session of Congress, held in the first week of January. If a candidate has received an absolute majority of electoral votes for president (currently 270 of 538), that person is declared the winner. Otherwise, the House of Representatives must meet to elect a president using a contingent election procedure in which representatives, voting by state delegation, with each state casting a single vote, choose between the top three electoral vote-getters for president. To win the presidency, a candidate must receive the votes of an absolute majority of states (currently 26 of 50).[139]
 There have been two contingent presidential elections in the nation's history. A 73–73 electoral vote tie between Thomas Jefferson and fellow Democratic-Republican Aaron Burr in the election of 1800 necessitated the first. Conducted under the original procedure established by Article II, Section 1, Clause 3 of the Constitution, which stipulates that if two or three persons received a majority vote and an equal vote, the House of Representatives would choose one of them for president; the runner-up would become vice president.[146] On February 17, 1801, Jefferson was elected president on the 36th ballot, and Burr elected vice president. Afterward, the system was overhauled through the Twelfth Amendment in time to be used in the 1804 election.[147] A quarter-century later, the choice for president again devolved to the House when no candidate won an absolute majority of electoral votes (131 of 261) in the election of 1824. Under the Twelfth Amendment, the House was required to choose a president from among the top three electoral vote recipients: Andrew Jackson, John Quincy Adams, and William H. Crawford. Held February 9, 1825, this second and most recent contingent election resulted in John Quincy Adams being elected president on the first ballot.[148]
 Pursuant to the Twentieth Amendment, the four-year term of office for both the president and the vice president begins at noon on January 20, in the year following the preceding presidential election.[149] The first presidential and vice presidential terms to begin on this date, known as Inauguration Day, were the second terms of President Franklin D. Roosevelt and Vice President John Nance Garner in 1937.[150] Previously, Inauguration Day was on March 4. As a result of the date change, the first term (1933–37) of both men had been shortened by 43 days.[151]
 Before executing the powers of the office, a president is required to recite the presidential Oath of Office, found in Article II, Section 1, Clause 8 of the Constitution. This is the only component in the inauguration ceremony mandated by the Constitution:
 Presidents have traditionally placed one hand upon a Bible while taking the oath, and have added ""So help me God"" to the end of the oath.[153][154] Although the oath may be administered by any person authorized by law to administer oaths, presidents are traditionally sworn in by the chief justice of the United States.[152]
 When the first president, George Washington, announced in his Farewell Address that he was not running for a third term, he established a ""two terms then out"" precedent. Precedent became tradition after Thomas Jefferson publicly embraced the principle a decade later during his second term, as did his two immediate successors, James Madison and James Monroe.[155] In spite of the strong two-term tradition, Ulysses S. Grant sought nomination at the 1880 Republican National Convention for a non-consecutive third term, but was unsuccessful.[156]
 In 1940, after leading the nation through the Great Depression and focused on supporting U.S. allied nations at war with the Axis powers, Franklin Roosevelt was elected to a third term, breaking the long-standing precedent. Four years later, with the U.S. engaged in World War II, he was re-elected again despite his declining physical health; he died 82 days into his fourth term on April 12, 1945.[157]
 In response to the unprecedented length of Roosevelt's presidency, the Twenty-second Amendment was adopted in 1951. The amendment bars anyone from being elected president more than twice, or once if that person served more than two years (24 months) of another president's four-year term. Harry S. Truman, the president at the time it was submitted to the states by the Congress, was exempted from its limitations. Without the exemption, he would not have been eligible to run for a second full term in 1952 (which he briefly sought), as he had served nearly all of Franklin Roosevelt's unexpired 1945–1949 term and had been elected to a full four-year term beginning in 1949.[157]
 Under Section 1 of the Twenty-fifth Amendment, ratified in 1967, the vice president becomes president upon the removal from office, death, or resignation of the president. Deaths have occurred a number of times, resignation has occurred only once, and removal from office has never occurred.
 Before the ratification of the Twenty-fifth amendment (which clarified the matter of succession), Article II, Section 1, Clause 6, stated only that the vice president assumes the ""powers and duties"" of the presidency in the event of a president's removal, death, resignation, or inability.[158] Under this clause, there was ambiguity about whether the vice president would actually become president in the event of a vacancy, or simply act as president,[159] potentially resulting in a special election. Upon the death of President William Henry Harrison in 1841, Vice President John Tyler declared that he had succeeded to the office itself, refusing to accept any papers addressed to the ""Acting President"", and Congress ultimately accepted it.
 In the event of a double vacancy, Article II, Section 1, Clause 6 also authorizes Congress to declare who shall become acting president in the ""Case of Removal, Death, Resignation or Inability, both of the president and vice president"".[159] The Presidential Succession Act of 1947 (codified as 3 U.S.C. § 19) provides that if both the president and vice president have left office or are both otherwise unavailable to serve during their terms of office, the presidential line of succession follows the order of: speaker of the House, then, if necessary, the president pro tempore of the Senate, and then if necessary, the eligible heads of federal executive departments who form the president's cabinet. The cabinet currently has 15 members, of which the secretary of state is first in line; the other Cabinet secretaries follow in the order in which their department (or the department of which their department is the successor) was created. Those individuals who are constitutionally ineligible to be elected to the presidency are also disqualified from assuming the powers and duties of the presidency through succession. No statutory successor has yet been called upon to act as president.[160]
 Under the Twenty-fifth Amendment, the president may temporarily transfer the presidential powers and duties to the vice president, who then becomes acting president, by transmitting to the speaker of the House and the president pro tempore of the Senate a statement that he is unable to discharge his duties. The president resumes his or her powers upon transmitting a second declaration stating that he is again able. The mechanism has been used by Ronald Reagan (once), George W. Bush (twice), and Joe Biden (once), each in anticipation of surgery.[161][162]
 The Twenty-fifth Amendment also provides that the vice president, together with a majority of certain members of the Cabinet, may transfer the presidential powers and duties to the vice president by transmitting a written declaration, to the speaker of the House and the president pro tempore of the Senate, to the effect that the president is unable to discharge his or her powers and duties. If the president then declares that no such inability exist, he or she resumes the presidential powers unless the vice president and Cabinet make a second declaration of presidential inability, in which case Congress decides the question.
 Article II, Section 4 of the Constitution allows for the removal of high federal officials, including the president, from office for ""treason, bribery, or other high crimes and misdemeanors"". Article I, Section 2, Clause 5 authorizes the House of Representatives to serve as a ""grand jury"" with the power to impeach said officials by a majority vote.[163] Article I, Section 3, Clause 6 authorizes the Senate to serve as a court with the power to remove impeached officials from office, by a two-thirds vote to convict.[164]
 Three presidents have been impeached by the House of Representatives: Andrew Johnson in 1868, Bill Clinton in 1998, and Donald Trump in 2019 and 2021; none have been convicted by the Senate. Additionally, the House Judiciary Committee conducted an impeachment inquiry against Richard Nixon in 1973–74 and reported  three articles of impeachment to the House of Representatives for final action; however, he resigned from office before the House voted on them.[163]
 Controversial measures have sometimes been taken short of removal to deal with perceived recklessness on the part of the president, or with a long-term disability. In some cases, staff have intentionally failed to deliver messages to or from the president, typically to avoid executing or promoting the president to write certain orders. This has ranged from Richard Nixon's Chief of Staff not transmitting orders to the Cabinet due to the president's heavy drinking, to staff removing memos from Donald Trump's desk.[165] Decades before the Twenty-fifth Amendment, in 1919, President Woodrow Wilson had a stroke that left him partly incapacitated.  First lady Edith Wilson kept this condition a secret from the public for a while, and controversially became the sole gatekeeper for access to the president (aside from his doctor), assisting him with paperwork and deciding which information was ""important"" enough to share with him.
 Since 2001, the president's annual salary has been $400,000, along with a $50,000 expense allowance; a $100,000 nontaxable travel account; and a $19,000 entertainment[clarification needed] account. The president's salary is set by Congress, and under Article II, Section 1, Clause 7 of the Constitution, any increase or reduction in presidential salary cannot take effect before the next presidential term of office.[169][10]
 The Executive Residence of the White House in Washington, D.C., is the official residence of the president. The site was selected by George Washington, and the cornerstone was laid in 1792. Every president since John Adams (in 1800) has lived there. At various times in U.S. history, it has been known as the ""President's Palace"", the ""President's House"", and the ""Executive Mansion"". Theodore Roosevelt officially gave the White House its current name in 1901.[170] The federal government pays for state dinners and other official functions, but the president pays for personal, family, and guest dry cleaning and food.[171]
 Camp David, officially titled Naval Support Facility Thurmont, a mountain-based military camp in Frederick County, Maryland, is the president's country residence. A place of solitude and tranquility, the site has been used extensively to host foreign dignitaries since the 1940s.[172]
 The President's Guest House, located next to the Eisenhower Executive Office Building at the White House Complex and Lafayette Park, serves as the president's official guest house and as a secondary residence for the president if needed. Four interconnected, 19th-century houses—Blair House, Lee House, and 700 and 704 Jackson Place—with a combined floor space exceeding 70,000 square feet (6,500 m2) constitute the property.[173]
 The primary means of long-distance air travel for the president is one of two identical Boeing VC-25 aircraft, which are extensively modified Boeing 747 airliners and are referred to as Air Force One while the president is on board (although any U.S. Air Force aircraft the president is aboard is designated as ""Air Force One"" for the duration of the flight). In-country trips are typically handled with just one of the two planes, while overseas trips are handled with both, one primary and one backup. The president also has access to smaller Air Force aircraft, most notably the Boeing C-32, which are used when the president must travel to airports that cannot support a jumbo jet. Any civilian aircraft the president is aboard is designated Executive One for the flight.[174]
 For short-distance air travel, the president has access to a fleet of U.S. Marine Corps helicopters of varying models, designated Marine One when the president is aboard any particular one in the fleet. Flights are typically handled with as many as five helicopters all flying together and frequently swapping positions as to disguise which helicopter the president is actually aboard to any would-be threats.
 For ground travel, the president uses the presidential state car, which is an armored limousine designed to look like a Cadillac sedan, but built on a truck chassis.[175][176] The U.S. Secret Service operates and maintains the fleet of several limousines. The president also has access to two armored motorcoaches, which are primarily used for touring trips.[177]
 The U.S. Secret Service is charged with protecting the president and the first family. As part of their protection, presidents, first ladies, their children and other immediate family members, and other prominent persons and locations are assigned Secret Service codenames.[178] The use of such names was originally for security purposes and dates to a time when sensitive electronic communications were not routinely encrypted; today, the names simply serve for purposes of brevity, clarity, and tradition.[179]
 Some former presidents have had significant careers after leaving office. Prominent examples include William Howard Taft's tenure as chief justice of the United States and Herbert Hoover's work on government reorganization after World War II. Grover Cleveland, whose bid for reelection failed in 1888, was elected president again four years later in 1892, and Donald Trump, whose bid for reelection failed in 2020, was elected president again four years later in 2024. Two former presidents served in Congress after leaving the White House: John Quincy Adams was elected to the House of Representatives, serving there for 17 years, and Andrew Johnson returned to the Senate in 1875, though he died soon after. Some ex-presidents were very active, especially in international affairs, most notably Theodore Roosevelt;[180] Herbert Hoover;[181] Richard Nixon;[182] and Jimmy Carter.[183][184]
 Presidents may use their predecessors as emissaries to deliver private messages to other nations or as official representatives of the United States to state funerals and other important foreign events.[185][186] Richard Nixon made multiple foreign trips to countries including China and Russia and was lauded as an elder statesman.[187] Jimmy Carter became a global human rights campaigner, international arbiter, and election monitor, as well as a recipient of the Nobel Peace Prize. Bill Clinton also worked as an informal ambassador, most recently in the negotiations that led to the release of two American journalists, Laura Ling and Euna Lee, from North Korea. During his presidency, George W. Bush called on former presidents George H. W. Bush and Clinton to assist with humanitarian efforts after the 2004 Indian Ocean earthquake and tsunami. President Obama followed suit by asking presidents Clinton and George W. Bush to lead efforts to aid Haiti after an earthquake devastated that country in 2010.
 Clinton has been active politically since his presidential term ended, working with his wife Hillary on her 2008 and 2016 presidential bids and President Obama on his 2012 reelection campaign. Obama has also been active politically since his presidential term ended, having worked with his former vice president Joe Biden on his 2020 election campaign. After losing his bid for the presidency in 2020, Trump remained politically active and was an outspoken critic of his successor and the Democratic Party. He also contended with four criminal cases. Trump announced his fourth bid to the presidency in 2022, ultimately becoming the nominee of his party for the third time and won a second presidential term in 2024.
 The Former Presidents Act (FPA), enacted in 1958, grants lifetime benefits to former presidents and their widows, including a monthly pension, medical care in military facilities, health insurance, and Secret Service protection; also provided is funding for a certain number of staff and for office expenses.  The act has been amended several times to provide increases in presidential pensions and in the allowances for office staff.  The FPA excludes any president who was removed from office by impeachment.[188]
 According to a 2008 report by the Congressional Research Service:[188]
 The pension has increased numerous times with congressional approval. Retired presidents receive a pension based on the salary of the current administration's cabinet secretaries, which was $199,700 per year in 2012.[189] Former presidents who served in Congress may also collect congressional pensions.[190] The act also provides former presidents with travel funds and franking privileges.
 Prior to 1997, all former presidents, their spouses, and their children until age 16 were protected by the Secret Service until the president's death.[191][192] In 1997, Congress passed legislation limiting Secret Service protection to no more than 10 years from the date a president leaves office.[193] On January 10, 2013, President Obama signed legislation reinstating lifetime Secret Service protection for him, George W. Bush, and all subsequent presidents.[194] A first spouse who remarries is no longer eligible for Secret Service protection.[193]
 Every president since Herbert Hoover has created a repository known as a presidential library for preserving and making available his papers, records, and other documents and materials. Completed libraries are deeded to and maintained by the National Archives and Records Administration (NARA); the initial funding for building and equipping each library must come from private, non-federal sources.[195] There are currently thirteen presidential libraries in the NARA system. There are also presidential libraries maintained by state governments and private foundations and Universities of Higher Education, including:
 Several former presidents have overseen the building and opening of their own presidential libraries. Some even made arrangements for their own burial at the site. Several presidential libraries contain the graves of the president they document: 
 These gravesites are open to the general public.
 Political parties have dominated American politics for most of the nation's history. Though the Founding Fathers generally spurned political parties as divisive and disruptive, and their rise had not been anticipated when the U.S. Constitution was drafted in 1787, organized political parties developed in the U.S. in the mid-1790s nonetheless. They evolved from political factions, which began to appear almost immediately after the Federal government came into existence. Those who supported the Washington administration were referred to as ""pro-administration"" and would eventually form the Federalist Party, while those in opposition largely joined the emerging Democratic-Republican Party.[196]
 Greatly concerned about the very real capacity of political parties to destroy the fragile unity holding the nation together, Washington remained unaffiliated with any political faction or party throughout his eight-year presidency. He was, and remains, the only U.S. president never to be affiliated with a political party.[197][198] Since Washington, every U.S. president has been affiliated with a political party at the time of assuming office.[199][200]
 The number of presidents per political party by their affiliation at the time they were first sworn into office (alphabetical, by last name) are:
 The following timeline depicts the progression of the presidents and their political affiliation at the time of assuming office.
"
Continental Army,https://en.wikipedia.org/wiki/Continental_Army,"

 The Continental Army was the army of the United Colonies representing the Thirteen Colonies and later the United States during the American Revolutionary War. It was formed on June 14, 1775, by a resolution passed by the Second Continental Congress, meeting in Philadelphia after the war's outbreak. The Continental Army was created to coordinate military efforts of the colonies in the war against the British, who sought to maintain control over the American colonies. General George Washington was appointed commander-in-chief of the Continental Army and maintained this position throughout the war.
 The Continental Army was supplemented by local militias and volunteer troops that were either loyal to individual states or otherwise independent. Most of the Continental Army was disbanded in 1783 after the Treaty of Paris formally ended the war. The Continental Army's 1st and 2nd Regiments went on to form what was to become the Legion of the United States in 1792, which ultimately served as the foundation for the creation of the United States Army.
 The Continental Army consisted of soldiers from all the Thirteen Colonies and, after 1776, from all 13 states. The American Revolutionary War began at the Battles of Lexington and Concord on April 19, 1775, at a time when the colonial revolutionaries had no standing army. Previously, each colony had relied upon the militia (which was made up of part-time citizen-soldiers) for local defense; or the raising of temporary provincial troops during such crises as the French and Indian War of 1754–1763. As tensions with Great Britain increased in the years leading to the war, colonists began to reform their militias in preparation for the perceived potential conflict. Training of militiamen increased after the passage of the Intolerable Acts in 1774. Colonists such as Richard Henry Lee proposed forming a national militia force, but the First Continental Congress rejected the idea.[2]
 On April 23, 1775, the Massachusetts Provincial Congress authorized the raising of a colonial army consisting of 26 company regiments. New Hampshire, Rhode Island, and Connecticut soon raised similar but smaller forces. On June 14, 1775, the Second Continental Congress decided to proceed with the establishment of a Continental Army for purposes of common defense, adopting the forces already in place outside Boston (22,000 troops) and New York (5,000). It also raised the first ten companies of Continental troops on a one-year enlistment, riflemen from Pennsylvania, Maryland, and Virginia to be used as light infantry. The Pennsylvania riflemen became the 1st Continental Regiment in January 1776. On June 15, 1775, Congress elected by unanimous vote George Washington as Commander-in-Chief, who accepted and served throughout the war without any compensation except for reimbursement of expenses.[3] As the Continental Congress increasingly adopted the responsibilities and posture of a legislature for a sovereign state, the role of the Continental Army became the subject of considerable debate. Some Americans had a general aversion to maintaining a standing army; but on the other hand, the requirements of the war against the British required the discipline and organization of a modern military. As a result, the army went through several distinct phases, characterized by official dissolution and reorganization of units.
 The Continental Army's forces included several successive armies or establishments:
 Military affairs were at first managed by the Continental Congress in plenary session, although specific matters were prepared by a number of ad hoc committees. In June 1776 a five-member standing committee, the Board of War and Ordnance, was established in order to replace the ad hoc committees. The five members who formed the Board fully participated in the plenary activities of Congress as well as in other committees and were unable to fully engage in the administrative leadership of the Continental Army. A new Board of War was therefore formed in October 1777, of three commissioners not member of Congress. Two more commissioners, not members of Congress, were shortly thereafter added, but in October 1778, the membership was set to three commissioners not members of Congress and two commissioners members of Congress. In early 1780, the Quartermaster General, the Commissary General of Purchase, and the Commissary General of Issue were put under the direction of the Board. The Office of the Secretary at War was created in February 1781, although the Office did not start its work until Benjamin Lincoln assumed the office in October 1781.[5]
 On June 15, 1775, Congress elected by unanimous vote George Washington as Commander-in-Chief, who accepted and served throughout the war without any compensation except for reimbursement of expenses.[3]  Washington, as commander-in-chief, was supported by a chief administrative officer, the Adjutant General. Horatio Gates held the position (1775–1776), Joseph Reed (1776–1777), George Weedon and Isaac Budd Dunn (1777), Morgan Connor 1777, Timothy Pickering (1777–1778), Alexander Scammell (1778–1781), and Edward Hand (1781–1783).[6] An Inspector General assisted the Commander-in-Chief through periodically inspecting and reporting on the condition of troops. The first incumbent was Thomas Conway (1777–1778), followed by Baron von Steuben 1778–1784, under whom the position became that of a de facto chief of staff.[7] The Judge Advocate General assisted the commander-in-chief with the administration of military justice, but he did not, as his modern counterpart, give legal advise. William Tudor was the first appointee.[8] He was followed by John Laurance in 1777 and Thomas Edwards in 1781[9] The Mustermaster General kept track by name of every officer and man serving in the army. The first mustermaster was Stephen Moylan.[10] He was followed by Gunning Bedford Jr. 1776–1777 and Joseph Ward.[9]
 Units of the Continental Army were assigned to any one of the territorial departments to decentralize command and administration. In general there were seven territorial departments,[11] although their boundaries were subject to change and they were not all in existence throughout the war. The Department of New York (later the Northern Department) was created when Congress made Philip Schuyler its commander on June 15, 1775. The Southern and Middle Departments were added in February 1776. Several others were added the same year. A major general appointed by Congress commanded each department. Under his command came all Continental Army units within the territorial limits of the department, as well as state troops and militia – if released by the governor of the state.[12]
 All troops under the department commander were designated as an army; hence troops in the Northern Department were called the Northern Army, in the Southern Department the Southern Army, etc. The department commander could be field commander or he could appoint another officer to command the troops in the field. Depending on the size of the army, it could be divided into wings or divisions (of typically three brigades) that were temporary organizations, and brigades (of two to five regiments) that in effect were permanent organizations and the basic tactical unit of the Continental Army.[13]
 An infantry regiment in the Continental Army typically consisted of 8 to 10 companies, each commanded by a captain. Field officers usually included a colonel, a lieutenant colonel, and a major. A regimental staff was made up of an adjutant, quartermaster, surgeon, surgeon's mate, paymaster, and chaplain. Infantry regiments were often called simply regiments or battalions.[14] The regiment's fighting strength consisted of a single battalion of 728 officers and enlisted men at full strength.[15] Cavalry and artillery regiments were organized in a similar manner. A company of cavalry was frequently called a troop. An artillery company contained specialized soldiers, such as bombardiers, gunners, and matrosses.[14] A continental cavalry regiment had a nominal strength of 280 officers and men, but the actual strength was usually less than 150 men and even fewer horses.[16] Artificers were civilian or military mechanics and artisans employed by the army  to provide services. They included blacksmiths, coopers, carpenters, harnessmakers, and wheelwrights.[14]
 In June 1775, Congress created the position of Quartermaster General, after the British example. He was charged with opening and maintaining the lines of advance and retreat, laying out camps and assigning quarters. His responsibilities included furnishing the army with materiel and supplies, although the supply of arms, clothing, and provisions fell under other departments. The transportation of all supplies, even those provided by other departments, came under his ambit. The Quartermaster General served with the main army under General Washington, but was directly responsible to Congress. Deputy quartermasters were appointed by Congress to serve with separate armies, and functioned independently of the Quartermaster General. Thomas Mifflin served as Quartermaster General (1775–1776 and 1776–1778), Stephen Moylan (1776), Nathanael Green (1778–1780), and Timothy Pickering (from 1780).[17]
 Congress also created the position of Commissary General of Stores and Provisions directly responsible to Congress, with Joseph Trumbull as the first incumbent. In 1777, Congress divided the department into two, a Commissary General of Purchases, with four deputies, and a Commissary General of Issues, with three deputies. William Buchanan was head of the Purchase Department (1777–1778), Jeremiah Wadsworth (1778–1779), and Ephraim Blaine (1779–1781). In 1780, the department became subordinated to the Superintendent of Finance, although Blaine retained his position. Charles Stewart served as Commissary General of Issues (1777–1782).[18]
 The responsibility for procuring arms and ammunition at first rested with various committees of Congress. In 1775, a field organization, usually known as the Military Branch of the Commissariat of Military Stores, was made responsible for distribution and care of ordnance in the field. In 1777, Congress established a Commissary General of Military Stores. Known as the Civil Branch, this organization was responsible for handling arsenals, laboratories, and some procurement under the general supervision of the Board of War. Later in the war, a Surveyor of Ordnance was made responsible for inspecting foundries, magazines, ordnance shops, and field ordnance. In July 1777, the Board of War was authorized to purchase artillery.[19]
 Congress created a hospital department in July 1775 as a part of the Continental Army's administrative structure. It came under the Director General of the Hospital Department, chosen by Congress but serving under the Commander-in-Chief, and was staffed by four surgeons, an apothecary, twenty surgeon's mates, a nurse for every ten patients, a matron to supervise the nurses, a clerk, and two storekeepers. The department was reorganized in 1777; deputy director generals were added to the administrative structure; commissaries of hospitals were established to provide food and  forage; and apothecary generals were established to procure and distribute medicines.[20] The first director general was Benjamin Church (1775), he was followed by John Morgan (1775–1777), William Shippen (1777–1781), and John Cochran (1781).[9]
 Keeping the continentals clothed was a difficult task and to do this Washington appointed James Mease, a merchant from Philadelphia, as Clothier General. Mease worked closely with state-appointed agents to purchase clothing and things such as cow hides to make clothing and shoes for soldiers. Mease eventually resigned in 1777 and had compromised much of the organization of the Clothing Department. After this, on many accounts, the soldiers of the Continental Army were often poorly clothed, had few blankets, and often did not even have shoes. The problems with clothing and shoes for soldiers were often not the result of not having enough but of organization and lack of transportation. To reorganize the Board of War was appointed to sort out the clothing supply chain. During this time they sought out the help of France and for the remainder of the war, clothing was coming from over-sea procurement.[21]
 The disbursing of money to pay soldiers and suppliers were the function of the Paymaster-General. James Warren was the first incumbent of this office.[22] His successor was William Palfrey in 1776, who was followed by John Pierce Jr. in 1781.[9]
 The Continental Army lacked the discipline typically expected of an army. When they first assembled, the count of how many soldiers George Washington had was delayed a little over a week. Instead of obeying their commanders and officers without question, each unit was a community that had democratically chosen its leaders. The regiments, coming from different states, were uneven in numbers. Logically, they should be evened, which would mean moving soldiers around. In the spirit of American republicanism, if George Washington separated the soldiers from the officers they had chosen they did not believe they should have to serve. George Washington had to give in to the soldiers and negotiate with them. He needed them to have an army.[24]
 Soldiers in the Continental Army were volunteers; they agreed to serve in the army and standard enlistment periods lasted from one to three years. Early in the war, the enlistment periods were short, as the Continental Congress feared the possibility of the Continental Army evolving into a permanent army. The army never numbered more than 48,000 men overall and 13,000 troops in one area. The turnover proved a constant problem, particularly in the winter of 1776–1777, and longer enlistments were approved. As the new country (not yet fully independent) had no money, the government agreed to give grants to the soldiers which they could exchange for money.[25] In 1781 and 1782, Patriot officials and officers in the Southern Colonies repeatedly implemented policies that offered slaves as rewards for recruiters who managed to enlist a certain number of volunteers in the Continental Army; in January 1781, Virginia's General Assembly passed a measure which announced that voluntary enlistees in the Virginia Line's regiments would be given a ""healthy sound negro"" as a reward.[25]
 The officers of both the Continental Army and the state militias were typically yeoman farmers with a sense of honor and status and an ideological commitment to oppose the policies of the British Crown.[26]  The enlisted men were very different. They came from the working class or minority groups (English, Ulster Protestant, Black or of African descent). They were motivated to volunteer by specific contracts that promised bounty money; regular pay at good wages; food, clothing, and medical care; companionship; and the promise of land ownership after the war. By 1780, more than 30,000 men served in the Continental army, but the lack of resources and proper training resulted in the deaths of over 13,000 soldiers.[27] By 1781–1782, threats of mutiny and actual mutinies were becoming serious.[28][29] Up to a fourth of Washington's army were of Scots-Irish (English and Scottish descent) Ulster origin, many being recent arrivals and in need of work.[30]
 The Continental Army was racially integrated, a condition the United States Army would not see again until the late 1940s. During the Revolution, African American slaves were promised freedom in exchange for military service by both the Continental and British armies.[31][32][33] Approximately 6,600 people of color (including African American, indigenous, and multiracial men) served with the colonial forces, and made up one-fifth of the Northern Continental Army.[34][35]
 In addition to the Continental Army regulars, state militia units were assigned for short-term service and fought in campaigns throughout the war. Sometimes the militia units operated independently of the Continental Army, but often local militias were called out to support and augment the Continental Army regulars during campaigns. The militia troops developed a reputation for being prone to premature retreats, a fact that General Daniel Morgan integrated into his strategy at the Battle of Cowpens and used to fool the British in 1781.[36]
 The financial responsibility for providing pay, food, shelter, clothing, arms, and other equipment to specific units was assigned to states as part of the establishment of these units. States differed in how well they lived up to these obligations. There were constant funding issues and morale problems as the war continued. This led to the army offering low pay, often rotten food, hard work, cold, heat, poor clothing and shelter, harsh discipline, and a high chance of becoming a casualty.[37]
 At the time of the siege of Boston, the Continental Army at Cambridge, Massachusetts, in June 1775, is estimated to have numbered from 14,000 to 16,000 men from New England (though the actual number may have been as low as 11,000 because of desertions). Until Washington's arrival, it remained under the command of Artemas Ward. The British force in Boston was increasing by fresh arrivals. It numbered then about 10,000 men. The British controlled Boston and defended it with their fleet, but they were outnumbered and did not attempt to challenge the American control of New England. Washington selected young Henry Knox, a self-educated strategist, to take charge of the artillery from an abandoned British fort in upstate New York, and dragged across the snow to and placed them in the hills surrounding Boston in March 1776.[38] The British situation was untenable. They negotiated an uneventful abandonment of the city and relocated their forces to Halifax in Canada. Washington relocated his army to New York. For the next five years, the main bodies of the Continental and British armies campaigned against one another in New York, New Jersey, and Pennsylvania. These campaigns included the notable battles of Trenton, Princeton, Brandywine, Germantown, and Morristown, among many others.
 The army increased its effectiveness and success rate through a series of trials and errors, often at a great human cost. General Washington and other distinguished officers were instrumental leaders in preserving unity, learning and adapting, and ensuring discipline throughout the eight years of war. In the winter of 1777–1778, with the addition of Baron von Steuben, a Prussian expert, the training and discipline of the Continental Army was dramatically upgraded to modern European standards through the Regulations for the Order and Discipline of the Troops of the United States.[39] This was during the infamous winter at Valley Forge. Washington always viewed the Army as a temporary measure and strove to maintain civilian control of the military, as did the Continental Congress, though there were minor disagreements about how this was to be carried out.
 Throughout its existence, the Army was troubled by poor logistics, inadequate training, short-term enlistments, interstate rivalries, and Congress's inability to compel the states to provide food, money, or supplies. In the beginning, soldiers enlisted for a year, largely motivated by patriotism; but as the war dragged on, bounties and other incentives became more commonplace. Major and minor mutinies—56 in all—diminished the reliability of two of the main units late in the war.[40]
 The French played a decisive role in 1781 as Washington's Army was augmented by a French expeditionary force under Lieutenant General Rochambeau and a squadron of the French navy under the Comte de Barras. By disguising his movements, Washington moved the combined forces south to Virginia without the British commanders in New York realizing it. This resulted in the capture of the main British invasion force in the south at the Siege of Yorktown, which resulted in the American and their allied victory in the land war in North America and assured independence.
 A small residual force remained at West Point and some frontier outposts until Congress created the United States Army by their resolution of June 3, 1784. Although Congress declined on May 12 to make a decision on the peace establishment, it did address the need for some troops to remain on duty until the British evacuated New York City and several frontier posts. The delegates told Washington to use men enlisted for fixed terms as temporary garrisons. A detachment of those men from West Point reoccupied New York without incident on November 25. When Steuben's effort in July to negotiate a transfer of frontier forts with Major General Frederick Haldimand collapsed, however, the British maintained control over them, as they would into the 1790s. That failure and the realization that most of the remaining infantrymen's enlistments were due to expire by June 1784 led Washington to order Knox, his choice as the commander of the peacetime army, to discharge all but 500 infantry and 100 artillerymen before winter set in. The former regrouped as 1st American Regiment,  under Colonel Henry Jackson of Massachusetts. The single artillery company, New Yorkers under Major John Doughty, came from remnants of the 2nd Continental Artillery Regiment.
 Congress issued a proclamation on October 18, 1783, which approved Washington's reductions. On November 2, Washington, then at Rockingham near Rocky Hill, New Jersey, released his Farewell Orders issued to the Armies of the United States of America to the Philadelphia newspapers for nationwide distribution to the furloughed men. In the message, he thanked the officers and men for their assistance and reminded them that ""the singular interpositions of Providence in our feeble condition were such, as could scarcely escape the attention of the most unobserving; while the unparalleled perseverance of the Armies of the United States, through almost every possible suffering and discouragement for the space of eight long years, was little short of a standing Miracle.""[60]
 Washington believed that the blending of persons from every colony into ""one patriotic band of Brothers"" had been a major accomplishment, and he urged the veterans to continue this devotion in civilian life. Washington said farewell to his remaining officers on December 4 at Fraunces Tavern in New York City. On December 23 he appeared in Congress, then sitting at Annapolis, and returned his commission as commander-in-chief: ""Having now finished the work assigned me, I retire from the great theatre of Action; and bidding an Affectionate farewell to this August body under whose orders I have so long acted, I here offer my Commission, and take my leave of all the employments of public life."" Congress ended the War of American Independence on January 14, 1784, by ratifying the definitive peace treaty that had been signed in Paris on September 3.
 Monthly pay of the officers and soldiers of the continental line as established by the resolutions of Congress, fixing the arrangement of the Continental Army May 27, 1778, which rate of pay continued to the end of the war.[61]
 During the American Revolutionary War, the Continental Army initially wore ribbons, cockades, and epaulettes of various colors as an ad hoc form of rank insignia, as General George Washington wrote in 1775:
 In 1776, captains were to have buff or white cockades.
 Later on in the war, the Continental Army established its own uniform with a black and white cockade among all ranks. Infantry officers had silver and other branches gold insignia:
"
Patriot (American Revolution),https://en.wikipedia.org/wiki/Patriot_(American_Revolution),"


 Patriots (also known as Revolutionaries, Continentals, Rebels, or Whigs) were colonists in the Thirteen Colonies who opposed the Kingdom of Great Britain's control and governance during the colonial era and supported and helped launch the American Revolution that ultimately established American independence. Patriot politicians led colonial opposition to British policies regarding the American colonies, eventually building support for the adoption of the Declaration of Independence, which was adopted unanimously by the Second Continental Congress on July 4, 1776. After the American Revolutionary War began the year before, in 1775, many patriots assimilated into the Continental Army, which was commanded by George Washington and which secured victory against the British Army, leading the British to acknowledge the sovereign independence of the colonies, reflected in the Treaty of Paris, which led to the establishment of the United States in 1783.
 The patriots were inspired by English and American republican ideology that was part of the Age of Enlightenment, and rejected monarchy and aristocracy and supported individual liberty and natural rights and legal rights. Prominent patriot political theorists such as Thomas Jefferson, John Adams, and Thomas Paine spearheaded the American Enlightenment, which was in turn inspired by European thinkers such as Francis Bacon, John Locke, and Jean-Jacques Rousseau. Though slavery existed in all of the Thirteen Colonies prior to the American Revolution, the issue divided patriots, with some supporting its abolition while others espoused proslavery thought.
 The patriots included members of every social and ethnic group in the colonies, though support for the patriot cause was strongest in the New England Colonies and weakest in the Southern Colonies. The American Revolution divided the colonial population into three groups: patriots, who supported the end of British rule; loyalists, who supported Britain's continued control over the colonies; and those who remained neutral. African Americans who supported the patriots were known as Black Patriots, with their counterparts on the British side being referred to as Black Loyalists. 
 The critics of British policy towards the Thirteen Colonies called themselves ""Whigs"" after 1768, identifying with members of the British Whig party who favored similar colonial policies.[citation needed] Samuel Johnson writes that, at the time, the word ""patriot"" had a negative connotation and was used as a negative epithet for ""a factious disturber of the government"".[1]
 Prior to the American Revolution, colonists who supported British authority called themselves Tories or royalists, identifying with the political philosophy of traditionalist conservatism as it existed in Great Britain. During the American Revolution, these persons became known primarily as Loyalists. Afterward, some 15% of Loyalists emigrated north to the remaining British territories in the Canadas. There they called themselves the United Empire Loyalists. 85% of the Loyalists decided to stay in the new United States and were granted American citizenship.
 Prior to the formal beginning of the American Revolution, many patriots were active in groups including the Sons of Liberty. The most prominent patriot leaders are referred to today as the Founding Fathers, who are generally defined as the 56 men who, as delegates to the Second Continental Congress in Philadelphia, signed the Declaration of Independence. 
 Patriots included a cross-section of the population of the Thirteen Colonies and came from varying backgrounds. Roughly 40 to 45 percent of the white population in the Thirteen Colonies supported the patriots' cause, between 15 and 20 percent supported the Loyalists, and the remainder were neutral or kept a low profile regarding their loyalties.[2]  The great majority of Loyalists remained in the Thirteen Colonies during the American Revolution; a minority, however, fled the nation for Canada, Great Britain, Florida, or the West Indies.[3]
 Historians have explored the motivations that pulled men to one side or the other.[4] Yale historian Leonard Woods Labaree used the published and unpublished writings and letters of leading men on each side, searching for how personality shaped their choice. He finds eight characteristics that differentiated the two groups. Loyalists were older, better established, and more likely to resist innovation than the patriots. Loyalists felt that the Crown was the legitimate government and resistance to it was morally wrong, while the patriots felt that morality was on their side because the British government had violated the constitutional rights of Englishmen. Men who were alienated by physical attacks on Royal officials took the Loyalist position, while those who were offended by British responses to actions such as the Boston Tea Party became patriots. Merchants in the port cities with long-standing financial attachments to Britain were likely to remain loyal, while few patriots were so deeply enmeshed in the system. Some Loyalists, according to Labaree, were ""procrastinators"" who believed that independence was bound to come some day but wanted to ""postpone the moment"", while the patriots wanted to ""seize the moment"". Loyalists were cautious and afraid of anarchy or tyranny that might come from mob rule; patriots made a systematic effort to take a stand against the British government. Finally, Labaree argues that Loyalists were pessimists who lacked the patriots' confidence that independence lay ahead.[5][6]
 The patriots rejected taxes imposed by legislatures in which the taxpayer was not represented. ""No taxation without representation"" was their slogan, referring to the lack of representation in the British Parliament. The British countered that there was ""virtual representation"" in the sense that all members of Parliament represented the interests of all the citizens of the British Empire. Some patriots declared that they were loyal to the king, but they insisted that they should be free to run their own affairs. In fact, they had been running their own affairs since the period of ""salutary neglect"" before the French and Indian War. Some radical patriots tarred and feathered tax collectors and customs officers, making those positions dangerous; according to Benjamin Irvin, the practice was especially prevalent in Boston where many patriots lived.[7]
"
American Revolutionary War,https://en.wikipedia.org/wiki/American_Revolutionary_War,"


 The American Revolutionary War (April 19, 1775 – September 3, 1783), also known as the Revolutionary War or American War of Independence, was an armed conflict that was part of the broader American Revolution, in which American Patriot forces organized as the Continental Army and commanded by George Washington defeated the British Army. The conflict was fought in North America, the Caribbean, and the Atlantic Ocean. The war ended with the Treaty of Paris (1783), which resulted in Great Britain ultimately recognizing the independence of the United States of America.
 After the British Empire gained dominance in North America with victory over the French in the Seven Years' War in 1763, tensions and disputes arose between Great Britain and the Thirteen Colonies over a variety of issues, including the Stamp and Townshend Acts. The resulting British military occupation led to the Boston Massacre in 1770. Among further tensions, the British Parliament imposed the Intolerable Acts in mid-1774. A British attempt to disarm the Americans and the resulting Battles of Lexington and Concord in April 1775 ignited the war. In June, the Second Continental Congress formalized Patriot militias into the Continental Army and appointed Washington its commander-in-chief. The British Parliament declared the colonies to be in a state of rebellion in August 1775. The stakes of the war were formalized with passage of the Lee Resolution by the Congress in Philadelphia on July 2, 1776, and the unanimous ratification of the Declaration of Independence on July 4, 1776.
 After a successful siege, Washington's forces drove the British Army out of Boston in March 1776, and British commander in chief William Howe responded by launching the New York and New Jersey campaign. Howe captured New York City in November. Washington responded by clandestinely crossing the Delaware River and winning small but significant victories at Trenton and Princeton. In the summer of 1777, as Howe was poised to capture Philadelphia, the Continental Congress fled to Baltimore. In October 1777, a separate northern British force under the command of John Burgoyne was forced to surrender at Saratoga in an American victory that proved crucial in convincing France and Spain that an independent United States was a viable possibility. France signed a commercial agreement with the rebels, followed by a Treaty of Alliance in February 1778. In 1779, the Sullivan Expedition undertook a scorched earth campaign against the Iroquois who were largely allied with the British. Indian raids on the American frontier, however, continued to be a problem. Also, in 1779, Spain allied with France against Great Britain in the Treaty of Aranjuez, though Spain did not formally ally with the Americans.
 Howe's replacement Henry Clinton intended to take the war against the Americans into the Southern Colonies. Despite some initial success, British General Cornwallis was besieged by a Franco-American force in Yorktown in September and October 1781. Cornwallis was forced to surrender in October. The British wars with France and Spain continued for another two years, but fighting largely ceased in North America. In the Treaty of Paris, ratified on September 3, 1783, Great Britain acknowledged the sovereignty and independence of the United States, bringing the American Revolutionary War to an end. The Treaties of Versailles resolved Great Britain's conflicts with France and Spain and forced Great Britain to cede Tobago, Senegal, and small territories in India to France, and Menorca, West Florida and East Florida to Spain.[43][44]
 The French and Indian War, part of the wider global conflict known as the Seven Years' War, ended with the 1763 Peace of Paris, which expelled France from their possessions in New France.[45] The Royal Proclamation of 1763 was designed to refocus colonial expansion north into Nova Scotia and south into Florida, with the Mississippi River as the dividing line between British and Spanish possessions in America. Settlement was tightly restricted beyond the 1763 limits, and claims west of this line, including by Virginia and Massachusetts, were rescinded.[46] With the exception of Virginia and others deprived of rights to western lands, the colonial legislatures agreed on the boundaries but disagreed on where to set them. Many settlers resented the restrictions entirely, and enforcement required permanent garrisons along the frontier, which led to increasingly bitter disputes over who should pay for them.[47]
 The huge debt incurred by the Seven Years' War and demands from British taxpayers for cuts in government expenditure meant Parliament expected the colonies to fund their own defense.[47] The 1763 to 1765 Grenville ministry instructed the Royal Navy to cease trading smuggled goods and enforce customs duties levied in American ports.[47] The most important was the 1733 Molasses Act; routinely ignored before 1763, it had a significant economic impact since 85% of New England rum exports were manufactured from imported molasses. These measures were followed by the Sugar Act and Stamp Act, which imposed additional taxes on the colonies to pay for defending the western frontier.[48] The taxes proved highly burdensome, particularly for the poorer classes, and quickly became a source of discontent.[49] In July 1765, the Whigs formed the First Rockingham ministry, which repealed the Stamp Act and reduced tax on foreign molasses to help the New England economy, but re-asserted Parliamentary authority in the Declaratory Act.[50]
 However, this did little to end the discontent; in 1768, a riot started in Boston when the authorities seized the sloop Liberty on suspicion of smuggling.[51] Tensions escalated in March 1770 when British troops fired on rock-throwing civilians, killing five in what became known as the Boston Massacre.[52] The Massacre coincided with the partial repeal of the Townshend Acts by the Tory-based North Ministry. North insisted on retaining duty on tea to enshrine Parliament's right to tax the colonies; the amount was minor, but ignored the fact it was that very principle Americans found objectionable.[53]
 In April 1772, colonialists staged the first American tax revolt against British royal authority in Weare, New Hampshire, later referred to as the Pine Tree Riot.[54] This would inspire the design of the Pine Tree Flag. Tensions escalated following the destruction of a customs vessel in the June 1772 Gaspee Affair, then came to a head in 1773. A banking crisis led to the near-collapse of the East India Company, which dominated the British economy; to support it, Parliament passed the Tea Act, giving it a trading monopoly in the Thirteen Colonies. Since most American tea was smuggled by the Dutch, the act was opposed by those who managed the illegal trade, while being seen as another attempt to impose the principle of taxation by Parliament.[55] In December 1773, a group called the Sons of Liberty disguised as Mohawks dumped crates of tea into Boston Harbor, an event later known as the Boston Tea Party. The British Parliament responded by passing the so-called Intolerable Acts, aimed specifically at Massachusetts, although many colonists and members of the Whig opposition considered them a threat to liberty in general. This increased sympathy for the Patriot cause locally, in the British Parliament, and in the London press.[56]
 Throughout the 18th century, the elected lower houses in the colonial legislatures gradually wrested power from their governors.[57] Dominated by smaller landowners and merchants, these assemblies now established ad-hoc provincial legislatures, effectively replacing royal control. With the exception of Georgia, twelve colonies sent representatives to the First Continental Congress to agree on a unified response to the crisis.[58] Many of the delegates feared that a boycott would result in war and sent a Petition to the King calling for the repeal of the Intolerable Acts.[59] After some debate, on September 17, 1774, Congress endorsed the Massachusetts Suffolk Resolves and on October 20 passed the Continental Association, which instituted economic sanctions and a boycott of goods against Britain.[60]
 While denying its authority over internal American affairs, a faction led by James Duane and future Loyalist Joseph Galloway insisted Congress recognize Parliament's right to regulate colonial trade.[60][w] Expecting concessions by the North administration, Congress authorized the colonial legislatures to enforce the boycott; this succeeded in reducing British imports by 97% from 1774 to 1775.[61] However, on February 9 Parliament declared Massachusetts to be in rebellion and instituted a blockade of the colony.[62] In July, the Restraining Acts limited colonial trade with the British West Indies and Britain and barred New England ships from the Newfoundland cod fisheries. The tension led to a scramble for control of militia stores, which each assembly was legally obliged to maintain for defense.[63] On April 19, a British attempt to secure the Concord arsenal culminated in the Battles of Lexington and Concord, which began the Revolutionary War.[64]
 After the Patriot victory at Concord, moderates in Congress led by John Dickinson drafted the Olive Branch Petition, offering to accept royal authority in return for George III mediating in the dispute.[65] However, since the petition was immediately followed by the Declaration of the Causes and Necessity of Taking Up Arms, Colonial Secretary Lord Dartmouth viewed the offer as insincere and refused to present the petition to the king.[66] Although constitutionally correct, since the monarch could not oppose his own government, it disappointed those Americans who hoped he would mediate in the dispute, while the hostility of his language annoyed even Loyalist members of Congress.[65] Combined with the Proclamation of Rebellion, issued on August 23 in response to the Battle at Bunker Hill, it ended hopes of a peaceful settlement.[67]
 Backed by the Whigs, Parliament initially rejected the imposition of coercive measures by 170 votes, fearing an aggressive policy would drive the Americans towards independence.[68] However, by the end of 1774 the collapse of British authority meant both Lord North and George III were convinced war was inevitable.[69] After Boston, Gage halted operations and awaited reinforcements; the Irish Parliament approved the recruitment of new regiments, while allowing Catholics to enlist for the first time.[70] Britain also signed a series of treaties with German states to supply additional troops.[71] Within a year, it had an army of over 32,000 men in America, the largest ever sent outside Europe at the time.[72] The employment of German soldiers against people viewed as British citizens was opposed by many in Parliament and by the colonial assemblies; combined with the lack of activity by Gage, opposition to the use of foreign troops allowed the Patriots to take control of the legislatures.[73]
 Support for independence was boosted by Thomas Paine's pamphlet Common Sense, which was published on January 10, 1776, and argued for American self-government and was widely reprinted.[74] To draft the Declaration of Independence, the Second Continental Congress appointed the Committee of Five: Thomas Jefferson, John Adams, Benjamin Franklin, Roger Sherman, and Robert Livingston.[75] The declaration was written almost exclusively by Jefferson.[76]
 Identifying inhabitants of the Thirteen Colonies as ""one people"", the declaration simultaneously dissolved political links with Britain, while including a long list of alleged violations of ""English rights"" committed by George III. This is also one of the first times that the colonies were referred to as ""United States"", rather than the more common United Colonies.[77]
 On July 2, Congress voted for independence and published the declaration on July 4.[78] At this point, the revolution ceased to be an internal dispute over trade and tax policies and had evolved into a civil war, since each state represented in Congress was engaged in a struggle with Britain, but also split between American Patriots and American Loyalists.[79] Patriots generally supported independence from Britain and a new national union in Congress, while Loyalists remained faithful to British rule. Estimates of numbers vary, one suggestion being the population as a whole was split evenly between committed Patriots, committed Loyalists, and those who were indifferent.[80] Others calculate the split as 40% Patriot, 40% neutral, 20% Loyalist, but with considerable regional variations.[81]
 At the onset of the war, the Second Continental Congress realized defeating Britain required foreign alliances and intelligence-gathering. The Committee of Secret Correspondence was formed for ""the sole purpose of corresponding with our friends in Great Britain and other parts of the world"". From 1775 to 1776, the committee shared information and built alliances through secret correspondence, as well as employing secret agents in Europe to gather intelligence, conduct undercover operations, analyze foreign publications, and initiate Patriot propaganda campaigns.[82] Paine served as secretary, while Benjamin Franklin and Silas Deane, sent to France to recruit military engineers,[83] were instrumental in securing French aid in Paris.[84]
 
 On April 14, 1775, Sir Thomas Gage, Commander-in-Chief, North America and Governor of Massachusetts, received orders to take action against the Patriots. He decided to destroy militia ordnance stored at Concord, Massachusetts, and capture John Hancock and Samuel Adams, who were considered the principal instigators of the rebellion. The operation was to begin around midnight on April 19, in the hope of completing it before the American Patriots could respond.[85][86] However, Paul Revere learned of the plan and notified Captain Parker, commander of the Concord militia, who prepared to resist.[87] The first action of the war, commonly referred to as the shot heard round the world, was a brief skirmish at Lexington, followed by the full-scale Battles of Lexington and Concord. British troops suffered around 300 casualties before withdrawing to Boston, which was then besieged by the militia.[88]
 In May 1775, 4,500 British reinforcements arrived under Generals William Howe, John Burgoyne, and Sir Henry Clinton.[89] On June 17, they seized the Charlestown Peninsula at the Battle of Bunker Hill, a frontal assault in which they suffered over 1,000 casualties.[90] Dismayed at the costly attack which had gained them little,[91] Gage appealed to London for a larger army,[92] but instead was replaced as commander by Howe.[90]
 On June 14, 1775, Congress took control of Patriot forces outside Boston, and Congressional leader John Adams nominated Washington as commander-in-chief of the newly formed Continental Army.[93] On June 16, Hancock officially proclaimed him ""General and Commander in Chief of the army of the United Colonies.""[94] He assumed command on July 3, preferring to fortify Dorchester Heights outside Boston rather than assaulting it.[95] In early March 1776, Colonel Henry Knox arrived with heavy artillery acquired in the Capture of Fort Ticonderoga.[96] Under cover of darkness, on March 5, Washington placed these on Dorchester Heights,[97] from where they could fire on the town and British ships in Boston Harbor. Fearing another Bunker Hill, Howe evacuated the city on March 17 without further loss and sailed to Halifax, Nova Scotia, while Washington moved south to New York City.[98]
 Beginning in August 1775, American privateers raided towns in Nova Scotia, including Saint John, Charlottetown, and Yarmouth. In 1776, John Paul Jones and Jonathan Eddy attacked Canso and Fort Cumberland respectively. British officials in Quebec began negotiating with the Iroquois for their support,[99] while US envoys urged them to remain neutral.[100] Aware of Native American leanings toward the British and fearing an Anglo-Indian attack from Canada, Congress authorized a second invasion in April 1775.[101] After the defeat at the Battle of Quebec on December 31,[102] the Americans maintained a loose blockade of the city until they retreated on May 6, 1776.[103] A second defeat at Trois-Rivières on June 8 ended operations in Quebec.[104]
 British pursuit was initially blocked by American naval vessels on Lake Champlain until victory at Valcour Island on October 11 forced the Americans to withdraw to Fort Ticonderoga, while in December an uprising in Nova Scotia sponsored by Massachusetts was defeated at Fort Cumberland.[105] These failures impacted public support for the Patriot cause,[106] and aggressive anti-Loyalist policies in the New England colonies alienated the Canadians.[107]
 In Virginia, Dunmore's Proclamation on November 7, 1775, promised freedom to any slaves who fled their Patriot masters and agreed to fight for the Crown.[108] British forces were defeated at Great Bridge on December 9 and took refuge on British ships anchored near Norfolk. When the Third Virginia Convention refused to disband its militia or accept martial law, Lord Dunmore ordered the Burning of Norfolk on January 1, 1776.[109]
 The siege of Savage's Old Fields began on November 19 in South Carolina between Loyalist and Patriot militias,[110] and the Loyalists were subsequently driven out of the colony in the Snow Campaign.[111] Loyalists were recruited in North Carolina to reassert British rule in the South, but they were decisively defeated in the Battle of Moore's Creek Bridge.[112] A British expedition sent to reconquer South Carolina launched an attack on Charleston in the Battle of Sullivan's Island on June 28, 1776,[113] but it failed.[114]
 A shortage of gunpowder led Congress to authorize a naval expedition against the Bahamas to secure ordnance stored there.[115] On March 3, 1776, an American squadron under the command of Esek Hopkins landed at the east end of Nassau and encountered minimal resistance at Fort Montagu. Hopkins' troops then marched on Fort Nassau. Hopkins had promised governor Montfort Browne and the civilian inhabitants that their lives and property would not be in any danger if they offered no resistance; they complied. Hopkins captured large stores of powder and other munitions that was so great he had to impress an extra ship in the harbor to transport the supplies back home, when he departed on March 17.[116] A month later, after a brief skirmish with HMS Glasgow, they returned to New London, Connecticut, the base for American naval operations.[117]
 After regrouping at Halifax in Nova Scotia,[118] Howe set sail for New York in June 1776 and began landing troops on Staten Island near the entrance to New York Harbor on July 2. The Americans rejected Howe's informal attempt to negotiate peace on July 30;[119] Washington knew that an attack on the city was imminent and realized that he needed advance information to deal with disciplined British regular troops.
 On August 12, 1776, Patriot Thomas Knowlton was ordered to form an elite group for reconnaissance and secret missions. Knowlton's Rangers, which included Nathan Hale, became the Army's first intelligence unit.[120][x] When Washington was driven off Long Island, he soon realized that he would need to professionalize military intelligence. With aid from Benjamin Tallmadge, Washington launched the six-man Culper spy ring.[123][y] The efforts of Washington and the Culper Spy Ring substantially increased the effective allocation and deployment of Continental regiments in the field.[123] Throughout the war, Washington spent more than 10 percent of his total military funds on military intelligence.[124]
 Washington split the Continental Army into positions on Manhattan and across the East River in western Long Island.[125] On August 27 at the Battle of Long Island, Howe outflanked Washington and forced him back to Brooklyn Heights, but he did not attempt to encircle Washington's forces.[126] Through the night of August 28, Knox bombarded the British. Knowing they were up against overwhelming odds, Washington ordered the assembly of a war council on August 29; all agreed to retreat to Manhattan. Washington quickly had his troops assembled and ferried them across the East River to Manhattan on flat-bottomed freight boats without any losses in men or ordnance, leaving General Thomas Mifflin's regiments as a rearguard.[127]
 Howe met with a delegation from the Second Continental Congress at the September Staten Island Peace Conference, but it failed to conclude peace, largely because the British delegates only had the authority to offer pardons and could not recognize independence.[128] On September 15, Howe seized control of New York City when the British landed at Kip's Bay and unsuccessfully engaged the Americans at the Battle of Harlem Heights the following day.[129] On October 18, Howe failed to encircle the Americans at the Battle of Pell's Point, and the Americans withdrew. Howe declined to close with Washington's army on October 28 at the Battle of White Plains and instead attacked a hill that was of no strategic value.[130]
 Washington's retreat isolated his remaining forces and the British captured Fort Washington on November 16. The British victory there amounted to Washington's most disastrous defeat with the loss of 3,000 prisoners.[131] The remaining American regiments on Long Island fell back four days later.[132] General Henry Clinton wanted to pursue Washington's disorganized army, but he was first required to commit 6,000 troops to capture Newport, Rhode Island, to secure the Loyalist port.[133][z] General Charles Cornwallis pursued Washington, but Howe ordered him to halt.[135]
 The outlook following the defeat at Fort Washington appeared bleak for the American cause. The reduced Continental Army had dwindled to fewer than 5,000 men and was reduced further when enlistments expired at the end of the year.[136] Popular support wavered, and morale declined. On December 20, 1776, the Continental Congress abandoned the revolutionary capital of Philadelphia and moved to Baltimore, where it remained until February 27, 1777.[137] Loyalist activity surged in the wake of the American defeat, especially in New York state.[138]
 In London, news of the victorious Long Island campaign was well received with festivities held in the capital. Public support reached a peak.[139] Strategic deficiencies among Patriot forces were evident: Washington divided a numerically weaker army in the face of a stronger one, his inexperienced staff misread the military situation, and American troops fled in the face of enemy fire. The successes led to predictions that the British could win within a year.[140] The British established winter quarters in the New York City area and anticipated renewed campaigning the following spring.[141]
 On the night of December 25–26, 1776, Washington crossed the Delaware River, leading a column of Continental Army troops from today's Bucks County, Pennsylvania, to today's Mercer County, New Jersey, in a logistically challenging and dangerous operation.
 Meanwhile, the Hessians were involved in numerous clashes with small bands of Patriots and were often aroused by false alarms at night in the weeks before the actual Battle of Trenton. By Christmas they were tired, while a heavy snowstorm led their commander, Colonel Johann Rall, to assume no significant attack would occur.[142] At daybreak on the 26th, the American Patriots surprised and overwhelmed Rall and his troops, who lost over 20 killed including Rall,[143] while 900 prisoners, German cannons and supplies were captured.[144]
 The Battle of Trenton restored the American army's morale, reinvigorated the Patriot cause,[145] and dispelled their fear of what they regarded as Hessian ""mercenaries"".[146] A British attempt to retake Trenton was repulsed at Assunpink Creek on January 2;[147] during the night, Washington outmaneuvered Cornwallis, then defeated his rearguard in the Battle of Princeton the following day. The two victories helped convince the French that the Americans were worthy military allies.[148]
 After his success at Princeton, Washington entered winter quarters at Morristown, New Jersey, where he remained until May[149] and received Congressional direction to inoculate all Patriot troops against smallpox.[150][aa] With the exception of a minor skirmishing between the two armies which continued until March,[152] Howe made no attempt to attack the Americans.[153]
 The 1776 campaign demonstrated that regaining New England would be a prolonged affair, which led to a change in British strategy to isolating the north by taking control of the Hudson River, allowing them to focus on the south where Loyalist support was believed to be substantial.[154] In December 1776, Howe wrote to the Colonial Secretary Lord Germain, proposing a limited offensive against Philadelphia, while a second force moved down the Hudson from Canada.[155] Burgoyne supplied several alternatives, all of which gave him responsibility for the offensive, with Howe remaining on the defensive. The option selected required him to lead the main force south from Montreal down the Hudson Valley, while a detachment under Barry St. Leger moved east from Lake Ontario. The two would meet at Albany, leaving Howe to decide whether to join them.[156] Reasonable in principle, this did not account for the logistical difficulties involved and Burgoyne erroneously assumed Howe would remain on the defensive; Germain's failure to make this clear meant he opted to attack Philadelphia instead.[157]
 With a mixed force of British regulars, professional German soldiers and Canadian militia Burgoyne set out on June 14, 1777 and captured Fort Ticonderoga on July 5. As General Horatio Gates retreated, his troops blocked roads, destroyed bridges, dammed streams, and stripped the area of food.[158] This slowed Burgoyne's progress and forced him to send out large foraging expeditions; one of more than 700 British troops were captured at the Battle of Bennington on August 16.[159] St Leger moved east and besieged Fort Stanwix; despite defeating an American relief force at the Battle of Oriskany on August 6, Burgoyne was abandoned by his Indian allies and withdrew to Quebec on August 22.[160] Now isolated and outnumbered by Gates, Burgoyne continued onto Albany rather than retreating to Fort Ticonderoga, reaching Saratoga on September 13. He asked Clinton for support while constructing defenses around the town.[161]
 Morale among his troops rapidly declined, and an unsuccessful attempt to break past Gates at the Battle of Freeman Farms on September 19 resulted in 600 British casualties.[162] When Clinton advised he could not reach them, Burgoyne's subordinates advised retreat; a reconnaissance in force on October 7 was repulsed by Gates at the Battle of Bemis Heights, forcing them back into Saratoga with heavy losses. By October 11, all hope of British escape had vanished; persistent rain reduced the camp to a ""squalid hell"" and supplies were dangerously low.[163] Burgoyne capitulated on October 17; around 6,222 soldiers, including German forces commanded by General Friedrich Adolf Riedesel, surrendered their arms before being taken to Boston, where they were to be transported to England.[164]
 After securing additional supplies, Howe made another attempt on Philadelphia by landing his troops in Chesapeake Bay on August 24.[165] He now compounded failure to support Burgoyne by missing repeated opportunities to destroy his opponent: despite defeating Washington at the Battle of Brandywine on September 11, he then allowed him to withdraw in good order.[166] After dispersing an American detachment at Paoli on September 20, Cornwallis occupied Philadelphia on September 26, with the main force of 9,000 under Howe based just to the north at Germantown.[167] Washington attacked them on October 4, but was repulsed.[168]
 To prevent Howe's forces in Philadelphia being resupplied by sea, the Patriots erected Fort Mifflin and nearby Fort Mercer on the east and west banks of the Delaware respectively, and placed obstacles in the river south of the city. This was supported by a small flotilla of Continental Navy ships on the Delaware, supplemented by the Pennsylvania State Navy, commanded by John Hazelwood. An attempt by the Royal Navy to take the forts in the October 20 to 22 Battle of Red Bank failed;[169][170] a second attack captured Fort Mifflin on November 16, while Fort Mercer was abandoned two days later when Cornwallis breached the walls.[171] His supply lines secured, Howe tried to tempt Washington into giving battle, but after inconclusive skirmishing at the Battle of White Marsh from December 5 to 8, he withdrew to Philadelphia for the winter.[172]
 On December 19, the Americans followed suit and entered winter quarters at Valley Forge. As Washington's domestic opponents contrasted his lack of battlefield success with Gates' victory at Saratoga,[173] foreign observers such as Frederick the Great were equally impressed with Washington's command at Germantown, which demonstrated resilience and determination.[174] Over the winter, poor conditions, supply problems and low morale resulted in 2,000 deaths, with another 3,000 unfit for duty due to lack of shoes.[175] However, Baron Friedrich Wilhelm von Steuben took the opportunity to introduce Prussian Army drill and infantry tactics to ""model companies"" in each Continental Army regiment, who then instructed their home units.[176] Despite Valley Forge being only twenty miles away, Howe made no effort to attack their camp, an action some critics argue could have ended the war.[177]
 Like his predecessors, French foreign minister Vergennes considered the 1763 Peace a national humiliation and viewed the war as an opportunity to weaken Britain. He initially avoided open conflict, but allowed American ships to take on cargoes in French ports, a technical violation of neutrality.[178] Vergennes persuaded Louis XVI to secretly fund a government front company to purchase munitions for the Patriots, carried in neutral Dutch ships and imported through Sint Eustatius in the Caribbean.[179]
 Many Americans opposed a French alliance, fearing to ""exchange one tyranny for another"", but this changed after a series of military setbacks in early 1776. As France had nothing to gain from the colonies reconciling with Britain, Congress had three choices: making peace on British terms, continuing the struggle on their own, or proclaiming independence, guaranteed by France. Although the Declaration of Independence had wide public support, over 20% of Congressmen voted against an alliance with France.[180] Congress agreed to the treaty with reluctance and as the war moved in their favor increasingly lost interest in it.[181]
 Silas Deane was sent to Paris to begin negotiations with Vergennes, whose key objectives were replacing Britain as the United States' primary commercial and military partner while securing the French West Indies from American expansion.[182] These islands were extremely valuable; in 1772, the value of sugar and coffee produced by Saint-Domingue on its own exceeded that of all American exports combined.[183] Talks progressed slowly until October 1777, when British defeat at Saratoga and their apparent willingness to negotiate peace convinced Vergennes only a permanent alliance could prevent the ""disaster"" of Anglo-American rapprochement. Assurances of formal French support allowed Congress to reject the Carlisle Peace Commission and insist on nothing short of complete independence.[184]
 On February 6, 1778, France and the United States signed the Treaty of Amity and Commerce regulating trade between the two countries, followed by a defensive military alliance against Britain, the Treaty of Alliance. In return for French guarantees of American independence, Congress undertook to defend their interests in the West Indies, while both sides agreed not to make a separate peace; conflict over these provisions would lead to the 1798 to 1800 Quasi-War.[181] Charles III of Spain was invited to join on the same terms but refused, largely due to concerns over the impact of the Revolution on Spanish colonies in the Americas. Spain had complained on multiple occasions about encroachment by American settlers into Louisiana, a problem that could only get worse once the United States replaced Britain.[185]
 Although Spain ultimately made important contributions to American success, in the Treaty of Aranjuez, Charles agreed only to support France's war with Britain outside America, in return for help in recovering Gibraltar, Menorca and Spanish Florida.[186] The terms were confidential since several conflicted with American aims; for example, the French claimed exclusive control of the Newfoundland cod fisheries, a non-negotiable for colonies like Massachusetts.[187] One less well-known impact of this agreement was the abiding American distrust of 'foreign entanglements'; the U.S. would not sign another treaty with France until their NATO agreement of 1949.[181] This was because the US had agreed not to make peace without France, while Aranjuez committed France to keep fighting until Spain recovered Gibraltar, effectively making it a condition of U.S. independence without the knowledge of Congress.[188]
 To encourage French participation in the struggle for independence, the U.S. representative in Paris, Silas Deane promised promotion and command positions to any French officer who joined the Continental Army. Such as Gilbert du Motier, Marquis de Lafayette, whom Congress via Dean appointed a major general,[189][190] on July 31, 1777.[191]
 When the war started, Britain tried to borrow the Dutch-based Scots Brigade for service in America, but pro-Patriot sentiment led the States General to refuse.[192] Although the Republic was no longer a major power, prior to 1774 they still dominated the European carrying trade, and Dutch merchants made large profits shipping French-supplied munitions to the Patriots. This ended when Britain declared war in December 1780, a conflict that proved disastrous to the Dutch economy.[193]
 The British government failed to take into account the strength of the American merchant marine and support from European countries, which allowed the colonies to import munitions and continue trading with relative impunity. While well aware of this, the North administration delayed placing the Royal Navy on a war footing for cost reasons; this prevented the institution of an effective blockade.[194] Traditional British policy was to employ European land-based allies to divert the opposition; in 1778, they were diplomatically isolated and faced war on multiple fronts.[195]
 Meanwhile, George III had given up on subduing America while Britain had a European war to fight.[196] He did not welcome war with France, but he held the British victories over France in the Seven Years' War as a reason to believe in ultimate victory over France.[197] Britain subsequently changed its focus into the Caribbean theater,[198] and diverted major military resources away from America.[199]
 At the end of 1777, Howe resigned and was replaced by Sir Henry Clinton on May 24, 1778; with French entry into the war, he was ordered to consolidate his forces in New York.[199] On June 18, the British departed Philadelphia with the reinvigorated Americans in pursuit; the Battle of Monmouth on June 28 was inconclusive but boosted Patriot morale. That midnight, the newly installed Clinton continued his retreat to New York.[200] A French naval force under Admiral Charles Henri Hector d'Estaing was sent to assist Washington; deciding New York was too formidable a target, in August they launched a combined attack on Newport, with General John Sullivan commanding land forces.[201] The resulting Battle of Rhode Island was indecisive; badly damaged by a storm, the French withdrew to avoid risking their ships.[202] 
 Further activity was limited to British raids on Chestnut Neck and Little Egg Harbor in October.[203] In July 1779, the Americans captured British positions at Stony Point and Paulus Hook.[204] Clinton unsuccessfully tried to tempt Washington into a decisive engagement by sending General William Tryon to raid Connecticut.[205] In July, a large American naval operation, the Penobscot Expedition, attempted to retake Maine but was defeated.[206]
Persistent Iroquois raids in New York and Pennsylvania led to the punitive Sullivan Expedition from July to September 1779. Involving more than 4,000 patriot soldiers, the scorched earth campaign destroyed more than 40 Iroquois villages and 160,000 bushels (4,000 mts) of maize, leaving the Iroquois destitute and destroying the Iroquois confederacy as an independent power on the American frontier. However, 5,000 Iroquois fled to Canada, where, supplied and supported by the British, they continued their raids.[207][208][209]
 During the winter of 1779–1780, the Continental Army suffered greater hardships than at Valley Forge.[210] Morale was poor, public support fell away, the Continental dollar was virtually worthless, the army was plagued with supply problems, desertion was common, and mutinies occurred in the Pennsylvania Line and New Jersey Line regiments over the conditions.[211]
 In June 1780, Clinton sent 6,000 men under Wilhelm von Knyphausen to retake New Jersey, but they were halted by local militia at the Battle of Connecticut Farms; although the Americans withdrew, Knyphausen felt he was not strong enough to engage Washington's main force and retreated.[212] A second attempt two weeks later ended in a British defeat at the Battle of Springfield, effectively ending their ambitions in New Jersey.[213] In July, Washington appointed Benedict Arnold commander of West Point; his attempt to betray the fort to the British failed due to incompetent planning, and the plot was revealed when his British contact John André was captured and executed.[214] Arnold escaped to New York and switched sides, an action justified in a pamphlet addressed ""To the Inhabitants of America""; the Patriots condemned his betrayal, while he found himself almost as unpopular with the British.[215]
 The Southern Strategy was developed by Lord Germain, based on input from London-based Loyalists, including Joseph Galloway. They argued that it made no sense to fight the Patriots in the north where they were strongest, while the New England economy was reliant on trade with Britain. On the other hand, duties on tobacco made the South far more profitable for Britain, while local support meant securing it required small numbers of regular troops. Victory would leave a truncated United States facing British possessions in the south, Canada to the north, and Ohio on their western border; with the Atlantic seaboard controlled by the Royal Navy, Congress would be forced to agree to terms. However, assumptions about the level of Loyalist support proved wildly optimistic.[216]
 Germain ordered Augustine Prévost, the British commander in East Florida, to advance into Georgia in December 1778. Lieutenant-Colonel Archibald Campbell, an experienced officer, captured Savannah on December 29, 1778. He recruited a Loyalist militia of nearly 1,100, many of whom allegedly joined only after Campbell threatened to confiscate their property.[217] Poor motivation and training made them unreliable troops, as demonstrated in their defeat by Patriot militia at the Battle of Kettle Creek on February 14, 1779, although this was offset by British victory at Brier Creek on March 3.[218]
 In June 1779, Prévost launched an abortive assault on Charleston, before retreating to Savannah, an operation notorious for widespread looting by British troops that enraged both Loyalists and Patriots. In October, a joint French and American operation under d'Estaing and General Benjamin Lincoln failed to recapture Savannah.[219] Prévost was replaced by Lord Cornwallis, who assumed responsibility for Germain's strategy; he soon realized estimates of Loyalist support were considerably over-stated, and he needed far more regular forces.[220]
 Reinforced by Clinton, Cornwallis's troops captured Charleston in May 1780, inflicting the most serious Patriot defeat of the war; over 5,000 prisoners were taken and the Continental Army in the south effectively destroyed. On May 29, Lieutenant-Colonel Banastre Tarleton's mainly Loyalist force routed a Continental Army force nearly three times its size under Colonel Abraham Buford at the Battle of Waxhaws. The battle is controversial for allegations of a massacre, which were later used as a recruiting tool by the Patriots.[221]
 Clinton returned to New York, leaving Cornwallis to oversee the south; despite their success, the two men left barely on speaking terms.[222] The Southern strategy depended on local support, but this was undermined by a series of coercive measures. Previously, captured Patriots were sent home after swearing not to take up arms against the king; they were now required to fight their former comrades, while the confiscation of Patriot-owned plantations led formerly neutral ""grandees"" to side with them.[223] Skirmishes at Williamson's Plantation, Cedar Springs, Rocky Mount, and Hanging Rock signaled widespread resistance to the new oaths throughout South Carolina.[224]
 In July 1780, Congress appointed Gates commander in the south; he was defeated at the Battle of Camden on August 16, leaving Cornwallis free to enter North Carolina.[225] Despite battlefield success, the British could not control the countryside and Patriot attacks continued; before moving north, Cornwallis sent Loyalist militia under Major Patrick Ferguson to cover his left flank, leaving their forces too far apart to provide mutual support.[226] In early October, Ferguson was defeated at the Battle of Kings Mountain, dispersing organized Loyalist resistance in the region.[227] Despite this, Cornwallis continued into North Carolina hoping for Loyalist support, while Washington replaced Gates with General Nathanael Greene in December 1780.[228]
 Greene divided his army, leading his main force southeast pursued by Cornwallis; a detachment was sent southwest under Daniel Morgan, who defeated Tarleton's British Legion at Cowpens on January 17, 1781, nearly eliminating it as a fighting force.[229] The Patriots now held the initiative in the south, with the exception of a raid on Richmond led by Benedict Arnold in January 1781.[230] Greene led Cornwallis on a series of countermarches around North Carolina; by early March, the British were exhausted and short of supplies and Greene felt strong enough to fight the Battle of Guilford Court House on March 15. Although victorious, Cornwallis suffered heavy casualties and retreated to Wilmington, North Carolina, seeking supplies and reinforcements.[231]
 The Patriots now controlled most of the Carolinas and Georgia outside the coastal areas; after a minor reversal at the Battle of Hobkirk's Hill, they recaptured Fort Watson and Fort Motte on April 15.[232] On June 6, Brigadier General Andrew Pickens captured Augusta, leaving the British in Georgia confined to Charleston and Savannah.[233] The assumption Loyalists would do most of the fighting left the British short of troops and battlefield victories came at the cost of losses they could not replace. Despite halting Greene's advance at the Battle of Eutaw Springs on September 8, Cornwallis withdrew to Charleston with little to show for his campaign.[234]
 From the beginning of the war, Bernardo de Gálvez, the Governor of Spanish Louisiana, allowed the Americans to import supplies and munitions into New Orleans, then ship them to Pittsburgh.[235] This provided an alternative transportation route for the Continental Army, bypassing the British blockade of the Atlantic Coast.[236]
 In February 1778, an expedition of militia to destroy British military supplies in settlements along the Cuyahoga River was halted by adverse weather.[237] Later in the year, a second campaign was undertaken to seize the Illinois Country from the British. Virginia militia, Canadien settlers, and Indian allies commanded by Colonel George Rogers Clark captured Kaskaskia on July 4 and then secured Vincennes, though Vincennes was recaptured by Quebec Governor Henry Hamilton. In early 1779, the Virginians counter-attacked in the siege of Fort Vincennes and took Hamilton prisoner. Clark secured western British Quebec as the American Northwest Territory in the Treaty of Paris brought the Revolutionary War to an end.[238]
 When Spain joined France's war against Britain in the Anglo-French War in 1779, their treaty specifically excluded Spanish military action in North America. Later that year, however, Gálvez initiated offensive operations against British outposts.[239] First, he cleared British garrisons in Baton Rouge, Louisiana, Fort Bute, and Natchez, Mississippi, and captured five forts.[240] In doing so, Gálvez opened navigation on the Mississippi River north to the American settlement in Pittsburgh.[241]
 On May 25, 1780, British Colonel Henry Bird invaded Kentucky as part of a wider operation to clear American resistance from Quebec to the Gulf Coast. Their advance on New Orleans was repelled by Spanish Governor Gálvez's offensive on Mobile. Simultaneous British attacks were repulsed on St. Louis by the Spanish Lieutenant Governor de Leyba, and on the Virginia County courthouse in Cahokia, Illinois, by Lieutenant Colonel Clark. The British initiative under Bird from Detroit was ended at the rumored approach of Clark.[ab] The scale of violence in the Licking River Valley, was extreme ""even for frontier standards."" It led to English and German settlements, who joined Clark's militia when the British and their hired German soldiers withdrew to the Great Lakes.[242] The Americans responded with a major offensive along the Mad River in August which met with some success in the Battle of Piqua but did not end Indian raids.[243]
 French soldier Augustin de La Balme led a Canadian militia in an attempt to capture Detroit, but they dispersed when Miami natives led by Little Turtle attacked the encamped settlers on November 5.[244][ac] The war in the west stalemated with the British garrison sitting in Detroit and the Virginians expanding westward settlements north of the Ohio River in the face of British-allied Indian resistance.[246]
 In 1781, Galvez and Pollock campaigned east along the Gulf Coast to secure West Florida, including British-held Mobile and Pensacola.[247] The Spanish operations impaired the British supply of armaments to British Indian allies, which effectively suspended a military alliance to attack settlers between the Mississippi River and the Appalachian Mountains.[248][ad]
 In 1782, large scale retaliations between settlers and Native Americans in the region included the Gnadenhutten massacre and the Crawford expedition. The 1782 Battle of Blue Licks was one of the last major engagements of the war. News of the treaty between Great Britain and the United States arrived late that year. By this time, about 7% of Kentucky settlers had been killed in battles against Native Americans, contrasted with 1% of the population killed in the Thirteen Colonies. Lingering resentments led to continued fighting in the west after the war officially ended.
 Clinton spent most of 1781 based in New York City; he failed to construct a coherent operational strategy, partly due to his difficult relationship with Admiral Marriot Arbuthnot.[249] In Charleston, Cornwallis independently developed an aggressive plan for a campaign in Virginia, which he hoped would isolate Greene's army in the Carolinas and cause the collapse of Patriot resistance in the South. This strategy was approved by Lord Germain in London, but neither informed Clinton.[250]
 Washington and Rochambeau discussed their options: Washington wanted to attack the British in New York, and Rochambeau wanted to attack them in Virginia, where Cornwallis's forces were less established.[251] Washington eventually gave way, and Lafayette took a combined Franco-American force into Virginia.[252] Clinton misinterpreted his movements as preparations for an attack on New York and instructed Cornwallis to establish a fortified sea base, where the Royal Navy could evacuate British troops to help defend New York.[253]
 When Lafayette entered Virginia, Cornwallis complied with Clinton's orders and withdrew to Yorktown, where he constructed strong defenses and awaited evacuation.[254] An agreement by the Spanish Navy to defend the French West Indies allowed Admiral François Joseph Paul de Grasse to relocate to the Atlantic seaboard, a move Arbuthnot did not anticipate.[249] This provided Lafayette naval support, while the failure of previous combined operations at Newport and Savannah meant their coordination was planned more carefully.[255] Despite repeated urging from his subordinates, Cornwallis made no attempt to engage Lafayette before he could establish siege lines.[256] Expecting to be withdrawn within a few days, he also abandoned the outer defenses, which were promptly occupied by the besiegers and hastened British defeat.[257]
 On August 31, a Royal Navy fleet under Thomas Graves left New York for Yorktown.[258] After landing troops and munitions for the besiegers on August 30, de Grasse remained in Chesapeake Bay and intercepted him on September 5; although the Battle of the Chesapeake was indecisive in terms of losses, Graves was forced to retreat, leaving Cornwallis isolated.[259] An attempted breakout over York River at Gloucester Point failed due to bad weather.[260] Under heavy bombardment with dwindling supplies, on October 16 Cornwallis sent emissaries to General Washington to negotiate surrender; after twelve hours of negotiations, the terms of surrender were finalized the following day.[261] Responsibility for defeat was the subject of fierce public debate between Cornwallis, Clinton, and Germain. Clinton ultimately took most of the blame and spent the rest of his life in relative obscurity.[262]
 Subsequent to Yorktown, American forces were assigned to supervise the armistice between Washington and Clinton made to facilitate British departure following the January 1782 law of Parliament forbidding any further British offensive action in North America. British-American negotiations in Paris led to signed preliminary agreements in November 1782, which acknowledged U.S. independence. The enacted Congressional war objective, a British withdrawal from North America and cession of these regions to the U.S., was completed in stages in East Coast cities.[263]
 In the U.S. South, Generals Greene and Wayne observed the British remove their troops from Charleston on December 14, 1782.[264] Loyalist provincial militias of whites and free Blacks and Loyalists with slaves were transported to Nova Scotia and the British West Indies.[ae] Native American allies of the British and some freed Blacks were left to escape unaided through the American lines.
 On April 9, 1783, Washington issued orders that ""all acts of hostility"" were to cease immediately. That same day, by arrangement with Washington, Carleton issued a similar order to British troops.[265]  As directed by a Congressional resolution of May 26, 1783, all non-commissioned officers and enlisted were furloughed ""to their homes"" until the ""definitive treaty of peace"", when they would be automatically discharged. The U.S. armies were directly disbanded in the field as of Washington's General Orders on June 2, 1783.[266] Once the Treaty of Paris was signed with Britain on September 3, 1783, Washington resigned as commander-in-chief of the Continental Army.[263] The last British occupation of New York City ended on November 25, 1783, with the departure of Clinton's replacement, General Sir Guy Carleton.[267]
 To win their insurrection, Washington and the Continental Army needed to outlast the British will to fight. To restore British America, the British had to defeat the Continental Army quickly and compel the Second Continental Congress to retract its claim to self-governance.[269] Historian Terry M. Mays of The Citadel identifies three separate types of warfare during the Revolutionary War. The first was a colonial conflict in which objections to imperial trade regulation were as significant as taxation policy. The second was a civil war between American Patriots, American Loyalists, and those who preferred to remain neutral. Particularly in the south, many battles were fought between Patriots and Loyalists with no British involvement, leading to divisions that continued after independence was achieved.[270]
 The third element was a global war between France, Spain, the Dutch Republic, and Britain, with America serving as one of several different war theaters.[270] After entering the Revolutionary War in 1778, France provided the Americans money, weapons, soldiers, and naval assistance, while French troops fought under U.S. command in North America. While Spain did not formally join the war in America, they provided access to the Mississippi River and captured British possessions on the Gulf of Mexico that denied bases to the Royal Navy, retook Menorca and besieged Gibraltar in Europe.[271] Although the Dutch Republic was no longer a major power prior to 1774, they still dominated the European carrying trade, and Dutch merchants made large profits by shipping French-supplied munitions to the Patriots. This ended when Britain declared war in December 1780, and the conflict proved disastrous to the Dutch economy.[272]
 The Second Continental Congress stood to benefit if the Revolution evolved into a protracted war. Colonial state populations were largely prosperous and depended on local production for food and supplies rather than on imports from Britain. The thirteen colonies were spread across most of North American Atlantic seaboard, stretching 1,000 miles. Most colonial farms were remote from the seaports, and control of four or five major ports did not give Britain control over American inland areas. Each state had established internal distribution systems.[273] Motivation was also a major asset: each colonial capital had its own newspapers and printers, and the Patriots enjoyed more popular support than the Loyalists. Britain hoped that the Loyalists would do much of the fighting, but found that the Loyalists did not engage as significantly as they had hoped.[14]
 When the Revolutionary War began, the Second Continental Congress lacked a professional army or navy. However, each of the colonies had a long-established system of local militia, which were combat-tested in support of British regulars in the French and Indian War. The colonial state legislatures independently funded and controlled their local militias.[273] 
 Militiamen were lightly armed, had little training, and usually did not have uniforms. Their units served for only a few weeks or months at a time and lacked the training and discipline of more experienced soldiers. Local county militias were reluctant to travel far from home and were unavailable for extended operations.[274] To compensate for this, the Continental Congress established a regular force known as the Continental Army on June 14, 1775, which proved to be the origin of the modern United States Army, and appointed Washington as its commander-in-chief. However, it suffered significantly from the lack of an effective training program and from largely inexperienced officers.[275]
Each state legislature appointed officers for both county and state militias and their regimental Continental line officers; although Washington was required to accept Congressional appointments, he was permitted to choose and command his own generals, such as Greene; his chief of artillery, Knox; and Alexander Hamilton, the chief of staff.[276] One of Washington's most successful general officer recruits was Steuben, a veteran of the Prussian general staff who wrote the Revolutionary War Drill Manual.[275] The development of the Continental Army was always a work in progress and Washington used both his regulars and state militias throughout the war; when properly employed, the combination allowed them to overwhelm smaller British forces, as they did in battles at Concord, Boston, Bennington, and Saratoga. Both sides used partisan warfare, but the state militias effectively suppressed Loyalist activity when British regulars were not in the area.[274][af]
 Washington designed the overall military strategy in cooperation with Congress, established the principle of civilian supremacy in military affairs, personally recruited his senior officer corps, and kept the states focused on a common goal.[279] Washington initially employed the inexperienced officers and untrained troops in Fabian strategies rather than risk frontal assaults against Britain's professional forces.[280] Over the course of the war, Washington lost more battles than he won, but he never surrendered his troops and maintained a fighting force in the face of British field armies.[281]
 By prevailing European standards, the armies in America were relatively small, limited by lack of supplies and logistics. The British were constrained by the logistical difficulty of transporting troops across the Atlantic and their dependence on local supplies. Washington never directly commanded more than 17,000 men,[282] and the combined Franco-American army in the decisive American victory at Yorktown was only about 19,000.[283] At the beginning of 1776, Patriot forces consisted of 20,000 men, with two-thirds in the Continental Army and the other third in the state militias. About 250,000 American men served as regulars or as militia for the revolutionary cause during the war, but there were never more than 90,000 men under arms at any time.[284]
 On the whole, American officers never equaled their British opponents in tactics and maneuvers, and they lost most of the pitched battles. The great successes at Boston (1776), Saratoga (1777), and Yorktown (1781) were won by trapping the British far from base with a greater number of troops.[276] After 1778, Washington's army was transformed into a more disciplined and effective force, mostly as a product of Baron von Steuben's military training.[275] Immediately after the Continental Army emerged from Valley Forge in June 1778, it proved its ability to match the military capabilities of the British at the Battle of Monmouth, including a Black Rhode Island regiment fending off a British bayonet attack and then counter charging the British for the first time as part of Washington's army.[285] After the Battle of Monmouth, Washington came to realize that saving entire towns was not necessary, but preserving his army and keeping the revolutionary spirit alive was more important. Washington informed Henry Laurens, then president of the Second Continental Congress,[ag] ""that the possession of our towns, while we have an army in the field, will avail them little.""[287]
 Although the Continental Congress was responsible for the war effort and provided supplies to the troops, Washington took it upon himself to pressure Congress and the state legislatures to provide the essentials of war; there was never nearly enough.[288] Congress evolved in its committee oversight and established the Board of War, which included members of the military.[289] Because the Board of War was also a committee ensnared with its own internal procedures, Congress also created the post of Secretary of War, appointing Major General Benjamin Lincoln to the position in February 1781. Washington worked closely with Lincoln to coordinate civilian and military authorities and took charge of training and supplying the army.[290][275]
 During the first summer of the war, Washington began outfitting schooners and other small seagoing vessels to prey on ships supplying the British in Boston.[291] The Second Continental Congress established the Continental Navy on October 13, 1775, and appointed Esek Hopkins as its first commander;[292] for most of the war, the Continental Navy included only a handful of small frigates and sloops, supported by privateers.[293] On November 10, 1775, Congress authorized the creation of the Continental Marines, which ultimately evolved into the United States Marine Corps.[278]
 John Paul Jones became the first American naval hero when he captured HMS Drake on April 24, 1778, the first victory for any American military vessel in British waters.[294] The last such victory was by the frigate USS Alliance, commanded by Captain John Barry. On March 10, 1783, the Alliance outgunned HMS Sybil in a 45-minute duel while escorting Spanish gold from Havana to the Congress in Philadelphia.[295] After Yorktown, all US Navy ships were sold or given away; it was the first time in America's history that it had no fighting forces on the high seas.[296]
 Congress primarily commissioned privateers to reduce costs and to take advantage of the large proportion of colonial sailors found in the British Empire. In total, they included 1,700 ships that successfully captured 2,283 enemy ships to damage the British effort and to enrich themselves with the proceeds from the sale of cargo and the ship itself.[297][ah] About 55,000 sailors served aboard American privateers during the war.[16]
 At the beginning of the war, the Americans had no major international allies, since most nation-states waited to see how the conflict unfolded. Over time, the Continental Army established its military credibility. Battles such as the Battle of Bennington, the Battles of Saratoga, and even defeats such as the Battle of Germantown, proved decisive in gaining the support of powerful European nations, including France, Spain, and the Dutch Republic; the Dutch moved from covertly supplying the Americans with weapons and supplies to overtly supporting them.[299]
 The decisive American victory at Saratoga convinced France, which was already a long-time rival of Britain, to offer the Americans the Treaty of Amity and Commerce. The two nations also agreed to a defensive Treaty of Alliance to protect their trade and also guaranteed American independence from Britain. To engage the United States as a French ally militarily, the treaty was conditioned on Britain initiating a war on France to stop it from trading with the U.S. Spain and the Dutch Republic were invited to join by both France and the United States in the treaty, but neither was responsive to the request.[300]
 On June 13, 1778, France declared war on Great Britain, and it invoked the French military alliance with the U.S., which ensured additional U.S. private support for French possessions in the Caribbean.[ai] Washington worked closely with the soldiers and navy that France would send to America, primarily through Lafayette on his staff. French assistance made critical contributions required to defeat Cornwallis at Yorktown in 1781.[303][aj]
 The British military had considerable experience fighting in North America.[305] However, in previous conflicts they benefited from local logistics and support from the colonial militia. In the American Revolutionary War, reinforcements had to come from Europe, and maintaining large armies over such distances was extremely complex; ships could take three months to cross the Atlantic, and orders from London were often outdated by the time they arrived.[306]
 Prior to the conflict, the colonies were largely autonomous economic and political entities, with no centralized area of ultimate strategic importance.[307] This meant that, unlike Europe where the fall of a capital city often ended wars, that in America continued even after the loss of major settlements such as Philadelphia, the seat of Congress, New York, and Charleston.[308] British power was reliant on the Royal Navy, whose dominance allowed them to resupply their own expeditionary forces while preventing access to enemy ports. However, the majority of the American population was agrarian, rather than urban; supported by the French navy and blockade runners based in the Dutch Caribbean, their economy was able to survive.[309]
Lord North, Prime Minister since 1770, delegated control of the war in North America to Lord George Germain and the Earl of Sandwich, who was head of the Royal Navy from 1771 to 1782. Defeat at Saratoga in 1777 made it clear the revolt would not be easily suppressed, especially after the Franco-American alliance of February 1778. With Spain also expected to join the conflict, the Royal Navy needed to prioritize either the war in America or in Europe; Germain advocated the former, Sandwich the latter.[310]
 North initially backed the Southern strategy attempting to exploit divisions between the mercantile north and slave-owning south, but after the defeat of Yorktown, he was forced to accept that this policy had failed.[311] It was clear the war was lost, although the Royal Navy forced the French to relocate their fleet to the Caribbean in November 1781 and resumed a close blockade of American trade.[312] The resulting economic damage and rising inflation meant the US was now eager to end the war, while France was unable to provide further loans; Congress could no longer pay its soldiers.[313]
The geographical size of the colonies and limited manpower meant the British could not simultaneously conduct military operations and occupy territory without local support. Debate persists over whether their defeat was inevitable; one British statesman described it as ""like trying to conquer a map"".[314] While Ferling argues Patriot victory was nothing short of a miracle,[315] Ellis suggests the odds always favored the Americans, especially after Howe squandered the chance of a decisive British success in 1776, an ""opportunity that would never come again"".[316] The US military history speculates the additional commitment of 10,000 fresh troops in 1780 would have placed British victory ""within the realm of possibility"".[317]
 The expulsion of France from North America in 1763 led to a drastic reduction in British troop levels in the colonies; in 1775, there were only 8,500 regular soldiers among a civilian population of 2.8 million.[318] The bulk of military resources in the Americas were focused on defending sugar islands in the Caribbean; Jamaica alone generated more revenue than all thirteen American colonies combined.[319] With the end of the Seven Years' War, the permanent army in Britain was also cut back, which resulted in administrative difficulties when the war began a decade later.[320]
 Over the course of the war, there were four separate British commanders-in-chief. The first was Thomas Gage, appointed in 1763, whose initial focus was establishing British rule in former French areas of Canada. Many in London blamed the revolt on his failure to take firm action earlier, and he was relieved after the heavy losses incurred at the Battle of Bunker Hill.[321] His replacement was Sir William Howe, a member of the Whig faction in Parliament who opposed the policy of coercion advocated by Lord North; Cornwallis, who later surrendered at Yorktown, was one of many senior officers who initially refused to serve in North America.[322]
 The 1775 campaign showed the British overestimated the capabilities of their own troops and underestimated the colonial militia, requiring a reassessment of tactics and strategy,[323] and allowing the Patriots to take the initiative.[324] Howe's responsibility is still debated; despite receiving large numbers of reinforcements, Bunker Hill seems to have permanently affected his self-confidence and lack of tactical flexibility meant he often failed to follow up opportunities.[325] Many of his decisions were attributed to supply problems, such as his failure to pursue Washington's beaten army.[326] Having lost the confidence of his subordinates, he was recalled after Burgoyne surrendered at Saratoga.[327]
 Following the failure of the Carlisle Commission, British policy changed from treating the Patriots as subjects who needed to be reconciled to enemies who had to be defeated.[328] In 1778, Howe was replaced by Sir Henry Clinton.[329] Regarded as an expert on tactics and strategy,[327] like his predecessors Clinton was handicapped by chronic supply issues.[330] In addition, Clinton's strategy was compromised by conflict with political superiors in London and his colleagues in North America, especially Admiral Mariot Arbuthnot, replaced in early 1781 by Rodney.[249] He was neither notified nor consulted when Germain approved Cornwallis's invasion of the south in 1781 and delayed sending him reinforcements believing the bulk of Washington's army was still outside New York City.[331] After the surrender at Yorktown, Clinton was relieved by Carleton, whose major task was to oversee the evacuation of Loyalists and British troops from Savannah, Charleston, and New York City.[332]
 During the 18th century, states commonly hired foreign soldiers, including Britain.[333] When it became clear additional troops were needed to suppress the revolt in America, it was decided to employ professional German soldiers. There were several reasons for this, including public sympathy for the Patriot cause, a historical reluctance to expand the British army and the time needed to recruit and train new regiments.[334] Many smaller states in the Holy Roman Empire had a long tradition of renting their armies to the highest bidder. The most important was Hesse-Kassel, known as ""the Mercenary State"".[335]
 The first supply agreements were signed by the North administration in late 1775; 30,000 Germans served in the American War.[336] Often generically referred to as ""Hessians"", they included men from many other states, including Hanover and Brunswick.[337] Sir Henry Clinton recommended recruiting Russian troops whom he rated very highly, having seen them in action against the Ottomans; however, negotiations with Catherine the Great made little progress.[338]
 Unlike previous wars their use led to intense political debate in Britain, France, and even Germany, where Frederick the Great refused to provide passage through his territories for troops hired for the American war.[339] In March 1776, the agreements were challenged in Parliament by Whigs who objected to ""coercion"" in general, and the use of foreign soldiers to subdue ""British subjects"".[340] The debates were covered in detail by American newspapers; in May 1776 they received copies of the treaties themselves, provided by British sympathizers and smuggled into North America from London.[341]
 The prospect of foreign German soldiers being used in the colonies bolstered support for independence, more so than taxation and other acts combined; the King was accused of declaring war on his own subjects, leading to the idea there were now two separate governments.[342][343] By apparently showing Britain was determined to go to war, it made hopes of reconciliation seem naive and hopeless, while the employment of what was regarded as ""foreign mercenaries"" became one of the charges levelled against George III in the Declaration of Independence.[339] The Hessian reputation within Germany for brutality also increased support for the Patriot cause among German American immigrants.[344]
 The presence of over 150,000 German Americans meant both sides felt the German soldiers might be persuaded to desert; one reason Clinton suggested employing Russians was that he felt they were less likely to defect. When the first German troops arrived on Staten Island in August 1776, Congress approved the printing of handbills, promising land and citizenship to any willing to join the Patriot cause. The British launched a counter-campaign claiming deserters could be executed.[345] Desertion among the Germans occurred throughout the war, with the highest rate of desertion occurring between the surrender at Yorktown and the Treaty of Paris.[346] German regiments were central to the British war effort; of the estimated 30,000 sent to America, some 13,000 became casualties.[347]
 Wealthy Loyalists convinced the British government that most of the colonists were sympathetic toward the Crown;[348] consequently, British military planners relied on recruiting Loyalists, but had trouble recruiting sufficient numbers as the Patriots had widespread support.[274][ak] Approximately 25,000 Loyalists fought for the British throughout the war.[31] Although Loyalists constituted about twenty percent of the colonial population,[81] they were concentrated in distinct communities. Many of them lived among large plantation owners in the Tidewater region and South Carolina.[81]
 When the British began probing the backcountry in 1777–1778, they were faced with a major problem: any significant level of organized Loyalist activity required a continued presence of British regulars.[349] The available manpower that the British had in America was insufficient to protect Loyalist territory and counter American offensives.[350] The Loyalist militias in the South were constantly defeated by neighboring Patriot militia. The Patriot victory at the Battle of Kings Mountain irreversibly impaired Loyalist militia capability in the South.[231]
 When the early war policy was administered by Howe, the Crown's need to maintain Loyalist support prevented it from using the traditional revolt suppression methods.[351] The British cause suffered when their troops ransacked local homes during an aborted attack on Charleston in 1779 that enraged both Patriots and Loyalists.[219] After Congress rejected the Carlisle Peace Commission in 1778 and Westminster turned to ""hard war"" during Clinton's command, neutral colonists in the Carolinas often allied with the Patriots.[352] Conversely, Loyalists gained support when Patriots intimidated suspected Tories by destroying property or tarring and feathering.[353]
 A Loyalist militia unit—the British Legion—provided some of the best troops in British service.[354] It was commanded by Tarleton and gained a fearsome reputation in the colonies for ""brutality and needless slaughter"".[355][better source needed]
 Women played various roles during the Revolutionary War; they often accompanied their husbands when permitted. For example, throughout the war Martha Washington was known to visit and provide aid to her husband George at various American camps.[356] Women often accompanied armies as camp followers to sell goods and perform necessary tasks in hospitals and camps, and numbered in the thousands during the war.[357]
 Women also assumed military roles: some dressed as men to directly support combat, fight, or act as spies on both sides.[358] Anna Maria Lane joined her husband in the Army. The Virginia General Assembly later cited her bravery: she fought while dressed as a man and ""performed extraordinary military services, and received a severe wound at the battle of Germantown ... with the courage of a soldier"".[359] On April 26, 1777, Sybil Ludington is said to have ridden to alert militia forces to the British's approach; she has been called the ""female Paul Revere"".[360] Whether the ride occurred is questioned.[361][362][363][364] A few others disguised themselves as men. Deborah Sampson fought until her gender was discovered and she was discharged as a result; Sally St. Clair was killed in action.[359]
 When war began, the population of the Thirteen Colonies included an estimated 500,000 slaves, predominantly used as labor on Southern plantations.[365] In November 1775, Lord Dunmore, the royal governor of Virginia, issued a proclamation that promised freedom to any Patriot-owned slaves willing to bear arms. Although the announcement helped to fill a temporary manpower shortage, white Loyalist prejudice meant recruits were eventually redirected to non-combatant roles. The Loyalists' motive was to deprive Patriot planters of labor rather than to end slavery; Loyalist-owned slaves were returned.[366]
 The 1779 Philipsburg Proclamation issued by Clinton extended the offer of freedom to Patriot-owned slaves throughout the colonies. It persuaded entire families to escape to British lines, many of which were employed growing food for the army by removing the requirement for military service. While Clinton organized the Black Pioneers, he also ensured fugitive slaves were returned to Loyalist owners with orders that they were not to be punished.[367] As the war progressed, service as regular soldiers in British units became increasingly common; Black Loyalists formed two regiments of the Charleston garrison in 1783.[368]
 Estimates of the numbers who served the British during the war vary from 25,000 to 50,000, excluding those who escaped during wartime. Thomas Jefferson estimated that Virginia may have lost 30,000 slaves to escapes.[369] In South Carolina, nearly 25,000 slaves (about 30 percent of the enslaved population) either fled, migrated, or died, which significantly disrupted the plantation economies both during and after the war.[370]
 Black Patriots were barred from the Continental Army until Washington convinced Congress in January 1778 that there was no other way to replace losses from disease and desertion. The 1st Rhode Island Regiment formed in February included former slaves whose owners were compensated; however, only 140 of its 225 soldiers were Black and recruitment stopped in June 1788.[371] Ultimately, around 5,000 African Americans served in the Continental Army and Navy in a variety of roles, while another 4,000 were employed in Patriot militia units, aboard privateers, or as teamsters, servants, and spies. After the war, a small minority received land grants or Congressional pensions; many others were returned to their masters post-war despite earlier promises of freedom.[372]
 As a Patriot victory became increasingly likely, the treatment of Black Loyalists became a point of contention; after the surrender of Yorktown in 1781, Washington insisted all escapees be returned but Cornwallis refused. In 1782 and 1783, around 8,000 to 10,000 freed Blacks were evacuated by the British from Charleston, Savannah, and New York; some moved onto London, while 3,000 to 4,000 settled in Nova Scotia.[373] White Loyalists transported 15,000 enslaved Blacks to Jamaica and the Bahamas. The free Black Loyalists who migrated to the British West Indies included regular soldiers from Dunmore's Ethiopian Regiment, and those from Charleston who helped garrison the Leeward Islands.[368]
 Most Native Americans east of the Mississippi River were affected by the war, and many tribes were divided over how to respond. A few tribes were friendly with the colonists, but most Natives opposed the union of the Colonies as a potential threat to their territory. Approximately 13,000 Natives fought on the British side, with the largest group coming from the Iroquois tribes who deployed around 1,500 men.[33]
 Early in July 1776, Cherokee allies of Britain attacked the short-lived Washington District of North Carolina. Their defeat splintered both Cherokee settlements and people, and was directly responsible for the rise of the Chickamauga Cherokee, who perpetuated the Cherokee–American wars against American settlers for decades after hostilities with Britain ended.[374]
 Muscogee and Seminole allies of Britain fought against Americans in Georgia and South Carolina. In 1778, a force of 800 Muscogee destroyed American settlements along the Broad River in Georgia. Muscogee warriors also joined Thomas Brown's raids into South Carolina and assisted Britain during the siege of Savannah.[375] Many Native Americans were involved in the fight between Britain and Spain on the Gulf Coast and along the British side of the Mississippi River. Thousands of Muscogee, Chickasaw, and Choctaw fought in major battles such as the Battle of Fort Charlotte, the Battle of Mobile, and the siege of Pensacola.[376]
 The Iroquois Confederacy was shattered as a result of the American Revolutionary War. The Seneca, Onondaga, and Cayuga tribes sided with the British; members of the Mohawks fought on both sides; and many Tuscarora and Oneida sided with the Americans. To retaliate against raids on American settlement by Loyalists and their Indian allies, the Continental Army dispatched the Sullivan Expedition throughout New York to debilitate the Iroquois tribes that had sided with the British. Mohawk leaders Joseph Louis Cook and Joseph Brant sided with the Americans and the British respectively, which further exacerbated the split.[377]
 In the western theater, conflicts between settlers and Native Americans led to lingering distrust.[378] In the 1783 Treaty of Paris, Great Britain ceded control of the disputed lands between the Great Lakes and the Ohio River, but Native inhabitants were not a part of the peace negotiations.[379] Tribes in the Northwest Territory joined as the Western Confederacy and allied with the British to resist American settlement, and their conflict continued after the Revolutionary War as the Northwest Indian War.[380]
 The terms presented by the Carlisle Peace Commission in 1778 included acceptance of the principle of self-government. Parliament would recognize Congress as the governing body, suspend any objectionable legislation, surrender its right to local colonial taxation, and discuss including American representatives in the House of Commons. In return, all property confiscated from Loyalists would be returned, British debts honored, and locally enforced martial law accepted. However, Congress demanded either immediate recognition of independence or the withdrawal of all British troops; they knew the commission were not authorized to accept these, bringing negotiations to a rapid end.[382]
 On February 27, 1782, a Whig motion to end the offensive war in America was carried by 19 votes.[383] North resigned, obliging the king to invite Lord Rockingham to form a government; a consistent supporter of the Patriot cause, he made a commitment to U.S. independence a condition of doing so. George III reluctantly accepted and the new government took office on March 27, 1782; however, Rockingham died unexpectedly on July 1, and was replaced by Lord Shelburne who acknowledged American independence.[384]
 When Lord Rockingham was elevated to Prime Minister, Congress consolidated its diplomatic consuls in Europe into a peace delegation at Paris. The dean of the delegation was Benjamin Franklin. He had become a celebrity in the French Court, but he was also influential in the courts of Prussia and Austria. Since the 1760s, Franklin had been an organizer of British American inter-colony cooperation, and then served as a colonial lobbyist to Parliament in London. John Adams had been consul to the Dutch Republic and was a prominent early New England Patriot. John Jay of New York had been consul to Spain and was a past president of the Continental Congress. As consul to the Dutch Republic, Henry Laurens had secured a preliminary agreement for a trade agreement. Although active in the preliminaries, he was not a signer of the conclusive treaty.[263]
 The Whig negotiators included long-time friend of Franklin David Hartley, and Richard Oswald, who had negotiated Laurens' release from the Tower of London.[263] The Preliminary Peace signed on November 30 met four key Congressional demands: independence, territory up to the Mississippi, navigation rights into the Gulf of Mexico, and fishing rights in Newfoundland.[263]
 British strategy was to strengthen the U.S. sufficiently to prevent France from regaining a foothold in North America, and they had little interest in these proposals.[385] However, divisions between their opponents allowed them to negotiate separately with each to improve their overall position, starting with the American delegation in September 1782.[386] The French and Spanish sought to improve their position by creating the U.S. dependent on them for support against Britain, thus reversing the losses of 1763.[387] Both parties tried to negotiate a settlement with Britain excluding the Americans; France proposed setting the western boundary of the U.S. along the Appalachians, matching the British 1763 Proclamation Line. The Spanish suggested additional concessions in the vital Mississippi River Basin, but required the cession of Georgia in violation of the Franco-American alliance.[387]
 Facing difficulties with Spain over claims involving the Mississippi River, and from France who was still reluctant to agree to American independence until all her demands were met, John Jay told the British that he was willing to negotiate directly with them, cutting off France and Spain, and Prime Minister Lord Shelburne, in charge of the British negotiations, agreed.[388] Key agreements for the United States in obtaining peace included recognition of US independence; all of the territory east of the Mississippi River, north of Florida and south of Canada; and fishing rights in the Grand Banks, off the coast of Newfoundland and in the Gulf of Saint Lawrence. The United States and Great Britain were each given perpetual access to the Mississippi River.[389][390]
 An Anglo-American Preliminary Peace was formally entered into in November 1782, and Congress endorsed the settlement on April 15, 1783. It announced the achievement of peace with independence, and the conclusive treaty was signed on September 2, 1783, in Paris, effective the following day when Britain signed its treaty with France. John Adams, who helped draft the treaty, claimed it represented ""one of the most important political events that ever happened on the globe"". Ratified respectively by Congress and Parliament, the final versions were exchanged in Paris the following spring.[391] On November 25, the last British troops remaining in the U.S. were evacuated from New York to Halifax.[392]
 The expanse of territory that was now the U.S. included millions of sparsely settled acres south of the Great Lakes line between the Appalachian Mountains and the Mississippi River, much of which was part of Canada. The tentative colonial migration west became a flood during the war.[393]
 Britain's extended post-war policy for the U.S. continued to try to establish an Indian barrier state below the Great Lakes as late as 1814 during the War of 1812. The formally acquired western American lands continued to be populated by Indigenous tribes that had mostly been British allies.[379] In practice the British refused to abandon the forts on territory they formally transferred. Instead, they provisioned military allies for continuing frontier raids and sponsored the Northwest Indian War (1785–1795). British sponsorship of local warfare on the U.S. continued until the Anglo-American Jay Treaty, authored by Hamilton, went into effect on February 29, 1796.[394][al]
 Of the European powers with American colonies adjacent to the newly created U.S., Spain was most threatened by American independence, and it was correspondingly the most hostile to it.[am] Its territory adjacent to the U.S. was relatively undefended, so Spanish policy developed a combination of initiatives. Spanish soft power diplomatically challenged the British territorial cession west to the Mississippi River and the previous northern boundaries of Spanish Florida.[396] It imposed a high tariff on American goods, then blocked American settler access to the port of New Orleans. At the same time, the Spanish also sponsored war within the U.S. by Indian proxies in its Southwest Territory ceded by France to Britain, then Britain to the Americans.[393]
 The total loss of life throughout the conflict is largely unknown. As was typical in wars of the era, diseases such as smallpox claimed more lives than battle. Between 1775 and 1782, a smallpox epidemic throughout North America killed an estimated 130,000.[41][an] Historian Joseph Ellis suggests that Washington having his troops inoculated against the disease was one of his most important decisions.[397]
 Up to 70,000 American Patriots died during active military service.[398] Of these, approximately 6,800 were killed in battle, while at least 17,000 died from disease. The majority of the latter died while prisoners of war of the British, mostly in the prison ships in New York Harbor.[399][ao] The number of Patriots seriously wounded or disabled by the war has been estimated from 8,500 to 25,000.[400]
 The French suffered 2,112 killed in combat in the United States.[401][ap] The Spanish lost 124 killed and 247 wounded in West Florida.[402][aq]
 A British report in 1781 puts their total Army deaths at 6,046 in North America (1775–1779).[41][ar] Approximately 7,774 Germans died in British service in addition to 4,888 deserters; among those labeled German deserters, however, it is estimated that 1,800 were killed in combat.[13][as]
 The American Revolution set an example to overthrow both monarchy and colonial governments. The United States has the world's oldest written constitution, which was used as a model in other countries, sometimes word-for-word. The Revolution inspired revolutions in France, Haiti, Latin America, and elsewhere.[410]
 Although the Revolution eliminated many forms of inequality, it did little to change the status of women, despite the role they played in winning independence. Most significantly, it failed to end slavery. While many were uneasy over the contradiction of demanding liberty for some, yet denying it to others, the dependence of southern states on slave labor made abolition too great a challenge. Between 1774 and 1780, many of the states banned the importation of slaves, but the institution itself continued.[411] In 1782, Virginia passed a law permitting manumission and over the next eight years more than 10,000 slaves were given their freedom.[412] The number of abolitionist movements greatly increased, and by 1804 all the northern states had outlawed it.[413] However,  slavery continued to be a serious social and political issue and caused divisions that would ultimately end in civil war.
 The body of historical writings on the American Revolution cite many motivations for the Patriot revolt.[414] American Patriots stressed the denial of their constitutional rights as Englishmen, especially ""no taxation without representation."" Contemporaries credit the American Enlightenment with laying the intellectual, moral, and ethical foundations for the American Revolution among the Founding Fathers, who were influenced by the classical liberalism of John Locke and other Enlightenment writers and philosophers.
 Two Treatises of Government has long been cited as a major influence on Revolutionary-era American thinking, but historians David Lundberg and Henry F. May contend that Locke's Essay Concerning Human Understanding was far more widely read.[415] Historians since the 1960s have emphasized that the Patriot constitutional argument was made possible by the emergence of an American nationalism that united the Thirteen Colonies. In turn, that nationalism was rooted in a Republican value system that demanded consent of the governed and deeply opposed aristocratic control.[416] In Britain, on the other hand, republicanism was largely a fringe ideology since it challenged the aristocratic control of the British monarchy and political system. Political power was not controlled by an aristocracy or nobility in the 13 colonies; instead, the colonial political system was based on the winners of free elections, which were open at the time to the majority of white men. In analysis of the Revolution, historians in recent decades have often cited three motivations behind it:[417]
 After the first U.S. postage stamp was issued in 1849, the U.S. Postal Service frequently issued commemorative stamps celebrating people and events of the Revolutionary War. The first such stamp was the Liberty Bell issue of 1926.[424]
"
British Empire,https://en.wikipedia.org/wiki/British_Empire,"


 The British Empire comprised the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states. It began with the overseas possessions and trading posts established by England in the late 16th and early 17th centuries. At its height in the 19th and early 20th centuries, it was the largest empire in history and, for a century, was the foremost global power.[1] By 1913, the British Empire held sway over 412 million people, 23 percent of the world population at the time,[2] and by 1920, it covered 35.5 million km2 (13.7 million sq mi),[3] 24 per cent of the Earth's total land area. As a result, its constitutional, legal, linguistic, and cultural legacy is widespread. At the peak of its power, it was described as ""the empire on which the sun never sets"", as the sun was always shining on at least one of its territories.[4]
 During the Age of Discovery in the 15th and 16th centuries, Portugal and Spain pioneered European exploration of the globe, and in the process established large overseas empires. Envious of the great wealth these empires generated,[5] England, France, and the Netherlands began to establish colonies and trade networks of their own in the Americas and Asia. A series of wars in the 17th and 18th centuries with the Netherlands and France left Britain the dominant colonial power in North America. Britain became a major power in the Indian subcontinent after the East India Company's conquest of Mughal Bengal at the Battle of Plassey in 1757.
 The American War of Independence resulted in Britain losing some of its oldest and most populous colonies in North America by 1783. While retaining control of British North America (now Canada) and territories in and near the Caribbean in the British West Indies, British colonial expansion turned towards Asia, Africa, and the Pacific. After the defeat of France in the Napoleonic Wars (1803–1815), Britain emerged as the principal naval and imperial power of the 19th century and expanded its imperial holdings. It pursued trade concessions in China and Japan, and territory in Southeast Asia. The ""Great Game"" and ""Scramble for Africa"" also ensued. The period of relative peace (1815–1914) during which the British Empire became the global hegemon was later described as Pax Britannica (Latin for ""British Peace""). Alongside the formal control that Britain exerted over its colonies, its dominance of much of world trade, and of its oceans, meant that it effectively controlled the economies of, and readily enforced its interests in, many regions, such as Asia and Latin America.[6] It also came to dominate the Middle East. Increasing degrees of autonomy were granted to its white settler colonies, some of which were formally reclassified as Dominions by the 1920s. By the start of the 20th century, Germany and the United States had begun to challenge Britain's economic lead. Military, economic and colonial tensions between Britain and Germany were major causes of the First World War, during which Britain relied heavily on its empire. The conflict placed enormous strain on its military, financial, and manpower resources. Although the empire achieved its largest territorial extent immediately after the First World War, Britain was no longer the world's preeminent industrial or military power.
 In the Second World War, Britain's colonies in East Asia and Southeast Asia were occupied by the Empire of Japan. Despite the final victory of Britain and its allies, the damage to British prestige and the British economy helped accelerate the decline of the empire. India, Britain's most valuable and populous possession, achieved independence in 1947 as part of a larger decolonisation movement, in which Britain granted independence to most territories of the empire. The Suez Crisis of 1956 confirmed Britain's decline as a global power, and the handover of Hong Kong to China on 1 July 1997 symbolised for many the end of the British Empire,[7] though fourteen overseas territories that are remnants of the empire remain under British sovereignty. After independence, many former British colonies, along with most of the dominions, joined the Commonwealth of Nations, a free association of independent states. Fifteen of these, including the United Kingdom, retain the same person as monarch, currently King Charles III.
 The foundations of the British Empire were laid when England and Scotland were separate kingdoms. In 1496, King Henry VII of England, following the successes of Spain and Portugal in overseas exploration, commissioned John Cabot to lead an expedition to discover a northwest passage to Asia via the North Atlantic.[8] Cabot sailed in 1497, five years after the first voyage of Christopher Columbus, and made landfall on the coast of Newfoundland. He believed he had reached Asia,[9] and there was no attempt to found a colony. Cabot led another voyage to the Americas the following year but did not return; it is unknown what happened to his ships.[10]
 No further attempts to establish English colonies in the Americas were made until well into the reign of Queen Elizabeth I, during the last decades of the 16th century.[11] In the meantime, Henry VIII's 1533 Statute in Restraint of Appeals had declared ""that this realm of England is an Empire"".[12] The Protestant Reformation turned England and Catholic Spain into implacable enemies.[8] In 1562, Elizabeth I encouraged the privateers John Hawkins and Francis Drake to engage in slave-raiding attacks against Spanish and Portuguese ships off the coast of West Africa[13] with the aim of establishing an Atlantic slave trade. This effort was rebuffed and later, as the Anglo-Spanish Wars intensified, Elizabeth I gave her blessing to further privateering raids against Spanish ports in the Americas and shipping that was returning across the Atlantic, laden with treasure from the New World.[14] At the same time, influential writers such as Richard Hakluyt and John Dee (who was the first to use the term ""British Empire"")[15] were beginning to press for the establishment of England's own empire. By this time, Spain had become the dominant power in the Americas and was exploring the Pacific Ocean, Portugal had established trading posts and forts from the coasts of Africa and Brazil to China, and France had begun to settle the Saint Lawrence River area, later to become New France.[16]
 Although England tended to trail behind Portugal, Spain, and France in establishing overseas colonies, it carried out its first modern colonisation, referred to as the Munster Plantations, in 16th century Ireland by settling it with English and Welsh Protestant settlers. England had already colonised part of the country following the Norman invasion of Ireland in 1169.[17] Several people who helped establish the Munster plantations later played a part in the early colonisation of North America, particularly a group known as the West Country Men.[18]
 In 1578, Elizabeth I granted a patent to Humphrey Gilbert for discovery and overseas exploration.[19] That year, Gilbert sailed for the Caribbean with the intention of engaging in piracy and establishing a colony in North America, but the expedition was aborted before it had crossed the Atlantic.[20] In 1583, he embarked on a second attempt. On this occasion, he formally claimed the harbour of the island of Newfoundland, although no settlers were left behind. Gilbert did not survive the return journey to England and was succeeded by his half-brother, Walter Raleigh, who was granted his own patent by Elizabeth in 1584. Later that year, Raleigh founded the Roanoke Colony on the coast of present-day North Carolina, but lack of supplies caused the colony to fail.[21]
 In 1603, James VI of Scotland ascended (as James I) to the English throne and in 1604 negotiated the Treaty of London, ending hostilities with Spain. Now at peace with its main rival, English attention shifted from preying on other nations' colonial infrastructures to the business of establishing its own overseas colonies.[22] The British Empire began to take shape during the early 17th century, with the English settlement of North America and the smaller islands of the Caribbean, and the establishment of joint-stock companies, most notably the East India Company, to administer colonies and overseas trade. This period, until the loss of the Thirteen Colonies after the American War of Independence towards the end of the 18th century, has been referred to by some historians as the ""First British Empire"".[23]
 England's early efforts at colonisation in the Americas met with mixed success. An attempt to establish a colony in Guiana in 1604 lasted only two years and failed in its main objective to find gold deposits.[24] Colonies on the Caribbean islands of St Lucia (1605) and Grenada (1609) rapidly folded.[25] The first permanent English settlement in the Americas was founded in 1607 in Jamestown by Captain John Smith, and managed by the Virginia Company; the Crown took direct control of the venture in 1624, thereby founding the Colony of Virginia.[26] Bermuda was settled and claimed by England as a result of the 1609 shipwreck of the Virginia Company's flagship,[27] while attempts to settle Newfoundland were largely unsuccessful.[28] In 1620, Plymouth was founded as a haven by Puritan religious separatists, later known as the Pilgrims.[29] Fleeing from religious persecution would become the motive for many English would-be colonists to risk the arduous trans-Atlantic voyage: Maryland was established by English Roman Catholics (1634), Rhode Island (1636) as a colony tolerant of all religions and Connecticut (1639) for Congregationalists. England's North American holdings were further expanded by the annexation of the Dutch colony of New Netherland in 1664, following the capture of New Amsterdam, which was renamed New York.[30] Although less financially successful than colonies in the Caribbean, these territories had large areas of good agricultural land and attracted far greater numbers of English emigrants, who preferred their temperate climates.[31]
 The British West Indies initially provided England's most important and lucrative colonies.[32] Settlements were successfully established in St. Kitts (1624), Barbados (1627) and Nevis (1628),[25] but struggled until the ""Sugar Revolution"" transformed the Caribbean economy in the mid-17th century.[33] Large sugarcane plantations were first established in the 1640s on Barbados, with assistance from Dutch merchants and Sephardic Jews fleeing Portuguese Brazil. At first, sugar was grown primarily using white indentured labour, but rising costs soon led English traders to embrace the use of imported African slaves.[34] The enormous wealth generated by slave-produced sugar made Barbados the most successful colony in the Americas,[35] and one of the most densely populated places in the world.[33] This boom led to the spread of sugar cultivation across the Caribbean, financed the development of non-plantation colonies in North America, and accelerated the growth of the Atlantic slave trade, particularly the triangular trade of slaves, sugar and provisions between Africa, the West Indies and Europe.[36]
 To ensure that the increasingly healthy profits of colonial trade remained in English hands, Parliament decreed in 1651 that only English ships would be able to ply their trade in English colonies. This led to hostilities with the United Dutch Provinces—a series of Anglo-Dutch Wars—which would eventually strengthen England's position in the Americas at the expense of the Dutch.[37] In 1655, England annexed the island of Jamaica from the Spanish, and in 1666 succeeded in colonising the Bahamas.[38]
In 1670, Charles II incorporated by royal charter the Hudson's Bay Company (HBC), granting it a monopoly on the fur trade in the area known as Rupert's Land, which would later form a large proportion of the Dominion of Canada. Forts and trading posts established by the HBC were frequently the subject of attacks by the French, who had established their own fur trading colony in adjacent New France.[39]
 Two years later, the Royal African Company was granted a monopoly on the supply of slaves to the British colonies in the Caribbean.[40] The company would transport more slaves across the Atlantic than any other, and significantly grew England's share of the trade, from 33 per cent in 1673 to 74 per cent in 1683.[41] The removal of this monopoly between 1688 and 1712 allowed independent British slave traders to thrive, leading to a rapid escalation in the number of slaves transported.[42] British ships carried a third of all slaves shipped across the Atlantic—approximately 3.5 million Africans[43]—until the abolition of the trade by Parliament in 1807 (see § Abolition of slavery).[44] To facilitate the shipment of slaves, forts were established on the coast of West Africa, such as James Island, Accra and Bunce Island. In the British Caribbean, the percentage of the population of African descent rose from 25 per cent in 1650 to around 80 per cent in 1780, and in the Thirteen Colonies from 10 per cent to 40 per cent over the same period (the majority in the southern colonies).[45] The transatlantic slave trade played a pervasive role in British economic life, and became a major economic mainstay for western port cities.[46] Ships registered in Bristol, Liverpool and London were responsible for the bulk of British slave trading.[47] For the transported, harsh and unhygienic conditions on the slaving ships and poor diets meant that the average mortality rate during the Middle Passage was one in seven.[48]
 At the end of the 16th century, England and the Dutch Empire began to challenge the Portuguese Empire's monopoly of trade with Asia, forming private joint-stock companies to finance the voyages—the English, later British, East India Company and the Dutch East India Company, chartered in 1600 and 1602 respectively. The primary aim of these companies was to tap into the lucrative spice trade, an effort focused mainly on two regions: the East Indies archipelago, and an important hub in the trade network, India. There, they competed for trade supremacy with Portugal and with each other.[49] Although England eclipsed the Netherlands as a colonial power, in the short term the Netherlands' more advanced financial system[50] and the three Anglo-Dutch Wars of the 17th century left it with a stronger position in Asia. Hostilities ceased after the Glorious Revolution of 1688 when the Dutch William of Orange ascended the English throne, bringing peace between the Dutch Republic and England. A deal between the two nations left the spice trade of the East Indies archipelago to the Netherlands and the textiles industry of India to England, but textiles soon overtook spices in terms of profitability.[50]
 Peace between England and the Netherlands in 1688 meant the two countries entered the Nine Years' War as allies, but the conflict—waged in Europe and overseas between France, Spain and the Anglo-Dutch alliance—left the English a stronger colonial power than the Dutch, who were forced to devote a larger proportion of their military budget to the costly land war in Europe.[51] The death of Charles II of Spain in 1700 and his bequeathal of Spain and its colonial empire to Philip V of Spain, a grandson of the King of France, raised the prospect of the unification of France, Spain and their respective colonies, an unacceptable state of affairs for England and the other powers of Europe.[52] In 1701, England, Portugal and the Netherlands sided with the Holy Roman Empire against Spain and France in the War of the Spanish Succession, which lasted for thirteen years.[52]
 In 1695, the Parliament of Scotland granted a charter to the Company of Scotland, which established a settlement in 1698 on the Isthmus of Panama. Besieged by neighbouring Spanish colonists of New Granada, and affected by malaria, the colony was abandoned two years later. The Darien scheme was a financial disaster for Scotland: a quarter of Scottish capital was lost in the enterprise.[53] The episode had major political consequences, helping to persuade the government of the Kingdom of Scotland of the merits of turning the personal union with England into a political and economic one under the Kingdom of Great Britain established by the Acts of Union 1707.[54]
 The 18th century saw the newly united Great Britain rise to be the world's dominant colonial power, with France becoming its main rival on the imperial stage.[55] Great Britain, Portugal, the Netherlands, and the Holy Roman Empire continued the War of the Spanish Succession, which lasted until 1714 and was concluded by the Treaty of Utrecht. Philip V of Spain renounced his and his descendants' claim to the French throne, and Spain lost its empire in Europe.[52] The British Empire was territorially enlarged: from France, Britain gained Newfoundland and Acadia, and from Spain, Gibraltar and Menorca. Gibraltar became a critical naval base and allowed Britain to control the Atlantic entry and exit point to the Mediterranean. Spain ceded the rights to the lucrative asiento (permission to sell African slaves in Spanish America) to Britain.[56] With the outbreak of the Anglo-Spanish War of Jenkins' Ear in 1739, Spanish privateers attacked British merchant shipping along the Triangle Trade routes. In 1746, the Spanish and British began peace talks, with the King of Spain agreeing to stop all attacks on British shipping; however, in the 1750 Treaty of Madrid Britain lost its slave-trading rights in Latin America.[57]
 In the East Indies, British and Dutch merchants continued to compete in spices and textiles. With textiles becoming the larger trade, by 1720, in terms of sales, the British company had overtaken the Dutch.[50] During the middle decades of the 18th century, there were several outbreaks of military conflict on the Indian subcontinent, as the English East India Company and its French counterpart, struggled alongside local rulers to fill the vacuum that had been left by the decline of the Mughal Empire. The Battle of Plassey in 1757, in which the British defeated the Nawab of Bengal and his French allies, left the British East India Company in control of Bengal and as a major military and political power in India.[58] France was left control of its enclaves but with military restrictions and an obligation to support British client states, ending French hopes of controlling India.[59] In the following decades the British East India Company gradually increased the size of the territories under its control, either ruling directly or via local rulers under the threat of force from the Presidency Armies, the vast majority of which was composed of Indian sepoys, led by British officers.[60] The British and French struggles in India became but one theatre of the global Seven Years' War (1756–1763) involving France, Britain, and the other major European powers.[39]
 The signing of the Treaty of Paris of 1763 had important consequences for the future of the British Empire. In North America, France's future as a colonial power effectively ended with the recognition of British claims to Rupert's Land,[39] and the ceding of New France to Britain (leaving a sizeable French-speaking population under British control) and Louisiana to Spain. Spain ceded Florida to Britain. Along with its victory over France in India, the Seven Years' War therefore left Britain as the world's most powerful maritime power.[61]
 During the 1760s and early 1770s, relations between the Thirteen Colonies and Britain became increasingly strained, primarily because of resentment of the British Parliament's attempts to govern and tax American colonists without their consent.[62] This was summarised at the time by the colonists' slogan ""No taxation without representation"", a perceived violation of the guaranteed Rights of Englishmen. The American Revolution began with a rejection of Parliamentary authority and moves towards self-government. In response, Britain sent troops to reimpose direct rule, leading to the outbreak of war in 1775. The following year, in 1776, the Second Continental Congress issued the Declaration of Independence proclaiming the colonies' sovereignty from the British Empire as the new United States of America. The entry of French and Spanish forces into the war tipped the military balance in the Americans' favour and after a decisive defeat at Yorktown in 1781, Britain began negotiating peace terms. American independence was acknowledged at the Peace of Paris in 1783.[63]
 The loss of such a large portion of British America, at the time Britain's most populous overseas possession, is seen by some historians as the event defining the transition between the first and second empires,[64] in which Britain shifted its attention away from the Americas to Asia, the Pacific and later Africa.[65]  Adam Smith's Wealth of Nations, published in 1776, had argued that colonies were redundant, and that free trade should replace the old mercantilist policies that had characterised the first period of colonial expansion, dating back to the protectionism of Spain and Portugal.[66] The growth of trade between the newly independent United States and Britain after 1783 seemed to confirm Smith's view that political control was not necessary for economic success.[67]
 The war to the south influenced British policy in Canada, where between 40,000 and 100,000[68] defeated Loyalists had migrated from the new United States following independence.[69] The 14,000 Loyalists who went to the Saint John and Saint Croix river valleys, then part of Nova Scotia, felt too far removed from the provincial government in Halifax, so London split off New Brunswick as a separate colony in 1784.[70] The Constitutional Act 1791 created the provinces of Upper Canada (mainly English speaking) and Lower Canada (mainly French-speaking) to defuse tensions between the French and British communities, and implemented governmental systems similar to those employed in Britain, with the intention of asserting imperial authority and not allowing the sort of popular control of government that was perceived to have led to the American Revolution.[71]
 Tensions between Britain and the United States escalated again during the Napoleonic Wars, as Britain tried to cut off American trade with France and boarded American ships to impress men into the Royal Navy. The United States Congress declared war, the War of 1812, and invaded Canadian territory. In response, Britain invaded the US, but the pre-war boundaries were reaffirmed by the 1814 Treaty of Ghent, ensuring Canada's future would be separate from that of the United States.[72]
 Since 1718, transportation to the American colonies had been a penalty for various offences in Britain, with approximately one thousand convicts transported per year.[73] Forced to find an alternative location after the loss of the Thirteen Colonies in 1783, the British government looked for an alternative, eventually turning to Australia.[74] On his first of three voyages commissioned by the government, James Cook reached New Zealand in October 1769. He was the first European to circumnavigate and map the country.[75] From the late 18th century, the country was regularly visited by explorers and other sailors, missionaries, traders and adventurers but no attempt was made to settle the country or establish possession. The coast of Australia had been discovered for Europeans by the Dutch in 1606,[76] but there was no attempt to colonise it. In 1770, after leaving New Zealand, James Cook charted the eastern coast, claimed the continent for Britain, and named it New South Wales.[77] In 1778, Joseph Banks, Cook's botanist on the voyage, presented evidence to the government on the suitability of Botany Bay for the establishment of a penal settlement, and in 1787 the first shipment of convicts set sail, arriving in 1788.[78] Unusually, Australia was claimed through proclamation. Indigenous Australians were considered too uncivilised to require treaties,[79] and colonisation brought disease and violence that together with the deliberate dispossession of land and culture were devastating to these peoples.[80] Britain continued to transport convicts to New South Wales until 1840, to Tasmania until 1853 and to Western Australia until 1868.[81] The Australian colonies became profitable exporters of wool and gold,[82] mainly because of the Victorian gold rush, making its capital Melbourne for a time the richest city in the world.[83]
 The British also expanded their mercantile interests in the North Pacific. Spain and Britain had become rivals in the area, culminating in the Nootka Crisis in 1789. Both sides mobilised for war, but when France refused to support Spain it was forced to back down, leading to the Nootka Convention. The outcome was a humiliation for Spain, which practically renounced all sovereignty on the North Pacific coast.[84] This opened the way to British expansion in the area, and a number of expeditions took place; firstly a naval expedition led by George Vancouver which explored the inlets around the Pacific North West, particularly around Vancouver Island.[85] On land, expeditions sought to discover a river route to the Pacific for the extension of the North American fur trade. Alexander Mackenzie of the North West Company led the first, starting out in 1792, and a year later he became the first European to reach the Pacific overland north of the Rio Grande, reaching the ocean near present-day Bella Coola. This preceded the Lewis and Clark Expedition by twelve years. Shortly thereafter, Mackenzie's companion, John Finlay, founded the first permanent European settlement in British Columbia, Fort St. John. The North West Company sought further exploration and backed expeditions by David Thompson, starting in 1797, and later by Simon Fraser. These pushed into the wilderness territories of the Rocky Mountains and Interior Plateau to the Strait of Georgia on the Pacific Coast, expanding British North America westward.[86]
 The East India Company fought a series of Anglo-Mysore wars in Southern India with the Sultanate of Mysore under Hyder Ali and then Tipu Sultan. Defeats in the First Anglo-Mysore war and stalemate in the Second were followed by victories in the Third and the Fourth.[87] Following Tipu Sultan's death in the fourth war in the Siege of Seringapatam (1799), the kingdom became a protectorate of the company.[87]
 The East India Company fought three Anglo-Maratha Wars with the Maratha Confederacy. The First Anglo-Maratha War ended in 1782 with a restoration of the pre-war status quo.[88] The Second and Third Anglo-Maratha wars resulted in British victories.[89] After the surrender of Peshwa Bajirao II on 1818, the East India Company acquired control of a large majority of the Indian subcontinent.[90]
 Britain was challenged again by France under Napoleon, in a struggle that, unlike previous wars, represented a contest of ideologies between the two nations.[91] It was not only Britain's position on the world stage that was at risk: Napoleon threatened to invade Britain itself, just as his armies had overrun many countries of continental Europe.[92]
 The Napoleonic Wars were therefore ones in which Britain invested large amounts of capital and resources to win. French ports were blockaded by the Royal Navy, which won a decisive victory over a French Imperial Navy-Spanish Navy fleet at the Battle of Trafalgar in 1805. Overseas colonies were attacked and occupied, including those of the Netherlands, which was annexed by Napoleon in 1810. France was finally defeated by a coalition of European armies in 1815.[93] Britain was again the beneficiary of peace treaties: France ceded the Ionian Islands, Malta (which it had occupied in 1798), Mauritius, St Lucia, the Seychelles, and Tobago; Spain ceded Trinidad; the Netherlands ceded Guiana, Ceylon and the Cape Colony, while the Danish ceded Heligoland. Britain returned Guadeloupe, Martinique, French Guiana, and Réunion to France; Menorca to Spain; Danish West Indies to Denmark and Java and Suriname to the Netherlands.[94]
 With the advent of the Industrial Revolution, goods produced by slavery became less important to the British economy.[95] Added to this was the cost of suppressing regular slave rebellions. With support from the British abolitionist movement, Parliament enacted the Slave Trade Act in 1807, which abolished the slave trade in the empire. In 1808, Sierra Leone Colony was designated an official British colony for freed slaves.[96] Parliamentary reform in 1832 saw the influence of the West India Committee decline. The Slavery Abolition Act, passed the following year, abolished slavery in the British Empire on 1 August 1834, finally bringing the empire into line with the law in the UK (with the exception of the territories administered by the East India Company and Ceylon, where slavery was ended in 1844). Under the Act, slaves were granted full emancipation after a period of four to six years of ""apprenticeship"".[97] Facing further opposition from abolitionists, the apprenticeship system was abolished in 1838.[98] The British government compensated slave-owners.[99][100]
 Between 1815 and 1914, a period referred to as Britain's ""imperial century"" by some historians,[101] around 10 million sq mi (26 million km2) of territory and roughly 400 million people were added to the British Empire.[102] Victory over Napoleon left Britain without any serious international rival, other than Russia in Central Asia.[103] Unchallenged at sea, Britain adopted the role of global policeman, a state of affairs later known as the Pax Britannica,[104] and a foreign policy of ""splendid isolation"".[105] Alongside the formal control it exerted over its own colonies, Britain's dominant position in world trade meant that it effectively controlled the economies of many countries, such as China, Argentina and Siam, which has been described by some historians as an ""Informal Empire"".[6]
 British imperial strength was underpinned by the steamship and the telegraph, new technologies invented in the second half of the 19th century, allowing it to control and defend the empire. By 1902, the British Empire was linked together by a network of telegraph cables, called the All Red Line.[106]
 The East India Company drove the expansion of the British Empire in Asia. The company's army had first joined forces with the Royal Navy during the Seven Years' War, and the two continued to co-operate in arenas outside India: the eviction of the French from Egypt (1799),[107] the capture of Java from the Netherlands (1811), the acquisition of Penang Island (1786), Singapore (1819) and Malacca (1824), and the defeat of Burma (1826).[103]
 From its base in India, the company had been engaged in an increasingly profitable opium export trade to Qing China since the 1730s. This trade, illegal since it was outlawed by China in 1729, helped reverse the trade imbalances resulting from the British imports of tea, which saw large outflows of silver from Britain to China.[108] In 1839, the confiscation by the Chinese authorities at Canton of 20,000 chests of opium led Britain to attack China in the First Opium War, and resulted in the seizure by Britain of Hong Kong Island, at that time a minor settlement, and other treaty ports including Shanghai.[109]
 During the late 18th and early 19th centuries, the British Crown began to assume an increasingly large role in the affairs of the company. A series of acts of Parliament were passed, including the Regulating Act 1773, East India Company Act 1784 and the Charter Act 1813 which regulated the company's affairs and established the sovereignty of the Crown over the territories that it had acquired.[110] The company's eventual end was precipitated by the Indian Rebellion in 1857, a conflict that had begun with the mutiny of sepoys, Indian troops under British officers and discipline.[111] The rebellion took six months to suppress, with heavy loss of life on both sides. The following year the British government dissolved the company and assumed direct control over India through the Government of India Act 1858, establishing the British Raj, where an appointed governor-general administered India and Queen Victoria was crowned the Empress of India.[112] India became the empire's most valuable possession, ""the Jewel in the Crown"", and was the most important source of Britain's strength.[113]
 A series of serious crop failures in the late 19th century led to widespread famines on the subcontinent in which it is estimated that over 15 million people died. The East India Company had failed to implement any coordinated policy to deal with the famines during its period of rule. Later, under direct British rule, commissions were set up after each famine to investigate the causes and implement new policies, which took until the early 1900s to have an effect.[114]
 On each of his three voyages to the Pacific between 1769 and 1777, James Cook visited New Zealand. He was followed by an assortment of Europeans and Americans which including whalers, sealers, escaped convicts from New South Wales, missionaries and adventurers. Initially, contact with the indigenous Māori people was limited to the trading of goods, although interaction increased during the early decades of the 19th century with many trading and missionary stations being set up, especially in the north. The first of several Church of England missionaries arrived in 1814 and as well as their missionary role, they soon become the only form of European authority in a land that was not subject to British jurisdiction: the closest authority being the New South Wales governor in Sydney. The sale of weapons to Māori resulted  from 1818 on in the intertribal warfare of the Musket Wars, with devastating consequences for the Māori population.[115]
 The UK government finally decided to act, dispatching Captain William Hobson with instructions to take formal possession after obtaining native consent. There was no central Māori authority able to represent all New Zealand so, on 6 February 1840, Hobson and many Māori chiefs signed the Treaty of Waitangi in the Bay of Islands; most other chiefs signing in stages over the following months.[116] William Hobson declared British sovereignty over all New Zealand on 21 May 1840, over the North Island by cession and over the South Island by discovery (the island was sparsely populated and deemed terra nullius). Hobson became Lieutenant-Governor, subject to Governor Sir George Gipps in Sydney,[117] with British possession of New Zealand initially administered from Australia as a dependency of the New South Wales colony. From 16 June 1840 New South Wales laws applied in New Zealand.[118] This transitional arrangement ended with the Charter for Erecting the Colony of New Zealand on 16 November 1840. The Charter stated that New Zealand would be established as a separate Crown colony on 3 May 1841 with Hobson as its governor.[119]
 During the 19th century, Britain and the Russian Empire vied to fill the power vacuums that had been left by the declining Ottoman Empire, Qajar dynasty and Qing dynasty. This rivalry in Central Asia came to be known as the ""Great Game"".[120] As far as Britain was concerned, defeats inflicted by Russia on Persia and Turkey demonstrated its imperial ambitions and capabilities and stoked fears in Britain of an overland invasion of India.[121] In 1839, Britain moved to pre-empt this by invading Afghanistan, but the First Anglo-Afghan War was a disaster for Britain.[122]
 When Russia invaded the Ottoman Balkans in 1853, fears of Russian dominance in the Mediterranean and the Middle East led Britain and France to enter the war in support of the Ottoman Empire and invade the Crimean Peninsula to destroy Russian naval capabilities.[122] The ensuing Crimean War (1854–1856), which involved new techniques of modern warfare,[123] was the only global war fought between Britain and another imperial power during the Pax Britannica and was a resounding defeat for Russia.[122] The situation remained unresolved in Central Asia for two more decades, with Britain annexing Baluchistan in 1876 and Russia annexing Kirghizia, Kazakhstan, and Turkmenistan. For a while, it appeared that another war would be inevitable, but the two countries reached an agreement on their respective spheres of influence in the region in 1878 and on all outstanding matters in 1907 with the signing of the Anglo-Russian Entente.[124] The destruction of the Imperial Russian Navy by the Imperial Japanese Navy at the Battle of Tsushima during the Russo-Japanese War of 1904–1905 limited its threat to the British.[125]
 The Dutch East India Company had founded the Dutch Cape Colony on the southern tip of Africa in 1652 as a way station for its ships travelling to and from its colonies in the East Indies. Britain formally acquired the colony, and its large Afrikaner (or Boer) population in 1806, having occupied it in 1795 to prevent its falling into French hands during the Flanders Campaign.[126] British immigration to the Cape Colony began to rise after 1820, and pushed thousands of Boers, resentful of British rule, northwards to found their own—mostly short-lived—independent republics, during the Great Trek of the late 1830s and early 1840s.[127] In the process the Voortrekkers clashed repeatedly with the British, who had their own agenda with regard to colonial expansion in South Africa and to the various native African polities, including those of the Sotho people and the Zulu Kingdom. Eventually, the Boers established two republics that had a longer lifespan: the South African Republic or Transvaal Republic (1852–1877; 1881–1902) and the Orange Free State (1854–1902).[128] In 1902 Britain occupied both republics, concluding a treaty with the two Boer Republics following the Second Boer War (1899–1902).[129]
 In 1869 the Suez Canal opened under Napoleon III, linking the Mediterranean Sea with the Indian Ocean. Initially the Canal was opposed by the British;[130] but once opened, its strategic value was quickly recognised and became the ""jugular vein of the Empire"".[131] In 1875, the Conservative government of Benjamin Disraeli bought the indebted Egyptian ruler Isma'il Pasha's 44 per cent shareholding in the Suez Canal for £4 million (equivalent to £480 million in 2023). Although this did not grant outright control of the strategic waterway, it did give Britain leverage. Joint Anglo-French financial control over Egypt ended in outright British occupation in 1882.[132] Although Britain controlled the Khedivate of Egypt into the 20th century, it was officially a vassal state of the Ottoman Empire and not part of the British Empire. The French were still majority shareholders and attempted to weaken the British position,[133] but a compromise was reached with the 1888 Convention of Constantinople, which made the Canal officially neutral territory.[134]
 With competitive French, Belgian and Portuguese activity in the lower Congo River region undermining orderly colonisation of tropical Africa, the Berlin Conference of 1884–85 was held to regulate the competition between the European powers in what was called the ""Scramble for Africa"" by defining ""effective occupation"" as the criterion for international recognition of territorial claims.[135] The scramble continued into the 1890s, and caused Britain to reconsider its decision in 1885 to withdraw from Sudan. A joint force of British and Egyptian troops defeated the Mahdist Army in 1896 and rebuffed an attempted French invasion at Fashoda in 1898. Sudan was nominally made an Anglo-Egyptian condominium, but a British colony in reality.[136]
 British gains in Southern and East Africa prompted Cecil Rhodes, pioneer of British expansion in Southern Africa, to urge a ""Cape to Cairo"" railway linking the strategically important Suez Canal to the mineral-rich south of the continent.[137] During the 1880s and 1890s, Rhodes, with his privately owned British South Africa Company, occupied and annexed territories named after him, Rhodesia.[138]
 The path to independence for the white colonies of the British Empire began with the 1839 Durham Report, which proposed unification and self-government for Upper and Lower Canada, as a solution to political unrest which had erupted in armed rebellions in 1837.[139] This began with the passing of the Act of Union in 1840, which created the Province of Canada. Responsible government was first granted to Nova Scotia in 1848, and was soon extended to the other British North American colonies. With the passage of the British North America Act, 1867 by the British Parliament, the Province of Canada, New Brunswick and Nova Scotia were formed into Canada, a confederation enjoying full self-government with the exception of international relations.[140] Australia and New Zealand achieved similar levels of self-government after 1900, with the Australian colonies federating in 1901.[141] The term ""dominion status"" was officially introduced at the 1907 Imperial Conference.[142] As the dominions gained greater autonomy, they would come to be recognized as distinct realms of the empire with unique customs and symbols of their own. Imperial identity, through imagery such as patriotic artworks and banners, began developing into a form that attempted to be more inclusive by showcasing the empire as a family of newly birthed nations with common roots.[143][144]
 The last decades of the 19th century saw concerted political campaigns for Irish home rule. Ireland had been united with Britain into the United Kingdom of Great Britain and Ireland with the Act of Union 1800 after the Irish Rebellion of 1798, and had suffered a severe famine between 1845 and 1852. Home rule was supported by the British prime minister, William Gladstone, who hoped that Ireland might follow in Canada's footsteps as a Dominion within the empire, but his 1886 Home Rule bill was defeated in Parliament. Although the bill, if passed, would have granted Ireland less autonomy within the UK than the Canadian provinces had within their own federation,[145] many MPs feared that a partially independent Ireland might pose a security threat to Great Britain or mark the beginning of the break-up of the empire.[146] A second Home Rule bill was defeated for similar reasons.[146] A third bill was passed by Parliament in 1914, but not implemented because of the outbreak of the First World War leading to the 1916 Easter Rising.[147]
 By the turn of the 20th century, fears had begun to grow in Britain that it would no longer be able to defend the metropole and the entirety of the empire while at the same time maintaining the policy of ""splendid isolation"".[148] Germany was rapidly rising as a military and industrial power and was now seen as the most likely opponent in any future war. Recognising that it was overstretched in the Pacific[149] and threatened at home by the Imperial German Navy, Britain formed an alliance with Japan in 1902 and with its old enemies France and Russia in 1904 and 1907, respectively.[150]
 Britain's fears of war with Germany were realised in 1914 with the outbreak of the First World War. Britain quickly invaded and occupied most of Germany's overseas colonies in Africa. In the Pacific, Australia and New Zealand occupied German New Guinea and German Samoa respectively. Plans for a post-war division of the Ottoman Empire, which had joined the war on Germany's side, were secretly drawn up by Britain and France under the 1916 Sykes–Picot Agreement. This agreement was not divulged to the Sharif of Mecca, who the British had been encouraging to launch an Arab revolt against their Ottoman rulers, giving the impression that Britain was supporting the creation of an independent Arab state.[151]
 The British declaration of war on Germany and its allies committed the colonies and Dominions, which provided invaluable military, financial and material support. Over 2.5 million men served in the armies of the Dominions, as well as many thousands of volunteers from the Crown colonies.[152] The contributions of Australian and New Zealand troops during the 1915 Gallipoli Campaign against the Ottoman Empire had a great impact on the national consciousness at home and marked a watershed in the transition of Australia and New Zealand from colonies to nations in their own right. The countries continue to commemorate this occasion on Anzac Day. Canadians viewed the Battle of Vimy Ridge in a similar light.[153] The important contribution of the Dominions to the war effort was recognised in 1917 by British prime minister David Lloyd George when he invited each of the Dominion prime ministers to join an Imperial War Cabinet to co-ordinate imperial policy.[154]
 Under the terms of the concluding Treaty of Versailles signed in 1919, the empire reached its greatest extent with the addition of 1.8 million sq mi (4.7 million km2) and 13 million new subjects.[155] The colonies of Germany and the Ottoman Empire were distributed to the Allied powers as League of Nations mandates. Britain gained control of Palestine, Transjordan, Iraq, parts of Cameroon and Togoland, and Tanganyika. The Dominions themselves acquired mandates of their own: the Union of South Africa gained South West Africa (modern-day Namibia), Australia gained New Guinea, and New Zealand Western Samoa. Nauru was made a combined mandate of Britain and the two Pacific Dominions.[156]
 The changing world order that the war had brought about, in particular the growth of the United States and Japan as naval powers, and the rise of independence movements in India and Ireland, caused a major reassessment of British imperial policy.[157] Forced to choose between alignment with the United States or Japan, Britain opted not to renew its Anglo-Japanese Alliance and instead signed the 1922 Washington Naval Treaty, where Britain accepted naval parity with the United States.[158] This decision was the source of much debate in Britain during the 1930s[159] as militaristic governments took hold in Germany and Japan helped in part by the Great Depression, for it was feared that the empire could not survive a simultaneous attack by both nations.[160] The issue of the empire's security was a serious concern in Britain, as it was vital to the British economy.[161]
 In 1919, the frustrations caused by delays to Irish home rule led the MPs of Sinn Féin, a pro-independence party that had won a majority of the Irish seats in the 1918 British general election, to establish an independent parliament in Dublin, at which Irish independence was declared. The Irish Republican Army simultaneously began a guerrilla war against the British administration.[162] The Irish War of Independence ended in 1921 with a stalemate and the signing of the Anglo-Irish Treaty, creating the Irish Free State, a Dominion within the British Empire, with effective internal independence but still constitutionally linked with the British Crown.[163] Northern Ireland, consisting of six of the 32 Irish counties which had been established as a devolved region under the 1920 Government of Ireland Act, immediately exercised its option under the treaty to retain its existing status within the United Kingdom.[164]
 A similar struggle began in India when the Government of India Act 1919 failed to satisfy the demand for independence.[165] Concerns over communist and foreign plots following the Ghadar conspiracy ensured that war-time strictures were renewed by the Rowlatt Acts. This led to tension,[166] particularly in the Punjab region, where repressive measures culminated in the Amritsar Massacre. In Britain, public opinion was divided over the morality of the massacre, between those who saw it as having saved India from anarchy, and those who viewed it with revulsion.[166] The non-cooperation movement was called off in March 1922 following the Chauri Chaura incident, and discontent continued to simmer for the next 25 years.[167]
 In 1922, Egypt, which had been declared a British protectorate at the outbreak of the First World War, was granted formal independence, though it continued to be a British client state until 1954. British troops remained stationed in Egypt until the signing of the Anglo-Egyptian Treaty in 1936,[168] under which it was agreed that the troops would withdraw but continue to occupy and defend the Suez Canal zone. In return, Egypt was assisted in joining the League of Nations.[169] Iraq, a British mandate since 1920, gained membership of the League in its own right after achieving independence from Britain in 1932.[170] In Palestine, Britain was presented with the problem of mediating between the Arabs and increasing numbers of Jews. The Balfour Declaration, which had been incorporated into the terms of the mandate, stated that a national home for the Jewish people would be established in Palestine, and Jewish immigration allowed up to a limit that would be determined by the mandatory power.[171] This led to increasing conflict with the Arab population, who openly revolted in 1936. As the threat of war with Germany increased during the 1930s, Britain judged the support of Arabs as more important than the establishment of a Jewish homeland, and shifted to a pro-Arab stance, limiting Jewish immigration and in turn triggering a Jewish insurgency.[151]
 The right of the Dominions to set their own foreign policy, independent of Britain, was recognised at the 1923 Imperial Conference.[172] Britain's request for military assistance from the Dominions at the outbreak of the Chanak Crisis the previous year had been turned down by Canada and South Africa, and Canada had refused to be bound by the 1923 Treaty of Lausanne.[173] After pressure from the Irish Free State and South Africa, the 1926 Imperial Conference issued the Balfour Declaration of 1926, declaring Britain and the Dominions to be ""autonomous Communities within the British Empire, equal in status, in no way subordinate one to another"" within a ""British Commonwealth of Nations"".[174] This declaration was given legal substance under the 1931 Statute of Westminster.[142] The parliaments of Canada, Australia, New Zealand, the Union of South Africa, the Irish Free State and Newfoundland were now independent of British legislative control, they could nullify British laws and Britain could no longer pass laws for them without their consent.[175] Newfoundland reverted to colonial status in 1933, suffering from financial difficulties during the Great Depression.[176] In 1937 the Irish Free State introduced a republican constitution renaming itself Ireland.[177]
 Britain's declaration of war against Nazi Germany in September 1939 included the Crown colonies and India but did not automatically commit the Dominions of Australia, Canada, New Zealand, Newfoundland and South Africa. All soon declared war on Germany. While Britain continued to regard Ireland as still within the British Commonwealth, Ireland chose to remain legally neutral throughout the war.[178]
 After the Fall of France in June 1940, Britain and the empire stood alone against Germany, until the German invasion of Greece on 7 April 1941. British Prime Minister Winston Churchill successfully lobbied President Franklin D. Roosevelt for military aid from the United States, but Roosevelt was not yet ready to ask Congress to commit the country to war.[179] In August 1941, Churchill and Roosevelt met and signed the Atlantic Charter, which included the statement that ""the rights of all peoples to choose the form of government under which they live"" should be respected. This wording was ambiguous as to whether it referred to European countries invaded by Germany and Italy, or the peoples colonised by European nations, and would later be interpreted differently by the British, Americans, and nationalist movements.[180] Nevertheless, Churchill rejected its universal applicability when it came to the self-determination of subject nations including the British Indian Empire. Churchill further added that he did not become Prime Minister to oversee the liquidation of the empire.[181]
 For Churchill, the entry of the United States into the war was the ""greatest joy"".[182] He felt that Britain was now assured of victory,[183] but failed to recognise that the ""many disasters, immeasurable costs and tribulations [which he knew] lay ahead""[184] in December 1941 would have permanent consequences for the future of the empire. The manner in which British forces were rapidly defeated in the Far East irreversibly harmed Britain's standing and prestige as an imperial power,[185] including, particularly, the Fall of Singapore, which had previously been hailed as an impregnable fortress and the eastern equivalent of Gibraltar.[186] The realisation that Britain could not defend its entire empire pushed Australia and New Zealand, which now appeared threatened by Japanese forces, into closer ties with the United States and, ultimately, the 1951 ANZUS Pact.[187] The war weakened the empire in other ways: undermining Britain's control of politics in India, inflicting long-term economic damage, and irrevocably changing geopolitics by pushing the Soviet Union and the United States to the centre of the global stage.[188]
 
Though Britain and the empire emerged victorious from the Second World War, the effects of the conflict were profound, both at home and abroad. Much of Europe, a continent that had dominated the world for several centuries, was in ruins, and host to the armies of the United States and the Soviet Union, who now held the balance of global power.[189] Britain was left essentially bankrupt, with insolvency only averted in 1946 after the negotiation of a US$3.75 billion loan from the United States,[190][191] the last instalment of which was repaid in 2006.[192] At the same time, anti-colonial movements were on the rise in the colonies of European nations. The situation was complicated further by the increasing Cold War rivalry of the United States and the Soviet Union. In principle, both nations were opposed to European colonialism.[193] In practice, American anti-communism prevailed over anti-imperialism, and therefore the United States supported the continued existence of the British Empire to keep Communist expansion in check.[194] At first, British politicians believed it would be possible to maintain Britain's role as a world power at the head of a re-imagined Commonwealth,[195] but by 1960 they were forced to recognise that there was an irresistible ""wind of change"" blowing. Their priorities changed to maintaining an extensive zone of British influence[196] and ensuring that stable, non-Communist governments were established in former colonies.[197] In this context, while other European powers such as France and Portugal waged costly and unsuccessful wars to keep their empires intact, Britain generally adopted a policy of peaceful disengagement from its colonies, although violence occurred in Malaya, Kenya and Palestine.[198] Between 1945 and 1965, the number of people under British rule outside the UK itself fell from 700 million to 5 million, 3 million of whom were in Hong Kong.[199]
 The pro-decolonisation Labour government, elected at the 1945 general election and led by Clement Attlee, moved quickly to tackle the most pressing issue facing the empire: Indian independence.[200] India's major political party—the Indian National Congress (led by Mahatma Gandhi) — had been campaigning for independence for decades, but disagreed with Muslim League (led by Muhammad Ali Jinnah) as to how it should be implemented. Congress favoured a unified secular Indian state, whereas the League, fearing domination by the Hindu majority, desired a separate Islamic state for Muslim-majority regions. Increasing civil unrest led Attlee to promise independence no later than 30 June 1948. When the urgency of the situation and risk of civil war became apparent, the newly appointed (and last) Viceroy, Lord Mountbatten, hastily brought forward the date to 15 August 1947.[201] The borders drawn by the British to broadly partition India into Hindu and Muslim areas left tens of millions as minorities in the newly independent states of India and Pakistan.[202] The princely states were provided with a choice to either remain independent or join India or Pakistan.[203] Millions of Muslims crossed from India to Pakistan and Hindus vice versa, and violence between the two communities cost hundreds of thousands of lives. Burma, which had been administered as part of British India until 1937 gained independence the following year in 1948 along with  Sri Lanka (formerly known as British Ceylon). India, Pakistan and Sri Lanka became members of the Commonwealth, while Burma chose not to join.[204] That same year, the British Nationality Act was enacted, in hopes of strengthening and unifying the Commonwealth: it provided British citizenship and right of entry to all those living within its jurisdiction.[205]
 The British Mandate in Palestine, where an Arab majority lived alongside a Jewish minority, presented the British with a similar problem to that of India.[206] The matter was complicated by large numbers of Jewish refugees seeking to be admitted to Palestine following the Holocaust, while Arabs were opposed to the creation of a Jewish state. Frustrated by the intractability of the problem, attacks by Jewish paramilitary organisations and the increasing cost of maintaining its military presence, Britain announced in 1947 that it would withdraw in 1948 and leave the matter to the United Nations to solve.[207] The UN General Assembly subsequently voted for a plan to partition Palestine into a Jewish and an Arab state. It was immediately followed by the outbreak of a civil war between the Arabs and Jews of Palestine, and British forces withdrew amid the fighting. The British Mandate for Palestine officially terminated at midnight on 15 May 1948 as the State of Israel declared independence and the 1948 Arab-Israeli War broke out, during which the territory of the former Mandate was partitioned between Israel and the surrounding Arab states. Amid the fighting, British forces continued to withdraw from Israel, with the last British troops departing from Haifa on 30 June 1948.[208]
 Following the surrender of Japan in the Second World War, anti-Japanese resistance movements in Malaya turned their attention towards the British, who had moved to quickly retake control of the colony, valuing it as a source of rubber and tin.[209] The fact that the guerrillas were primarily Malaysian Chinese Communists meant that the British attempt to quell the uprising was supported by the Muslim Malay majority, on the understanding that once the insurgency had been quelled, independence would be granted.[209] The Malayan Emergency, as it was called, began in 1948 and lasted until 1960, but by 1957, Britain felt confident enough to grant independence to the Federation of Malaya within the Commonwealth. In 1963, the 11 states of the federation together with Singapore, Sarawak and North Borneo joined to form Malaysia, but in 1965 Chinese-majority Singapore was expelled from the union following tensions between the Malay and Chinese populations and became an independent city-state.[210] Brunei, which had been a British protectorate since 1888, declined to join the union.[211]
 In the 1951 general election, the Conservative Party returned to power in Britain under the leadership of Winston Churchill. Churchill and the Conservatives believed that Britain's position as a world power relied on the continued existence of the empire, with the base at the Suez Canal allowing Britain to maintain its pre-eminent position in the Middle East in spite of the loss of India. Churchill could not ignore Gamal Abdul Nasser's new revolutionary government of Egypt that had taken power in 1952, and the following year it was agreed that British troops would withdraw from the Suez Canal zone and that Sudan would be granted self-determination by 1955, with independence to follow[212] Sudan was granted independence on 1 January 1956.[213]
 In July 1956, Nasser unilaterally nationalised the Suez Canal. The response of Anthony Eden, who had succeeded Churchill as Prime Minister, was to collude with France to engineer an Israeli attack on Egypt that would give Britain and France an excuse to intervene militarily and retake the canal.[214] Eden infuriated US President Dwight D. Eisenhower by his lack of consultation, and Eisenhower refused to back the invasion.[215] Another of Eisenhower's concerns was the possibility of a wider war with the Soviet Union after it threatened to intervene on the Egyptian side. Eisenhower applied financial leverage by threatening to sell US reserves of the British pound and thereby precipitate a collapse of the British currency.[216] Though the invasion force was militarily successful in its objectives,[217] UN intervention and US pressure forced Britain into a humiliating withdrawal of its forces, and Eden resigned.[218][219]
 The Suez Crisis very publicly exposed Britain's limitations to the world and confirmed Britain's decline on the world stage and its end as a first-rate power,[220][221] demonstrating that henceforth it could no longer act without at least the acquiescence, if not the full support, of the United States.[222] The events at Suez wounded British national pride, leading one Member of Parliament (MP) to describe it as ""Britain's Waterloo""[223] and another to suggest that the country had become an ""American satellite"".[224] Margaret Thatcher later described the mindset she believed had befallen Britain's political leaders after Suez where they ""went from believing that Britain could do anything to an almost neurotic belief that Britain could do nothing"", from which Britain did not recover until the successful recapture of the Falkland Islands from Argentina in 1982.[225]
 While the Suez Crisis caused British power in the Middle East to weaken, it did not collapse.[226] Britain again deployed its armed forces to the region, intervening in Oman (1957), Jordan (1958) and Kuwait (1961), though on these occasions with American approval,[227] as the new Prime Minister Harold Macmillan's foreign policy was to remain firmly aligned with the United States.[223] Although Britain granted Kuwait independence in 1961, it continued to maintain a military presence in the Middle East for another decade. On 16 January 1968, a few weeks after the devaluation of the pound, Prime Minister Harold Wilson and his Defence Secretary Denis Healey announced that British Armed Forces troops would be withdrawn from major military bases East of Suez, which included the ones in the Middle East, and primarily from Malaysia and Singapore by the end of 1971, instead of 1975 as earlier planned.[228] By that time over 50,000 British military personnel were still stationed in the Far East, including 30,000 in Singapore.[229] The British granted independence to the Maldives in 1965 but continued to station a garrison there until 1976, withdrew from Aden in 1967, and granted independence to Bahrain, Qatar, and the United Arab Emirates in 1971.[230]
 Macmillan gave a speech in Cape Town, South Africa in February 1960 where he spoke of ""the wind of change blowing through this continent"".[231] Macmillan wished to avoid the same kind of colonial war that France was fighting in Algeria, and under his premiership decolonisation proceeded rapidly.[232] To the three colonies that had been granted independence in the 1950s—Sudan, the Gold Coast and Malaya—were added nearly ten times that number during the 1960s.[233] Owing to the rapid pace of decolonisation during this period, the cabinet post of Secretary of State for the Colonies was abolished in 1966, along with the Colonial Office, which merged with the Commonwealth Relations Office to form the Foreign and Commonwealth Office (now the Foreign, Commonwealth and Development Office) in October 1968.[234]
 Britain's remaining colonies in Africa, except for self-governing Southern Rhodesia, were all granted independence by 1968. British withdrawal from the southern and eastern parts of Africa was not a peaceful process. From 1952 the Kenya Colony saw the eight-year long Mau Mau rebellion, in which tens of thousands of suspected rebels were interned by the colonial government in detention camps to suppress the rebellion and over 1000 convicts executed, with records systematically destroyed.[235][236] Throughout the 1960s, the British government took a ""No independence until majority rule"" policy towards decolonising the empire, leading the white minority government of Southern Rhodesia to enact the 1965 Unilateral Declaration of Independence from Britain, resulting in a civil war that lasted until the British-mediated Lancaster House Agreement of 1979.[237] The agreement saw the British Empire temporarily re-establish the Colony of Southern Rhodesia from 1979 to 1980 as a transitionary government to a majority rule Republic of Zimbabwe. This was the last British possession in Africa.
 In Cyprus, a guerrilla war waged by the Greek Cypriot organisation EOKA against British rule, was ended in 1959 by the London and Zürich Agreements, which resulted in Cyprus being granted independence in 1960. The UK retained the military bases of Akrotiri and Dhekelia as sovereign base areas. The Mediterranean colony of Malta was amicably granted independence from the UK in 1964 and became the country of Malta, though the idea had been raised in 1955 of integration with Britain.[238]
 Most of the UK's Caribbean territories achieved independence after the departure in 1961 and 1962 of Jamaica and Trinidad from the West Indies Federation, established in 1958 in an attempt to unite the British Caribbean colonies under one government, but which collapsed following the loss of its two largest members.[239] Jamaica attained independence in 1962, as did Trinidad and Tobago. Barbados achieved independence in 1966 and the remainder of the eastern Caribbean islands, including the Bahamas, in the 1970s and 1980s,[239] but Anguilla and the Turks and Caicos Islands opted to revert to British rule after they had already started on the path to independence.[240] The British Virgin Islands,[241] The Cayman Islands and Montserrat opted to retain ties with Britain,[242] while Guyana achieved independence in 1966. Britain's last colony on the American mainland, British Honduras, became a self-governing colony in 1964 and was renamed Belize in 1973, achieving full independence in 1981. A dispute with Guatemala over claims to Belize was left unresolved.[243]
 British Overseas Territories in the Pacific acquired independence in the 1970s beginning with Fiji in 1970 and ending with Vanuatu in 1980. Vanuatu's independence was delayed because of political conflict between English and French-speaking communities, as the islands had been jointly administered as a condominium with France.[244] Fiji, Papua New Guinea, Solomon Islands and Tuvalu became Commonwealth realms.[245]
 By 1981, aside from a scattering of islands and outposts, the process of decolonisation that had begun after the Second World War was largely complete. In 1982, Britain's resolve in defending its remaining overseas territories was tested when Argentina invaded the Falkland Islands, acting on a long-standing claim that dated back to the Spanish Empire.[246] Britain's successful military response to retake the Falkland Islands during the ensuing Falklands War contributed to reversing the downward trend in Britain's status as a world power.[247]
 The 1980s saw Canada, Australia, and New Zealand sever their final constitutional links with Britain. Although granted legislative independence by the Statute of Westminster 1931, vestigial constitutional links had remained in place. The British Parliament retained the power to amend key Canadian constitutional statutes, meaning that an act of the British Parliament was required to make certain changes to the Canadian Constitution.[248] The British Parliament had the power to pass laws extending to Canada at Canadian request. Although no longer able to pass any laws that would apply to Australian Commonwealth law, the British Parliament retained the power to legislate for the individual Australian states. With regard to New Zealand, the British Parliament retained the power to pass legislation applying to New Zealand with the New Zealand Parliament's consent. In 1982, the last legal link between Canada and Britain was severed by the Canada Act 1982, which was passed by the British parliament, formally patriating the Canadian Constitution. The act ended the need for British involvement in changes to the Canadian constitution.[249] Similarly, the Australia Act 1986 (effective 3 March 1986) severed the constitutional link between Britain and the Australian states, while New Zealand's Constitution Act 1986 (effective 1 January 1987) reformed the constitution of New Zealand to sever its constitutional link with Britain.[250]
 On 1 January 1984, Brunei, Britain's last remaining Asian protectorate, was granted full independence.[251] Independence had been delayed due to the opposition of the Sultan, who had preferred British protection.[252]
 In September 1982 the Prime Minister, Margaret Thatcher, travelled to Beijing to negotiate with the Chinese Communist government, on the future of Britain's last major and most populous overseas territory, Hong Kong.[253] Under the terms of the 1842 Treaty of Nanking and 1860 Convention of Peking, Hong Kong Island and Kowloon Peninsula had been respectively ceded to Britain in perpetuity, but the majority of the colony consisted of the New Territories, which had been acquired under a 99-year lease in 1898, due to expire in 1997.[254] Thatcher, seeing parallels with the Falkland Islands, initially wished to hold Hong Kong and proposed British administration with Chinese sovereignty, though this was rejected by China.[255] A deal was reached in 1984—under the terms of the Sino-British Joint Declaration, Hong Kong would become a special administrative region of the People's Republic of China.[256] The handover ceremony in 1997 marked for many,[257] including King Charles III, then Prince of Wales, who was in attendance, ""the end of Empire"", though many British territories that are remnants of the empire still remain.[249]
 Britain retains sovereignty over 14 territories outside the British Isles. In 1983, the British Nationality Act 1981 renamed the existing Crown Colonies as ""British Dependent Territories"",[a] and in 2002 they were renamed the British Overseas Territories.[260] Most former British colonies and protectorates are members of the Commonwealth of Nations, a voluntary association of equal members, comprising a population of around 2.2 billion people.[261] The United Kingdom and 14 other countries, all collectively known as the Commonwealth realms, voluntarily continue to share the same person— King Charles III—as their respective head of state. These 15 nations are distinct and equal legal entities: the United Kingdom, Australia, Canada, New Zealand, Antigua and Barbuda, The Bahamas, Belize, Grenada, Jamaica, Papua New Guinea, Saint Kitts and Nevis, Saint Lucia, Saint Vincent and the Grenadines, Solomon Islands and Tuvalu.[262]
 During the colonial era, emphasis was given to study of the classical Greco-Roman heritage and their experience with empire, aiming to parse how that heritage could be applied to improve the future of the colonies.[264] American hegemony, which throughout its early rise had challenged British claims of being the ""New Rome"",[265] became the successor to British dominance in the mid-20th century; the two countries' historical ties and wartime collaboration supported a peaceful handoff of power after World War II.[266] As for the United Kingdom itself, British views of the former Empire are more positive than is the case with other post-imperial nations;[267] discourse around the former Empire has continued to impact the nation's present-day understanding of itself, as seen in the debate leading up to its decision to leave the European Union in 2016.[268]
 Decades, and in some cases centuries, of British rule and emigration have left their mark on the independent nations that rose from the British Empire. The empire established the use of the English language in regions around the world. Today it is the primary language of up to 460 million people and is spoken by about 1.5 billion as a first, second or foreign language.[269] It has also significantly influenced other languages.[270] Individual and team sports developed in Britain, particularly football, cricket, lawn tennis, and golf were exported.[271] Some sports were also invented or standardised in the former colonies, such as badminton, polo, and snooker in India.[272] British missionaries who travelled around the globe often in advance of soldiers and civil servants spread Protestantism (including Anglicanism) to all continents. The British Empire provided refuge for religiously persecuted continental Europeans for hundreds of years.[273] British educational institutions also remain popular in the present day, in part due to the importance of the English language and similarity of British curriculums to those in the former colonies.[274]
 Political boundaries drawn by the British did not always reflect homogeneous ethnicities or religions, contributing to conflicts in formerly colonised areas. The British Empire was responsible for large migrations of peoples (see also: Commonwealth diaspora). Millions left the British Isles, with the founding settler colonist populations of the United States, Canada, Australia and New Zealand coming mainly from Britain and Ireland. Millions of people moved between British colonies, with large numbers of South Asian people emigrating to other parts of the empire, such as Malaysia and Fiji, and Overseas Chinese people to Malaysia, Singapore and the Caribbean;[275] about half of all modern immigration to the Commonwealth nations continues to occur between them.[276] The demographics of the United Kingdom changed after the Second World War owing to immigration to Britain from its former colonies.[277]
 In the 19th century, innovation in Britain led to revolutionary changes in manufacturing, the development of factory systems, and the growth of transportation by railway and steamship.[278] Debate has also occurred as to what extent the Industrial Revolution, originating from the United Kingdom, was facilitated by or dependent on imperialism.[279] British colonial architecture, such as in churches, railway stations and government buildings, can be seen in many cities that were once part of the British Empire;[280] Western technologies and architecture had been globalised in part due to the Empire's military and administrative requirements.[281] Integration of former colonies into the global economy was also a major legacy.[282] The British choice of system of measurement, the imperial system, continues to be used in some countries in various ways. The convention of driving on the left-hand side of the road has been retained in much of the former empire.[283]
 The Westminster system of parliamentary democracy has served as the template for the governments of many former colonies,[284][285] and English common law for legal systems.[286] It has been observed that almost every former colony that emerged as an independent democratic state is a former British colony,[287] though this correlation greatly declines in strength after 30 years of an ex-colony's independence.[288] International commercial contracts are often based on English common law.[289] The British Judicial Committee of the Privy Council still serves as the highest court of appeal for twelve former colonies.[290]
 Historians' approaches to understanding the British Empire are diverse and evolving.[291] Two key sites of debate over recent decades have been the impact of post-colonial studies, which seek to critically re-evaluate the history of imperialism, and the continued relevance of historians Ronald Robinson and John Gallagher, whose work greatly influenced imperial historiography during the 1950s and 1960s. In addition, differing assessments of the empire's legacy remain relevant to debates over recent history and politics, such as the Anglo-American invasions of Iraq and Afghanistan, as well as Britain's role and identity in the contemporary world.[292][293]
 Historians such as Caroline Elkins have argued against perceptions of the British Empire as a primarily liberalising and modernising enterprise, criticising its widespread use of violence and emergency laws to maintain power.[293][294] Common criticisms of the empire include the use of detention camps in its colonies, massacres of indigenous peoples,[295] and famine-response policies.[296][297] Some scholars, including Amartya Sen, assert that British policies worsened the famines in India that killed millions during British rule.[298] Conversely, historians such as Niall Ferguson say that the economic and institutional development the British Empire brought resulted in a net benefit to its colonies.[299] Other historians treat its legacy as varied and ambiguous.[293] Public attitudes towards the empire within 21st-century Britain have been broadly positive although sentiment towards the Commonwealth has been one of apathy and decline.[297][300][205]
"
Father of the Nation,https://en.wikipedia.org/wiki/Father_of_the_Nation,"


 The Father of the Nation is an honorific title given to a person considered the driving force behind the establishment of a country, state, or nation. Pater Patriae (plural Patres Patriae), also seen as Parens Patriae, was a Roman honorific meaning the ""Father of the Fatherland"", bestowed by the Senate on heroes, and later on emperors. In monarchies, the monarch is often considered the ""father/mother of the nation"" or as a patriarch to guide his family. This concept is expressed in the divine right of kings espoused in some monarchies, while in others it is codified into constitutional law.
 In Spain, the monarch is considered the personification and embodiment, the symbol of unity and permanence of the nation. In Thailand, the monarch is given the same recognition, and any person who expresses disrespect toward the reigning monarch faces severe criminal penalties.
 Many dictators bestow titles upon themselves, which rarely survive the end of their regime. Gnassingbé Eyadéma of Togo's titles included ""father of the nation"", ""older brother"", and ""Guide of the People"".[1] Mobutu Sese Seko of Zaire's included ""Father of the nation"", ""the Guide"", ""the Messiah"", ""the Leopard"", ""the Sun-President"", and ""the Cock who Jumps on Anything That Moves"".[2] In postcolonial Africa, ""father of the nation"" was a title used by many leaders both to refer to their role in the independence movement as a source of legitimacy, and to use paternalist symbolism as a source of continued popularity.[3] On Joseph Stalin's seventieth birthday in 1949, he was bestowed with the title ""Father of Nations"" for his establishment of ""people's democracies"" in countries occupied by the USSR after World War II.[4]
 The title ""Father of the Nation"" is sometimes politically contested. The 1972 Constitution of Bangladesh declared Sheikh Mujibur Rahman to be ""father of the nation"".[5] A motion in the Parliament of Slovakia to proclaim controversial pre-war leader Andrej Hlinka ""father of the nation"" barely failed in September 2007.[6]
 The following people are still often called the ""Father"" or ""Mother"" of their respective nations.
"
Colony of Virginia,https://en.wikipedia.org/wiki/Colony_of_Virginia,"

 The Colony of Virginia was a British colonial settlement in North America from 1606 to 1776.
 The first effort to create an English settlement in the area was chartered in 1584 and established in 1585; the resulting Roanoke Colony lasted for three attempts totaling six years. In 1590, the colony was abandoned. But nearly 20 years later, the colony was re-settled at Jamestown, not far north of the original site. A second charter was issued in 1606 and settled in 1607, becoming the first enduring English colony in North America. It followed failed attempts at settlement on Newfoundland by Sir Humphrey Gilbert[4] in 1583 and the Roanoke Colony (in modern eastern North Carolina) by Sir Walter Raleigh in the late 1580s.
 The founder of the Jamestown colony was the Virginia Company,[5] chartered by King James I, with its first two settlements being in Jamestown on the north bank of the James River and Popham Colony on the Kennebec River in modern-day Maine, both in 1607. The Popham colony quickly failed because of famine, disease, and conflicts with local Native American tribes in the first two years. Jamestown occupied land belonging to the Powhatan Confederacy; it was also on the brink of failure before the arrival of a new group of settlers and supplies by ship in 1610. Tobacco became Virginia's first profitable export, the production of which had a significant impact on the society and settlement patterns.
 In 1624, the Virginia Company's charter was revoked by King James I, and the Virginia Colony was transferred to royal authority as a crown colony. After the English Civil War in the 1640s and 1650s, the Virginia colony was nicknamed ""The Old Dominion"" by King Charles II for its perceived loyalty to the English monarchy during the era of the Protectorate and Commonwealth of England.[6]
 From 1619 to 1775/1776, the colonial legislature of Virginia was the General Assembly, which governed in conjunction with a colonial governor. Jamestown remained the capital of the Virginia Colony until 1699; from 1699 until its dissolution, the capital was in Williamsburg. The colony experienced its first significant political turmoil with Bacon's Rebellion of 1676.
 After declaring independence from the Kingdom of Great Britain in 1775, before the Declaration of Independence was officially adopted, the Virginia Colony became the Commonwealth of Virginia, one of the original thirteen states of the United States, adopting as its official slogan ""The Old Dominion"". The entire modern states of West Virginia, Kentucky, Indiana, and Illinois, and portions of Ohio and Western Pennsylvania were later created from the territory encompassed, or claimed by, the colony of Virginia at the time of further American independence in July 1776.
 ""Virginia"" is the oldest designation for English claims in North America. In 1584, Sir Walter Raleigh sent Philip Amadas and Arthur Barlowe to explore what is now the North Carolina coast. They returned with word of a regional king (weroance) named Wingina, who ruled a land supposedly called Wingandacoa. ""Virginia"" was originally a term used to refer to England's entire North American possession and claim along the east coast from the 34th parallel (close to Cape Fear) north to 45th parallel. This area included a large section of Canada and the shores of Acadia.[7]
 The name Virginia for a region in North America may have been originally suggested by Raleigh, who named it for Queen Elizabeth I in approximately 1584.[8] In addition, the term Wingandacoa may have influenced the name Virginia.""[9][10] On his next voyage, Raleigh learned that while the chief of the Secotans was indeed called Wingina, the expression wingandacoa heard by the English upon arrival actually meant ""What good clothes you wear!"" in Carolina Algonquian and was not the name of the country as previously misunderstood.[11]
 The colony was also known as the Virginia Colony, the Province of Virginia, and occasionally as the Dominion and Colony of Virginia or His Majesty's Most Ancient Colloney and Dominion of Virginia.[12][13]
 According to tradition, in gratitude for the loyalty of Virginians to the crown during the English Civil War, Charles II gave it the title of ""Old Dominion"".[14][15] The colony seal stated from Latin en dat virginia quintum, in English 'Behold, Virginia gives the fifth', with Virginia claimed as the fifth English dominion after England, France[broken anchor], Scotland and Ireland.
 The Commonwealth of Virginia maintains ""Old Dominion"" as its state nickname. The athletic teams of the University of Virginia are known as the ""Cavaliers"", referring to supporters of Charles II, and Virginia has a public university called ""Old Dominion University"".
 Although Spain, France, Sweden, and the Netherlands all had competing claims to the region, none of these prevented the English from becoming the first European power to colonize successfully the Mid-Atlantic coastline. The Spanish had made earlier attempts in what is now Georgia (San Miguel de Gualdape, 1526–1527; several Spanish missions in Georgia between 1568 and 1684), South Carolina (Santa Elena, 1566–1587), North Carolina (Joara, 1567–1568) and Virginia (Ajacán Mission, 1570–1571); and by the French in South Carolina (Charlesfort, 1562–1563). Farther south, the Spanish colony of Spanish Florida, centered on St. Augustine, was established in 1565, while to the north, the French were establishing settlements in what is now Canada (Charlesbourg-Royal briefly occupied 1541–1543; Port Royal, established in 1605).
 In 1583, Sir Humphrey Gilbert established a charter in Newfoundland. Once established, he and his crew abandoned the site and returned to England. On the return trip, Gilbert's ship capsized, and all aboard perished. The charter was abandoned.
 In 1585, Raleigh sent his first colonization mission to Roanoke Island (in present-day North Carolina) with over 100 male settlers. However, when Sir Francis Drake arrived at the colony in the summer of 1586, the colonists opted to return to England because there was a lack of supply ships, abandoning the colony. Supply ships arrived at the abandoned colony later in 1586; 15 soldiers were left behind to hold the island, but no trace of these men was later found.[16]
 In 1587, Raleigh sent another group to attempt to establish a permanent settlement. The expedition leader, John White, returned to England for supplies that same year but was unable to return to the colony because of the war between England and Spain. When he finally did return in 1590, he found the colony abandoned. The houses were intact, but the colonists had disappeared. Although there are numerous theories about the fate of the colony, it remains a mystery and has come to be known as the ""Lost Colony"". Two English children were born in this colony; the first was named Virginia Dare (Dare County, North Carolina, was named in her honor), who was among those whose fate is unknown. The word Croatoan was found carved into a tree, the name of a tribe on a nearby island.[16]
 Following the failure of the previous colonization attempts, England resumed attempts to set up colonies. This time, joint-stock companies were used rather than giving extensive grants to a landed proprietor such as Gilbert or Raleigh.[5]
 King James granted a proprietary charter to two competing branches of the Virginia Company, which investors supported. These were the Plymouth Company and the Virginia Company of London.[17] By the terms of the charter, the Plymouth Company was permitted to establish a colony of 100 sq mi (260 km2) between the 38th parallel and the 45th parallel (roughly between Chesapeake Bay and the current U.S.–Canada border). The London Company was permitted to establish between the 34th parallel and the 41st parallel (approximately between Cape Fear and Long Island Sound) and also owned a large portion of Atlantic and Inland Canada. In the area of overlap, the two companies were not permitted to establish colonies within one hundred miles of each other.[17] During 1606, each company organized expeditions to establish settlements within the area of their rights.
 The London company formed Jamestown in its exclusive territory, while the Plymouth company formed the Popham Colony in its exclusive territory near what is now Phippsburg, Maine.[18] The Popham colony quickly failed because of famine, disease, and conflicts with local Native American tribes in the first two years.
 The London Company hired Captain Christopher Newport to lead its expedition. On December 20, 1606, he set sail from England with his flagship, the Susan Constant, and two smaller ships, the Godspeed, and the Discovery, with 105 men and boys, plus 39 sailors.[19] After an unusually long voyage of 144 days, they arrived at the mouth of the Chesapeake Bay and came ashore at the point where the southern side of the bay meets the Atlantic Ocean, an event that has come to be called the ""First Landing"". They erected a cross and named the point of land Cape Henry in honor of Henry Frederick, Prince of Wales, the eldest son of King James.[20]
 They were instructed to select a location inland along a waterway where they would be less vulnerable to the Spanish or other Europeans seeking to establish colonies. They sailed westward into the Bay and reached the mouth of Hampton Roads, stopping at a location now known as Old Point Comfort. Keeping the shoreline to their right, they then ventured up the largest river, which they named the James, for their king. After exploring at least as far upriver as the confluence of the Appomattox River at present-day Hopewell, they returned downstream to Jamestown Island, which offered a favorable defensive position against enemy ships and deep water anchorage adjacent to the land. Within two weeks, they had constructed their first fort and named their settlement Jamestown.
 In addition to securing gold and other precious minerals to send back to the waiting investors in England, the survival plan for the Jamestown colonists depended upon regular supplies from England and trade with the Native Americans. They selected a location largely cut off from the mainland with little game for hunting, no natural fresh drinking water, and minimal ground for farming. Captain Newport returned to England twice, delivering the first and second supply missions during 1608 and leaving the Discovery for the colonists' use. However, death from disease and conflicts with the Native Americans took a fearsome toll on the colonists. Despite attempts at mining minerals, growing silk, and exporting the native Virginia tobacco, no profitable exports had been identified, and it was unclear whether the settlement would survive financially.[citation needed]
 The Powhatan Confederacy was a confederation of numerous linguistically related tribes in the eastern part of Virginia. The Powhatan Confederacy controlled a territory known as Tsenacommacah, which roughly corresponded with the Tidewater region of Virginia. It was in this territory that the English established Jamestown. At the time of the English arrival, the Powhatan were led by the paramount chief Wahunsenacawh, known to the English as Chief Powhatan.
 On May 31, 1607, about 100 men and boys left England for what is now Maine. Approximately three months later, the group landed on a wooded peninsula where the Kennebec River meets the Atlantic Ocean and began building Fort St. George. By the end of the year, limited resources caused half of the colonists to return to England. The remaining 45 sailed home late the next year, and the Plymouth company fell dormant.[21]
 In 1609, with the abandonment of the Plymouth Company settlement, the London Company's Virginia charter was adjusted to include the territory north of the 34th parallel and south of the 40th parallel, with its original coastal grant extended ""from sea to sea"". Thus, according to James I's writ, the Virginia Colony in its original sense extended to the coast of the Pacific Ocean, in what is now California, with all the land in between belonging to Virginia. For practical purposes, though, the colonists rarely ventured far inland to what was known as the ""Virginia Wilderness.""
 For the third supply, the London Company had a new ship built. The Sea Venture was designed to emit additional colonists and transport supplies. It became the flagship of the admiral of the convoy, Sir George Somers. The third supply was the largest, with eight other ships joining the Sea Venture. The captain of the Sea Venture was the mission's Vice Admiral Christopher Newport. Hundreds of new colonists were aboard the ships. However, the weather was to affect the mission drastically.
 A few days out of London, the nine ships of the third supply mission encountered a hurricane in the Atlantic Ocean. They became separated during the three days the storm lasted. Admiral Somers had the Sea Venture, carrying most of the mission's supplies, deliberately driven aground onto the reefs of Bermuda to avoid sinking. However, while there was no loss of life, the ship was wrecked beyond repair, stranding its survivors on the uninhabited archipelago, to which they laid claim for England.[22]
 The survivors at Bermuda eventually built two smaller ships, and most of them continued to Jamestown, leaving a few on Bermuda to secure the claim. The company's possession of Bermuda was made official in 1612 when the third and final charter extended the boundaries of Virginia far enough out to sea to encompass Bermuda.[23]
 Upon their arrival at Jamestown, the survivors of the Sea Venture discovered that the 10-month delay had greatly aggravated other adverse conditions. Seven of the other ships had arrived carrying more colonists but little in the way of food and supplies. Combined with drought and hostile relations with the Native Americans, the loss of the supplies that had been aboard the Sea Venture resulted in the Starving Time in late 1609 to May 1610, during which over 80% of the colonists perished. Conditions were so adverse it appears, from skeletal evidence, that the survivors engaged in cannibalism.[24] The survivors from Bermuda had brought few supplies and food with them, and it appeared to all that Jamestown must be abandoned, and it would be necessary to return to England.
 Samuel Argall was the captain of one of the seven ships of the third supply that arrived at Jamestown in 1609 after being separated from the Sea Venture, whose fate was unknown. Depositing his passengers and limited supplies, he returned to England with word of the colonists' plight at Jamestown. The king authorized another leader, Thomas West, 3rd Baron De La Warr, later better known as ""Lord Delaware"", to have greater powers, and the London Company organized another supply mission. They set sail from London on April 1, 1610.
 Just after the survivors of the Starving Time and those who had joined them from Bermuda had abandoned Jamestown, the ships of the new supply mission sailed up the James River with food, supplies, a doctor, and more colonists. Lord Delaware was determined that the colony was to survive, and he intercepted the departing ships about 10 miles (16 km) downstream of Jamestown. The colonists thanked Providence for the colony's salvation.
 West proved far harsher and more belligerent toward the Indians than any of his predecessors, engaging in wars of conquest against them. He first sent Thomas Gates to drive off the Kecoughtan from their village on July 9, 1610, then gave Chief Powhatan an ultimatum to either return all English subjects and property, or face war. Powhatan responded by insisting that the English either stay in their fort or leave Virginia. Enraged, De la Warr had the hand of a Paspahegh captive cut off and sent him to the paramount chief with another ultimatum: Return all English subjects and property, or the neighboring villages would be burned. This time, Powhatan did not respond.
 On August 9, 1610, tired of waiting for a response from the Powhatan, West sent George Percy with 70 men to attack the Paspahegh capital, burning the houses and cutting down their cornfields. They killed 65 to 75 Powhatan and captured one of Wowinchopunk's wives and her children. Returning downstream, Percy's men threw the children overboard and shot out ""their Braynes in the water"". The queen was put to the sword in Jamestown. The Paspahegh never recovered from this attack and abandoned their town. Another small force sent with Argall against the Warraskoyaks found that they had already fled, and they destroyed an abandoned Warraskoyak village and the surrounding cornfields. This event triggered the first Anglo-Powhatan War.
 Among the individuals who had briefly abandoned Jamestown was John Rolfe, a Sea Venture survivor who had lost his wife and son in Bermuda. He was a businessman from London with some untried seeds for new, sweeter strains of tobacco he brought from Bermuda and some novel marketing ideas. It would turn out that Rolfe held the key to the colony's economic success. By 1612, Rolfe's strains of tobacco had been successfully cultivated and exported, establishing a first cash crop for export. Plantations and new outposts sprung up starting with Henricus, initially both upriver and downriver along the navigable portion of the James and thereafter along the other rivers and waterways of the area. The settlement at Jamestown could finally be considered permanently established.[26] A period of peace followed the marriage in 1614 of colonist Rolfe to Pocahontas, the daughter of Chief Powhatan.
 Another colonial charter was issued in 1611.[27]
 The relations with the Natives took a turn for the worse after the death of Pocahontas in England and the return of Rolfe and other colonial leaders in May 1617. Disease, poor harvests, and the growing demand for land to cultivate tobacco caused hostilities to escalate. After Chief Powhatan died in 1618, he was succeeded by his own younger brother, Opechancanough. On the surface, he maintained friendly relations with the English, negotiating with them through his warrior Nemattanew. Still, by 1622, after Nemattanew had been slain, Opechancanough was ready to order a limited surprise attack on the colonists, hoping to persuade them to move on and settle elsewhere.
 Chief Opechancanough organized and led a well-coordinated series of surprise attacks on multiple English colonial settlements along both sides of a 50-mile (80 km) long stretch of the James River, which took place early on the morning of March 22, 1622. This event resulted in the deaths of 347 colonists (including men, women, and children) and the abduction of many others. The massacre caught most of the Virginia Colony by surprise and virtually wiped out several entire communities, including Henricus and Wolstenholme Towne at Martin's Hundred. Jamestown was spared from destruction because an Indian boy named Chanco learned of the planned attacks from his brother and warned colonist Richard Pace with whom he lived. Pace, after securing himself and his neighbors on the south side of the James River, took a canoe across the river to warn Jamestown, which narrowly escaped destruction. However, there was no time to warn the other settlements.
 A year later, Captain William Tucker and John Pott worked out a truce with the Powhatan and proposed a toast using liquor laced with poison. 200 Virginia Indians were killed or made ill by the poison, and 50 more were slaughtered by the colonists. For over a decade, the English settlers attacked the Powhatan, targeting their settlements as part of a scorched earth policy. The settlers systematically razed villages, captured children, and seized or destroyed crops.
 By 1634, a six-mile-long palisade was completed across the Virginia Peninsula. The palisade provided some security from attacks by the Virginia Indians for colonists farming and fishing lower on the Peninsula from that point. On April 18, 1644, Opechancanough again tried to force the English to abandon the region with another series of coordinated attacks, killing almost 500 colonists. However, this was a smaller proportion of the growing population than had been killed in the 1622 attacks.
 In 1620, a successor to the Plymouth Company sent colonists to the New World aboard the Mayflower. Known as Pilgrims, they successfully established a settlement in what became Massachusetts. The portion of what had been Virginia north of the 40th parallel became known as New England, according to books written by Captain John Smith, who had made a voyage there.
 In 1624, the charter of the Virginia Company was revoked by King James I, and the Virginia Colony was transferred to royal authority in the form of a crown colony. Subsequent charters for the Maryland Colony in 1632 and to the eight lords proprietors of the Province of Carolina in 1663 and 1665 further reduced the Virginia Colony to roughly the coastal borders it held until the American Revolution. (The border with North Carolina was disputed until surveyed by William Byrd II in 1728.)
 After twelve years of peace following the Indian Wars of 1622–1632, another Anglo–Powhatan War began on March 18, 1644, as a last effort by the remnants of the Powhatan Confederacy, still under Opechancanough, to dislodge the English settlers of the Virginia Colony. Around 500 colonists were killed, but that number represented a relatively low percentage of the overall population, as opposed to the earlier massacre (the 1622 attack had wiped out a third; that of 1644 barely a tenth).
 This was followed by an effort by the settlers to decimate the Powhatan. In July, they marched against the Pamunkey, Chickahominy, and Powhatan proper; and south of the James, against the Appomattoc, Weyanoke, Warraskoyak, and Nansemond, as well as two Carolina tribes, the Chowanoke and Secotan. In February–March 1645, the colony ordered the construction of four frontier forts: Fort Charles at the falls of the James, Fort James on the Chickahominy, Fort Royal at the falls of the York and Fort Henry at the falls of the Appomattox, where the modern city of Petersburg is located.
 In August 1645, the forces of Governor William Berkeley stormed Opechancanough's stronghold. All captured males in the village over age 11 were deported to Tangier Island.[28] Opechancanough, variously reported to be 92 to 100 years old, was taken to Jamestown. While a prisoner, Opechancanough was shot in the back and killed by a soldier assigned to guard him.[29] His death disintegrated the Powhatan Confederacy into its component tribes, whom the colonists continued to attack.
 In the peace treaty of October 1646, the weroance Necotowance and the subtribes formerly in the confederacy each became tributaries to the King of England. At the same time, a racial frontier was delineated between Indian and English settlements, with members of each group forbidden to cross to the other side except by a special pass obtained at one of the newly erected border forts.
 The extent of the Virginia Colony open to patent by English colonists was defined as: All the land between the Blackwater and York rivers, and up to the navigable point of each of the major rivers – which were connected by a straight line running directly from modern Franklin on the Blackwater, northwesterly to the Appomattoc village beside Fort Henry, and continuing in the same direction to the Monocan village above the falls of the James, where Fort Charles was built, then turning sharp right, to Fort Royal on the York (Pamunkey) river. Necotowance thus ceded the English vast tracts of still-uncolonized land, much of it between the James and Blackwater. English settlements on the peninsula north of the York and below the Poropotank were also allowed, as they had already been there since 1640.
 While the newer Puritan colonies, most notably Massachusetts, were dominated by Parliamentarians, the older colonies sided with the Crown. The Virginia Company's two settlements, Virginia and Bermuda (Bermuda's Puritans were expelled as the Eleutheran Adventurers, settling the Bahamas under William Sayle), Antigua and Barbados were conspicuous in their loyalty to the Crown and were singled out by the Rump Parliament in An Act for prohibiting Trade with the Barbadoes, Virginia, Bermuda and Antego in October 1650. This dictated that:
 The act authorized Parliamentary privateers to act against English vessels trading with the rebellious colonies: ""All Ships that Trade with the Rebels may be surprized. Goods and tackle of such ships not to be embezeled, till judgement in the Admiralty; Two or three of the Officers of every ship to be examined upon oath.""
 Virginia's population swelled with Cavaliers during and after the English Civil War. Under the tenure of Crown Governor William Berkeley (1642–1652; 1660–1677), the population expanded from 8,000 in 1642 to 40,000 in 1677.[30] Despite the resistance of the Virginia Cavaliers, Virginian Puritan Richard Bennett was made governor answering to Oliver Cromwell in 1652, followed by two more nominal ""commonwealth governors"". Nonetheless, the colony was rewarded for its loyalty to the Crown by Charles II following the Stuart Restoration when he dubbed it the ''Old Dominion''.
 With the Restoration in the English colonies in 1660, the governorship returned to Berkeley. In 1676, Bacon's Rebellion challenged the political order of the colony. While a military failure, its handling resulted in Governor Berkeley being recalled to England. In 1679, the Treaty of Middle Plantation was signed between King Charles II and several Native American groups.
 Virginia was the most prominent, wealthiest, and most influential of the American colonies, where conservatives controlled the colonial and local governments. At the local level, Church of England parishes handled many local affairs, and they, in turn, were controlled not by the minister but rather by a closed circle of wealthy landowners who comprised the parish vestry. Ronald L. Heinemann emphasizes the ideological conservatism of Virginia while noting some religious dissenters were gaining strength by the 1760s:
 In actual practice, colonial Virginia never had a bishop to represent God nor a hereditary aristocracy with titles like 'duke' or 'baron'. However, it had a royal governor appointed by the king and a powerful landed gentry. The status quo was strongly reinforced by what Thomas Jefferson called ""feudal and unnatural distinctions"" that were vital to the maintenance of aristocracy in Virginia. He promoted laws such as entail and primogeniture by which the oldest son inherited all the land. As a result, increasingly large plantations, worked by white tenant farmers and by enslaved Black people, gained in size, wealth, and political power in the eastern (""Tidewater"") tobacco areas. Maryland and South Carolina had similar hierarchical systems, as did New York and Pennsylvania.[32] During the American Revolutionary era, all such laws were repealed by the new states.[33] The most fervent Loyalists left for Canada or Britain or other parts of the British Empire. They introduced primogeniture in Upper Canada in 1792, lasting until 1851. Such laws lasted in England until 1926.[34]
 As the English expanded out from Jamestown, encroachment of the new arrivals and their ever-growing numbers on what had been Indian lands resulted in several conflicts with the Virginia Indians. For much of the 17th century, English contact and conflict were mainly with the Algonquian peoples that populated the coastal regions, primarily the Powhatan Confederacy. Following a series of wars and the decline of the Powhatan as a political entity, the colonists expanded westward in the late 17th and 18th centuries, encountering the Shawnee, Iroquoian-speaking peoples such as the Nottoway, Meherrin, Iroquois and Cherokee, as well as Siouan-speaking peoples such as the Tutelo, Saponi, and Occaneechi.
 As the English settlements expanded beyond the Tidewater territory traditionally occupied by the Powhatan, they encountered new groups with which there had been minimal relations with the colony. In the late 17th century, the Iroquois Confederacy expanded into the western region of Virginia as part of the Beaver Wars. They arrived shortly before the English settlers and displaced the resident Siouan tribes.
 Lt. Gov. Alexander Spotswood made further advances in policy with the Virginia Indians along the frontier. In 1714, he established Fort Christanna to help educate and trade with several tribes with which the colony had friendly relations and to help protect them from hostile tribes. In 1722, the Treaty of Albany was signed by leaders of the Five Nations of Iroquois, Province of New York, Colony of Virginia, and Province of Pennsylvania.
 The geography of the Virginia settlement expanded as the boundaries of European colonization extended over time. Its cultural geography gradually evolved, with various settlement and jurisdiction models employed. By the late 17th century and the early 18th century, the primary settlement pattern was based on plantations (to grow tobacco), farms, and some towns (mostly ports or courthouse villages).
 The fort at Jamestown, founded in 1607, remained the primary settlement of the colonists for several years. A few strategic outposts were constructed, including Fort Algernon (1609) at the entrance to the James River. Early attempts to occupy strategic locations already inhabited by natives at what is now Richmond and Suffolk failed owing to native resistance.
 A short distance farther up the James, in 1611, Thomas Dale began the construction of a progressive development at Henricus on and about what was later known as Farrar's Island. Henricus was envisioned as a possible replacement capital for Jamestown and was to have the first college in Virginia. (The ill-fated Henricus was destroyed during the Indian massacre of 1622). In addition to creating the settlement at Henricus, Dale also established the port town of Bermuda Hundred, as well as ""Bermuda Cittie"" in 1613, now part of Hopewell, Virginia. He began the excavation work at Dutch Gap, using methods he had learned while serving in Holland.
 Once tobacco had been established as an export cash crop, investors became more interested, and groups of them united to create largely self-sufficient ""hundreds."" The term ""hundred"" is a traditional English name for an administrative division of a shire (or county) to define an area that would support one hundred heads of household.[35] In the colonial era in Virginia, the ""hundreds"" were large developments of many acres, necessary to support tobacco crops. The ""hundreds"" were required to be at least several miles from any existing community. Soon, these patented tracts of land sprang up along the rivers. The investors sent shiploads of settlers and supplies to Virginia to establish the new developments. The administrative centers of Virginia's hundreds were essentially small towns or villages and were often palisaded for defense.
 An example was Martin's Hundred, located downstream from Jamestown on the north bank of the James River. The Martin's Hundred Society, a group of investors in London, sponsored it. It was settled in 1618, and Wolstenholme Towne was its administrative center, named for John Wolstenholme, one of the investors.
 Bermuda Hundred (now in Chesterfield County) and Flowerdew Hundred (now in Prince George County) are other names which have survived over centuries. Others included Berkeley Hundred, Bermuda Nether Hundred, Bermuda Upper Hundred, Smith's Hundred, Digges Hundred, West Hundred, and Shirley Hundred (and, in Bermuda, Harrington Hundreds). Including the creation of the ""hundreds"", the various incentives to investors in the Virginia Colony finally paid off by 1617. By this time, the colonists were exporting 50,000 pounds of tobacco to England per year and were beginning to generate enough profit to ensure the economic survival of the colony.
 In 1619, the plantations and developments were divided into four ""incorporations"" or ""citties"", as they were called. These were Charles Cittie, Elizabeth Cittie, Henrico Cittie, and James Cittie, which included the relatively small seat of government for the colony at Jamestown Island. Each of the four ""citties"" (sic) extended across the James River, the main conduit of transportation of the era. Elizabeth Cittie, known initially as Kecoughtan (a Native word with many variations in spelling by the English), also included the areas now known as South Hampton Roads and the Eastern Shore.
 In 1634, a local government system was created in the Virginia Colony by order of the King of England. Eight shires were designated, each with local officers. Within a few years, the shires were renamed counties, a system that has remained to the present day.
 In 1630, under the governorship of John Harvey, the first settlement on the York River was founded. In 1632, the Virginia legislature voted to build a fort to link Jamestown and the York River settlement of Chiskiack and protect the colony from Indian attacks. In 1634, a palisade was built near Middle Plantation. This wall stretched across the peninsula between the York and James rivers and protected the settlements on the eastern side of the lower peninsula from Indians. The wall also served to contain cattle.
 In 1699, a capital was established and built at Middle Plantation, soon renamed Williamsburg.
 In the period following the English Civil War, the exiled King Charles II hoped to shore up the loyalty of several of his supporters by granting them a significant area of mostly uncharted land to control as a proprietary in Virginia (a claim that would only be valid were the king to return to power). While under the jurisdiction of the Virginia Colony, the proprietary maintained complete control of the granting of land within that territory (and revenues obtained from it) until after the American Revolution. The grant was for the land between the Rappahannock and Potomac Rivers, which included the titular Northern Neck, but as time went on, also would include all of what is today Northern Virginia and into West Virginia. Due to ambiguities of the text of the various grants causing disputes between the proprietary and the colonial government, the tract was finally demarcated via the Fairfax Line in 1746.
 In the initial years under the Virginia Company, the colony was governed by a council, headed by a council president. From 1611 to 1618, under the orders of Sir Thomas Dale, the settlers of the colony were under a regime of civil law that became known as Dale's Code.[36] Under a charter from the company in 1618, a new model of governance was put in place in 1619, which created a House of Burgesses.[36] On July 30, 1619, burgesses met at Jamestown Church as the first elected representative legislative assembly in the New World.[36] The legal system in the colony was thereafter based on the provisions of its royal charter and English common law.
 For much of the history of the royal colony, the formally appointed governor was absentee, often remaining in England. In his stead, a series of acting or lieutenant governors who were physically present held actual authority. In the later years of its history, as it became increasingly civilized, more governors made the journey.
 The first settlement in the colony, Jamestown, served as the capital and main port of entry from its founding until 1699. During this time, a series of statehouses (capitols) were used and subsequently consumed by fires (accidental and intentional in the case of Bacon's Rebellion). Following such a fire, in 1699, the capital was relocated inland, away from the swampy clime of Jamestown, to Middle Plantation, renamed Williamsburg. The capital of Virginia remained in Williamsburg until it was moved further inland to Richmond in 1779 during the American Revolution.
 The entrepreneurs of the Virginia Company experimented with several means of making the colony profitable. The orders sent with the first colonists instructed that they search for precious metals (specifically gold). While no gold was found, various products were sent back, including pitch and clapboard. In 1608, early attempts were made at breaking the Continental hold on glassmaking through the creation of a glassworks. In 1619, the colonists built the first ironworks in North America.
 In 1612, settler John Rolfe planted tobacco obtained from Bermuda (during his stay there as part of the third supply). Within a few years, the crop proved extremely lucrative in the European market. As the English increasingly used tobacco products, the production of tobacco in the American colonies became a significant economic driver, especially in the tidewater region surrounding the Chesapeake Bay. From 1616 to 1619, the only exports of the colony were tobacco and sassafras.[37]
 Colonists developed plantations along the rivers of Virginia, and social/economic systems developed to grow and distribute this cash crop. Some elements of this system included the importation and use of enslaved Africans to cultivate and process crops, which included harvesting and drying periods. Planters would have their workers fill large hogsheads with tobacco and convey them to inspection warehouses. In 1730, the Virginia House of Burgesses standardized and improved the quality of tobacco exported by establishing the Tobacco Inspection Act of 1730, which required inspectors to grade tobacco at 40 specified locations.
 England supplied most colonists; a later migration of Scots-Irish filled the backcountry. The Virginia Colony was always predominantly British in ethnic descent, with only minor contributions from other ethnic groups, particularly Palatinate Germans. In 1608, the first Poles and Slovaks arrived as part of a group of skilled craftsmen.[40][41][42][43] In 1619, the first Africans arrived. Many more Africans were imported as enslaved people, such as Angela.[44] In the early 17th century, French Huguenots arrived in the colony as refugees from religious warfare.[45]
 In the early 18th century, indentured German-speaking colonists from the iron-working region of Nassau-Siegen arrived to establish the Germanna settlement.[46] Scots-Irish settled on the Virginia frontier.[47] Some Welsh arrived, including some ancestors of Thomas Jefferson.[48]
 With the boom in tobacco planting, there was a severe shortage of laborers to work the labor-intensive crop. One method to solve the shortage was using indentured servants.
 By the 1640s, legal documents started to define indentured servants' changing nature and status as servants. In 1640, John Punch was sentenced to lifetime servitude as punishment for trying to escape from his enslaver, Hugh Gwyn. This is the earliest legal sanctioning of slavery in Virginia.[49] After this trial, the relationship between indentured servants and their masters changed, as planters saw permanent servitude a more appealing and profitable prospect than seven-year indentures.
 As many indentured workers were illiterate, especially Africans, there were opportunities for abuse by planters and other indenture holders. Some ignored the expiration of servants' indentured contracts and tried to keep them as lifelong workers. One example is with Anthony Johnson, who argued with Robert Parker, another planter, over the status of John Casor, formerly an indentured servant of his. Johnson argued that his indenture was for life and Parker had interfered with his rights. The court ruled in favor of Johnson and ordered that Casor be returned to him, where he served the rest of his life as an enslaved person.[50] Such documented cases marked the transformation of Black Africans from indentured servants into slaves.
 In the late 17th century, the Royal African Company, which the King of England established to supply the great demand for labor to the colonies, had a monopoly on providing enslaved Africans to the colony.[51] As plantation agriculture was established earlier in Barbados, in the early years, enslaved people were shipped from Barbados (where they were seasoned) to the colonies of Virginia and Carolina.
 In 1619, the Anglican Church was formally established as the official religion in the colony and would remain so until shortly after the American Revolution. Establishment meant that local tax funds paid the parish costs and that the parish had local civic functions such as poor relief. The upper-class planters controlled the vestry, which ran the parish and chose the minister. The church in Virginia was controlled by the Bishop of London, who sent priests and missionaries, but there were never enough, and they reported deficient standards of personal morality.[52] By the 1760s, dissenting Protestants, especially Baptists and Methodists, were proliferating and started challenging the Anglicans for moral leadership.[53][54][55]
 All 26 churches with regular services in Virginia in 1650 were Anglican, which included all but 4 of the 30 Anglican churches in the colonies at the time (with the remainder located in Maryland).[56] Following the First Great Awakening (1730–1755), the number of regular places of worship in Virginia grew to 126 in 1750 (96 Anglican, 17 Presbyterian, 5 Lutheran, 5 German Reformed, and 3 Baptist),[57] with the colony gaining an additional 251 regular places of worship to a total of 377 by 1776 (101 Baptist, 95 Presbyterian, 94 Episcopal, 42 Friends, 17 Lutheran, 14 German Reformed, 10 Methodism, 2 German Baptist Brethren, and 2 Mennonite).[58]
 The first printing press used in Virginia began operation in Jamestown on June 8, 1680, though within a few years, it was shut down by the Governor and Crown of England for want of a license.[59] It was not until 1736 that the first newspaper, the Virginia Gazette, began circulation under printer William Parks of Williamsburg.[59]
 The Syms-Eaton Academy, started in 1634, became America's first free public school. Private tutors were often favored among those families who could afford them.[60]
 For most of the 17th century, a university education for settlers of Virginia required a journey to England or Scotland.[60] Such journeys were undertaken by wealthy young men. In the early years, many settlers received their education before immigrating to the colony.[60]
 In 1693, the College of William & Mary was founded at Middle Plantation (soon renamed Williamsburg). The college included a common school for Virginia Indians, supplemented by local pupils, which lasted until a 1779 overhaul of the institution's curriculum.[60] The college, located in the capital and heart of the Tidewater region, dominated the colony's intellectual climate until after independence.[60][61]
 After 1747, some Virginians began to attend institutions at Princeton and Philadelphia. Generations began to move west into the Piedmont and Blue Ridge areas.[60] In this region of Virginia, two future Presbyterian colleges trace their origins to lower-level institutions founded in this period. First, what would become Hampden–Sydney College was founded in 1775, before the American Revolution. Likewise, Augusta Academy was a classical school that would evolve into Washington and Lee University (though it would not grant its first bachelor's degree until 1785).
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}37°07′52″N 76°38′28″W﻿ / ﻿37.131°N 76.641°W﻿ / 37.131; -76.641
"
Virginia Regiment,https://en.wikipedia.org/wiki/Virginia_Regiment,"The Virginia Regiment was an infantry unit of the Virginia Provincial Forces raised in 1754 by the Virginia General Assembly and Governor Robert Dinwiddie for service in the French and Indian War. The sole provincial unit raised by the British colony of Virginia during the conflict, it initially consisted of 300 men under the command of Colonel George Washington and fought in the battles of Jumonville Glen and Fort Necessity. After the Virginia Regiment's defeat at Fort Necessity, the General Assembly voted to double the size of the unit, which participated in the failed Braddock Expedition to capture Fort Duquesne from the French. 
 Under orders from General Edward Braddock, the unit was re-organized into two carpenter companies, six ranger companies, and one troop of mounted rangers, fighting at the Battle of the Monongahela in 1755. The Virginia Regiment was subsequently expanded into two regiments for the 1758 Forbes Expedition. As a result of the outbreak of the Anglo-Cherokee War in 1762, the unit remained on the Virginia frontier for longer than expected, but was disbanded by Governor Francis Fauquier in 1762. Although Washington resigned from the regiment in 1758, upset over not being made an officer in the British Army, the experience he gained in the conflict greatly helped him during the American Revolutionary War.
 The Anglo-French conflict over the Ohio Country led to raising of the first provincial regiment in the British colony of Virginia. In 1754, the Virginia General Assembly voted to raise a regiment of 300 men and send it to the confluence of the Alleghany and Monongahela rivers. After the battle of Fort Necessity, the General Assembly voted to increase the size of the regiment from five companies to ten.[1][2] The Virginian provincial troops who participated in the Braddock Expedition of 1755 and suffered defeat at the Battle of the Monongahela were unregimented: at the behest of General Edward Braddock, they had been organized into two companies of carpenters, six companies of rangers, and one troop of mounted rangers, about 450 men in all. The remaining 350 men from the original ten companies of the Virginia Regiment had been allocated to the two regular regiments of the expedition.[3][4]
 After the defeat of the expedition, the Virginia Regiment was immediately reformed, with the General Assembly voting in 1755 to increase its size again, to 1,500 men organized in 16 companies. The actual strength of the Regiment in 1756 was 1,400 men, but in 1757 it was reduced to 1,000 men. In 1758, Virginia raised two additional regiments of a thousand men each for the Forbes Expedition. The enlistment period for the first regiment expired in May 1759, and for the second in December 1758. After the fall of Fort Duquesne, the General Assembly voted in 1759 to fill the one regiment still in service, and to raise a force of another 500 men that would remain in the province for its immediate defense. The regiment would remain in service until May 1760.[5][6]
 With the outbreak of the Anglo-Cherokee War, the General Assembly prolonged the Regiment's service, adding 300 men in three companies as frontier guards. It remained on the Cherokee frontier until early 1762, when Governor Francis Fauquier disbanded it. When, later in 1762, the British government indicated its wish for Virginia to raise a regiment which would be put on the British establishment, the General Assembly instead voted to re-raise the Virginia Regiment. This re-raised Regiment was finally disbanded in May 1763, just before the outbreak of Pontiac's War, as the province could not maintain it without a supply of paper money, which the Board of Trade had disallowed.[7]
 Most recruits were characterized by Washington as ""loose, Idle Persons ... quite destitute of House, and Home.""[8] Hampered by frequent desertions because of poor supplies, extremely low pay and hazardous duty, Virginia Regiment recruiters went to Pennsylvania and Maryland for men. Washington said of them, "" and not a few... have Scarce a Coat, or Waistcoat, to their Backs ...""[8] Later drafts pulled only those who could not provide a substitute or pay the £10 exemption fee, ensuring that only Virginia's poor would be drafted. White males between 16 and 50 were permitted to serve, although the regiment's size rolls report men as young as 15 and as old as 60 in the ranks, along with references to a small number of drafts with partial African and Native American ancestry.[citation needed]
 The First Virginia Regiment is memorialized in a statue in Meadow Park, a triangular park in Richmond’s (VA) Fan District by sculptor Ferruccio Legnaioli. Dedicated on 1 May 1930, to commemorate the regiment for fighting in seven American Wars, including the Civil War when they served in the Confederate Army. The statue is a seven foot high bronze standing figure of a colonial infantryman that lists the founding date of the Regiment (1754) at its base. The figure is mounted on a pedestal eight feet high which is lined with bronze plaques describing the history and service of the Regiment through seven wars.[9]
 The statue was pulled down from its pedestal during the night of 19–20 June 2020. It was the fifth statue toppled in Richmond during a series of civil rights protests.[10]
 Source:[14]
 
"
French and Indian War,https://en.wikipedia.org/wiki/French_and_Indian_War,"


 The French and Indian War (1754–1763) was a theater of the Seven Years' War, which pitted the North American colonies of the British Empire against those of the French, each side being supported by various Native American tribes. At the start of the war, the French colonies had a population of roughly 60,000 settlers, compared with 2 million in the British colonies.[5] The outnumbered French particularly depended on their native allies.[6]
 Two years into the war, in 1756, Great Britain declared war on France, beginning the worldwide Seven Years' War. Many view the French and Indian War as being merely the American theater of this conflict; however, in the United States the French and Indian War is viewed as a singular conflict which was not associated with any European war.[7] French Canadians call it the guerre de la Conquête ('War of the Conquest').[8][9]
 The British colonists were supported at various times by the Iroquois, Catawba, and Cherokee tribes, and the French colonists were supported by Wabanaki Confederacy members Abenaki and Mi'kmaq, and the Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot (Huron).[10] Fighting took place primarily along the frontiers between New France and the British colonies, from the Province of Virginia in the south to Newfoundland in the north. It began with a dispute over control of the confluence of the Allegheny River and Monongahela River called the Forks of the Ohio, and the site of the French Fort Duquesne at the location that later became Pittsburgh, Pennsylvania. The dispute erupted into violence in the Battle of Jumonville Glen in May 1754, during which Virginia militiamen under the command of 22-year-old George Washington ambushed a French patrol.[11]
 In 1755, six colonial governors met with General Edward Braddock, the newly arrived British Army commander, and planned a four-way attack on the French.  None succeeded, and the main effort by Braddock proved a disaster; he lost the Battle of the Monongahela on July 9, 1755, and died a few days later. British operations failed in the frontier areas of the Province of Pennsylvania and the Province of New York during 1755–57 due to a combination of poor management, internal divisions, effective Canadien scouts, French regular forces, and Native warrior allies. In 1755, the British captured Fort Beauséjour on the border separating Nova Scotia from Acadia, and they ordered the expulsion of the Acadians (1755–64) soon afterwards. Orders for the deportation were given by Commander-in-Chief William Shirley without direction from Great Britain. The Acadians were expelled, both those captured in arms and those who had sworn the loyalty oath to the king. Natives likewise were driven off the land to make way for settlers from New England.[12]
 The British Pitt government fell due to disastrous campaigns in 1757, including a failed expedition against Louisbourg and the Siege of Fort William Henry; this last was followed by the Natives torturing and massacring their colonial victims. William Pitt came to power and significantly increased British military resources in the colonies at a time when France was unwilling to risk large convoys to aid the limited forces that they had in New France, preferring to concentrate their forces against Prussia and its allies who were now engaged in the Seven Years' War in Europe. The conflict in Ohio ended in 1758 with the British–American victory in the Ohio Country. Between 1758 and 1760, the British military launched a campaign to capture French Canada. They succeeded in capturing territory in surrounding colonies and ultimately the city of Quebec (1759). The following year the British were victorious in the Montreal Campaign in which the French ceded Canada in accordance with the Treaty of Paris (1763).
 France also ceded its territory east of the Mississippi to Great Britain, as well as French Louisiana west of the Mississippi River to its ally Spain in compensation for Spain's loss to Great Britain of Spanish Florida (Spain had ceded Florida to Britain in exchange for the return of Havana, Cuba). France's colonial presence north of the Caribbean was reduced to the islands of Saint Pierre and Miquelon, confirming Great Britain's position as the dominant colonial power in northern America.
 In British America, wars were often named after the sitting British monarch, such as King William's War or Queen Anne's War. There had already been a King George's War in the 1740s during the reign of King George II, so British colonists named this conflict after their opponents, and it became known as the French and Indian War.[13] This continues as the standard name for the war in the United States, although indigenous peoples fought on both sides of the conflict. It also led into the Seven Years' War overseas, a much larger conflict between France and Great Britain that did not involve the American colonies; some historians make a connection between the French and Indian War and the Seven Years' War overseas, but most residents of the United States consider them as two separate conflicts—only one of which involved the American colonies,[14] and American historians generally use the traditional name. Less frequently used names for the war include the Fourth Intercolonial War and the Great War for the Empire.[13]
 In Europe, the French and Indian War is conflated into the Seven Years' War and not given a separate name. ""Seven Years"" refers to events in Europe, from the official declaration of war in 1756—two years after the French and Indian War had started—to the signing of the peace treaty in 1763. The French and Indian War in America, by contrast, was largely concluded in six years from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.[13]
 Canadians conflate both the European and American conflicts into the Seven Years' War (Guerre de Sept Ans).[8] French Canadians also use the term ""War of Conquest"" (Guerre de la Conquête), since it is the war in which New France was conquered by the British and became part of the British Empire. In Quebec, this term was promoted by popular historians Jacques Lacoursière and Denis Vaugeois, who borrowed from the ideas of Maurice Séguin in considering this war as a dramatic tipping point of French Canadian identity and nationhood.[15]
 At this time, North America east of the Mississippi River was largely claimed by either Great Britain or France. Large areas had no colonial settlements. The French population numbered about 75,000 and was heavily concentrated along the St. Lawrence River valley, with some also in Acadia (present-day New Brunswick and parts of Nova Scotia), including Île Royale (Cape Breton Island). Fewer lived in New Orleans; Biloxi, Mississippi; Mobile, Alabama; and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries. French fur traders and trappers traveled throughout the St. Lawrence and Mississippi watersheds, did business with local Indian tribes, and often married Indian women.[16] Traders married daughters of chiefs, creating high-ranking unions.
 British settlers outnumbered the French 20 to 1[17] with a population of about 1.5 million ranged along the Atlantic coast of the continent from Nova Scotia and the Colony of Newfoundland in the north to the Province of Georgia in the south.[18] Many of the older colonies' land claims extended arbitrarily far to the west, as the extent of the continent was unknown at the time when their provincial charters were granted. Their population centers were along the coast, but the settlements were growing into the interior. The British captured Nova Scotia from France in 1713, which still had a significant French-speaking population. Britain also claimed Rupert's Land where the Hudson's Bay Company traded for furs with local Indian tribes.
 Between the French and British colonists, large areas were dominated by Indian tribes. To the north, the Mi'kmaq and the Abenakis were engaged in Father Le Loutre's War and still held sway in parts of Nova Scotia, Acadia, and the eastern portions of the province of Canada, as well as much of Maine.[19] The Iroquois Confederation dominated much of upstate New York and the Ohio Country, although Ohio also included Algonquian-speaking populations of Delaware and Shawnee, as well as Iroquoian-speaking Mingos. These tribes were formally under Iroquois rule and were limited by them in their authority to make agreements.[20] The Iroquois Confederation initially held a stance of neutrality to ensure continued trade with both French and British. Though maintaining this stance proved difficult as the Iroquois Confederation tribes sided and supported French or British causes depending on which side provided the most beneficial trade.[21]
 The Southeast interior was dominated by Siouan-speaking Catawbas, Muskogee-speaking Creeks and Choctaw, and the Iroquoian-speaking Cherokee tribes.[22] When war broke out, the French colonists used their trading connections to recruit fighters from tribes in western portions of the Great Lakes region, which was not directly subject to the conflict between the French and British; these included the Hurons, Mississaugas, Ojibwas, Winnebagos, and Potawatomi.
 The British colonists were supported in the war by the Iroquois Six Nations and also by the Cherokees, until differences sparked the Anglo-Cherokee War in 1758. In 1758, the Province of Pennsylvania successfully negotiated the Treaty of Easton in which a number of tribes in the Ohio Country promised neutrality in exchange for land concessions and other considerations. Most of the other northern tribes sided with the French, their primary trading partner and supplier of arms. The Creeks and Cherokees were subject to diplomatic efforts by both the French and British to gain either their support or neutrality in the conflict.[23][additional citation(s) needed]
 At this time, Spain claimed only the province of Florida in eastern America. It controlled Cuba and other territories in the West Indies that became military objectives in the Seven Years' War. Florida's European population was a few hundred, concentrated in St. Augustine.[24]
 There were no French regular army troops stationed in America at the onset of war. New France was defended by about 3,000 troupes de la marine, companies of colonial regulars (some of whom had significant woodland combat experience). The colonial government recruited militia support when needed. The British had few troops. Most of the British colonies mustered local militia companies to deal with Indian threats, generally ill trained and available only for short periods, but they did not have any standing forces. Virginia, by contrast, had a large frontier with several companies of British regulars.[citation needed]
 When hostilities began, the British colonial governments preferred operating independently of one another and of the government in London. This situation complicated negotiations with Indian tribes, whose territories often encompassed land claimed by multiple colonies. As the war progressed, the leaders of the British Army establishment tried to impose constraints and demands on the colonial administrations.[citation needed]
 New France's Governor-General Roland-Michel Barrin de La Galissonière was concerned about the incursion and expanding influence in the Ohio Country of British colonial traders such as George Croghan.  In June 1747, he ordered Pierre-Joseph Céloron to lead a military expedition through the area. Its objectives were:
 Céloron's expedition force consisted of about 200 Troupes de la marine and 30 Indians, and they covered about 3,000 miles (4,800 km) between June and November 1749. They went up the St. Lawrence, continued along the northern shore of Lake Ontario, crossed the portage at Niagara, and followed the southern shore of Lake Erie. At the Chautauqua Portage near Barcelona, New York, the expedition moved inland to the Allegheny River, which it followed to the site of Pittsburgh. There Céloron buried lead plates engraved with the French claim to the Ohio Country.[25] Whenever he encountered British colonial merchants or fur-traders, he informed them of the French claims on the territory and told them to leave.[25]
 Céloron's expedition arrived at Logstown where the Indians in the area informed him that they owned the Ohio Country and that they would trade with the British colonists regardless of the French.[26] He continued south until his expedition reached the confluence of the Ohio and the Miami rivers, which lay just south of the village of Pickawillany, the home of the Miami chief known as ""Old Briton"". Céloron threatened Old Briton with severe consequences if he continued to trade with British colonists, but Old Briton ignored the warning. Céloron returned disappointedly to Montreal in November 1749.[27]
 Céloron wrote an extensively detailed report. ""All I can say is that the Natives of these localities are very badly disposed towards the French,"" he wrote, ""and are entirely devoted to the English. I don't know in what way they could be brought back.""[26] Even before his return to Montreal, reports on the situation in the Ohio Country were making their way to London and Paris, each side proposing that action be taken. Massachusetts governor William Shirley was particularly forceful, stating that British colonists would not be safe as long as the French were present.[28]
 The War of the Austrian Succession ended in 1748 with the signing of the Treaty of Aix-la-Chapelle, which was primarily focused on resolving issues in Europe. The issues of conflicting territorial claims between British and French colonies were turned over to a commission, but it reached no decision. Frontier areas were claimed by both sides, from Nova Scotia and Acadia in the north to the Ohio Country in the south. The disputes also extended into the Atlantic Ocean, where both powers wanted access to the rich fisheries of the Grand Banks off Newfoundland.[citation needed]
 In 1749, the British government gave land to the Ohio Company of Virginia for the purpose of developing trade and settlements in the Ohio Country.[29] The grant required that it settle 100 families in the territory and construct a fort for their protection. But the territory was also claimed by Pennsylvania, and both colonies began pushing for action to improve their respective claims.[30] In 1750, Christopher Gist explored the Ohio territory, acting on behalf of both Virginia and the company, and he opened negotiations with the Indian tribes at Logstown.[31] He completed the 1752 Treaty of Logstown in which the local Indians agreed to terms through their ""Half-King"" Tanacharison and an Iroquois representative. These terms included permission to build a strong house at the mouth of the Monongahela River on the modern site of Pittsburgh, Pennsylvania.[32]
 Governor-General of New France Marquis de la Jonquière died on March 17, 1752, and he was temporarily replaced by Charles le Moyne de Longueuil. His permanent replacement was to be the Marquis Duquesne, but he did not arrive in New France until 1752 to take over the post.[33] The continuing British activity in the Ohio territories prompted Longueuil to dispatch another expedition to the area under the command of Charles Michel de Langlade, an officer in the Troupes de la Marine. Langlade was given 300 men, including French-Canadians and warriors of the Ottawa tribe. His objective was to punish the Miami people of Pickawillany for not following Céloron's orders to cease trading with the British. On June 21, the French war party attacked the trading center at Pickawillany, capturing three traders[27] and killing 14 Miami Indians, including Old Briton. He was reportedly ritually cannibalized by some Indians in the expedition party.
 In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier. Céloron, however, had limited the record of French claims to the burial of lead plates, whereas Marin constructed and garrisoned forts. He first constructed Fort Presque Isle on Lake Erie's south shore near Erie, Pennsylvania, and he had a road built to the headwaters of LeBoeuf Creek. He then constructed a second fort at Fort Le Boeuf in Waterford, Pennsylvania, designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson was a chief of the Mingo Indians, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French whom he accused of killing and eating his father. He traveled to Fort Le Boeuf and threatened the French with military action, which Marin contemptuously dismissed.[34]
 The Iroquois sent runners to the manor of William Johnson in upstate New York, who was the British Superintendent for Indian Affairs in the New York region and beyond. Johnson was known to the Iroquois as Warraghiggey, meaning ""he who does great things."" He spoke their languages and had become a respected honorary member of the Iroquois Confederacy in the area, and he was made a colonel of the Iroquois in 1746; he was later commissioned as a colonel of the Western New York Militia.
 The Indian representatives and Johnson met with Governor George Clinton and officials from some of the other American colonies at Albany, New York. Mohawk Chief Hendrick was the speaker of their tribal council, and he insisted that the British abide by their obligations[which?] and block French expansion. Clinton did not respond to his satisfaction, and Hendrick said that the ""Covenant Chain"" was broken, a long-standing friendly relationship between the Iroquois Confederacy and the British Crown.
 Governor Robert Dinwiddie of Virginia was an investor in the Ohio Company, which stood to lose money if the French held their claim.[35] He ordered 21-year-old Major George Washington (whose brother was another Ohio Company investor) of the Virginia Regiment to warn the French to leave Virginia territory in October 1753.[36] Washington left with a small party, picking up Jacob Van Braam as an interpreter, Christopher Gist (a company surveyor working in the area), and a few Mingos led by Tanaghrisson. On December 12, Washington and his men reached Fort Le Boeuf.[37][38]
 Jacques Legardeur de Saint-Pierre succeeded Marin as commander of the French forces after Marin died on October 29, and he invited Washington to dine with him. Over dinner, Washington presented Saint-Pierre with the letter from Dinwiddie demanding an immediate French withdrawal from the Ohio Country. Saint-Pierre said, ""As to the Summons you send me to retire, I do not think myself obliged to obey it.""[39] He told Washington that France's claim to the region was superior to that of the British, since René-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.[40]
 Washington's party left Fort Le Boeuf early on December 16 and arrived in Williamsburg on January 16, 1754. He stated in his report, ""The French had swept south"",[41] detailing the steps which they had taken to fortify the area, and their intention to fortify the confluence of the Allegheny and Monongahela rivers.[42]
 Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point where they began construction of a small stockaded fort in the early months of 1754.[43] Governor Duquesne sent additional French forces under Claude-Pierre Pécaudy de Contrecœur to relieve Saint-Pierre during the same period, and Contrecœur led 500 men south from Fort Venango on April 5, 1754.[44] These forces arrived at the fort on April 16, but Contrecœur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.[45]
 Dinwiddie had ordered Washington to lead a larger force to assist Trent in his work, and Washington learned of Trent's retreat while he was en route.[46] Mingo sachem Tanaghrisson had promised support to the British, so Washington continued toward Fort Duquesne and met with him. He then learned of a French scouting party in the area from a warrior sent by Tanaghrisson, so he added Tanaghrisson's dozen Mingo warriors to his own party. Washington's combined force of 52 ambushed 40 Canadiens (French colonists of New France) on the morning of May 28 in what became known as the Battle of Jumonville Glen.[47] They killed many of the Canadiens, including their commanding officer Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. Historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and to regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.[48] Historians generally consider the Battle of Jumonville Glen as the opening battle of the French and Indian War in North America, and the start of hostilities in the Ohio valley.
 Following the battle, Washington pulled back several miles and established Fort Necessity, which the Canadians attacked under the command of Jumonville's brother at the Battle of Fort Necessity on July 3. Washington surrendered and negotiated a withdrawal under arms. One of his men reported that the Canadian force was accompanied by Shawnee, Delaware, and Mingo warriors—just those whom Tanaghrisson was seeking to influence.[49]
 News of the two battles reached England in August. After several months of negotiations, the government of the Duke of Newcastle decided to send an army expedition the following year to dislodge the French.[50] They chose Major General Edward Braddock to lead the expedition.[51] Word of the British military plans leaked to France well before Braddock's departure for North America. In response, King Louis XV dispatched six regiments to New France under the command of Baron Dieskau in 1755.[52] The British sent out their fleet in February 1755, intending to blockade French ports, but the French fleet had already sailed. Admiral Edward Hawke detached a fast squadron to North America in an attempt to intercept them.
 In a second British action, Admiral Edward Boscawen fired on the French ship Alcide on June 8, 1755, capturing her and two troop ships.[53] The British harassed French shipping throughout 1755, seizing ships and capturing seamen. These actions contributed to the eventual formal declarations of war in spring 1756.[54]
 An early important political response to the opening of hostilities was the convening of the Albany Congress in June and July, 1754.  The goal of the congress was to formalize a unified front in trade and negotiations with the Indians, since the allegiance of the various tribes and nations was seen to be pivotal in the war that was unfolding.  The plan that the delegates agreed to was neither ratified by the colonial legislatures nor approved by the Crown.  Nevertheless, the format of the congress and many specifics of the plan became the prototype for confederation during the War of Independence.
 The British formed an aggressive plan of operations for 1755. General Braddock was to lead the expedition to Fort Duquesne,[55] while Massachusetts governor William Shirley was given the task of fortifying Fort Oswego and attacking Fort Niagara. Sir William Johnson was to capture Fort St. Frédéric at Crown Point, New York,[56] and Lieutenant Colonel Robert Monckton was to capture Fort Beauséjour to the east on the frontier between Nova Scotia and Acadia.[57]
 Braddock led about 1,500 army troops and provincial militia on the Braddock expedition in June 1755 to take Fort Duquesne, with George Washington as one of his aides. The expedition was a disaster. It was attacked by French regulars, Canadian Militiamen, and Indian warriors ambushing them from hiding places up in trees and behind logs, and Braddock called for a retreat. He was killed and approximately 1,000 British soldiers were killed or injured.[55] The remaining 500 British troops retreated to Virginia, led by Washington. Washington and Thomas Gage played key roles in organizing the retreat—two future opponents in the American Revolutionary War.
 The British government initiated a plan to increase their military capability in preparation for war following news of Braddock's defeat and the start of parliament's session in November 1755. Among the early legislative measures were the Recruiting Act 1756,[58] the Commissions to Foreign Protestants Act 1756[59] for the Royal American Regiment, the Navigation Act 1756,[60] and the Continuance of Laws Act 1756.[61] England passed the Naval Prize Act 1756 following the proclamation of war on May 17 to allow the capture of ships and establish privateering.[62]
 The French acquired a copy of the British war plans, including the activities of Shirley and Johnson. Shirley's efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by his inexperience in managing large expeditions. In conjunction, he was made aware that the French were massing for an attack on Fort Oswego in his absence when he planned to attack Fort Niagara. As a response, he left garrisons at Oswego, Fort Bull, and Fort Williams, the last two located on the Oneida Carry between the Mohawk River and Wood Creek at Rome, New York. Supplies were cached at Fort Bull for use in the projected attack on Niagara.
 Johnson's expedition was better organized than Shirley's, which was noticed by New France's governor the Marquis de Vaudreuil. Vaudreuil had been concerned about the extended supply line to the forts on the Ohio, and he had sent Baron Dieskau to lead the defenses at Frontenac against Shirley's expected attack. Vaudreuil saw Johnson as the larger threat and sent Dieskau to Fort St. Frédéric to meet that threat. Dieskau planned to attack the British encampment at Fort Edward at the upper end of navigation on the Hudson River, but Johnson had strongly fortified it, and Dieskau's Indian support was reluctant to attack. The two forces finally met in the bloody Battle of Lake George between Fort Edward and Fort William Henry. The battle ended inconclusively, with both sides withdrawing from the field. Johnson's advance stopped at Fort William Henry, and the French withdrew to Ticonderoga Point, where they began the construction of Fort Carillon (later renamed Fort Ticonderoga after the British captured it in 1759).
 Colonel Monckton captured Fort Beauséjour in June 1755 in the sole British success that year, cutting off the French Fortress Louisbourg from land-based reinforcements. To cut vital supplies to Louisbourg, Nova Scotia's Governor Charles Lawrence ordered the deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians, chasing down many who resisted and sometimes committing atrocities. Cutting off supplies to Louisbourg led to its demise.[63] The Acadian resistance was sometimes quite stiff, in concert with Indian allies including the Mi'kmaq, with ongoing frontier raids against Dartmouth and Lunenburg, among others. The only clashes of any size were at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757, other than the campaigns to expel the Acadians ranging around the Bay of Fundy, on the Petitcodiac and St. John rivers, and Île Saint-Jean.
 Following the death of Braddock, William Shirley assumed command of British forces in North America, and he laid out his plans for 1756 at a meeting in Albany in December 1755. He proposed renewing the efforts to capture Niagara, Crown Point, and Duquesne, with attacks on Fort Frontenac on the north shore of Lake Ontario and an expedition through the wilderness of the Maine district and down the Chaudière River to attack the city of Quebec. His plan, however, got bogged down by disagreements and disputes with others, including William Johnson and New York's Governor Sir Charles Hardy, and consequently gained little support.
 Newcastle replaced him in January 1756 with Lord Loudoun, with Major General James Abercrombie as his second in command. Neither of these men had as much campaign experience as the trio of officers whom France sent to North America.[54] French regular army reinforcements arrived in New France in May 1756, led by Major General Louis-Joseph de Montcalm and seconded by the Chevalier de Lévis and Colonel François-Charles de Bourlamaque, all experienced veterans from the War of the Austrian Succession. On May 17, 1756, Britain formally declared war on France, which expanded the war into Europe and came to be known as the Seven Years' War.
 Governor Vaudreuil had ambitions to become the French commander in chief, in addition to his role as governor, and he acted during the winter of 1756 before those reinforcements arrived. Scouts had reported the weakness of the British supply chain, so he ordered an attack against the forts which Shirley had erected at the Oneida Carry. In the Battle of Fort Bull, French forces destroyed the fort and large quantities of supplies, including 45,000 pounds of gunpowder. They set back any British hopes for campaigns on Lake Ontario and endangered the Oswego garrison, already short on supplies. French forces in the Ohio valley also continued to intrigue with Indians throughout the area, encouraging them to raid frontier settlements. This led to ongoing alarms along the western frontiers, with streams of refugees returning east to get away from the action.
 The new British command was not in place until July. Abercrombie arrived in Albany but refused to take any significant actions until Loudoun approved them, and Montcalm took bold action against his inertia. He built on Vaudreuil's work harassing the Oswego garrison and executed a strategic feint by moving his headquarters to Ticonderoga, as if to presage another attack along Lake George. With Abercrombie pinned down at Albany, Montcalm slipped away and led the successful attack on Oswego in August. In the aftermath, Montcalm and the Indians under his command disagreed about the disposition of prisoners' personal effects. The Europeans did not consider them prizes and prevented the Indians from stripping the prisoners of their valuables, which angered the Indians.
 Loudoun was a capable administrator but a cautious field commander, and he planned one major operation for 1757: an attack on New France's capital of Quebec. He left a sizable force at Fort William Henry to distract Montcalm and began organizing for the expedition to Quebec. He was then ordered to attack Louisbourg first by William Pitt, the Secretary of State responsible for the colonies. The expedition was beset by delays of all kinds but was finally ready to sail from Halifax, Nova Scotia, in early August. In the meantime, French ships had escaped the British blockade of the French coast, and a fleet awaited Loudoun at Louisbourg which outnumbered the British fleet. Faced with this strength, Loudoun returned to New York amid news that a massacre had occurred at Fort William Henry.
 French irregular forces (Canadian scouts and Indians) harassed Fort William Henry throughout the first half of 1757. In January, they ambushed British rangers near Ticonderoga. In February, they launched a raid against the position across the frozen Lake George, destroying storehouses and buildings outside the main fortification. In early August, Montcalm and 7,000 troops besieged the fort, which capitulated with an agreement to withdraw under parole. When the withdrawal began, some of Montcalm's Indian allies attacked the British column because they were angry about the lost opportunity for loot, killing and capturing several hundred men, women, children, and slaves. The aftermath of the siege may have contributed to the transmission of smallpox into remote Indian populations, as some Indians were reported to have traveled from beyond the Mississippi to participate in the campaign and returned afterward. Modern writer William Nester believes that the Indians might have been exposed to European carriers, although no proof exists.[64]
 Vaudreuil and Montcalm were minimally resupplied in 1758, as the British blockade of the French coastline limited French shipping. The situation in New France was further exacerbated by a poor harvest in 1757, a difficult winter, and the allegedly corrupt machinations of François Bigot, the intendant of the territory. His schemes to supply the colony inflated prices and were believed by Montcalm to line his pockets and those of his associates. A massive outbreak of smallpox among western Indian tribes led many of them to stay away from trading in 1758. The disease probably spread through the crowded conditions at William Henry after the battle;[65] yet the Indians blamed the French for bringing ""bad medicine"" as well as denying them prizes at Fort William Henry.
 Montcalm focused his meager resources on the defense of the St. Lawrence, with primary defenses at Carillon, Quebec, and Louisbourg, while Vaudreuil argued unsuccessfully for a continuation of the raiding tactics that had worked quite effectively in previous years.[66] The British failures in North America combined with other failures in the European theater and led to Newcastle's fall from power along with the Duke of Cumberland, his principal military advisor.
 Newcastle and Pitt joined in an uneasy coalition in which Pitt dominated the military planning. He embarked on a plan for the 1758 campaign that was largely developed by Loudoun. He had been replaced by Abercrombie as commander in chief after the failures of 1757. Pitt's plan called for three major offensive actions involving large numbers of regular troops supported by the provincial militias, aimed at capturing the heartlands of New France. Two of the expeditions were successful, with Fort Duquesne and Louisbourg falling to sizable British forces.
 The Forbes Expedition was a British campaign in September–October 1758, with 6,000 troops led by General John Forbes sent to drive out the French from the contested Ohio Country. The French withdrew from Fort Duquesne and left the British in control of the Ohio River Valley.[67] The great French fortress at Louisbourg in Nova Scotia was captured after a siege.[68]
 The third invasion was stopped with the improbable French victory in the Battle of Carillon, in which 3,600 Frenchmen defeated Abercrombie's force of 18,000 regulars, militia, and Indian allies outside the fort which the French called Carillon and the British called Ticonderoga. Abercrombie saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac, including caches of supplies destined for New France's western forts and furs destined for Europe. Abercrombie was recalled and replaced by Jeffery Amherst, victor at Louisbourg.
 The French had generally poor results in 1758 in most theaters of the war. The new foreign minister was the duc de Choiseul, and he decided to focus on an invasion of Britain to draw British resources away from North America and the European mainland. The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France and sent funds to Britain's mainland ally of Prussia, while the French Navy failed in the 1759 naval battles at Lagos and Quiberon Bay. In one piece of good fortune, some French supply ships did manage to depart France and elude the British blockade of the French coast.
 The British proceeded to wage a campaign in the northwest frontier of Canada in an effort to cut off the French frontier forts to the west and south. They captured Ticonderoga and Fort Niagara, and they defeated the French at the Thousand Islands in the summer of 1759. In September 1759, James Wolfe defeated Montcalm in the Battle of the Plains of Abraham which claimed the lives of both commanders. After the battle, the French capitulated the city to the British.
 In April 1760, François Gaston de Lévis led French forces to launch an attack to retake Quebec. Although he won the Battle of Sainte-Foy, Lévis' subsequent siege of Quebec ended in defeat when British ships arrived to relieve the garrison. After Lévis had retreated he was given another blow when a British naval victory at Restigouche brought the loss of French ships meant to resupply his army. In July Jeffrey Amherst then led British forces numbering around 18,000 men in a three pronged attack on Montreal. After eliminating French positions along the way all three forces met up and surrounded Montreal in September. Many Canadians deserted or surrendered their arms to British forces while the Native allies of the French sought peace and neutrality. De Lévis and the Marquis de Vaudreuil reluctantly signed the Articles of Capitulation of Montreal on September 8 which effectively completed the British conquest of New France.
 Most of the fighting ended in America in 1760, although it continued in Europe between France and Britain. The notable exception was the French seizure of St. John's, Newfoundland. General Amherst heard of this surprise action and immediately dispatched troops under his nephew William Amherst, who regained control of Newfoundland after the Battle of Signal Hill in September 1762.[69] Many of the British troops who were stationed in America were reassigned to participate in further British actions in the West Indies, including the capture of Spanish Havana when Spain belatedly entered the conflict on the side of France, and a British expedition against French Martinique in 1762 led by Major General Robert Monckton.[70]
 Governor Vaudreuil in Montreal negotiated a capitulation with General Amherst in September 1760. Amherst granted his requests that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, to own property, and to remain undisturbed in their homes. The British provided medical treatment for the sick and wounded French soldiers, and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.[71]
 General Amherst also oversaw the transfer of French fortifications to British control on the western frontier. The policies which he introduced in those lands disturbed large numbers of Natives and contributed to the outbreak of Pontiac's War in 1763,[72] in which a series of Native attacks on frontier forts occurred, such as that on Fort Miami which effectively brought a nearly half-century long period of European garrisoning at Kekionga to an end. The frontier settlements required the continued deployment of British forces, and the conflict was not fully concluded until 1766.[73]
 Beginning from the 1750s and lasting until the 1760s, a smallpox outbreak devastated several Native communities throughout the American Midwest. The outbreak was brought on in part by victorious Native warriors who had fought on the side of the French bringing home prizes of war which had been infected with the disease; the Ojibwe, Odawa and Potawatomi peoples were most affected by the outbreak. An oral account from Odawa tribal leader and historian Andrew Blackbird claimed that the outbreak had ""entirely depopulated and laid waste"" to Waganagisi, a large Odawa settlement.[74][75]
 The war in North America, along with the global Seven Years' War, officially ended with the signing of the Treaty of Paris on 10 February 1763, by the kingdoms of Great Britain, France and Spain, with Portugal in agreement. The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique, which had been occupied by the British. France chose to cede the former but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area. They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent. French philosopher Voltaire referred to Canada disparagingly as nothing more than a few acres of snow. The British, however, were happy to take New France, as defence of their North American colonies would no longer be an issue (though the absence of that threat caused many colonists to conclude they no longer needed British protection). Britain also had ample places from which to obtain sugar. Spain traded Florida to Britain in order to regain Cuba, but they also gained Louisiana from France, including New Orleans, in compensation for their losses. Great Britain and Spain also agreed that navigation on the Mississippi River was to be open to vessels of all nations.[76]
 The war changed economic, political, governmental, and social relations among the three European powers, their colonies, and the people who inhabited those territories. France and Britain both suffered financially because of the war, with significant long-term consequences.
 Britain gained control of French Canada and Acadia, colonies containing approximately 80,000 primarily French-speaking Roman Catholic residents. The deportation of Acadians beginning in 1755 made land available to immigrants from Europe and migrants from the colonies to the south. The British resettled many Acadians throughout its American provinces, but many went to France and some went to New Orleans, which they expected to remain French. Some were sent to colonize places as diverse as French Guiana and the Falkland Islands, but these efforts were unsuccessful. The Louisiana population contributed to founding the Cajun population. (The French word ""Acadien"" changed to ""Cadien"" then to ""Cajun"".)[77]
 King George III issued the Royal Proclamation of 1763 on October 7, 1763, which outlined the division and administration of the newly conquered territory, and it continues to govern relations to some extent between the government of Canada and the First Nations. Included in its provisions was the reservation of lands west of the Appalachian Mountains to its Indian population,[78] a demarcation that was only a temporary impediment to a rising tide of westward-bound settlers.[79] The proclamation also contained provisions that prevented civic participation by the Roman Catholic Canadians.[80]
 The Quebec Act 1774 addressed issues brought forth by Roman Catholic French Canadians from the 1763 proclamation, and it transferred the Indian Reserve into the Province of Quebec. The Act maintained French Civil law, including the seigneurial system, a medieval code removed from France within a generation by the French Revolution. The Quebec Act was a major concern for the largely Protestant Thirteen Colonies over the advance of ""popery"". It is typically associated with other Intolerable Acts, legislation that eventually led to the American Revolutionary War. The Quebec Act served as the constitutional document for the province of Quebec until it was superseded by the Constitutional Act 1791.
 The Seven Years' War nearly doubled Great Britain's national debt. The Crown sought sources of revenue to pay it off and attempted to impose new taxes on its colonies. These attempts were met with increasingly stiff resistance, until troops were called in to enforce the Crown's authority, and they ultimately led to the start of the American Revolutionary War.[81] France attached comparatively little value to its American possessions, apart from the highly profitable sugar-producing Antilles islands which it retained. Minister Choiseul considered that he had made a good deal at the Treaty of Paris, and Voltaire wrote that Louis XV had lost a few acres of snow.[82] However, the military defeat and the financial burden of the war weakened the French monarchy and contributed to the advent of the French Revolution in 1789.[83]
 The elimination of French power in America meant the disappearance of a strong ally for some Indian tribes.[83] The Ohio Country was now more available to colonial settlement due to the construction of military roads by Braddock and Forbes.[84] The Spanish takeover of the Louisiana territory was not completed until 1769, and it had modest repercussions. The British takeover of Spanish Florida resulted in the westward migration of Indian tribes who did not want to do business with them. This migration also caused a rise in tensions between the Choctaw and the Creek, historic enemies who were competing for land.[85] The change of control in Florida also prompted most of its Spanish Catholic population to leave. Most went to Cuba, although some Christianized Yamasee were resettled to the coast of Mexico.[86]
 France returned to America in 1778 with the establishment of a Franco-American alliance against Great Britain in the American Revolutionary War, in what historian Alfred A. Cave describes as French ""revenge for Montcalm's death"".[87]
"
Virginia House of Burgesses,https://en.wikipedia.org/wiki/Virginia_House_of_Burgesses,"The House of Burgesses (/ˈbɜːrdʒəsɪz/) was the lower house of the Virginia General Assembly from 1619 to 1776. It existed during the colonial history of the United States when Virginia was a British colony. From 1642 to 1776, the House of Burgesses was an important feature of Virginian politics, alongside the Crown-appointed colonial governor and the Virginia Governor's Council, the upper house of the General Assembly.[1]
 When Virginia declared its independence from the Kingdom of Great Britain at the Fifth Virginia Convention in 1776 and became the independent Commonwealth of Virginia, the House of Burgesses was transformed into the House of Delegates, which continues to serve as the lower house of the General Assembly.[2]
 Burgess originally referred to a freeman of a borough, a self-governing town or settlement in England.
 The Colony of Virginia was founded by a joint-stock company, the Virginia Company, as a private venture, though under a royal charter. Early governors provided the stern leadership and harsh judgments required for the colony to survive its early difficulties.[citation needed]
 Early crises with famine, disease, Native American raids, the need to establish cash crops, and lack of skilled or committed labor, meant the colony needed to attract enough new and responsible settlers if it were to grow and prosper.[citation needed]
 To encourage settlers to come to Virginia, in November 1618 the Virginia Company's leaders gave instructions to the new governor, Sir George Yeardley, which became known as ""the great charter.""[3]
 It established that immigrants who paid their own way to Virginia would receive fifty acres of land and not be mere tenants. The civil authority would control the military. In 1619, based on the instructions, Governor Yeardley initiated the election of 22 burgesses by the settlements and Jamestown. They, together with the royally appointed Governor and six-member Council of State, would form the first General Assembly as a unicameral body.[4]
 The governor could veto its actions and the Company still maintained overall control of the venture, but the settlers would have a limited say in the management of their own affairs, including their finances.[4]
 A House of Assembly was created at the same time in Bermuda (which had also been settled by the Virginia Company, and was by then managed by its offshoot, the Somers Isles Company) and held its first session in 1620.[citation needed]
 A handful of Polish craftsmen, brought to the colony to supply skill in the manufacture of pitch, tar, potash, and soap ash, were initially denied full political rights. They downed their tools in protest but returned to work after being declared free and enfranchised, apparently by agreement with the Virginia Company.[5]
 On July 30, 1619, Governor Yeardley convened the Virginia General Assembly as the first representative legislature in the Americas for a six-day meeting at the new timber church on Jamestown Island, Virginia. The unicameral Assembly was composed of the Governor, a Council of State appointed by the Virginia Company, and the 22 locally elected representatives.[6][7]
 The Assembly's first session of July 30, 1619, was cut short by an outbreak of malaria and adjourned after five days.[8] On the third day of the assembly, the assembly's Journal noted ""Mr. Shelley, one of the Burgesses, deceased.""[9] Twenty-two (22) members were sent to the assembly from the following constituencies:[10] 
 The latter two burgesses were excluded from the assembly because John Martin refused to give up a clause in his land patent that exempted his borough ""from any command of the colony except it be aiding and assisting the same against any foreign or domestic enemy.""[11][12]
 Especially after the massacre of almost 400 colonists on March 22, 1622, by Native Americans, and epidemics in the winters before and after the massacre, the governor and council ruled arbitrarily, showing great contempt for the assembly and allowing no dissent.[13]
 By 1624, the royal government in London had heard enough about the problems of the colony and revoked the charter of the Virginia Company. Virginia became a crown colony and the governor and council would be appointed by the Crown. Nonetheless, the Assembly maintained management of local affairs with some informal royal assent, although it was not royally confirmed until 1639.[4]
 In 1634, the General Assembly divided the colony into eight shires (later renamed counties) for purposes of government, administration, and the judicial system. By 1643, the expanding colony had 15 counties. All of the county offices, including a board of commissioners, judges, sheriff, constable, and clerks, were appointed positions. Only the burgesses were elected by a vote of the people. Women had no right to vote. Only free and white men originally were given the right to vote, by 1670 only property owners were allowed to vote.[4]
 In 1642, Governor William Berkeley urged the creation of a bicameral legislature which the Assembly promptly implemented; the House of Burgesses was thus formed and met separately from the Council of State.[14][page needed]
 In 1652, the parliamentary forces of Oliver Cromwell forced the colony to submit to being taken over by the English government. Again, the colonists were able to retain the General Assembly as their governing body. Only taxes agreed to by the assembly were to be levied. Still, most Virginia colonists were loyal to Prince Charles and were pleased with his restoration as King Charles II in 1660. He went on to directly or indirectly restrict some of the liberties of the colonists, such as requiring tobacco to be shipped only to England, only on English ships, with the price set by the English merchant buyers;[15] but the General Assembly remained.[4]
 A majority of the members of the General Assembly of 1676 were supporters of Nathaniel Bacon. They enacted legislation designed to further popular sovereignty and representative government and to equalize opportunities.[16] Bacon took little part in the deliberations since he was busy fighting the Native Americans.[17]
 In 1691, the House of Burgesses abolished the enslavement of Native peoples; however, many Powhatans were held in servitude well into the 18th century.[18]
 The statehouse in Jamestown burned down for the fourth time on October 20, 1698. The General Assembly met temporarily in Middle Plantation, 11 miles (18 km) inland from Jamestown, and then in 1699 permanently moved the capital of the colony to Middle Plantation, which they renamed Williamsburg.[19]
 The French and Indian War in North America from 1754 to 1763 resulted in local colonial losses and economic disruption. Higher taxes were to follow, and adverse local reactions to these and how they were determined would drive events well into the next decade.[20]
 In 1764, desiring revenue from its North American colonies, Parliament passed the first law specifically aimed at raising colonial money for the Crown. The Sugar Act increased duties on non-British goods shipped to the colonies.[21] The same year, the Currency Act prohibited American colonies from issuing their own currency.[22] These angered many American colonists and began colonial opposition with protests. By the end of the year, many colonies were practicing non-importation, a refusal to use imported British goods.[21]
 In 1765, the British Quartering Act, which required the colonies to provide barracks and supplies to British troops, further angered American colonists; and to raise more money for Britain, Parliament enacted the Stamp Act on the American colonies, to tax newspapers, almanacs, pamphlets, broadsides, legal documents, dice, and playing cards.[23] American colonists responded to Parliament's acts with organized protest throughout the colonies. A network of secret organizations known as the Sons of Liberty was created to intimidate the stamp agents collecting the taxes, and before the Stamp Act could take effect, all the appointed stamp agents in the colonies had resigned.[24]  The Massachusetts Assembly suggested a meeting of all colonies to work for the repeal of the Stamp Act, and all but four colonies were represented.[25] The colonists also increased their non-importation efforts,[26][better source needed] and sought to increase in local production.
 In May 1765, Patrick Henry presented a series of resolves that became known as the Virginia Resolves, denouncing the Stamp Act and denying the authority of the British parliament to tax the colonies, since they were not represented by elected members of parliament. Newspapers around the colonies published all his resolves, even the most radical ones which had not been passed by the assembly.[27] The assembly also sent a 1768 Petition, Memorial, and Remonstrance to Parliament.[citation needed]
 From 1769–1775 Thomas Jefferson represented Albemarle County as a delegate in the Virginia House of Burgesses.[28] He pursued reforms to slavery and introduced legislation allowing masters to take control over the emancipation of slaves in 1769, taking discretion away from the royal Governor and General Court. Jefferson persuaded his cousin Richard Bland to spearhead the legislation's passage, but the reaction was strongly negative.[29]
 In 1769 the Virginia House of Burgesses passed several resolutions condemning Britain's stationing troops in Boston following the Massachusetts Circular Letter of the previous year; these resolutions stated that only Virginia's governor and legislature could tax its citizens.[30][page needed] The members also drafted a formal letter to the King, completing it just before the legislature was dissolved by Virginia's royal governor.[31]
 In 1774, after Parliament passed the Boston Port Act to close Boston Harbor, the House of Burgesses adopted resolutions in support of the Boston colonists which resulted in Virginia's royal governor, John Murray, 4th Earl of Dunmore, dissolving the assembly. The burgesses then reassembled on their own and issued calls for the first of five Virginia Conventions. These conventions were essentially meetings of the House of Burgesses without the governor and Council, Peyton Randolph the Speaker of the House would serve as the President of the convention, and they would elect delegates to the Continental Congress.[2] The First Continental Congress passed their Declaration and Resolves, which inter alia claimed that American colonists were equal to all other British citizens, protested against taxation without representation, and stated that Britain could not tax the colonists since they were not represented in Parliament.[32][page needed]
 In 1775 the burgesses, meeting in conventions, listened to Patrick Henry deliver his ""give me liberty or give me death!"" speech and raised regiments. The House of Burgesses was called back by Lord Dunmore one last time in June 1775 to address British Prime Minister Lord North's Conciliatory Resolution. Randolph, who was a delegate to the Continental Congress, returned to Williamsburg to take his place as Speaker. Randolph indicated that the resolution had not been sent to the Congress (it had instead been sent to each colony individually in an attempt to divide them and bypass the Continental Congress). The House of Burgesses rejected the proposal, which was also later rejected by the Continental Congress.[33] The burgesses formed a Committee of Safety to take over governance in the absence of the royal governor, Dunmore, who had organized loyalists forces but after defeats, he took refuge on a British warship.[34]
 In 1776 the House of Burgesses ended. The final entry in the Journals of the House of Burgesses is ""6th of May. 16 Geo. III. 1776 … FINIS.""[35] Edmund Pendleton, a member of the House of Burgesses (and President of the Committee of Safety) who was present at the final meeting, wrote in a letter to Richard Henry Lee on the following day, ""We met in an assembly yesterday and determined not to adjourn, but let that body die."" Later on the same morning, the members of the fifth and final Virginia Revolutionary Convention met in the chamber of the House of Burgesses in Williamsburg and elected Pendleton its president. The convention voted for independence from Britain.[36] The former colony had become the independent Commonwealth of Virginia and the convention created the Constitution of Virginia with a new General Assembly, composed of an elected Senate and an elected House of Delegates. The House of Delegates acceded to the role of the former House of Burgesses.[2]
 In 1619, the General Assembly first met in the church in Jamestown. Subsequent meetings continued to take place in Jamestown.[37]
 In 1700, the seat of the House of Burgesses was moved from Jamestown to Middle Plantation, near what was soon renamed Williamsburg.[38] The Burgesses met there, first (1700 to 1704) in the Great Hall of what is now called the Wren Building at the College of William and Mary, while the Capitol was under construction. When the Capitol burned in 1747, the legislature moved back into the college until the second Capitol was completed in 1754. The present Capitol building at Colonial Williamsburg is a reconstruction of the earlier of the two lost buildings.[citation needed]
 In 1779, and effective in April 1780, the House of Delegates moved the capital city to Richmond during the American Revolutionary War for safety reasons.[39]
 The House of Burgesses became the House of Delegates in 1776, retaining its status as the lower house of the General Assembly, the legislative branch of the Commonwealth of Virginia. Through the General Assembly and House of Burgesses, the Virginia House of Delegates is considered the oldest continuous legislative body in the New World.[40]
 In honor of the original House of Burgesses, every four years, the Virginia General Assembly traditionally leaves the current Capitol in Richmond and meets for one day in the restored Capitol building at Colonial Williamsburg. The most recent commemorative session (the 26th) was held in January 2016.[41][42]
 In January 2007, the Assembly held a special session at Jamestown to mark the 400th anniversary of its founding as part of the Jamestown 2007 celebration, including an address by then-Vice-President Dick Cheney.[43]
 In January 2019, to mark the 400th anniversary of the House of Burgesses, the Virginia House of Representatives Clerk's Office announced a new Database of House Members called ""DOME"" that ""[chronicles] the 9,700-plus men and women who served as burgesses or delegates in the Virginia General Assembly over the past four centuries.""[44][45][46]
"
Commanding General of the United States Army,https://en.wikipedia.org/wiki/Commanding_General_of_the_United_States_Army,"

 The Commanding General of the United States Army was the title given to the service chief and highest-ranking officer of the United States Army (and its predecessor the Continental Army), prior to the establishment of the Chief of Staff of the United States Army in 1903.  During the American Revolutionary War (1775–1783), the title was Commander-in-Chief of the Continental Army.  In 1783, the title was simplified to Senior Officer of the United States Army.  In 1821, the title was changed to Commanding General of the United States Army.  The office was often referred to by various other titles, such as ""Major General Commanding the Army"" or ""General-in-Chief"".
 From 1789 until its abolition in 1903, the position of commanding general was legally subordinate to the United States Secretary of War; (senior member of the President's Cabinet), but was replaced by the creation of the statutory Chief of Staff of the Army by action of the United States Congress in 1903, under the 26th President Theodore Roosevelt (1901–1909).
 † denotes people who died in office.
"
Siege of Boston,https://en.wikipedia.org/wiki/Siege_of_Boston,"

 The siege of Boston (April 19, 1775 – March 17, 1776) was the opening phase of the American Revolutionary War.[5] In the siege, American patriot militia led by newly-installed Continental Army commander George Washington prevented the British Army, which was garrisoned in Boston, from moving by land. Both sides faced resource, supply, and personnel challenges during the siege. British resupply and reinforcement was limited to sea access, which was impeded by American vessels. The British ultimately abandoned Boston after eleven months, moving their troops and equipment north to Nova Scotia. 
 The siege began on April 19 after the Revolutionary War's first battles at Lexington and Concord, when Massachusetts militias blocked land access to Boston. The Continental Congress, meeting in Philadelphia, formed the Continental Army from the militias involved in the fighting and appointed George Washington as commander in chief. In June 1775, the British seized Bunker Hill and Breed's Hill, which Washington and the Continental Army was preparing to bombard, but their casualties were heavy and their gains insufficient to break the Continental Army's control over land to Boston. After this, the Americans laid siege to Boston; no major battles were fought during this time, and the conflict was limited to occasional raids, minor skirmishes, and sniper fire. British efforts to supply their troops were significantly impeded by the smaller but more agile Continental Army and patriot forces that were operating on land and sea. The British suffered from a continual lack of food, fuel, and supplies.
 In November 1775, George Washington sent Henry Knox on a mission to bring the heavy artillery that had recently been captured at Fort Ticonderoga. In a technically complex and demanding operation, Knox brought the cannons to Boston in January 1776, and this artillery fortified Dorchester Heights which overlooked Boston harbor. This development threatened to cut off the British supply lifeline from the sea. British commander William Howe saw his position as indefensible, and he withdrew his forces from Boston to Halifax, Nova Scotia on March 17.
 Before 1775, the British imposed taxes and import duties on the American colonies, to which the Americans objected since they lacked British Parliamentary representation. In response to the Boston Tea Party and other acts of protest, 4,000 British troops were sent to occupy Boston under the command of General Thomas Gage and to pacify the restive Province of Massachusetts Bay.[7] Parliament authorized Gage to disband the government of Massachusetts Bay, led by John Hancock and Samuel Adams, among numerous other powers, but the Americans formed the Massachusetts Provincial Congress and continued to meet. The Provincial Congress called for the organization of local militias and coordinated the accumulation of weapons and other military supplies.[8] Under the terms of the Boston Port Act, Gage closed the Boston port, which caused much unemployment and discontent.[9]
 British forces went to seize military supplies from the town of Concord on April 19, 1775, but militia companies from surrounding towns opposed them at the Battles of Lexington and Concord.[10] At Concord, some of the British forces were routed in a confrontation at the North Bridge. The British troops were then engaged in a running battle during their march back to Boston, suffering heavy casualties.[11]  All of the New England colonies raised militias in response to this alarm and sent them to Boston.[12]
 The British Army order of battle in July 1775 was:[13]
 Immediately after the battles of April 19, the Massachusetts militia formed a siege line extending from Chelsea, around the peninsulas of Boston and Charlestown, to Roxbury, effectively surrounding Boston on three sides. The siege line was under the loose leadership of William Heath, who was superseded by General Artemas Ward late on April 20.[16] They particularly blocked the Charlestown Neck, the only land access to Charlestown, and the Boston Neck, the only land access to Boston, which was then a peninsula, leaving the British in control only of the harbor and sea access.[12]
 The size of the colonial forces grew in the following days, as militias arrived from New Hampshire, Rhode Island, and Connecticut.[12] General Gage wrote of his surprise at the number of Patriots surrounding the city: ""The rebels are not the despicable rabble too many have supposed them to be.... In all their wars against the French they never showed such conduct, attention, and perseverance as they do now.""[17]
 General Gage turned his attention to fortifying easily defensible positions. He ordered lines of defenses with ten 24-pound guns in Roxbury. In Boston proper, four hills were quickly fortified. They were to be the main defense of the city.[18] Over time, each of these hills was strengthened.[19] Gage also decided to abandon Charlestown, removing the beleaguered forces that had retreated from Concord.  The town of Charlestown itself was entirely vacant, and Bunker Hill and Breed's Hill were left undefended, as were the heights of Dorchester, which had a commanding view of the harbor and the city.[20]
 The British at first greatly restricted movement in and out of the city, fearing infiltration of weapons.  Besieged and besiegers eventually reached an informal agreement allowing traffic on the Boston Neck, provided that no firearms were carried.  Residents of Boston turned in almost 2,000 muskets, and most of the Patriot residents left the city.[21]  Many Loyalists who lived outside the city of Boston left their homes and fled into the city. Most of them felt that it was not safe to live outside of the city, because the Patriots were now in control of the countryside.[22]  Some of the men arriving in Boston joined Loyalist regiments attached to the British army.[23]
 The siege did not blockade the harbor and the city remained open for the Royal Navy to bring in supplies from Nova Scotia and other places under Vice Admiral Samuel Graves. Colonial forces could do little to stop these shipments due to the superiority of the British fleet. Nevertheless, American privateers were able to harass supply ships, and food prices rose quickly. Soon the shortages meant that the British forces were on short rations. Generally, the American forces were able to gather information about what was happening in the city from people escaping the privations of Boston, but General Gage had no effective intelligence of American activities.[24]
 On May 3, the Massachusetts Provincial Congress authorized Benedict Arnold to raise forces for taking Fort Ticonderoga near the southern end of Lake Champlain in the Province of New York, which was known to have heavy weapons and only lightly defended.  Arnold arrived in Castleton, New Hampshire on the 9th, where he joined with Ethan Allen and a militia company from Connecticut, all of whom had independently arrived at the idea of taking Ticonderoga. This company captured Fort Ticonderoga and Fort Crown Point under the joint leadership of Arnold and Allen. They also captured the one large military vessel on Lake Champlain in a raid on Fort Saint-Jean.[25]  They recovered more than 180 cannons and other weaponry and supplies that the Continental Army used to tighten their grip on Boston.[26]
 Boston lacked a regular supply of fresh meat, and many horses needed hay. On May 21, Gage ordered a party to go to Grape Island in the outer harbor and bring hay to Boston.[27] The Continentals on the mainland noticed this and called out the militia. As the British party arrived, they came under fire from the militia. The militia set fire to a barn on the island, destroying 80 tons of hay and preventing the British from taking more than three tons.[27]
 Continental forces worked to clear the harbor islands of livestock and supplies useful to the British. The Royal Marines attempted to stop removal of livestock from some of the islands on May 27 in the Battle of Chelsea Creek. The Americans resisted and the British schooner Diana ran aground and was destroyed.[28] Gage issued a proclamation on June 12 offering to pardon all of those who would lay down their arms, with the exception of John Hancock and Samuel Adams,[29][30] but this merely ignited anger among the Patriots, and more people began to take up arms.[29]
 The British had been receiving reinforcements throughout May until they reached a strength of about 6,000 men.  On May 25, generals William Howe, John Burgoyne, and Henry Clinton arrived on HMS Cerberus, and Gage began planning to break out of the city.[28]
 The British plan was to fortify Bunker Hill and Dorchester Heights. They fixed the date for taking Dorchester Heights at June 18, but the colonists' committee of safety learned of the British plans on June 15. In response, they sent instructions to General Ward to fortify Bunker Hill and the heights of Charlestown, and he ordered Colonel William Prescott to do so.  On the night of June 16, Prescott led 1,200 men over the Charlestown Neck and constructed fortifications on Bunker Hill and Breed's Hill.[31]
 British forces under General Howe took the Charlestown peninsula on June 17 in the Battle of Bunker Hill.[32] The British succeeded in their tactical objective of taking the high ground on the Charlestown peninsula, but they suffered significant losses with some 1,000 men killed or wounded, including 92 officers killed. The British losses were so heavy that there were no further direct attacks on American forces.[33]  The Americans lost the battle but had again stood against the British regulars with some success, as they successfully repelled two assaults on Breed's Hill during the engagement.[34]
 General George Washington arrived at Cambridge on July 2. He set up his headquarters at the Benjamin Wadsworth House at Harvard College[35] and took command of the newly formed Continental Army the following day. By this time, forces and supplies were arriving, including four independent companies of riflemen (not part of any state line) from Maryland and Virginia.[36][37] Washington began the work of molding the militias into an army, appointing senior officers (the militias had typically elected their leaders), and introducing more organization and disciplinary measures.[38]
 Washington required officers of different ranks to wear differentiating apparel so that they might be distinguished from their underlings and superiors.[39] On July 16, he moved his headquarters to the John Vassall House in Cambridge, that became well known as the home of Henry Wadsworth Longfellow. Toward the end of July, about 2,000 riflemen arrived in units raised in Pennsylvania, Maryland, and Virginia by Congressional mandate.[40] The accuracy of the rifle was previously unknown in New England, and these forces were used to harass the besieged forces.[41]
 Washington also ordered the defenses to be improved, so the army dug trenches on Boston Neck and then extended toward Boston. However, these activities had little effect on the British occupation.[42]  The working parties were fired on from time to time, as were sentries guarding the works. The British pushed back an American advanced guard on July 30, and they burned a few houses in Roxbury.[43] An American rifleman was killed on August 2, and the British hung his body by the neck. In retaliation, American riflemen marched to the lines and began to attack the British troops. They continued their sharpshooting all day, killing and wounding many of the British while losing only one Patriot.[44]
 On August 30, the British made a surprise breakout from Boston Neck, set fire to a tavern, and withdrew to their defenses.[44] On the same night, 300 Americans attacked Lighthouse Island and burned the lighthouse, killing several British soldiers and capturing 23, with the loss of only one American.[44] On another August night, Washington sent 1,200 men to dig entrenchments on a hill near the Charlestown Neck. Despite a British bombardment, the Americans successfully dug the trenches.[45]
 In early September, Washington began drawing up plans for two moves: to dispatch 1,000 men from Boston to invade Quebec, and to launch an attack on Boston.[46] He felt that he could afford to send some troops to Quebec, as he had received intelligence from British deserters and American spies that the British had no intention of launching an attack from Boston until they were reinforced.[47]  On September 11, about 1,100 troops under the command of Benedict Arnold left for Quebec.[48]  Washington summoned a council of war and made a case for an amphibious assault on Boston by sending troops across Back Bay in flat-bottomed boats which could hold 50 men each.[49] He believed that it would be extremely difficult to keep the men together when winter came. His war council unanimously rejected the plan, and the decision was not to attack ""for the present at least"".[49]
 In early September, Washington authorized the appropriation and outfitting of local fishing vessels for intelligence-gathering and interdiction of supplies to the British. This activity was a precursor to the Continental Navy, which was established in the aftermath of the British Burning of Falmouth in Portland, Maine. The provincial assemblies of Connecticut and Rhode Island began arming ships and authorized privateering.[50]
 In early November, 400 British soldiers went to Lechmere's Point on a raiding expedition to acquire some livestock. They made off with 10 head of cattle but lost two lives in the skirmish with colonial troops sent to defend the point.[51][52]  On November 29, colonial Captain John Manley commanding the schooner Lee captured one of the most valuable prizes of the siege: the British brigantine Nancy just outside Boston Harbor. She was carrying a large supply of ordnance and military stores intended for the British troops in Boston.[53]
 As winter approached, the Americans were so short of gunpowder that some of the soldiers were given spears instead of guns.[54] Many of the American troops remained unpaid and many of their enlistments were set to expire at the end of the year. 
 Howe had replaced Gage in October as commander of the British forces, and was faced with different problems. Firewood was so scarce that British soldiers resorted to cutting down trees and tearing down wooden buildings, including the Old North Meeting House.[55] The city had become increasingly difficult because of winter storms and the rise in American privateers.[54] An improvised American war fleet of about 12 converted merchant ships captured 55 British ships over the course of the winter. Many of the captured ships had been carrying food and supplies to the British troops.[56] The British troops were so hungry that many were ready to desert as soon as they could, and scurvy and smallpox had broken out in the city.[57] Washington's army faced similar problems with smallpox, as soldiers from rural communities were exposed to the disease. He moved infected troops to a separate hospital, the only option available given the public stigma against inoculation.[58]
 Washington again proposed to assault Boston in October, but his officers thought it best to wait until the harbor had frozen over.[59] In February, the water froze between Roxbury and Boston Common, and Washington thought that he would try an assault by rushing across the ice in spite of his shortage in powder; but his officers again advised against it. Washington's desire to launch an attack on Boston arose from his fear that his army would desert in the winter, and he knew that Howe could easily break the lines of his army in its present condition. He abandoned an attack across the ice with great reluctance in exchange for a more cautious plan of fortifying Dorchester Heights using cannon arrived from Fort Ticonderoga.[60][61]
 British major general, Henry Clinton, and a small fleet set sail for the Carolinas in mid-January with 1,500 men. Their objective was to join forces with additional troops arriving from Europe, and to take a port in the southern colonies for further military operations.[62] In early February, a British raiding party crossed the ice and burned several farmhouses in Dorchester.[63]
 Between November 1775 and February 1776, Colonel Henry Knox and a team of engineers used sledges to retrieve 60 tons of heavy artillery that had been captured at Fort Ticonderoga, bringing them across the frozen Hudson and Connecticut rivers in a difficult, complex operation. They arrived back at Cambridge on January 24, 1776.[64]
 Some of the Ticonderoga cannons were of a size and range not previously available to the Americans. They were placed in fortifications around the city, and the Americans began to bombard the city on the night of March 2, 1776, to which the British responded with cannonades of their own.[65] The American guns under the direction of Colonel Knox continued to exchange fire with the British until March 4. The exchange of fire did little damage to either side, although it did damage houses and kill some British soldiers in Boston.[66]
 On March 5, Washington moved more of the Ticonderoga cannon and several thousand men overnight to occupy Dorchester Heights, overlooking Boston. The ground was frozen, which made it impossible to dig trenches, so Rufus Putnam developed a plan to fortify the heights using defenses made of heavy timbers and fascines.[67]   These were prefabricated out of sight of the British and brought in overnight.[67][68][69][70]  General Howe is said to have exclaimed, ""My God, these fellows have done more work in one night than I could make my army do in three months.""[71][67] The British fleet was within range of the American guns on Dorchester Heights, putting it and the troops in the city at risk.[72]
 The immediate response of the British was a two-hour cannon barrage at the heights, which had no effect because the British guns could not reach the American guns.[73] After the failure of the barrage, Howe and his officers agreed that the colonists must be removed from the heights if they were to hold Boston. They planned an assault on the heights, but the attack never took place because of a storm, and the British elected instead to withdraw.[74]
 On March 8, some prominent Bostonians sent a letter to Washington, stating that the British would not destroy the town if they were allowed to depart unmolested. Washington formally rejected the letter, as it was not addressed to him by either name or title.[75] However, the letter had the intended effect; when the evacuation began, there was no American fire to hinder the British departure.  On March 9, the British saw movement on Nook's Hill in Dorchester and opened a massive artillery barrage that lasted all night. It killed four men with one cannonball, but that was all the damage that was done.[76] The next day, the colonists went out and collected the 700 cannonballs that had been fired at them.[76]
 On March 10, 1776, General Howe issued a proclamation ordering the inhabitants of Boston to give up all linen and woolen goods that could be used by the colonists to continue the war. Loyalist Crean Brush was authorized to receive these goods, in return for which he gave certificates that were effectively worthless.[77] Over the next week, the British fleet sat in Boston harbor waiting for favorable winds, while Loyalists and British soldiers were loaded onto the ships. During this time, American naval vessels outside the harbor successfully captured several British supply ships.[78]
 On March 15, the wind became favorable for the British, but it turned against them before they could leave. On March 17, the wind once again turned favorable. The troops were authorized to burn the town if there were any disturbances while they were marching to their ships;[77] they began to move out at 4:00 a.m. By 9:00 a.m., all ships were underway.[79] The fleet departing from Boston included 120 ships, with more than 11,000 people on board. Of those, 9,906 were British troops, 667 were women, and 553 were children.[80]
 Once the British fleet sailed away, the Americans moved to reclaim Boston and Charlestown. At first, they thought that the British were still on Bunker Hill, but it turned out that the British had left dummies in place.[80] Initially, Artemas Ward led a troop of men into Boston who had already been exposed to smallpox, out of fear that others might be exposed who had no resistance to the disease.  More of the colonial army entered on March 20, 1776, once the risk of disease was judged to be low.[81] Washington had not hindered the British departure from the city by land, but he did not make their escape easy from the outer harbor. He directed Captain Manley to harass the departing British fleet, in which he had some success, capturing the ship carrying Crean Brush and his plunder, among other prizes.[82]
 General Howe left in his wake a small contingent of vessels whose primary purpose was to intercept any arriving British vessels. They redirected numerous ships to Halifax that were carrying British troops originally destined for Boston. Some unsuspecting British troop ships landed in Boston, only to fall into American hands.[83]
 The British departure ended major military activities in the New England colonies. Washington feared that the British were going to attack New York City and so departed on April 4 with his army for Manhattan, beginning the New York and New Jersey campaign.[84]
 There are six units of the Army National Guard derived from American units that participated in the siege of Boston: 101st Eng Bn,[85] 125th MP Co,[86] 181st Inf,[87] 182nd Inf,[88] 197th FA,[89] and 201st FA.[90] There are 30 units in the U.S. Army with lineages that go back to the colonial era.
 ""Had Sir William Howe fortified the hills round Boston, he could not have been disgracefully driven from it,"" wrote his replacement Sir Henry Clinton.[91] General Howe was severely criticized in the British press and Parliament for his failures in the Boston campaign, but he remained in command for another two years for the New York and New Jersey campaign and the Philadelphia campaign. General Gage never received another combat command. General Burgoyne saw action in the Saratoga campaign, a disaster that resulted in the capture of Burgoyne and 7,500 troops under his command. General Clinton commanded the British forces in America for four years (1778–1782).[92]
 Many Massachusetts Loyalists left with the British when they evacuated Boston. Some went to England to rebuild lives there, and some returned to America after the war. Many went to Saint John, New Brunswick.[93]
 Following the siege, Boston ceased to be a military target but continued to be a focal point for revolutionary activities, with its port acting as an important point for fitting ships of war and privateers. Its leading citizens had important roles in the development of the United States.[94] Boston and other area communities mark March 17 as Evacuation Day.
"
New York and New Jersey campaign,https://en.wikipedia.org/wiki/New_York_and_New_Jersey_campaign,"The New York and New Jersey campaign in 1776 and the winter months of 1777 was a series of American Revolutionary War battles for control of the Port of New York and the state of New Jersey, fought between British forces under General Sir William Howe and the Continental Army under General George Washington.  Howe was successful in driving Washington out of New York, but overextended his reach into New Jersey, and ended the New York and New Jersey campaign in January 1777 with only a few outposts near New York City under British control. The British held New York Harbor for the rest of the Revolutionary War, using it as a base for expeditions against other targets.
 Landing unopposed on Staten Island on July 3, 1776, Howe had assembled an army that included components that had withdrawn from Boston in March following the British failure to hold that city, combined with additional British troops, and Hessian troops hired from several German principalities. Washington's Continental Army included New England soldiers and regiments from the Thirteen Colonies as far south as the Colony of Virginia. Landing on Long Island in August, Howe defeated Washington in the largest battle of the war in North America, but the Continental Army was able to regroup and make an orderly and covert retreat to Manhattan that night under a cover of darkness and fog. Washington suffered a series of further defeats in Manhattan but prevailed in a skirmish at the Battle of Harlem Heights and eventually withdrew his troops successfully to White Plains, New York. Howe, meanwhile, returned to Manhattan and captured those forces Washington had left on the island.
 Washington and much of his army crossed the Hudson River to Rockland County and then south into New Jersey, retreated across the state, and then crossed the Delaware River into Pennsylvania. Along the way, his army shrunk due to the ending of enlistment periods, desertions, and poor morale. Howe ordered his troops into winter quarters in December, establishing a chain of outposts from New York City to Burlington, New Jersey.  Washington, in a tremendous boost to American morale, launched a successful strike against the Trenton garrison on the morning of December 26, 1776, prompting Howe to withdraw his chain of outposts back to New Brunswick and the coast near New York. Washington, in turn, established his winter camp at Morristown.  During the following winter months and through the rest of the war, both sides skirmished frequently around New York City and New Jersey as the British sought forage and provisions.
 Britain maintained control of New York City and some of the surrounding territory until the war ended in 1783, using it as a base for operations elsewhere in North America.  In 1777, General Howe launched a campaign to capture the revolutionary capital of Philadelphia, leaving General Sir Henry Clinton in command of the New York area, while General John Burgoyne led an attempt to gain control of the Hudson River valley, moving south from Quebec and being defeated at Saratoga.
 When the American Revolutionary War broke out in April 1775, British troops were under siege in Boston. They defeated Patriot forces in the Battle of Bunker Hill, suffering very high casualties.  When news of this expensive British victory reached London, General William Howe and Lord George Germain, the British official responsible, determined that a ""decisive action"" should be taken against New York City using forces recruited from throughout the British Empire as well as troops hired from small German states.[4]
 Washington, who was named commander-in-chief of the Continental Army by the Second Continental Congress in Philadelphia, echoed the sentiments of other in the Congress that New York was ""a post of infinite importance"",[5] and began the task of organizing military companies in the New York area when he stopped there on his way from Philadelphia to take command of the siege of Boston.[6] In January 1776, Washington ordered Charles Lee to raise troops and take command of New York's defenses.[7] Lee made some progress on the city's defenses when word arrived in late March that the British army had left Boston after Washington threatened them from heights south of the city. Concerned that General Howe was sailing directly to New York, Washington hurried regiments from Boston, including General Israel Putnam, who commanded the troops until Washington himself arrived in mid-April.[8] At the end of April, Washington dispatched General John Sullivan with six regiments to the north to bolster the faltering Quebec campaign.[9]
 General Howe, rather than moving against New York, withdrew his army to Halifax in Nova Scotia, and regrouped while transports full of British troops, shipped from bases around Europe and intended for New York, began gathering at Halifax. In June, Howe set sail for New York with the 9,000 men assembled there, before all of the transports arrived.[10] German troops, primarily from Hesse-Kassel, and British troops from Henry Clinton's ultimately unsuccessful expedition to the Carolinas, were supposed to meet with Howe's fleet when it reached New York. General Howe's brother, Admiral Lord Howe, arrived at Halifax with further transports after the general sailed, and immediately followed.[10]
 When General Howe arrived in the outer harbor of New York, the ships began sailing up the undefended Narrows between Staten Island and Long Island on July 2, and started landing troops on the undefended shores of Staten Island. Washington learned from prisoners taken that Howe had landed 10,000 men, but was awaiting the arrival of another 15,000.[11] General Washington, with a smaller army of about 19,000 effective troops, lacked significant military intelligence on the British force and plans, and was uncertain exactly where in the New York area the Howes intended to strike. He consequently split the Continental Army between fortified positions on Long Island, Manhattan, and mainland locations,[12] and also established a Flying Camp in northern New Jersey that was intended as a reserve force that could support operations anywhere along the New Jersey side of the Hudson River.[13]
 The Howe brothers had been granted authority as peace commissioners by the British Parliament with limited powers to pursue a peaceful resolution to the conflict. King George III was not optimistic about the possibility for peace, however, saying, ""yet I think it right to be attempted, whilst every act of vigour is unremittingly carried on"".[14]  Their powers were limited to granting of ""general and special pardons"" and to ""confer with any of his Majesty's subjects"".[14]  On July 14, pursuant to these powers, Admiral Howe sent a messenger with a letter addressed to ""George Washington, Esq."" across the harbor.[15]  Washington's adjutant, Joseph Reed, politely informed the messenger that no person with that title was in their army.  Admiral Howe's aide wrote that ""the Punctilio of an Address"" should not have prevented the letter's delivery, and Howe was said to be visibly annoyed by the rejection.[16]  A second request, addressed to ""George Washington, Esq., etc."" was similarly rejected, although the messenger was told that Washington would receive one of Howe's adjutants.[16]  In that fruitless meeting, held July 20, Washington pointed out that the limited powers the Howe brothers had been given were not of much use, as the rebels had done no wrong requiring an amnesty.[16]
 In late August, the British transported about 22,000 men, including 9,000 Hessians, from Staten Island to Long Island. In the Battle of Long Island on August 27, the British outflanked the American positions, driving the Americans back to their Brooklyn Heights fortifications. General Howe then began to lay siege to the works, but Washington skillfully managed a nighttime retreat through his unguarded rear across the East River to the island of Manhattan. Howe then paused to consolidate his position and consider his next move.[17]
 During the Battle of Long Island, the British captured General John Sullivan. Admiral Howe convinced him to deliver a message to Congress in Philadelphia, and released him on parole.  Washington also gave his permission, and on September 2 Sullivan told the Congress that the Howes wanted to negotiate, and had been given much broader powers to treat than those they actually held.  This created a diplomatic problem for Congress, which did not want to be seen as aggressive, which is how some representatives felt a direct rejection of the appeal would appear.[18]  Consequently, Congress agreed to send a committee to meet with the Howes in a move they did not think would bear any fruit.  On September 11, the Howe brothers met with John Adams, Benjamin Franklin, and Edward Rutledge in the Staten Island Peace Conference.  The positions expressed by the two groups in the three-hour meeting were irreconcilable.[19]
 Washington, who had previously been ordered by the Continental Congress to hold New York City, was concerned that he might have escaped one trap for another, since the army was still vulnerable to being surrounded on Manhattan.  To keep his escape routes open to the north, he placed 5,000 troops in the city (which then only occupied the lower portion of Manhattan), and took the rest of the army to Harlem Heights.  In the first recorded use of a submarine in warfare, he also attempted a novel attack on the Royal Navy, launching Turtle in a failed attempt to sink HMS Eagle, Admiral Howe's flagship.[20]
 On September 15, General Howe landed about 12,000 men on Lower Manhattan, quickly taking control of New York City. The Americans withdrew to Harlem, where they skirmished the next day, but held their ground.[21] Rather than attempting to dislodge Washington from his strong position a second time, Howe again opted for a flanking maneuver.  Landing troops with some opposition in October in Westchester County across the Harlem River and north of Manhattan, he sought once again to encircle Washington.  To defend against this move, Washington withdrew most of his army to White Plains, where after a short battle on October 28 he retreated further north. The retreat of Washington's forces was aided by a dense fog which concealed their movement to the British troops.  This isolated the remaining Continental Army troops in upper Manhattan, so Howe returned to Manhattan and captured Fort Washington in mid-November, taking almost 3,000 prisoners.
 Four days later, November 20, Fort Lee, across the Hudson River from Fort Washington, was also taken.  Washington brought much of his army across the Hudson into New Jersey, but was immediately forced to retreat by the aggressive British advance.[22]
 General Howe, after consolidating British positions around New York harbor, detached 6,000 men under the command of two of his more difficult subordinates, Henry Clinton, and Hugh, Earl Percy to take Newport, Rhode Island and its strategic port east across Long Island Sound (which they did without opposition on December 8),[23] while he sent General Lord Cornwallis to chase Washington's army through New Jersey. The Americans withdrew across the Delaware River into Pennsylvania in early December.[24]
 The outlook of the Continental Army—and thus the revolution itself—was bleak. ""These are the times that try men's souls"", wrote Thomas Paine in The American Crisis.[25] Washington's army had dwindled to fewer than 5,000 men fit for duty and would be significantly reduced further after enlistments expired at the end of the year.[26] Spirits were low, popular support was wavering, and Congress had abandoned Philadelphia, fearing a British attack.[27]  Washington ordered some of the troops that returned from the failed invasion of Quebec to join him, and also ordered General Lee's troops, which he had left north of New York City, to join him.[28]  Lee, whose relationship with Washington was at times difficult, made excuses and only traveled as far as Morristown, New Jersey.  When Lee strayed too far from his army on December 12, his exposed position was betrayed by Loyalists, and a British company led by Lieutenant Colonel Banastre Tarleton surrounded the inn where he was staying and took him prisoner.  Lee's command was taken over by John Sullivan, who finished marching the army to Washington's camp across the river from Trenton.[29]
 The capture of Lee presented the Howes with a problematic prisoner.  As with a number of other Continental Army leaders, he had previously served in the British Army. Because of this the Howes at first treated him as a deserter, with threats of military punishment.  However, Washington intervened, tying the treatment of Lee to the treatment of prisoners held by the Americans.  Lee was ultimately treated well, and apparently offered the British commanders advice on how to win the war.[30]  Because the Americans did not have a prisoner of comparable rank, Lee remained a prisoner in New York until 1778, when he was exchanged for Richard Prescott.[31]
 The failure of the Continental Army to hold New York also brought about a rise in Loyalist activity, as the city became a haven for refugee supporters of the Crown from elsewhere across the region.  The British therefore actively recruited in New York and New Jersey to build regiments of provincial militia, with some success.  Loyalists in these areas may have been motivated by seeing elements of the rebel army head home after their enlistments ended.[32]  One New York Patriot militia leader wrote that thirty of his men, rather than reenlisting with him, had instead signed up with the enemy.[33]  On November 30 Admiral Howe offered amnesty to anyone that had taken up arms against the Crown, provided they swore an oath to it.  Washington responded with his own proclamation suggesting that those who did not renounce such oaths should immediately go behind British lines.[34]  As a result, New Jersey became a civil battlefield, with militia activity as well as spying and counterspying continuing for the rest of the war.[35]
 News of the capture of New York was favorably received in London, and General Howe was awarded the Order of the Bath for his work.[36]  Combined with news of the recovery of Quebec, circumstances suggested to British leaders that the war could be ended with one more year's campaigning.[37]  News of Admiral Howe's amnesty proclamation was met with some surprise, as its terms were more lenient than the hardliners in the government expected. Politicians opposed to the war pointed out that the proclamation failed to mention the primacy of the Parliament.  Furthermore, the Howes were criticized for failing to keep Parliament informed of the various peace efforts they embarked on.[38]
 With the campaign at an apparent conclusion for the season, the British established a chain of outposts in New Jersey stretching from Perth Amboy to Bordentown, and entered winter quarters. They controlled New York harbor and much of New Jersey, and were in a good position to resume operations in the spring, with the rebel capital of Philadelphia in striking distance.[24]  Howe detached General Clinton with 6,000 additional men to occupy Newport as a base for future operations against Boston and Connecticut.[40]  Howe then sketched a campaign for the following year in a letter to Lord Germain: 10,000 men at Newport, 10,000 for an expedition to Albany (to meet an army descending from Quebec), 8,000 to cross New Jersey and threaten Philadelphia, and 5,000 to defend New York.  If additional foreign forces were available, operations could also be considered against the southern states.[41]
 While worrying over how to hold his army together, Washington organized attacks on the relatively exposed British outposts, which were as a result continually on edge due to ongoing militia and army raids.  German commanders Carl von Donop and Johann Rall, whose brigades were at the end of the chain of outposts, were frequent targets of these raids, but their repeated warnings and requests for support from General James Grant were dismissed.[42]
 Beginning in mid-December 1776, Washington planned a two-pronged attack on Rall's outpost in Trenton, with a third diversionary attack on Donop's outpost in Bordentown.  The plan was aided by the fortuitous presence of a militia company that drew Donop's entire 2,000-man force away from Bordentown to the south that resulted in a skirmish at Mount Holly on December 23.  The consequence of this action was that Donop was not in a position to assist Rall when Washington's attack on Trenton took place.[43]  Washington and 2,400 men stealthily crossed the Delaware River and surprised Rall's outpost on the morning of December 26 in a street-to-street battle, killing or capturing nearly 1,000 Hessians.  This action not only significantly boosted the army's morale; it also brought Cornwallis out of New York.  He reassembled an army of more than 6,000 men and marched most of them against a position Washington had taken south of Trenton. Leaving a garrison of 1,200 at Princeton, Cornwallis then attacked Washington's position on January 2, 1777, and was three times repulsed before darkness set in.[44]  During the night Washington once again stealthily moved his army, going around that of Cornwallis with the intention of attacking the Princeton garrison.[45]
 On January 3, Hugh Mercer, leading the American advance guard, encountered British soldiers from Princeton under the command of Charles Mawhood. The British troops engaged Mercer and in the ensuing battle, Mercer was mortally wounded. Washington sent reinforcements under General John Cadwalader, which were successful in driving Mawhood and the British from Princeton, with many of them fleeing to Cornwallis in Trenton. The British lost more than one quarter of their force in the battle, and American morale further rose with the victory.[46] This period, from December 25, 1776, through January 3, 1777, has become known as the Ten Crucial Days.[47]
 The defeats convinced General Howe to withdraw most of his army from New Jersey, only leaving outposts at New Brunswick and Perth Amboy.  Washington entered winter quarters at Morristown, having retaken most of the state from the British.  However, provisions for both armies were limited, and commanders on both sides sent out parties to forage for food and other supplies.  For the next few months, they engaged in a forage war, in which each targeted the foraging parties of the other.  This led to numerous skirmishes and minor confrontations including the Battle of Millstone.  The British also sniped with each other over the subject of provisions.  Lord Percy resigned his command after a series of disagreements with Howe came to a head over the ability of the Newport station to provide forage to the New York and New Jersey forces.[48]
 The British gained control of New York Harbor and the surrounding agricultural areas, and held New York City and Long Island until the war ended in 1783.[49][50] The Americans suffered significant casualties and lost important supplies, but Washington managed to retain the core of his army and avoid a decisive confrontation that could have ended the war.  With the bold strokes of Trenton and Princeton, he had regained initiative and boosted morale.[51] The areas around New York City in New York, New Jersey, and Connecticut were an ongoing battleground for the rest of the war.[52]
 The early reports that General Howe sent to his superiors in London concerning the battles at Trenton and Princeton attempted to minimize their significance, blaming Rall for Trenton, and trying to recast Princeton as a nearly successful defense.  Not everyone was fooled by his accounts, particularly not Lord Germain.  In a letter to the Hessian General Leopold Philip von Heister Germain wrote that ""the officer who commanded [the forces at Trenton] and to whom this misfortune is to be attributed has lost his life by his rashness.""[53]  Heister in turn had to report the loss to his ruler, Frederick II, Landgrave of Hesse-Kassel, with the news that not only had an entire brigade been lost, but sixteen regimental colors and six cannon as well.  The news reportedly enraged Frederick, who broadly suggested that Heister return home (which he did, turning over command of the Hessian forces to Wilhelm von Knyphausen).[54] Frederick also ordered extensive inquiries into the events of 1776, that took place in New York from 1778 to 1782.  These inquiries created a unique archive of materials about the campaign.[55]
 The news of Washington's successes reached Paris at a critical time.  Britain's ambassador to France, Lord Stormont, was preparing complaints to France's foreign minister, the Comte de Vergennes, concerning the semi-secret financial and logistical support France had been giving to the revolutionaries.  Stormont had learned that supplies bound for America were to be shipped under French flags, where they had previously sent under American colors.  He wrote that the French court was extremely happy with the news, and that the French diplomatic position noticeably hardened: ""that M. de Vergennes is hostile in his heart and anxious for the success of the Rebels I have not a shadow of a doubt.""[56]
 The British planned two major operations for the 1777 campaign season. The first was an ambitious plan to gain control of the Hudson River valley, whose central thrust was a move along Lake Champlain by the army from Quebec under General John Burgoyne.  Execution of this plan ultimately failed, ending with the surrender of Burgoyne's army at Saratoga, New York, in October.  The second operation was General Howe's plan to take Philadelphia, which, after a difficult start, met with success in September.[57]
 Washington's strategy in 1777 continued to be a basically defensive one.  He successfully fended off an attempt by Howe to draw him into a general engagement in northern New Jersey, but was unable to prevent Howe's later success taking Philadelphia.[57]  He did send material help to General Horatio Gates, who was tasked with defending against Burgoyne's movements.[58]  Major General Benedict Arnold and Daniel Morgan's riflemen all played a notable role in the defeat of Burgoyne, following which France entered the war.[59]
 In the urban environments of Manhattan, Brooklyn, The Bronx, Westchester and Trenton there are plaques and other memorials placed to commemorate the actions that took place in and around those locations.[60][61]  The Princeton Battlefield and Washington's Crossing are National Historic Landmarks, with state parks also preserving all or part of the locations where events of this campaign occurred in those areas.[62][63] Morristown National Historical Park preserves locations occupied by the Continental Army during the winter months at the end of the campaign.[64]
 
"
George Washington's crossing of the Delaware River,https://en.wikipedia.org/wiki/George_Washington%27s_crossing_of_the_Delaware_River,"
 George Washington's crossing of the Delaware River, which occurred on the night of December 25–26, 1776, during the American Revolutionary War, was the first move in a complex and surprise military maneuver organized by George Washington, the commander-in-chief of the Continental Army, which culminated in their attack on Hessian forces garrisoned at Trenton. The Hessians were German mercenaries hired by the British.
 Washington and his troops successfully attacked the Hessian forces in the Battle of Trenton on the morning of December 26, 1776. The military campaign was organized in great secrecy by Washington, who led a column of Continental Army troops from today's Bucks County, Pennsylvania across the icy Delaware River to today's Mercer County, New Jersey in what was one of the Revolutionary War's most logistically challenging and dangerous clandestine operations.
 Other planned crossings in support of the operation were either called off or ineffective, but this did not prevent Washington from surprising and defeating the Hessian troops encamped in Trenton under the command of Johann Rall. After prevailing in the Battle of Trenton, Washington and his Continental Army troops crossed the Delaware River again, returning to Pennsylvania west-bound with Hessian prisoners and military stores taken in the battle.
 Washington's army then crossed the Delaware River a third time at the end of 1776 under difficult circumstances by the uncertain thickness of the ice on the river. They defeated British reinforcements under Lord Cornwallis at Trenton on January 2, 1777, and were also triumphant over his rear guard at Princeton the following day prior to retreating to his winter quarters in Morristown, New Jersey.
 As a celebrated location and development in the ultimately victorious Revolutionary War, the unincorporated communities of Washington Crossing, Pennsylvania and Washington Crossing, New Jersey are both presently named in honor of Washington and the logistically complicated covert crossing of Delaware River.
 Although 1776 started well for Washington and the Continental Army with the evacuation of British troops from Boston in March, attempts to defend New York City from the British were unsuccessful. British General William Howe and his troops landed on Long Island in August and pushed Washington's Continental Army completely out of New York by mid-November, when he captured the remaining troops on Manhattan.[1]
 The main force of British troops returned to New York for the winter season, and they left their allied Hessian troops in New Jersey under the command of Colonels Rall and Von Donop. Both colonels were ordered to form small outposts in and around Trenton.[2] Howe then sent troops under the command of Charles Cornwallis across the Hudson River, where they chased Washington and his troops across New Jersey.
 Washington's army was shrinking because of expiring enlistments and desertions. The remaining troops were suffering from poor morale because of the defeats in the New York area.  Most of Washington's army crossed the Delaware River into Pennsylvania north of Trenton, and destroyed or moved to the western shore all boats for miles in both directions.
 Rather than attempting to immediately chase Washington further, Cornwallis, under Howe's orders, established a chain of New Jersey outposts from New Brunswick to Burlington, including one at Bordentown and one at Trenton, and ordered his troops to bunker there in winter quarters.[3] The British were happy to end the campaign season when they were ordered to winter quarters.  This was a time for the generals to regroup, resupply, and strategize for the upcoming campaign season the following spring.[2]
 Washington encamped his Continental Army near McConkey's Ferry in present-day Upper Makefield Township, not far from the crossing site.  Washington at first took quarters across the river from Trenton, but on December 15 he moved his headquarters to the home of William Keith in present-day Upper Makefield Township so he was closer to his forces. When Washington's army first arrived at McConkey's Ferry, the Continental Army had between 4,000 and 6,000 men, but approximately 1,700 were unfit for duty and needed hospital care. In the retreat across New Jersey, Washington lost precious supplies and also lost contact with two important divisions of the Continental Army.
 Two of Washington's top generals were potentially poised to aid Washington in the crossing and attack. Horatio Gates was in the Hudson River Valley. Charles Lee was in western New Jersey, where he had 2,000 Continental Army troops under his command.[4] Washington ordered both generals to join him, but Gates was delayed by heavy snows in transit to McConkey's Ferry, and Lee, who did not have a high opinion of Washington, delayed following repeated orders, and remained on the British flank near Morristown.[5][6]
 Other problems hampered Washington's forces. Many of his troop's enlistments were due to expire at the end of 1776, then only a week away, and many of them were inclined to leave the Continental Army when their commission ended.[7] Several Continental Army troops deserted prior to expiration of their enlistment commitment.[8]
 The pending loss of forces, the series of lost battles, the loss of New York, and the resulting flight of the Continental Army and many New Yorkers from the British, led some in the Second Continental Congress in Philadelphia to begin doubting the war's direction under Washington's leadership.[9] But Washington persisted, successfully procuring supplies and dispatching men to recruit new members for the Continental Army,[10] which was successful partly because of British and Hessian soldiers' drunken behavior while in New Jersey and Pennsylvania.[11]
 The losses at Fort Lee placed a heavy toll on Washington and the Continental Army. When they evacuated their forts, they were forced to leave behind critical supplies and munitions. Many troops were killed or taken prisoner, and the morale of the remaining troops suffered even further. Few believed that Washington and the Continental Army could win the war and gain independence.[2]
 On December 19, 1776, just a week prior to Washington's covert crossing of the Delaware River, the morale of the Continental Army was lifted by the publication of The American Crisis, a pamphlet authored by Thomas Paine, the author of Common Sense.[12] In The American Crisis, Paine wrote the famed phrase:
 The day following its publication in Philadelphia, Washington ordered all his troops to read it. In The American Crisis, Paine encouraged the soldiers to look more optimistically at their prospects for victory. The pamphlet also enhanced public understanding across the Thirteen Colonies of the challenging conditions confronting the Continental Army but arguing that victory was possible and necessary.[13]
 On December 20, General Lee's division of 2,000 troops arrived in Washington's camp under the command of General John Sullivan.[14] Lee was captured by the British on December 12, when he ventured too far outside the protection of his troops in search of more comfortable lodgings.[15] Later that day, Gates' division arrived in camp, which by then included only 600 Continental Army forces following the end of many enlistments, to secure the northern frontier.[14] Soon after, another 1,000 Continental Army troops arrived from Philadelphia under Colonel John Cadwalader's command to support Washington and the existing troops he then was commanding.
 With these reinforcements and a smaller number of local volunteers who joined his forces, Washington's forces totaled about 6,000 troops fit for duty. This total was then reduced by a large portion because some forces were detailed to guard the ferries at Dunk's Ferry, currently bordered by present-day Neshaminy State Park in Bensalem Township, Pennsylvania and New Hope, Pennsylvania. Another group was sent to protect supplies at Newtown, Pennsylvania, and to guard the sick and wounded who had to remain behind as the Continental Army began crossing the Delaware River.[11] This left Washington with about 2,400 men able to take offensive action against the Hessian and British troops in and around Trenton.[16]
 The Continental Army's morale was boosted further by the arrival of some provisions, including much-needed blankets, on December 24.[17]
 Washington was considering some form of bold maneuver since arriving in Pennsylvania. With the arrival of Sullivan's and Gates' forces and the influx of militia companies, he felt the time was finally right for some sort of action.
 Washington first considered an attack on the southernmost British positions near Mount Holly, where a Continental Army militia force had gathered. He sent his adjutant Joseph Reed to meet with Samuel Griffin, the militia's commander. Reed arrived in Mount Holly on December 22, and found Griffin to be ill and his men in relatively poor condition, but willing to undertake some form of military diversion.[18] They did this at the Battle of Iron Works Hill the next day, drawing the Hessians at Bordentown far enough south that they would be unable to come to the assistance of the Trenton garrison.[19] The intelligence gathered by Reed and others led Washington to abandon the idea of attacking at Mount Holly, and he began focusing instead on targeting the Hessian garrison in Trenton.
 On December 23, Washington announced to his staff that he had decided to attack Trenton just prior to sunrise on December 26.[20] Washington told Reed that ""dire necessity"" justified the risky assault, which included the logistically complicated task of crossing the Delaware River.[21]
 Washington's final plan included plans for three crossings of the river, with his troops, the largest contingent, to lead the attack on Trenton. A second column under Cadwalader was to cross at Dunk's Ferry and create a diversion to the south. A third column under Brigadier General James Ewing was to cross at Trenton Ferry and hold the bridge across the Assunpink Creek, just south of Trenton, in order to prevent the enemy's escape by that route. Once Trenton was secure, the combined Continental Army would move against the British posts in Princeton and New Brunswick. A planned fourth crossing, by men provided by General Israel Putnam to assist Cadwalader, was aborted after Putnam indicated that he did not feel he had enough men fit for such a military operation.[22]
 Preparations for the attack began immediately, on December 23. The following day, on December 24, boats were used to begin bringing the Continental Army across the Delaware from New Jersey were brought down from Malta Island near present-day New Hope, Pennsylvania. The boats were hidden behind Taylor Island at McConkey's Ferry, Washington's planned crossing site, and security was tightened at and around the crossing. A final planning meeting took place that day, with all of the general officers present. Washington outlined the detailed plans for the crossing of the river and planned attacks on the Hessians in Trenton on December 25, 1776[23]
 A wide variety of watercraft were assembled for the crossing of the Delaware River, primarily through the work of militia men from surrounding counties in New Jersey and Pennsylvania with assistance from the Pennsylvania Navy.
 Captain Daniel Bray, along with Captain Jacob Gearhart and Captain Jacob Ten Eyck, were chosen by Washington to take charge of the boats used in the crossing, supervising the transport of infantry, cavalry, and cannon. In addition to the large ferry vessels, which were big enough to carry large coaches, and likely served for carrying horses and artillery during the crossing, a large number of Durham boats were used to transport soldiers across the river. These boats were designed to carry heavy loads from the Durham Iron Works, featured high sides and a shallow draft, and could be poled across the river.[24][25]
 The boats were operated by experienced watermen, including John Glover's Marblehead Regiment, a company of experienced seamen from Marblehead, Massachusetts. These men were joined by seamen, dockworkers, and shipbuilders from Philadelphia and local ferry operators and boatsmen who knew the Delaware River well, including Kirby Francis Kane from Rhode Island.[26]
 On the morning of December 25, Washington ordered his Continental Army troops to prepare three days' food and issued orders that every soldier be outfitted with fresh flints for their muskets.[27] Washington was somewhat worried by intelligence reports that the British were planning their own crossing once the Delaware was frozen over. At 4 pm on December 25, Washington's army arrived to begin the crossing of the river. The troops were issued ammunition, and even the officers and musicians were ordered to carry muskets. They were told that they were departing on a secret mission.[28]  Marching eight abreast in close formations and ordered to be as quiet as possible, they left the camp for McConkey's Ferry.[16]  Washington's plan required the crossing to begin as soon as it was dark enough to conceal their movements on the river, but most of the troops did not reach the crossing point until about 6 pm, about ninety minutes after sunset.[29]  As the evening progressed, the weather became progressively worse, turning from drizzle to rain and then to sleet and snow. ""It blew a hurricane,"" one soldier recalled.[30]
 Washington had given charge of the crossing to his chief of artillery, Henry Knox.  In addition to the crossing of large numbers of troops (most of whom could not swim), he had to safely transport horses and eighteen pieces of artillery over the river.  Knox wrote that the crossing was accomplished ""with almost infinite difficulty"", and that its most significant danger was floating ice in the river.[31]  One observer noted that the whole operation might well have failed ""but for the stentorian lungs of Colonel Knox"".[31] The unusually cold weather of the 1770s and the icy river were likely related to the Little Ice Age.[32]
 Washington was among the first of the troops to cross, going with Virginia troops led by General Adam Stephen.  These troops formed a sentry line around the landing area in New Jersey, with strict instructions that no one was to pass through. The password was ""Victory or Death"".[33][34]  The rest of the army crossed without significant incident, although a few men, including Delaware's Colonel John Haslet, fell into the water.[35]
 Others who crossed included Arthur St. Clair, who later served as President of the Continental Congress and Governor of the Northwest Territory[36] and John Gano, a brigade chaplain and friend of George Washington, who later served as the first chaplain in the Kentucky state legislature.[37]
 The amount of ice on the river prevented the artillery from finishing the crossing until 3 am on December 26. The troops were ready to march around 4 am.[38]
 The two other crossings fared less well. The treacherous weather and ice jams on the river stopped General Ewing from even attempting a crossing below Trenton. Colonel Cadwalader crossed a significant portion of his men to New Jersey, but when he found that he could not get his artillery across the river he recalled his men from New Jersey. When he received word about Washington's victory, he crossed his men over again but retreated when he found out that Washington had not stayed in New Jersey.[39]
 On the morning of December 26, as soon as the Continental Army was ready, Washington ordered it split into two columns, one Washington personally commanded with General Greene, and a second led by General Sullivan. The Sullivan column would take River Road from Bear Tavern to Trenton while Washington's column would follow Pennington Road, a parallel route that lay a few miles inland from the Delaware River.
 Meanwhile, the Hessians were held up at Trenton. In the days approaching Christmas, they experienced numerous skirmishes around Trenton, and were subjected to frequent gunfire at night, along with repeated false alarms. By Christmas Eve, the Hessians were tired and weary. As a storm and heavy snowfall began Christmas night, Colonel Rall assumed there would be no attack of any consequence to worry about. While Rall was in Trenton, he and some of his top officers spent Christmas evening at the home of Abraham Hunt, Trenton's postmaster, where Hunt played the role of a Loyalist and placated Rall and his officers with food and plenty of drink into the late hours of the evening and morning, which, by many accounts, compromised Rall's ability to respond to Washington's surprise attack at daybreak.[40]
 Washington attacked an unsuspecting Rall and his troops and in little time had scattered and divided them and ultimately won the battle. Only three Americans were killed and six wounded, while 22 Hessians were killed, with 98 wounded.[41] During the battle Colonel Rall was mortally wounded, and died the next day. The Americans captured nearly 1,000 prisoners, and seized muskets, gunpowder, artillery pieces, and drums.[41][42][21]
 Following the battle, Washington had to execute a second crossing that was in some ways more difficult than the first. In the aftermath of the battle, the Hessian supplies had been plundered, and, in spite of Washington's explicit orders for its destruction, casks of captured rum were opened, so some of the celebrating troops got drunk,[43] probably contributing to the larger number of troops that had to be pulled from the icy Delaware River waters on the return crossing.[44] They also had to transport the large numbers of prisoners across the river while keeping them under guard.  One American acting as a guard on one of the crossings observed that the Hessians, who were standing in knee-deep ice water, were ""so cold that their underjaws quivered like an aspen leaf.""[45]
 The victory had a marked effect on the troops' morale. Soldiers celebrated the victory, Washington's role as a leader was secured, and Congress gained renewed enthusiasm for the war.[2]
 In a war council on December 27, Washington learned that all of the British and Hessian forces had withdrawn as far north as Princeton, something Cadwalader had learned when his militia company crossed the river that morning. In his letter, Cadwalader proposed that the British could be driven entirely from the area, magnifying the victory.  After much debate, the council decided on action and planned a third crossing for December 29. On December 28 it snowed, but the weather cleared that night, although its remained bitter cold.
 As this effort involved most of the army, eight crossing points were used.  At some of crossing points, the ice had frozen two to three inches (4 to 7 cm) thick and was capable of supporting soldiers, who crossed the ice on foot. At other crossings, the conditions were so bad that the attempts were abandoned for the day.  It was New Year's Eve before the army and all of its baggage was back in New Jersey.[46]  This was somewhat fortunate, as the enlistment period of John Glover's regiment, along with a significant number of others, was expiring at the end of the year, and many of these men, including most of Glover's, wanted to go home, where a lucrative privateering trade awaited them.[47]  Only by offering a bounty to be paid immediately from Congressional coffers in Philadelphia did a significant number of men agree to stay with the army another six weeks.[48]
 Washington then adopted a fortified position just south of the Assunpink Creek, across the creek from Trenton.[49]  In this position, he beat back one assault on January 2, 1777, which he followed up with a decisive victory at Princeton the next day, although General Hugh Mercer was killed in the battle.[50] In the following days, the British withdrew to New Brunswick, and the Continental Army entered winter quarters in Morristown, New Jersey.[51]
 Both sides of the Delaware River where the crossing took place have been preserved, in an area designated as the Washington's Crossing National Historic Landmark. In this district, Washington Crossing Historic Park in Washington Crossing, Pennsylvania, preserves the area in Pennsylvania, and Washington Crossing State Park in Washington Crossing, New Jersey preserves the area in New Jersey.[52] The two areas are connected by the Washington Crossing Bridge.[53] 
 In 1851, the artist Emmanuel Leutze painted Washington Crossing the Delaware, an idealized and inspirational portrait of the crossing.[54]
 Fictional portrayals in film of the crossing have also been made, with perhaps the most notable recent one being The Crossing, a 2000 television movie starring Jeff Daniels as George Washington.[55]
 A marble statue of George Washington, displayed at the Centennial Exposition in 1876, is located near the Douglass House in the Mill Hill neighborhood of Trenton. He is shown standing on a boat, symbolically representing the crossing.[56] An image of Washington Crossing the Delaware has also appeared on the 1999 New Jersey State Quarter and on the reverse of the 2021 Quarter.
 In 1970, the Vietnam Veterans Against the War invoked the crossing when they marched from Morristown to Valley Forge, performing guerilla theatre, holding press conferences, and passing out flyers in a three-day event called Operation RAW.[57]
 
.mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}40°18′0″N 74°52′24″W﻿ / ﻿40.30000°N 74.87333°W﻿ / 40.30000; -74.87333﻿ (Washington's Crossing)
"
Battle of Trenton,https://en.wikipedia.org/wiki/Battle_of_Trenton,"
 
 The Battle of Trenton was a small but pivotal American Revolutionary War battle on the morning of December 26, 1776, in Trenton, New Jersey. After General George Washington's crossing of the Delaware River north of Trenton the previous night, Washington led the main body of the Continental Army against Hessian auxiliaries garrisoned at Trenton. After a brief battle, almost two-thirds of the Hessian force were captured, with negligible losses to the Americans. The battle significantly boosted the Continental Army's waning morale, and inspired re-enlistments.
 The Continental Army had previously suffered several defeats in New York and had been forced to retreat through New Jersey to Pennsylvania. Morale in the army was low; to end the year on a positive note, George Washington, Commander-in-Chief of the Continental Army, devised a plan to cross the Delaware River on the night of December 25–26 (Christmas and St. Stephen's Day)[6] and surround the Hessians' garrison.
 Because the river was icy and the weather severe, the crossing proved dangerous. Two detachments were unable to cross the river, leaving Washington with only 2,400 men under his command in the assault, 3,000 fewer than planned. The army marched 9 mi (14.5 km) south to Trenton. The Hessians had lowered their guard, thinking they were safe from the Americans' army, and had no long-distance outposts or patrols. Washington's forces caught them off guard, and after a short but fierce resistance, most of the Hessians surrendered and were captured, with just over a third escaping across Assunpink Creek.
 Despite the battle's small numbers, the victory inspired patriots and sympathizers of the newly formed United States. With the success of the ongoing revolution in doubt a week earlier, the army had seemed on the verge of collapse. The dramatic victory inspired soldiers to believe the war was not a lost cause and serve longer. It also attracted new recruits to the ranks.[6]
 In early December 1776, American morale was very low.[7] The Americans had been ousted from New York by the British and their Hessian auxiliaries, and the Continental Army was forced to retreat across New Jersey. Ninety percent of the Continental Army soldiers who had served at Long Island were gone.[8] Men had deserted, feeling that the cause for independence was lost. Washington, Commander-in-Chief of the Continental Army, expressed some doubts, writing to his cousin in Virginia, ""I think the game is pretty near up.""[9]
 At the time a small town in New Jersey, Trenton was occupied by four regiments of Hessian soldiers (numbering about 1,400 men) commanded by Colonel Johann Rall. Washington's force comprised 2,400 men, with infantry divisions commanded by Major Generals Nathanael Greene and John Sullivan, and artillery under the direction of Brigadier General Henry Knox.[10]
 George Washington had stationed a spy named John Honeyman, posing as a Tory, in Trenton. Honeyman had served with Major General James Wolfe in Quebec at the Battle of the Plains of Abraham on September 13, 1759, and had no trouble establishing his credentials as a Tory. Honeyman was a butcher and bartender, who traded with the British and Hessians. This enabled him to gather intelligence and to convince the Hessians that the Continental Army was in such a low state of morale that they would not attack Trenton. Shortly before Christmas, he arranged to be captured by the Continental Army, who had orders to bring him to Washington unharmed. After being questioned by Washington, he was imprisoned in a hut to be tried as a Tory in the morning, but a small fire broke out nearby, enabling him to ""escape"".[11] On the evening before the battle, Hessian commander Rall was celebrating Christmas with his troops at the farm of Abraham Hunt, a wealthy merchant and farmer of Trenton. Hunt played the role of a friendly Loyalist host, giving Rall a false sense of serenity while Washington and his troops were preparing for a surprise attack.[12]
 The U.S. plan relied on launching coordinated attacks from three directions. General John Cadwalader would launch a diversionary attack against the British garrison at Bordentown, New Jersey, to block off reinforcements from the south. General James Ewing would take 700 militia across the river at Trenton Ferry, seize the bridge over the Assunpink Creek and prevent enemy troops from escaping. The main assault force of 2,400 men would cross the river 9 mi (14 km) north of Trenton and split into two groups, one under Greene and one under Sullivan, to launch a pre-dawn attack.[13] Sullivan would attack the town from the south, and Greene from the north.[8] Depending on the success of the operation, the Americans would possibly follow up with separate attacks on Princeton and New Brunswick.[7]
 During the week before the battle, U.S. advance parties began to ambush enemy cavalry patrols, capturing dispatch riders and attacking Hessian pickets. The Hessian commander, to emphasize the danger his men were facing, sent 100 infantry and an artillery detachment to deliver a letter to the British commander at Princeton.[7] Washington ordered Ewing and his Pennsylvania militia to try to gain information on Hessian movements and technology.[14] Ewing instead made three successful raids across the river. On December 17 and 18, 1776, they attacked an outpost of jägers and on the 21st, they set fire to several houses.[14] Washington put constant watches on all possible crossings near the Continental Army encampment on the Delaware, as he believed William Howe would launch an attack from the north on Philadelphia if the river froze over.[15]
 On December 20, 1776, some 2,000 troops led by General Sullivan arrived in Washington's camp.[16] They had been under the command of Charles Lee and had been moving slowly through northern New Jersey when Lee was captured. That same day, an additional 800 troops arrived from Fort Ticonderoga under the command of Horatio Gates.[16]
 On December 14, 1776, the Hessians arrived in Trenton to establish their winter quarters.[17] At the time, Trenton was a small town with about 100 houses and two main streets, King (now Warren) Street and Queen (now Broad) Street.[18] Carl von Donop, Rall's superior, had marched south to Mount Holly on December 22 to deal with the resistance in New Jersey, and had clashed with some New Jersey militia there on December 23.[19]
 Donop, who despised Rall, was reluctant to give command of Trenton to him.[20] Rall was known to be loud and unacquainted with the English language,[20] but he was also a 36-year veteran with a great deal of battle experience. His request for reinforcements had been turned down by British commander General James Grant, who disdained the American rebels and thought them poor soldiers. Despite Rall's experience, the Hessians at Trenton did not admire their commander.[21]
 Trenton lacked city walls or fortifications, which was typical of U.S. settlements.[22] Some Hessian officers advised Rall to fortify the town, and two of his engineers advised that a redoubt be constructed at the upper end of town and fortifications be built along the river.[22] The engineers went so far as to draw up plans, but Rall disagreed with them.[22] When Rall was again urged to fortify the town, he replied, ""Let them come ... We will go at them with the bayonet.""[22]
 As Christmas approached, Loyalists came to Trenton to report the Americans were planning action.[9] U.S. deserters told the Hessians that rations were being prepared for an advance across the river. Rall publicly dismissed such talk as nonsense, but privately in letters to his superiors, he said he was worried about an imminent attack.[9] He wrote to Donop that he was ""liable to be attacked at any moment"". Rall said that Trenton was ""indefensible"" and asked that British troops establish a garrison in Maidenhead (now Lawrenceville). Close to Trenton, this would help defend the roads from Americans. His request was denied.[23] As the Americans disrupted Hessian supply lines, the officers started to share Rall's fears. One wrote, ""We have not slept one night in peace since we came to this place.""[24] On December 22, 1776, a spy reported to Grant that Washington had called a council of war; Grant told Rall to ""be on your guard"".[25]
 The main Hessian force of 1,500 men was divided into three regiments: Knyphausen, Lossberg and Rall. That night, they did not send out any patrols because of the severe weather.[26]
 Before Washington and his troops left, Benjamin Rush came to cheer up the general. While he was there, he saw a note Washington had written, saying, ""Victory or Death"".[24] Those words would be the password for the surprise attack.[27] Each soldier carried 60 rounds of ammunition, and three days of rations.[28] When the army arrived at the shores of the Delaware, they were already behind schedule, and clouds began to form above them.[29] It began to rain. As the air's temperature dropped, the rain changed to sleet, and then to snow.[29] The Americans began to cross the river, with John Glover in command. The men went across in Durham boats, while the horses and artillery went across on large ferries.[30] The 14th Continental Regiment of Glover manned the boats. During the crossing, several men fell overboard, including Colonel John Haslet. Haslet was quickly pulled out of the water. No one died during the crossing, and all the artillery pieces made it over in good condition.[31]
 Two small detachments of infantry of about 40 men each were ordered ahead of main columns.[32] They set roadblocks ahead of the main army and were to take prisoner whoever came into or left the town.[32] One of the groups was sent north of Trenton, and the other was sent to block River Road, which ran along the Delaware River to Trenton.[33]
 The terrible weather conditions delayed the landings in New Jersey until 3:00 am; the plan was that they were supposed to be completed by 12:00 am. Washington realized it would be impossible to launch a pre-dawn attack. Another setback occurred for the Americans, as generals Cadwalader and Ewing were unable to join the attack because of the weather conditions.[13]
 At 4:00 am, the soldiers began to march towards Trenton.[34] Along the way, several civilians joined as volunteers and led as guides (such as John Mott) because of their knowledge of the terrain.[35] After marching 1.5 mi (2.4 km) through winding roads into the wind, they reached Bear Tavern, where they turned south onto Bear Tavern Road .[36] The ground was slippery, but it was level, making it easier for the horses and artillery. They began to make better time.[36] They soon reached Jacobs Creek, where, with difficulty, the Americans made it across.[37] The two groups stayed together until they reached Birmingham (now West Trenton), where they split apart, with Greene's force heading east to approach Trenton by the Scotch and Pennington roads and Sullivan's heading southwest to approach via River Road.[8] Soon after, they reached the house of Benjamin Moore, where the family offered food and drink to Washington. At this point, the first signs of daylight began to appear.[38] Many of the troops did not have boots, so they were forced to wear rags around their feet. Some of the men's feet bled, turning the snow to a dark red. Two men died on the march.[39]
 As they marched, Washington rode up and down the line, encouraging the men to continue.[30] General Sullivan sent a courier to tell Washington that the weather was wetting his men's gunpowder. Washington replied, ""Tell General Sullivan to use the bayonet. I am resolved to take Trenton.""[40]
 About 2 mi (3 km) outside the town, the main columns reunited with the advance parties.[41] They were startled by the sudden appearance of 50 armed men, but they were American. Led by Adam Stephen, they had not known about the plan to attack Trenton and had attacked a Hessian outpost.[42] Washington feared the Hessians would have been put on guard, and shouted at Stephen, ""You sir! You Sir, may have ruined all my plans by having them put on their guard.""[42] Despite this, Washington ordered the advance continue to Trenton. In the event, Rall thought the first raid was the attack which Grant had warned him about, and that there would be no further action that day.[43]
 At 8 am on the morning of December 26, the Hessians established an outpost at a cooper shop on Pennington Road about one mile northwest of Trenton.[44] Washington then led the assault on it, riding in front of his soldiers.[45] As the Hessian commander of the outpost, Lieutenant Andreas Wiederholdt, left the shop, an American fired at him but missed.[45] Wiederholdt immediately shouted, ""Der Feind!"" (The Enemy!) and other Hessians came out.[46] The Americans fired three volleys, and the Hessians returned one of their own.[45] Washington ordered Edward Hand's Pennsylvania Riflemen and a battalion of German-speaking infantry to block the road that led to Princeton. They attacked the Hessian outpost there.[46] Wiederholdt soon realized that this was more than a raiding party; seeing other Hessians retreating from the outpost, he led his men to do the same.[47] Both Hessian detachments made organized retreats, firing as they fell back.[46] On the high ground at the north end of Trenton, they were joined by a duty company from the Lossberg Regiment.[46] They engaged the Americans, retreating slowly, keeping up continuous fire and using houses for cover.[48] Once in Trenton, they gained covering fire from other Hessian guard companies on the outskirts of the town. Another guard company nearer to the Delaware River rushed east to their aid, leaving open the River Road into Trenton. Washington ordered the escape route to Princeton be cut off, sending infantry in battle formation to block it, while artillery formed at the head of King and Queen streets.[49]
 Leading the southern U.S. column, General Sullivan entered Trenton by the abandoned River Road and blocked the only crossing over the Assunpink Creek to cut off the Hessian escape.[50] Sullivan briefly held up his advance to make sure Greene's division had time to drive the Hessians from their outposts in the north.[50] Soon after, they continued their advance, attacking the Hermitage, home of Philemon Dickinson, where 50 jägers under the command of Lieutenant von Grothausen were stationed.[50] Lieutenant von Grothausen brought 12 of his jägers into action against the advanced guard but had only advanced a few hundred yards when he saw a column of Americans advancing to the Hermitage.[50] Pulling back to the Hessian barracks, he was joined by the rest of the jägers. After the exchange of one volley, they turned and ran, some trying to swim across the creek, while others escaped over the bridge, which had not yet been cut off. The 20 British dragoons also fled.[50] As Greene and Sullivan's columns pushed into the town, Washington moved to high ground north of King and Queens streets to see the action and direct his troops.[51] By this time, U.S. artillery from the other side of the Delaware River had come into action, devastating the Hessian positions.[52]
 With the sounding of the alarm, the three Hessian regiments began to prepare for battle.[53] The Rall regiment formed on lower King Street along with the Lossberg regiment, while the Knyphausen regiment formed at the lower end of Queen Street.[53] Lieutenant Piel, Rall's brigade adjutant, woke his commander, who found that the rebels had taken the ""V"" of the main streets of the town. This is where the engineers had recommended building a redoubt. Rall ordered his regiment to form up at the lower end of King Street, the Lossberg regiment to prepare for an advance up Queen Street, and the Knyphausen regiment to stand by as a reserve for Rall's advance up King Street.[50]
 The U.S. cannon stationed at the head of the two main streets soon came into action. In reply, Rall directed his regiment, supported by a few companies of the Lossberg regiment, to clear the guns.[54] The Hessians formed ranks and began to advance up the street, but their formations were quickly broken by the U.S. guns and fire from Mercer's men who had taken houses on the left side of the street.[54] Breaking ranks, the Hessians fled. Rall ordered two three-pound cannons into action. After getting off six rounds each, within just a few minutes, half of the Hessians manning their guns were killed by the U.S. cannon.[54] After the men fled to cover behind houses and fences, their cannons were taken by the Americans.[55] Following capture of the cannons, men under the command of George Weedon advanced down King Street.[50]
 On Queen Street, all Hessian attempts to advance up the street were repulsed by guns under the command of Thomas Forrest. After firing four rounds each, two more Hessian guns were silenced. One of Forrest's howitzers was put out of action with a broken axle.[50] The Knyphausen regiment became separated from the Lossberg and the Rall regiments. The Lossberg and the Rall regiments fell back to a field outside of town, taking heavy losses from grapeshot and musket fire. In the southern part of the town, Americans under command of Sullivan began to overwhelm the Hessians. John Stark led a bayonet charge at the Knyphausen regiment, whose resistance broke because their weapons would not fire. Sullivan led a column of men to block off escape of troops across the creek.[55]
 The Hessians in the field attempted to reorganize and make one last attempt to retake the town so they could make a breakout.[1] Rall decided to attack the U.S. flank on the heights north of the town.[56] Rall yelled ""Forward! Advance! Advance!"", and the Hessians began to move, with the brigade's band playing fifes, bugles and drums to help the Hessians' spirit.[56][57]
 Washington, still on high ground, saw the Hessians approaching the U.S. flank. He moved his troops to assume battle formation against the enemy.[56] The two Hessian regiments began marching toward King Street but were caught in U.S. fire that came at them from three directions.[56] Some Americans had taken up defensive positions inside houses, reducing their exposure. Some civilians joined the fight against the Hessians.[58] Despite this, they continued to push, recapturing their cannons. At the head of King Street, Knox saw the Hessians had retaken the cannons and ordered his troops to take them. Six men ran and, after a brief struggle, seized the cannons, turning them on the Hessians.[59] With most of the Hessians unable to fire their guns, the attack stalled. The Hessians' formations broke, and they began to scatter.[58] Rall was mortally wounded.[60] Washington led his troops down from high ground while yelling, ""March on, my brave fellows, after me!""[58] Most of the Hessians retreated into an orchard, with the Americans in close pursuit. Quickly surrounded,[61] the Hessians were offered terms of surrender, to which they agreed.
 Although ordered to join Rall, the remains of the Knyphausen regiment mistakenly marched in the opposite direction.[61] They tried to escape across the bridge but found it had been taken. The Americans quickly swept in, defeating a Hessian attempt to break through their lines. Surrounded by Sullivan's men, the regiment surrendered, just minutes after the rest of the brigade.[62]
 The Hessian forces lost 22 killed in action, including their commander Colonel Johann Rall, 83 wounded, and 896 captured–including the wounded.[63] The Americans suffered only two deaths during the march and five wounded from battle, including a near-fatal shoulder wound to future president James Monroe. Other losses incurred by the patriots from exhaustion, exposure, and illness in the following days may have raised their fatalities above those of the Hessians.[64]
 The captured Hessians were sent to Philadelphia and later Lancaster. In 1777 they were moved to Virginia.[65] Rall was mortally wounded and died later that night at his headquarters.[64] All four Hessian colonels in Trenton were killed in the battle. The Lossberg regiment was effectively removed from the British forces. Parts of the Knyphausen regiment escaped to the south, but Sullivan captured some 200 additional men, along with the regiment's cannon and supplies. They also captured approximately 1,000 arms and much-needed ammunition.[66] The Americans also captured their entire store of provisions—tons of flour, dried and salted meats, ale and other liquors, as well as shoes, boots, clothing and bedding—things that were as much needed by the ragtag Continental forces as weapons and horses.
 Among those captured by the Patriots was Christian Strenge, later to become a schoolmaster and fraktur artist in Pennsylvania.[67]
 An officer in Washington's staff wrote before the battle, ""They make a great deal of Christmas in Germany, and no doubt the Hessians will drink a great deal of beer and have a dance to-night. They will be sleepy tomorrow morning.""[68] Popular history commonly portrays the Hessians as drunk from Christmas celebrations. However, historian David Hackett Fischer quotes Patriot John Greenwood, who fought in the battle and supervised Hessians afterward, who wrote, ""I am certain not a drop of liquor was drunk during the whole night, nor, as I could see, even a piece of bread eaten.""[69] Military historian Edward G. Lengel wrote, ""The Germans were dazed and tired but there is no truth to the legend claiming that they were helplessly drunk.""[70]
 After the Hessians' surrender, Washington is reported to have shaken the hand of a young officer and said, ""This is a glorious day for our country.""[71] On December 28, General Washington interviewed Lieutenant (later Colonel) Andreas Wiederhold, who detailed the failures of Rall's preparation.[72] Washington soon learned, however, that Cadwalader and Ewing had been unable to complete their crossing, leaving his worn-out army of 2,400 men isolated.[73] Without their 2,400 men, Washington realized he did not have the forces to attack Princeton and New Brunswick.[73]
 By noon, Washington's force had moved across the Delaware back into Pennsylvania, taking their prisoners and captured supplies with them.[73] Washington would follow up his success a week later in the Battle of the Assunpink Creek and the Battle of Princeton solidifying Patriot gains.
 This small but decisive battle, as with the later Battle of Cowpens, had an effect disproportionate to its size. The Patriot victory gave the Continental Congress new confidence, as it proved colonial forces could defeat the British in the future. It also increased re-enlistments in the Continental Army forces. By defeating a European army, the colonials reduced the fear that the Hessians had caused earlier that year after the fighting in New York.[1] Howe was stunned that the Patriots so easily surprised and overwhelmed the Hessian garrison.[62] Colonial support for the rebellion was further buoyed significantly at this time by writings of Thomas Paine and additional successful actions by the New Jersey Militia.[74]
 Two notable U.S. officers were wounded while leading the charge down King Street: William Washington, cousin of General Washington, and Lieutenant James Monroe, the future President of the United States. Monroe was carried from the field bleeding badly after he was struck in the left shoulder by a musket ball, which severed an artery. Doctor John Riker clamped the artery, preventing him from bleeding to death.[59]
 The Trenton Battle Monument, erected at ""Five Points"" in Trenton, stands as a tribute to this U.S. victory.[75] The crossing of the Delaware River and battle are reenacted by local enthusiasts every year (unless the weather is too severe on the river).[76]
 Eight current Army National Guard units (101st Eng Bn,[77] 103rd Eng Bn,[78] A/1-104th Cav,[79] 111th Inf,[80] 125th QM Co,[81] 175th Inf,[82] 181st Inf[83] and 198th Sig Bn[84]) and one currently-active Regular Army Artillery battalion (1–5th FA)[85] are derived from U.S. units that participated in the Battle of Trenton. There are thirty current units of the U.S. Army with colonial roots.
 In 1851, German-American artist Emanuel Leutze painted the second of three paintings depicting Washington crossing the Delaware. It is in the Metropolitan Museum of Art, and is ""one of the most famous American paintings."" At the time of its first exhibition it caused a sensation, in Europe and the United States. Leutze hoped it would stir revolutionary sentiments in Germany. After six months in Germany it was shipped to New York City where the New-York Mirror newspaper lauded it with the words, ""the grandest, most majestic, and most effective painting ever exhibited in America.""[86] The painting is the center-piece of the collections in the American Wing. It is still one of the most recognizable paintings at the Metropolitan. It is central to the canon of American historical art images, its monumental popularity undimmed in the years since it was first exhibited.[87]
"
Battle of Princeton,https://en.wikipedia.org/wiki/Battle_of_Princeton,"

 The Battle of Princeton was a battle of the American Revolutionary War, fought near Princeton, New Jersey on January 3, 1777, and ending in a small victory for the Colonials. General Lord Cornwallis had left 1,400 British troops under the command of Lieutenant Colonel Charles Mawhood in Princeton. Following a surprise attack at Trenton early in the morning of December 26, 1776, General George Washington of the Continental Army decided to attack the British in New Jersey before entering the winter quarters. On December 30, he crossed the Delaware River back into New Jersey. His troops followed on January 3, 1777. Washington advanced to Princeton by a back road, where he pushed back a smaller British force but had to retreat before Cornwallis arrived with reinforcements. The battles of Trenton and Princeton boosted the morale of the patriot cause, leading many recruits to join the Continental Army in the spring.
 After defeating the Hessians at the Battle of Trenton on the morning of December 26, 1776, Washington withdrew back to Pennsylvania. He subsequently decided to attack the British forces before going into winter quarters. On December 29, he led his army back into Trenton. On the night of January 2, 1777, Washington repulsed a British attack at the Battle of the Assunpink Creek. That night, he evacuated his position, circled General Cornwallis' army, and attacked the British garrison at Princeton.
 On January 3, Brigadier General Hugh Mercer of the Continental Army clashed with two regiments under the command of Mawhood. Mercer and his troops were overrun, and Mercer was mortally wounded. Washington sent a brigade of militia under Brigadier General John Cadwalader to help them. On seeing the flight of Mercer's men, the militia began to flee. Washington rode up with reinforcements and rallied the fleeing militia. He then led the attack on Mawhood's troops, driving them back. Mawhood gave the order to retreat, and most of the troops tried to flee to Cornwallis in Trenton.
 In Princeton, Brigadier General John Sullivan encouraged some British troops who had taken refuge in Nassau Hall to surrender, ending the battle.  After the battle, Washington moved his army to Morristown, and with their third defeat in 10 days, the British evacuated Central Jersey. The battle (while considered minor by British standards)[7][8] was the last major action of Washington's winter New Jersey campaign.
 On the night of December 25–26, 1776, General George Washington, Commander-in-chief of the Continental Army, led 2,400 men across the Delaware River.[9] After a nine-mile march, they seized the town of Trenton on the morning of December 26, killing or wounding over 100 Hessians and capturing 900 more. Soon after capturing the town, Washington led the army back across the Delaware into Pennsylvania.[10] On December 29, Washington once again led the army across the river and established a defensive position at Trenton. On December 31, Washington appealed to his men, whose enlistments expired at the end of the year, ""Stay for just six more weeks for an extra bounty of ten dollars."" His appeal worked, and most of the men agreed to stay.[11] Also that day, Washington learned that Congress had voted to give him wide-ranging powers for six months that are often described as dictatorial.[12]
 In response to the loss at Trenton, General Cornwallis left New York City and reassembled a British force of more than 9,000 at Princeton to oppose Washington. Leaving 1,200 men under the command of Lieutenant Colonel Mawhood at Princeton, Cornwallis left Princeton on January 2 in command of 8,000 men to attack Washington's army of 6,000 troops.[2] Washington sent troops to skirmish with the approaching British to delay their advance.  It was almost nightfall by the time the British reached Trenton. After three failed attempts to cross the bridge over the Assunpink Creek, beyond which were the primary American defenses, Cornwallis called off the attack until the next day.[13]
 During the night, Washington called a council of war and asked his officers whether they should stand and fight, attempt to cross the river somewhere, or take the back roads to attack Princeton. Although the idea had already occurred to Washington, he learned from Arthur St. Clair and John Cadwalader that his plan to attack Princeton was indeed possible. Two intelligence collection efforts, both of which came to fruition at the end of December 1776, supported such a surprise attack. After consulting with his officers, they agreed that the best option was to attack Princeton.[14]
 Washington ordered that the excess baggage be taken to Burlington where it could be sent to Pennsylvania. The ground had frozen, making it possible to move the artillery without it sinking into the ground. By midnight, the plan was complete, with the baggage on its way to Burlington and the guns wrapped in heavy cloth to stifle noise and prevent the British from learning of the evacuation. Washington left 500 men behind with two cannon to patrol, keep the fires burning, and to work with picks and shovels to make the British think that they were digging in. Before dawn, these men were to join up with the main army.[15]
 By 2:00 am, the entire army was in motion roughly along Quaker Bridge Road through what is now Hamilton Township. The men were ordered to march with silence. Along the way, a rumor was spread that they were surrounded, and some frightened militiamen fled for Philadelphia. The march was difficult, as some of the route ran through thick woods and it was icy, causing horses to slip and men to break through ice on ponds.[16]
 As dawn came, the army approached a stream called Stony Brook. The road the army took followed Stony Brook for a mile farther until it intersected the Post Road from Trenton to Princeton. However, off to the right of this road, there was an unused road that crossed the farmland of Thomas Clark. The road was not visible from the Post Road and ran through cleared land to a stretch from which the town could be entered at any point because the British had left it undefended.[17]
 However, Washington was running behind schedule as he had planned to attack and capture the British outposts before dawn and capture the garrison shortly afterward. By the time dawn broke he was still two miles from the town. According to the standard account of the battle, Washington sent 350 men under the command of Brigadier General Hugh Mercer to destroy the bridge over Stony Brook in order to delay Cornwallis's army when he found out that Washington had escaped. According to a newer analysis, however, Brigadier General Thomas Mifflin was tasked with the bridge, and Mercer's forces did not break off from the main column until later.[18]
 Shortly before 8:00 am, Washington wheeled the rest of the army to the right down the unused road. First in the column went General John Sullivan's division consisting of Arthur St. Clair's and Isaac Sherman's brigades. Following them were John Cadwalader's brigade and then Daniel Hitchcock's.[19]
 Cornwallis had sent orders to Mawhood to bring the 17th and 55th British regiments to join his army in the morning.  Mawhood had moved out from Princeton to fulfill these orders when his troops climbed the hill south of Stony Brook and sighted the main American army. Unable to figure out the size of the American army because of the wooded hills, he sent a rider to warn the 40th British Regiment, which he had left in Princeton, then wheeled the 17th and 55th Regiments around and headed back to Princeton. That day, Mawhood had called off the patrol which was to reconnoiter the area from which Washington was approaching.[20]
 Mercer received word that Mawhood was leading his troops back to Princeton.[21] Mercer, on orders from Washington, moved his column to the right in order to hit the British before they could confront Washington's main army.[22] Mercer moved towards Mawhood's rear, but when he realized he would not be able to cut off Mawhood in time, he decided to join Sullivan. When Mawhood learned that Mercer was in his rear and moving to join Sullivan, Mawhood detached part of the 55th Regiment to join the 40th Regiment in the town and then moved the rest of the 55th, the 17th, fifty cavalry, and two artillery pieces to attack Mercer.[23]
 Mawhood ordered his light troops to delay Mercer, while he brought up the other detachments. Mercer was walking through William Clark's orchard when the British light troops appeared. The British light troops' volley went high, which gave time for Mercer to wheel his troops around into battle line. Mercer's troops advanced, pushing back the British light troops. The Americans took up a position behind a fence at the upper end of the orchard. However, Mawhood had brought up his troops and his artillery.[24] The American gunners opened fire first, and for about ten minutes, the outnumbered American infantry exchanged fire with the British. However, many of the Americans had rifles which took longer to load than muskets. Mawhood ordered a bayonet charge, and because many of the Americans had rifles, which could not be equipped with bayonets, they were overrun.[2] Both of the Americans' cannon were captured, and the British turned them on the fleeing troops. Mercer was surrounded by British soldiers, and they shouted at him ""Surrender, you damn rebel!"" Declining to ask for quarter, Mercer chose to resist instead. The British, thinking they had caught Washington, bayoneted him and then left him for dead. Mercer's second in command, Colonel John Haslet, was shot through the head and killed.[25]
 Fifty light infantrymen were in pursuit of Mercer's men when a fresh brigade of 1,100 militiamen under the command of Cadwalader appeared.[26] Mawhood gathered his men who were all over the battlefield and put them into battle line formation. Meanwhile, Sullivan was at a standoff with the detachment of the 55th Regiment that had come to assist the 40th Regiment, neither daring to move towards the main battle for risk of exposing its flank. Cadwalader attempted to move his men into a battle line, but they had no combat experience and did not know even the most basic military maneuvers. When his men reached the top of the hill and saw Mercer's men fleeing from the British, most of the militia turned around and ran back down the hill.[27]
 As Cadwalader's men began to flee, the American guns opened fire onto the British, who were preparing to attack, and the guns were able to hold them off for several minutes. Cadwalader was able to get one company to fire a volley but it fled immediately afterwards. At this point, Washington arrived with the Virginia Continentals and Edward Hand's riflemen.[28] Washington ordered the riflemen and the Virginians to take up a position on the right hand side of the hill, and then Washington quickly rode over to Cadwalader's fleeing men. Washington shouted, ""Parade with us my brave fellows! There is but a handful of the enemy and we shall have them directly!"".[29] Cadwalader's men formed into battle formation at Washington's direction. When Daniel Hitchcock's New England Continentals arrived, Washington sent them to the right, where he had put the riflemen and the Virginians.[30]
 Washington, with his hat in his hand, rode forward and waved the Americans forward. At this point, Mawhood had moved his troops slightly to the left to get out of the range of the American artillery fire. Washington gave orders not to fire until he gave them the signal, and when they were thirty yards away, he turned around on his horse, facing his men and said ""Halt!"" and then ""Fire!"".[31] At this moment, the British also fired, obscuring the field in a cloud of smoke. One of Washington's officers, John Fitzgerald, pulled his hat over his eyes to avoid seeing Washington killed, but when the smoke cleared, Washington appeared, unharmed, waving his men forward.[32]
 On the right, Hitchcock's New Englanders fired a volley and then advanced again, threatening to turn the British flank.[33] The riflemen were slowly picking off British soldiers while the American artillery was firing grapeshot at the British lines. At this point, Hitchcock ordered his men to charge, and the British began to flee. The British attempted to save their artillery, but the militia also charged, and Mawhood gave the order to retreat. The British fled towards the Post Road followed by the Americans. Washington reportedly shouted, ""It's a fine fox chase my boys!"" Some Americans had swarmed onto the Post Road in order to block a British retreat across the bridge, but Mawhood ordered a bayonet charge and broke through the American lines, escaping across the bridge. Some of the Americans, Hand's riflemen among them, continued to pursue the British, and Mawhood ordered his dragoons to buy them some time to retreat; however, the dragoons were pushed back. Some Americans continued to pursue the fleeing British until nightfall, killing some and taking some prisoners.[34] After some time, Washington turned around and rode back to Princeton.[33]
 At the edge of town, the 55th Regiment received orders from Mawhood to fall back and join the 40th Regiment in town. The 40th had taken up a position just outside town, on the north side of a ravine. The 55th formed up to the left of the 40th. The 55th sent a platoon to flank the oncoming Americans, but it was cut to pieces. When Sullivan sent several regiments to scale the ravine, they fell back to a breastwork. After making a brief stand, the British fell back again, some leaving Princeton and others taking up refuge in Nassau Hall.[35] Alexander Hamilton brought three cannons up and had them blast away at the building. Then some Americans rushed the front door, broke it down, and the British put a white flag outside one of the windows. 194 British soldiers walked out of the building and laid down their arms.[33]
 After entering Princeton, the Americans began to loot the abandoned British supply wagons and the town.[36] With news that Cornwallis was approaching, Washington knew he had to leave Princeton. Washington wanted to push on to New Brunswick and capture a British pay chest of 70,000 pounds, but Major Generals Henry Knox and Nathanael Greene talked him out of it.[37] Instead, Washington moved his army to Somerset Courthouse on the night of January 3, then marched to Pluckemin by January 5, and arrived at Morristown by sunset the next day for winter encampment.[3][38] After the battle, Cornwallis abandoned many of his posts in New Jersey and ordered his army to retreat to New Brunswick. The next several months of the war consisted of a series of small scale skirmishes known as the Forage War.
 General Howe's official casualty report for the battle stated 18 killed, 58 wounded and 200 missing.[39] Mark Boatner says that the Americans took 194 prisoners during the battle, while the remaining 6 ""missing"" men may have been killed.[6] A civilian eyewitness (the anonymous writer of A Brief Narrative of the Ravages of the British and Hessians at Princeton in 1776–1777) wrote that 24 British soldiers were found dead on the field. Washington claimed that the British had more than 100 killed and 300 captured.[40] William S. Stryker follows Washington in stating that the British loss was 100 men killed, 70 wounded and 280 captured.[5]
 Washington reported his own army's casualties as 6 or 7 officers and 25 to 30 enlisted men killed, giving no figures for the wounded.[41] Richard M. Ketchum states that the Americans had ""30 enlisted men and 14 officers killed"";[4] Henry B. Dawson gives 10 officers and 30 enlisted men killed;[42] while Edward G. Lengel gives total casualties as 25 killed and 40 wounded.[3] The Loyalist newspaper, New York Gazette and Weekly Mercury, reported on January 17, 1777, that the American losses at Princeton had been 400 killed and wounded.[43]
 The colonnade on the Princetown Battlefield Monument marks the common grave of 15 Americans and 21 British killed.[44] In addition, one British officer, Captain William Leslie, died of his wounds and was buried at Pluckemin, New Jersey.[45][46]
 The British viewed Trenton and Princeton as minor American victories, but with these victories, the Americans believed that they could win the war.[37] American historians often consider the Battle of Princeton a great victory, on par with the Battle of Trenton, because of the subsequent loss of control of most of New Jersey by the Crown forces. Ira D. Gruber pronounced that the battles of ""Trenton and Princeton were supremely important: destroying the illusion of British invincibility, making patriots of potential loyalists, and spoiling [British] hopes for an end to the war and a start toward a lasting reunion.""[47] Some other historians, such as Edward Lengel, consider Princeton to be even more impressive than Trenton.[3] A century later, British historian Sir George Otto Trevelyan wrote in a study of the American Revolution, when talking about the impact of the victories at Trenton and Princeton, that ""It may be doubted whether so small a number of men ever employed so short a space of time with greater and more lasting effects upon the history of the world.""[48]
 Part of the battlefield is now preserved in Princeton Battlefield State Park, which was designated a National Historic Landmark in 1961.[49] Another section of the battlefield adjacent to the state park was embroiled in a development controversy. The Institute for Advanced Study, which owns the property, had planned a housing project on land where George Washington charged with his men during the battle.[50] Historians, the Department of the Interior and archaeological evidence confirm the land's significance.[51][52]  Several national and local preservation organizations worked to prevent construction on the property, and the Princeton Battlefield Society had legal action pending as of summer 2016. On December 12, 2016, the American Battlefield Trust announced that through its Campaign 1776 project to preserve land at battlefields of the Revolutionary War and War of 1812, it had reached an agreement to purchase almost 15 acres of land from the Institute for Advanced Study valued at $4 million. This purchase would increase the size of the state park by 16%. Seven of the planned single family dwellings would be replaced with townhouses and a total of 16 housing units would be constructed. The compromise arrangement was subject to approval by the Princeton Planning Board and Delaware and Raritan Canal Commission.[53] The Trust had already acquired and preserved nine other acres of the Princeton battlefield.[54] On May 30, 2018, the Trust announced that it had finalized the purchase after raising almost $3.2 million from private donors, which was matched by an $837,000 grant from the National Park Service and the Mercer County Open Space Assistance Program. The completed purchase ended the long dispute over how and whether the battlefield land would be developed.[55] As of mid-2023, the Trust and its partners had preserved more than 24 acres of the battlefield.[56]
 The equestrian statue of George Washington at Washington Circle in Washington, D.C. depicts him at the Battle of Princeton.  Sculptor Clark Mills said in his speech at the statue's dedication ceremony on February 22, 1860, ""The incident selected for representation of this statue was at the battle of Princeton where Washington, after several ineffectual attempts to rally his troops, advanced so near the enemy's lines that his horse refused to go further, but stood and trembled while the brave rider sat undaunted with reins in hand.  But while his noble horse is represented thus terror stricken, the dauntless hero is calm and dignified, ever believing himself the instrument in the hand of Providence to work out the great problem of liberty.""[57]
 Eight current Army National Guard units (101st Eng Bn,[58] 103rd Eng Bn,[59] A/1-104th Cav,[60] 111th Inf,[61] 125th QM Co,[62] 175th Inf,[63] 181st Inf[64] and 198th Sig Bn[65]) and one currently-active Regular Army Artillery battalion (1–5th FA[66]) are derived from American units that participated in the Battle of Princeton.
 A famous story, possibly apocryphal, states that during the Battle of Princeton, Alexander Hamilton ordered his cannon to fire upon the British soldiers taking refuge in Nassau Hall. As a result, one of the cannonballs was shot through the head of the portrait of King George II that hung in the chapel, which was subsequently replaced with a portrait of George Washington.[67] Tangentially, a few years earlier Hamilton had been refused accelerated study at the College of New Jersey (now Princeton University) housed in Nassau Hall. He attended King's College (now Columbia University) in New York, instead.[68]
"
Battle of Brandywine,https://en.wikipedia.org/wiki/Battle_of_Brandywine,"
 The Battle of Brandywine, also known as the Battle of Brandywine Creek, was fought between the American Continental Army of General George Washington and the British Army of General Sir William Howe on September 11, 1777, as part of the American Revolutionary War (1775–1783). The forces met near Chadds Ford, Pennsylvania. More troops fought at Brandywine than at any other battle of the American Revolution.[5] It was also the second longest single-day battle of the war, after the Battle of Monmouth, with continuous fighting for 11 hours.[5]
 As Howe moved to take Philadelphia, then the American capital, the British forces routed the Continental Army and forced them to withdraw, first, to the City of Chester, Pennsylvania, and then northeast toward Philadelphia.
 Howe's army departed from Sandy Hook, New Jersey, across New York Bay from the occupied town of New York City on the southern tip of Manhattan Island, on July 23, 1777, and landed near present-day Elkton, Maryland, at the point of the ""Head of Elk"" by the Elk River at the northern end of the Chesapeake Bay, at the southern mouth of the Susquehanna River.[6] Marching north, the British Army brushed aside American light forces in a few skirmishes. General Washington offered battle with his army posted behind Brandywine Creek, off the Christina River. While part of his army demonstrated in front of Chadds Ford, Howe took the bulk of his troops on a long march that crossed the Brandywine far beyond Washington's right flank. Due to poor scouting, the Americans did not detect Howe's column until it reached a position in rear of their right flank. Belatedly, three divisions were shifted to block the British flanking force at Birmingham Friends Meetinghouse and School, a Quaker meeting house.
 After a stiff fight, Howe's wing broke through the newly formed American right wing, which was deployed on several hills. At this point Lieutenant General Wilhelm von Knyphausen attacked Chadds Ford and crumpled the American left wing. As Washington's army streamed away in retreat, he brought up elements of General Nathanael Greene's division, which held off Howe's column long enough for his army to escape to the northeast. Polish General Casimir Pulaski defended Washington's rear, assisting in his escape.[7] The defeat and subsequent maneuvers left Philadelphia vulnerable. The British captured it two weeks later on September 26, resulting in the city falling under British control for nine months, until June 1778.
 In late August 1777, after a distressing 34-day journey from Sandy Hook on the coast of New Jersey, a Royal Navy fleet of more than 260 ships carrying some 17,000 British troops under the command of British General Sir William Howe landed at the head of the Elk River, on the northern end of the Chesapeake Bay near present-day Elkton, Maryland (then known as Head of Elk), approximately 40–50 miles (60–80 km) southwest of Philadelphia. Unloading the ships proved to be a logistical problem because the narrow river neck was shallow and muddy.
 General George Washington had placed the American forces, about 20,300-strong, between Head of Elk and Philadelphia. His forces were able to reconnoiter the British landing from Iron Hill near Newark, Delaware, about 9 miles (14 km) to the northeast. Because of the delay disembarking from the ships, Howe did not set up a typical camp but quickly moved forward with the troops. As a result, Washington was not able to accurately gauge the strength of the opposing forces.
 After a skirmish at Cooch's Bridge south of Newark, the British troops moved north and Washington abandoned a defensive encampment along the Red Clay Creek near Newport, Delaware, to deploy against the British at Chadds Ford. This site was important as it was the most direct passage across the Brandywine River on the road from Baltimore to Philadelphia. On September 9, Washington positioned detachments to guard other fords above and below Chadds Ford, hoping to force the battle there. Washington employed General John Armstrong, commanding about 1,000 Pennsylvania militia, to cover Pyle's Ford, 5.8 miles south of Chadds Ford, which was covered by Major Generals Anthony Wayne's and Nathanael Greene's divisions. Major General John Sullivan's division extended northward along the Brandywine's east banks, covering the high ground north of Chadds Ford along with Major General Adam Stephen's division and Major General Lord Stirling's divisions. Further upstream was a brigade under Colonel Moses Hazen covering Buffington's Ford and Wistar's Ford. Washington was confident that the area was secure.
 The British grouped forces at nearby Kennett Square.[8] Howe, who had better information about the area than Washington, had no intention of mounting a full-scale frontal attack against the prepared American defenses. He instead employed a flanking maneuver, similar to that used in the Battle of Long Island. About 6,800 men under the command of Wilhelm von Knyphausen advanced to meet Washington's troops at Chadds Ford.  The remainder of Howe's troops, about 9,000 men, under the command of Charles, Lord Cornwallis, marched north to Trimble's Ford across the West Branch of the Brandywine Creek, then east to Jefferis Ford across the East Branch (two fords that Washington had overlooked as a result of a poor understanding of the area and lack of credible reconnaissance), and then south to flank the American forces.[9]
 September 11 began with a heavy fog, which provided cover for the British troops. Washington received contradictory reports about the British troop movements and continued to believe that the main force was moving to attack at Chadds Ford.
 Knyphausen's Column
 At 5:30 a.m. the British and Hessian troops began marching east along the ""Great Road"" (now Route 1) from Kennett Square, advancing on the American troops positioned where the road crossed Brandywine Creek. The first shots of the battle took place about 4 miles west of Chadds Ford, at Welch's Tavern.  Elements of Maxwell's continental light infantry skirmished with the British vanguard (primarily the Queen's Rangers – a battalion of loyalists). The British continued to advance and encountered a greater force of continentals behind the stone walls on the Old Kennett Meetinghouse grounds. The battle was fought at mid-morning around the meeting house while the pacifist Quakers continued to hold their midweek service. One of the Quakers later wrote, ""While there was much noise and confusion without, all was quiet and peaceful within.""[10]
 From the Meetinghouse grounds, the battle continued for three miles to the Brandywine Creek, at Chadds Ford. Eventually the British pushed the Americans back but not before suffering heavy losses.
 Cornwallis's Column
 The main British column under General Cornwallis (and accompanied by General Howe) set out from Kennett Square at 5:00 a.m..  Local loyalist sources had provided Howe with knowledge of two unguarded fords, above the forks of the Brandywine.  The 17-mile flank march took approximately 9 hours to complete. The British appeared on the Americans' right flank at around 2 p.m. and took a much-needed rest on Osbourne's Hill, a commanding position north of the Continental army.  Having received intelligence from Colonel Bland's scouts, Washington ordered Sullivan to take overall command of Stirling and Stephen's divisions (in addition to his own) and quickly march north to meet the British flank attack.  As they were forming their lines north of Dilworth, Howe launched his attack.  Having taken overall command of the right wing of the army, Sullivan left his division to confer with the other generals.  His own division he left under the command of Preudhomme de Borre, with orders to shift to the right in order to link up with Stirling and Stephen's divisions (from left to right the divisions were arranged as Sullivan, Stirling, Stephen).  As the British lines advanced, the Hessian Jaegers threatened to flank the American right forcing Stephen and Stirling to shift right. Howe was slow to attack, which bought time for the Americans to position some of their men on high ground near Birmingham Meetinghouse, about a mile (1.6 km) north of Chadds Ford.[11] By 4 p.m., the British attacked.  The British Brigade of Guards caught de Borre by surprise on the American left, before de Borre had time to fully form, and immediately sent them in to disarray, causing the entire division to rout.  Initially,  Stephen's and Stirling's divisions held firm, aided by a battery of artillery on a knoll between their divisions.  However, the British light infantry battalions, aided by the Jaegers, eventually caused Stephen's division to fall back.  A bayonet charge by the British grenadier battalions, in the center, similarly forced Stirling to retreat.  The Marquis de Lafayette had only just arrived, joining Stirling's division, when he received a wound while trying to rally the retreating troops.
 Around 6 p.m., Washington and Greene arrived with reinforcements to try to hold off the British, who now occupied Meeting House Hill.  Washington conferred with Greene and Knox, the latter of whom was head of artillery, in the yard of the William Brinton house.[12] The 2nd Battalion of Grenadiers was nearing their position, and was joined by a fresh reserve brigade (the 4th British Brigade).  It was determined that Knox would deploy artillery to slow the British advance.  Greene's reinforcements, combined with the remnants of Sullivan's, Stephen's, and Stirling's divisions, formed south of Dilworth and stopped the pursuing British for nearly an hour, letting the rest of the army retreat.  When darkness fell, Greene's division finally began the march to Chester along with the rest of the army.  The British army was not able to pursue due to the onset of night.  The Americans were also forced to leave behind many of their cannons on Meeting House Hill because almost all of their artillery horses were killed.
 Upon hearing the attack of Cornwallis's column, Knyphausen launched an attack against the weakened American center across Chadds Ford, breaking through the divisions commanded by Wayne and William Maxwell and forcing them to retreat and leave behind most of their cannons. Armstrong's militia, never engaged in the fighting, also decided to retreat from their positions. Further north, Greene sent Brigadier General George Weedon's troops to cover the road just outside the town of Dilworth to hold off the British long enough for the rest of the Continental Army to retreat. Darkness brought the British pursuit to a standstill, which then allowed Weedon's force to retreat. The defeated Americans retreated to Chester where most of them arrived at midnight, with stragglers arriving until morning. The American retreat was well organized, largely because of the efforts of Lafayette, who, although wounded, created a rally point that allowed for a more orderly retreat before being treated for his wound.[13]
 The official British casualty list detailed 587 casualties: 93 killed (eight officers, seven sergeants and 78 rank and file); 488 wounded (49 officers, 40 sergeants,  four drummers and 395 rank and file); and six rank and file missing unaccounted for.[3] Only 40 of the British Army's casualties were Hessians.[14] Historian Thomas J. McGuire writes that, ""American estimates of British losses run as high as 2,000, based on distant observation and sketchy, unreliable reports"".[3]
 Most accounts of the American loss were from the British. One initial report by a British officer recorded American casualties at over 200 killed, around 750 wounded, and 400 prisoners were taken, many of them wounded. A member of General Howe's staff claimed that 400 rebels were buried on the field by the victors.[15] Another British officer wrote that, ""The Enemy had 502 dead in the field"".[3] General Howe's report to the British colonial secretary, Lord George Germain, said that the Americans, ""had about 300 men killed, 600 wounded, and near 400 made prisoners"".[3]
 No casualty return for the American army at Brandywine survives and no figures, official or otherwise, were ever released. The nearest thing to a hard figure from the American side was by Major General Nathanael Greene, who estimated that Washington's army had lost between 1,200 and 1,300 men.[16] On September 14, about 350 wounded Americans were taken from the British camp at Dilworth to a newly established hospital at Wilmington, Delaware.[17] This would suggest that of the ""near 400"" prisoners reported by Howe, only about 50 had surrendered unwounded. If General Greene's estimate of the total American loss was accurate, then they had between 1,160 and 1,260 killed, wounded or deserted during the battle. The British also captured 11 out of 14 of the American artillery pieces. Among the American wounded was the Marquis de Lafayette.
 In addition to losses in battle, 315 men were posted as deserters from Washington's camp during this stage of the campaign.[18]
 Washington had committed a serious error in leaving his right flank wide open and could have brought about his army's annihilation had it not been for Sullivan, Stirling and Stephen's divisions, which bought them time. Evening was approaching and, in spite of the early start Cornwallis had made in the flanking maneuver, most of the American army was able to escape. In his report to the Continental Congress detailing the battle, Washington stated: ""despite the day's misfortune, I am pleased to announce that most of my men are in good spirits and still have the courage to fight the enemy another day.""
 British and American forces maneuvered around each other for the next several days with only a few encounters such as the Battle of the Clouds on September 16 and the Battle of Paoli on the night of September 20–21.  In a matter of days, the Battles of Saratoga, hundreds of miles to the north, provided a victory over a British force which Howe was supposed to join.
 In preparation for the fall of Philadelphia, Pennsylvania's Supreme Executive Council ordered that eleven bells in the city, including the State House bell (known today as the Liberty Bell) and the bells from Christ Church and St. Peter's Church, be taken down and removed from Philadelphia to prevent the British Army from taking possession of them and melting them down to cast into munitions for use in the war. The Liberty Bell was transported to Allentown, where it was hidden for nine months under floor boards at Zion United Church of Christ in the city.[19]
 The Continental Congress then abandoned Philadelphia, moving first to Lancaster, for one day and then to York.
 On September 26, 1777, British forces marched into Philadelphia unopposed.
 Eight Army National Guard units (103rd Eng Bn,[20] A/1-104th Cav,[21] 109th FA,[22] 111th Inf,[23] 113th Inf,[24] 116th Inf,[25] 1–175th Inf[26] and 198th Sig Bn[27]) and one active Regular Army Field Artillery battalion (1–5th FA[28]) are derived from American units that participated in the Battle of Brandywine. There are thirty current U.S. Army units with lineages that go back to the colonial era.
 Brandywine Battlefield Historic Site is a National Historical Landmark. The historic park is owned and operated by the Pennsylvania Historical and Museum Commission, on 52 acres (210,000 m2), near Chadds Ford, Delaware County, part of the site of the Battle of Brandywine.
 The American Battlefield Trust and its partners have acquired and preserved more than 187 acres (0.76 km2) of the battlefield as of mid-2023.[29]
"
Battle of Germantown,https://en.wikipedia.org/wiki/Battle_of_Germantown,"
 The Battle of Germantown was a major engagement in the Philadelphia campaign of the American Revolutionary War. It was fought on October 4, 1777, at Germantown, Pennsylvania, between the British Army led by Sir William Howe, and the American Continental Army under George Washington.
 After defeating the Continental Army at the Battle of Brandywine on September 11, and the Battle of Paoli on September 20, Howe outmaneuvered Washington, seizing Philadelphia, the capital of the United States, on September 26. Howe left a garrison of some 3,000 troops in Philadelphia, while moving the bulk of his force to Germantown, then an outlying community to the city. Learning of the division, Washington determined to engage the British. His plan called for four separate columns to converge on the British position at Germantown. The two flanking columns were composed of 3,000 militia, while the center-left, under Nathanael Greene, the center-right under John Sullivan, and the reserve under Lord Stirling were made up of regular troops. The ambition behind the plan was to surprise and destroy the British force, much in the same way as Washington had surprised and decisively defeated the Hessians at Trenton. In Germantown, Howe had his light infantry and the 40th Foot spread across his front as pickets. In the main camp, Wilhelm von Knyphausen commanded the British left, while Howe himself personally led the British right.
 A heavy fog caused a great deal of confusion among the approaching Americans. After a sharp contest, Sullivan's column routed the British pickets. Unseen in the fog, around 120 men of the British 40th Foot barricaded the Chew House. When the American reserve moved forward, Washington made the decision to launch repeated assaults on the position, all of which failed with heavy casualties. Penetrating several hundred yards beyond the mansion, Sullivan's wing became dispirited, running low on ammunition and hearing cannon fire behind them. As they withdrew, Anthony Wayne's division collided with part of Greene's late-arriving wing in the fog. Mistaking each other for the enemy, they opened fire, and both units retreated. Meanwhile, Greene's left-center column threw back the British right. With Sullivan's column repulsed, the British left outflanked Greene's column. The two militia columns had only succeeded in diverting the attention of the British, and had made no progress before they withdrew.
 Despite the defeat, France, already impressed by the American success at Saratoga, decided to lend greater aid to the Americans. Howe did not vigorously pursue the defeated Americans, instead turning his attention to clearing the Delaware River of obstacles at Red Bank and Fort Mifflin. After unsuccessfully attempting to draw Washington into combat at White Marsh, Howe withdrew to Philadelphia. Washington, his army intact, withdrew to Valley Forge, where he wintered and re-trained his forces.
 The Philadelphia campaign had begun badly for the Americans. Washington's Continental Army suffered a string of defeats at Cooch's Bridge,[6] Brandywine,[7] and Paoli. After inflicting a stinging defeat on Anthony Wayne's division at Paoli on September 20,[8] the British army marched north to Valley Forge then west to the French Creek bridge.[9] At this point, Howe's right wing faced Fatland Ford on the Schuylkill River near Valley Forge while the left wing was opposite Gordon's Ford at French Creek and the left center faced Richardson's Ford. The American army defended all these Schuylkill crossings, plus one farther downstream at Swede's Ford[10] near Norristown.[11] On September 22, a small British force under Sir William Erskine feinted north and another force mounted a demonstration at Gordon's Ford.[12] Howe's moves convinced Washington that the British commander was trying to seize his supply base at Reading and turn his right flank. Washington moved north, but in the night of September 22–23, the British army reversed direction.[13] They crossed the Schuylkill at Fatland and Richardson's Fords without opposition, and after a brief rest, headed downstream toward Swede's Ford where the American militia abandoned three cannons.[12]
 Charles Cornwallis subsequently seized Philadelphia for the British on September 26, dealing a blow to the revolutionary cause. Howe left a garrison of 3,462 men to defend the city, moving the bulk of his force north, some 9,728 men, to the outlying community of Germantown.[2] With the campaigning season drawing to a close, Howe determined to locate and destroy the main American army. Howe established his headquarters at the Stenton Mansion, the former country home of James Logan.
 Despite having suffered successive defeats, Washington saw an opportunity to entrap and decisively defeat the divided British army. He resolved to attack the Germantown garrison, as the last effort of the year before entering winter quarters. His plan called for a complex, ambitious assault; four columns of troops were to assail the British garrison from different directions, at night, with the goal of creating a double-envelopment. Washington's hope was that the British would be surprised and overwhelmed much how the Hessians were at Trenton.
 Germantown was a hamlet of stone houses, spreading from what is now known as Mount Airy on the north, to what is now Market Square in the south.[14] Extending southwest from Market Square was Schoolhouse Lane, running 1.5 miles (2.4 km) to the point where Wissahickon Creek emptied from a steep gorge, into the Schuylkill River. Howe had established his main camp along the high ground of Schoolhouse and Church lanes. The western wing of the camp, under the command of Hessian general Wilhelm von Knyphausen, had a picket of two Jäger battalions, positioned on the high ground above the mouth of the Wissahickon to the far left. A brigade of Hessians, and two brigades of British regulars camped along Market Square. East of the Square, two British brigades under the command of General James Grant had encamped, with two squadrons of dragoons, and the 1st battalion of Light Infantry. The Queen's Rangers, a unit of loyalist Americans recruited from New York, covered the right flank.
 After dusk on October 3, the American force began the 16 miles (26 km) march southward toward Germantown in complete darkness. To differentiate friend from foe in the darkness, the troops were instructed to put a piece of white paper in their hats to mark them out.[15] The Americans remained undetected by the Jäger pickets, and the main British camp was, subsequently, unaware of the American advance. For the Americans, it seemed their attempt to repeat their victory at Trenton was on the road to success. However, the darkness made communications between the American columns extremely difficult, and progress was far slower than expected. At dawn, most of the American forces had fallen too short of their intended positions, losing the element of surprise they otherwise enjoyed.
 The Pennsylvania Militia, led by Brigadier General John Armstrong Sr., advanced down the Manatawny Road (Ridge Avenue) to the confluence of the Wissahickon Creek and Schuylkill River. There on the cliffs opposite General Knyphausen's Hessian encampment, the militia set up their artillery and began a desultory fire until withdrawing back up the Manatawny road. Armstrong's Brigade played no further part in the battle.  The three remaining American columns continued their advance. One column, under the command of General John Sullivan moved down Germantown Road. A column of New Jersey militia under Brigadier General William Smallwood moved down Skippack Road to Whitemarsh Church Road, and from there to Old York Road to attack the British right. General Nathanael Greene's column, consisting of Greene's, General Adam Stephen's divisions and General Alexander McDougall's brigade, moved down Limekiln Road.
 A thick fog[16] clouded the battlefield throughout the day, greatly hampering coordination. The vanguard of Sullivan's column, upon Germantown Road, opened fire upon the British pickets on Mount Airy, just after sunrise at 05:00. The British pickets fired their cannon in alarm, and resisted the American advance. Howe rode forward, thinking they were being attacked by foraging or skirmishing parties, and ordered his men to hold their ground. It took a substantial part of Sullivan's division to finally overwhelm the British pickets, and drive them back into Germantown.
 Howe, still believing his men were facing only light opposition, called out; ""For shame, Light Infantry! I never saw you retreat before! Form! Form! It is only a scouting party!"" Just then, three American guns came into action, opening fire with grapeshot. Howe and his staff quickly withdrew out of range. Several British officers were shocked to see their own soldiers rapidly falling back before the enemy attack. One British officer later described the number of attacking Americans as ""overwhelming"".[17]
 Cut off from the main force, Lieutenant Colonel Thomas Musgrave, of the British 40th Regiment of Foot, ordered his six companies of troops, around 120 men, to barricade and fortify the stone house of Pennsylvania Chief Justice Benjamin Chew, called Cliveden. The American troops launched a determined assault against Cliveden; however, the outnumbered defenders repulsed their attempts, inflicting heavy casualties. Washington called a council of war to decide how to deal with the fortification. Some of his subordinates favoured bypassing Clivden entirely, leaving a regiment behind to besiege it. However, Washington's artillery commander, Brigadier General Henry Knox, advised it was unwise to allow a fortified garrison to remain under enemy control in the rear of a forward advance. Washington concurred.
 General William Maxwell's brigade, which had been held in reserve, was brought forward to storm Cliveden, partially led by a volunteer aide from General Washington's own staff, Lieutenant Colonel John Laurens, who had been shot through his right and therefore dominant shoulder earlier in the battle; he had continued to fight with his aide riband wrapped around his arm and his sword in his left and non-dominant hand. Knox positioned four 3-pound cannon out of musket range to bombard the mansion. However, the thick walls of Cliveden withstood the bombardment from the light field guns. The Americans launched a second wave of infantry assaults, all of which were repulsed with heavy losses. The few Americans who managed to get inside the mansion were shot or bayoneted. It was becoming clear to the Americans that Cliveden was not going to be taken easily. Among this assault was Lieutenant John Marshall of the Virginia Line, the future Chief Justice of the United States, who was wounded during the attack.
 Before completely disregarding the notion of taking Cliveden, Laurens was offered the idea of burning or smoking the British out of the house. He took the idea and, according Gregory Massey's biography, he sent men to gather firewood to pile at the door. Once enough had gathered, Laurens and Marshall began to command the attack of charging to the front steps, dropping the firewood, and in groups, rushing to set fire to the wood. Laurens himself was sent forward with a torch with his companion, Major John White. The fire did not catch. While on the front steps of Cliveden, however, Laurens was under a barrage of gunfire and supposedly wounded a second time by a bayonet in his left side. Major White was shot and later pronounced dead. Both men were forced from the door and the Chevalier de Mauduit convinced Laurens to cease his fighting before he later was moved to have a surgeon see to his wounds. 
 Prior to Maxwell's futile attack against Cliveden, Sullivan's division advanced beyond in the fog. Sullivan deployed Brigadier General Thomas Conway's brigade to the right, and Brigadier General Anthony Wayne's brigade to the left before advancing on the British center-left.[18] The 1st and 2nd Maryland Brigades of Sullivan's column paused frequently to fire volleys into the fog. While the tactic was effective in suppressing enemy opposition, his troops rapidly ran low on ammunition. Wayne's brigade to the left of the road moved ahead, and became precariously separated from Sullivan's main line. As the Americans launched their attack on Cliveden, Wayne's brigade heard the disquieting racket from Knox's artillery pieces to their rear. To their right, the firing from Sullivan's men died down as the Marylanders ran low on ammunition. Wayne's men began to panic in their apparent isolation, and so he ordered them to fall back. Sullivan was subsequently forced back, although the regiments fought a stubborn rear-guard action. Since the British units pursuing them were redirected to fight Greene's column, Sullivan's men fell back in good order.[19]
 Meanwhile, Nathanael Greene's column on Limekiln Road had finally caught up with the bulk of the Americans at Germantown. Greene's vanguard engaged the British pickets at Luken's Mill, driving them back after a savage skirmish. The fog that clung to the field was compounded by palls of smoke from the cannon and musket fire, throwing Greene's column into disarray and confusion. One of Greene's brigades, under Brigadier General Adam Stephen, veered off-course and began following Meetinghouse Road, instead of rendezvousing at Market Square with the rest of Greene's troops. The wayward brigade collided with Wayne's brigade, and mistook them for redcoats. The two American brigades opened fire on each other in the fog, causing both to flee. The withdrawal of Wayne's New Jersey Brigade, having suffered heavy losses attacking Cliveden, left Conway's right flank exposed.
 To the north, an American column led by McDougall came under attack by the Loyalist troops of the Queen's Rangers, and the Guards of the British reserve. After a brutal contest, McDougall's brigade was forced to retreat, having suffered heavy losses. Despite the reversal in fortune, the Continentals were still convinced of a possible victory. The 9th Virginia Regiment of Greene's column launched a determined attack on the British lines as planned, managing to break through and capturing a number of prisoners. However, they were soon surrounded by two arriving British brigades under Cornwallis. Cornwallis then launched a counter-charge, cutting off the Virginians completely, forcing them to surrender. Greene, upon learning of the main army's defeat and withdrawal, realized he stood alone against Howe's entire army, and so withdrew.
 The primary attacks on the British and Hessian camp had all been repulsed with heavy casualties. Washington ordered Armstrong and Smallwood's men to withdraw. Maxwell's brigade, still having failed to capture Cliveden, was forced to fall back. Howe ordered a pursuit, harrying the retreating Americans for some 9 miles (14 km), though he did not follow up on his victory. The pursuing British forces were finally forced to retire in the face of resistance from Greene's infantry, Wayne's artillery, and a detachment of dragoons, as well as the coming of the night.
 On October 6, there was a brief cease-fire. A little terrier that was identified from its collar as belonging to General Howe was formally transferred from Washington's camp to Howe's under a flag of truce.  The little terrier that had been found wandering on the battlefield was brought to Washington, who had the dog fed, cleaned and brushed before being returned to Howe.[20][21]
 Of the 11,000 men Washington led into battle, 30 officers and 122 men were killed, and 117 officers and 404 men were wounded.[23] According to a Hessian staff officer, some 438 had been taken prisoner by the British, including Colonel George Mathews and the entire 9th Virginia Regiment.[23][24] Brigadier General Francis Nash, whose North Carolina brigade covered the American retreat, had his left leg taken off by a cannonball, and died on October 8 at the home of Adam Gotwals. His body was interred with military honors on October 9 at the Mennonite Meetinghouse in Towamencin.[25]  Major John White, who was shot at Cliveden, died on October 10.[26] Lieutenant-Colonel William Smith, who was wounded carrying the flag of truce to Cliveden, also died from his wounds.[26] In total, 57 Americans, over one-third of all those killed in the battle, died in the attack on Cliveden.[27]
 British casualties in the battle were 71 killed, 448 wounded and 14 missing; only 24 of whom were Hessians.[4] British officers killed in action include Brigadier General James Agnew and Lieutenant-Colonel John Bird. Lieutenant-Colonel William Walcott of the 5th Regiment of Foot was mortally wounded, and later died.[28]
 Wyck House served as a hospital during the battle.
 Washington's ambitious plan failed for several factors:
 Washington had intended for his attack to be a second Trenton. Had everything gone according to plan, Washington may have trapped and destroyed a second major British force. Coupled with Burgoyne's defeat at Saratoga, the defeat of Howe at Germantown could have compelled Lord North and the British government to sue for peace.[31]
 The battle was a victory for the British, but the long-term strategic consequences favored the Americans. Howe had, once again, failed to follow up on his success and allowed Washington to escape with his army, leading to their encampment at Valley Forge.
 The battle in particular made a strong impression upon the French court that the Americans would prove worthy allies. Sir George Otto Trevelyan, in Volume IV of his History of the American Revolution, concluded that although the battle had unquestionably been a defeat for the Americans, it was of ""great and enduring service to the American cause"". In particular, the engagement persuaded the Comte de Vergennes to vouch for the United States against Britain.[33] He continues:
 John Fiske, in The American Revolution (1891), wrote:[31]
 Eight Army National Guard units (103rd Eng Bn,[34] A/1-104th Cav,[35] 109th FA,[36] 111th Inf,[37] 113th Inf,[38] 116th Inf,[39] 175th Inf[40] and 198th Sig Bn[41]) and one active Regular Army Field Artillery battalion (1-5th FA)[42] are derived from American units that participated in the Battle of Germantown. There are only thirty currently existing units in the U.S. Army with lineages that go back to the colonial era.
"
Siege of Yorktown,https://en.wikipedia.org/wiki/Siege_of_Yorktown,"
 The siege of Yorktown, also known as the Battle of Yorktown and the surrender at Yorktown, began September 28, 1781, and ended on October 19, 1781, at exactly 10:30 am in Yorktown, Virginia. It was a decisive victory by a combined force of the American Continental Army troops led by General George Washington with support from the Marquis de Lafayette and French Army troops led by the Comte de Rochambeau and a French naval force commanded by the Comte de Grasse over the British Army commanded by British Lieutenant General Charles Cornwallis.
 The siege of Yorktown was the last major land battle of the American Revolutionary War in North America, and led to the surrender of General Cornwallis and the capture of both him and his army. The Continental Army's victory at Yorktown prompted the British government to negotiate an end to the conflict.[b]
 In 1780, about 5,500 French soldiers landed in Rhode Island to help their American allies fight the British troops controlling New York City. Following the arrival of dispatches from France that included the possibility of support from the French West Indies fleet of the Comte de Grasse, disagreements arose between Washington and Rochambeau on whether to ask de Grasse for assistance in besieging New York or in military operations against a British army in Virginia. On the advice of Rochambeau, de Grasse informed them of his intent to sail to the Chesapeake Bay, where Cornwallis had taken command of the army. Cornwallis, at first given confusing orders by his superior officer, Henry Clinton, was eventually ordered to build a defensible deep-water port, which he began to do in Yorktown. Cornwallis' movements in Virginia were shadowed by a Continental Army force led by Marquis de Lafayette.
 The French and American armies united north of New York City during the summer of 1781. When word of de Grasse's decision arrived, both armies began moving south toward Virginia, engaging in deception tactics to lead the British to believe a siege of New York was planned. De Grasse sailed from the West Indies and arrived at the Chesapeake Bay at the end of August, bringing additional troops and creating a naval blockade of Yorktown.  He was transporting 500,000 silver reales collected from the Spanish subject residents of Havana, Cuba, to fund supplies for the siege and payroll for the Continental Army.[8] While in Santo Domingo, de Grasse met with Francisco Saavedra de Sangronis, an agent of King Charles III of Spain. De Grasse had planned to leave several of his warships in Santo Domingo. Saavedra promised the assistance of the Spanish Navy to protect the French merchant fleet, enabling de Grasse to sail north with all of his warships.[9] In the beginning of September, he defeated a British fleet led by Sir Thomas Graves that came to relieve Cornwallis at the Battle of the Chesapeake. As a result of this victory, de Grasse blocked any reinforcement or escape by sea for Cornwallis and also disembarked the heavy siege guns required by the allied land forces. By late September, Washington and Rochambeau arrived, and the army and naval forces completely surrounded Cornwallis.
 After initial preparations, the Americans and French built their first parallel and began the bombardment. With the British defense weakened, on October 14, 1781, Washington sent two columns to attack the last major remaining British outer defenses. A French column under Vicomte de Deux-Ponts took Redoubt No. 9 and an American column under Lieutenant Colonel Alexander Hamilton took Redoubt No. 10. With these defenses taken, the allies were able to finish their second parallel. With the Franco-American artillery closer and its bombardment more intense than ever, the British position began to deteriorate rapidly. Cornwallis asked for capitulation terms on October 17. After two days of negotiation, the surrender ceremony occurred on October 19; Cornwallis was absent from the ceremony. With the capture of more than 7,000 British soldiers, negotiations between the United States and Great Britain began, resulting in the Treaty of Paris of 1783.
 The battlegrounds are preserved and interpreted today as part of Colonial National Historical Park.
 On December 20, 1780, Benedict Arnold sailed from New York with 1,500 British troops to Portsmouth, Virginia. He first raided Richmond, defeating the defending militia, from January 5–7 before falling back to Portsmouth.[10] Admiral Destouches, who arrived in Newport, Rhode Island, in July 1780 with a fleet transporting 5,500 soldiers, was encouraged by Washington and French Lieutenant General Rochambeau to move his fleet south, and launch a joint land-naval attack on Arnold's troops.[10] The Marquis de Lafayette was sent south with 1,200 men to help with the assault.[11] However, Destouches was reluctant to dispatch many ships, and in February sent only three. After they proved ineffective, he took a larger force of eight ships in March 1781, and fought a tactically inconclusive battle with the British fleet of Marriot Arbuthnot at the mouth of the Chesapeake Bay. Destouches withdrew due to the damage sustained to his fleet, leaving Arbuthnot and the British fleet in control of the bay's mouth.[11]
 On March 26, Arnold was joined by 2,300 troops under command of Major General William Phillips, who took command of the combined forces.[11] Phillips resumed raiding, defeating the militia at Blandford, then burning the tobacco warehouses at Petersburg on April 25. Richmond was about to suffer the same fate, but Lafayette arrived. The British, not wanting to engage in a major battle, withdrew to Petersburg on May 10.[11]
 On May 20, Charles Cornwallis arrived at Petersburg with 1,500 men after suffering heavy casualties at the Battle of Guilford Courthouse. He immediately assumed command, as Phillips had recently died of a fever.[11][12] Cornwallis had not received permission to abandon the Carolinas from his superior, Henry Clinton, but he believed that Virginia would be easier to capture, feeling that it would approve of an invading British army.[11]
 With the arrival of Cornwallis and more reinforcements from New York, the British Army numbered 7,200 men.[5] Cornwallis wanted to push Lafayette, whose force now numbered 3,000 men with the arrival of Virginia militia.[5] On May 24, he set out after Lafayette, who withdrew from Richmond, and linked forces with those under the command of Baron von Steuben and Anthony Wayne.[5] Cornwallis did not pursue Lafayette. Instead, he sent raiders into central Virginia, where they attacked depots and supply convoys, before being recalled on June 20. Cornwallis then headed for Williamsburg, and Lafayette's force of now 4,500 followed him.[13] General Clinton, in a confusing series of orders, ordered Cornwallis first to Portsmouth and then Yorktown, where he was instructed to build fortifications for a deep water port.[14][15]
 On July 6, the French and American armies met at White Plains, north of New York City.[16] Although Rochambeau had almost 40 years of warfare experience, he never challenged Washington's authority, telling Washington he had come to serve, not to command.[17]
 Washington and Rochambeau discussed where to launch a joint attack.[18] Washington believed an attack on New York was the best option, since the Americans and French now outnumbered the British defenders 3 to 1. Rochambeau disagreed, arguing the fleet in the West Indies under Admiral de Grasse was going to sail to the American coast, where easier options than attacking New York could be attempted.[18]
 In early July, Washington suggested an attack be made at the northern part of Manhattan Island, but his officers and Rochambeau all disagreed.[19] Washington continued to probe the New York area until August 14, when he received a letter from de Grasse stating he was headed for Virginia with 28 warships and 3,200 soldiers, but could only remain there until October 14.[19] De Grasse encouraged Washington to move south so they could launch a joint operation. Washington abandoned his plan to take New York, and began to prepare his army for the march south to Virginia.[20]
 On August 19, the ""celebrated march"" to Yorktown led by Washington and Rochambeau began.[20] 7,000 soldiers (4,000 French and 3,000 American) began the march in Newport, Rhode Island, while the rest remained behind to protect the Hudson Valley. Washington wanted to maintain complete secrecy of their destination.[21] To ensure this, he sent out fake dispatches that reached Clinton revealing that the Franco-American army was going to launch an attack on New York, and that Cornwallis was not in danger.[22]
 The French and American armies marched through Philadelphia from September 2 to 4, where the American soldiers announced they would not leave Maryland until they received one month's pay in coin, rather than in the worthless Continental paper currency. ""Count de Rochambeau very readily agreed at Chester to supply at the Head of Elk twenty thousand hard dollars"",[23] half of his supply of gold Spanish coins. This would be the last time the men would be paid. This strengthened French and American relations.[24] On September 5, Washington learned of the arrival of de Grasse's fleet off the Virginia Capes. De Grasse debarked his French troops to join Lafayette, and then sent his empty transports to pick up the American troops.[20] Washington made a visit to his home, Mount Vernon, on his way to Yorktown.[25]
 In August, Admiral Sir Thomas Graves led a fleet from New York to attack de Grasse's fleet. Graves did not realize how large the French fleet was, and neither did Cornwallis.[25] The British fleet was defeated by de Grasse's fleet in the Battle of the Chesapeake on September 5, and forced to fall back to New York.[25] On September 14, Washington arrived in Williamsburg, Virginia.[25]
 On September 26, transports with artillery, siege tools, and some French infantry and shock troops from Head of Elk, the northern end of the Chesapeake Bay, arrived, giving Washington command of an army of 7,800 Frenchmen, 3,100 militia, and 8,000 Continentals.[3] Early on September 28, Washington led the army out of Williamsburg to surround Yorktown.[26] The French took the positions on the left while the Americans took the position of honor on the right.[3] Cornwallis had a chain of seven redoubts and batteries linked by earthworks along with batteries that covered the narrows of the York River at Gloucester Point.[3] That day, Washington reconnoitered the British defenses, and decided that they could be bombarded into submission.[27] The Americans and the French spent the night of the 28th sleeping out in the open, while work parties built bridges over the marsh. Some of the American soldiers hunted down wild hogs to eat.[28]
 On September 29, Washington moved the army closer to Yorktown, and British gunners opened fire on the infantry.[29] Throughout the day, several British cannon fired on the Americans, but there were few casualties. Fire was also exchanged between American riflemen and Hessian Jägers.[29]
 Cornwallis pulled back from all of his outer defenses, except for the Fusilier's redoubt on the west side of the town and redoubts 9 and 10 in the east.[3] Cornwallis had his forces occupy the earthworks immediately surrounding the town because he had received a letter from Clinton that promised relief force of 5,000 men within a week and he wished to tighten his lines.[3][30] The Americans and the French occupied the abandoned defenses and began to establish their batteries there.[31] With the British outer defenses in their hands, allied engineers began to lay out positions for the artillery. The men improved their works and deepened their trenches.[32] The British also worked on improving their defenses.[32]
 On September 30, the French attacked the British Fusiliers redoubt.[33] The skirmish lasted two hours, in which the French were repulsed, suffering several casualties. On October 1, the allies learned from British deserters that, to preserve their food, the British had slaughtered hundreds of horses and thrown them on the beach.[33] In the American camp, thousands of trees were cut down to provide wood for earthworks. Preparations for the parallel also began.[34]
 As the allies began to put their artillery into place, the British kept up a steady fire to disrupt them.[35] British fire increased on the 2nd and the allies suffered moderate casualties. General Washington continued to make visits to the front, despite concern shown by several of his officers over the increasing enemy fire.[36] On the night of October 2, the British opened a storm of fire to cover up the movement of the British cavalry to Gloucester where they were to escort infantrymen on a foraging party.[36] On the 3rd, the foraging party, led by Banastre Tarleton, went out but collided with Lauzun's Legion, and John Mercer's Virginia militia, led by the Marquis de Choisy. The British cavalry quickly retreated behind their defensive lines, losing 50 men.[37]
 By October 5, Washington was almost ready to open the first parallel.[38] That night the sappers and miners worked, putting strips of pine on the wet sand to mark the path of the trenches. The main/ initial movements of this battle were walking and riding horses.[38]
 After nightfall on October 6, troops moved out in stormy weather to dig the first parallel: the heavily overcast sky negated the waning full moon and shielded the massive digging operation from the eyes of British sentries.[d] Washington ceremoniously struck several blows with his pickaxe to begin the trench. The trench was to be 2,000 yd (1,800 m) long, running from the head of Yorktown to the York River.[40] Half of the trench was to be commanded by the French, the other half by the Americans. On the northernmost end of the French line, a support trench was dug so that they could bombard the British ships in the river.[40] The French were ordered to distract the British with a false attack, but the British were told of the plan by a French deserter and the British artillery fire turned on the French from the Fusiliers redoubt.[41]
 On October 7, the British saw the new allied trench just out of musket-range.[41] Over the next two days, the allies completed the gun placements and dragged the artillery into line. The British fire began to weaken when they saw the large number of guns the allies had.[42]
 By October 9, all of the French and American guns were in place.[42] Among the American guns there were three twenty-four pounders, three eighteen pounders, two eight-inch (203 mm) howitzers and six mortars, totaling fourteen guns. At 3:00 pm, the French guns opened the barrage and drove the British frigate HMS Guadeloupe across the York River, where she was scuttled to prevent capture. At 5:00 pm, the Americans opened fire.[42] Washington fired the first gun; legend has it that this shot smashed into a table where British officers were eating. The Franco-American guns began to tear apart the British defenses.[43] Washington ordered that the guns fire all night so that the British could not make repairs.[43] All of the British guns on the left were soon silenced. The British soldiers began to pitch their tents in their trenches and soldiers began to desert in large numbers.[44] Some British ships were also damaged by cannonballs that flew across the town into the harbor.[44]
 On October 10, the Americans spotted a large house in Yorktown.[45] Believing that Cornwallis might be stationed there, they aimed at it and quickly destroyed it. Cornwallis sank more than a dozen of his ships in the harbor. The French began to fire at the British ships and scored a hit on the British HMS Charon, which caught fire, and in turn set two or three other ships on fire.[46] Cornwallis received word from Clinton that the British fleet was to depart on October 12, however Cornwallis responded by saying that he would not be able to hold out for long.[47]
 On the night of October 11, Washington ordered that the Americans dig a second parallel.[47] It was 400 yd (370 m) closer to the British lines, but could not be extended to the river because the British number 9 and 10 redoubts were in the way. During the night, the British fire continued to land in the old line; Cornwallis did not suspect that a new parallel was being dug.[47] By morning of the 12th, the allied troops were in position on the new line.[47]
 By October 14, the trenches were within 150 yd (140 m) of Redoubt Nos. 9 and 10.[48] Washington ordered that all guns within range begin blasting the redoubts to weaken them for an assault that evening.[49] Washington planned to use the cover of a moonless night to gain the element of surprise.[e] To reinforce the darkness, he added silence, ordering that no soldier should load his musket until reaching the fortifications; the advance would be made with only ""cold steel."" Redoubt No. 10 was near the river and held by only 70 men, while redoubt 9 was a quarter-mile inland, and was held by 120 British and Germans.[49] Both redoubts were heavily fortified with rows of abatis surrounding them, along with muddy ditches that surrounded the redoubts at about 25 yd (23 m).[48] Washington’s officers devised a plan in which the French would launch a diversionary attack on the Fusiliers redoubt, and then a half an hour later, the French would assault Redoubt No. 9 and the Americans Redoubt No. 10.[49][51] Redoubt No. 9 would be assaulted by 400 French regular soldiers of the Royal Deux-Ponts Regiment under the command of the Count of Deux-Ponts and redoubt 10 would be assaulted by 400 light infantry troops under the command of Alexander Hamilton.[51] There was a brief dispute as to who should lead the attack on Redoubt No. 10. Lafayette named his aide, Jean-Joseph Sourbader de Gimat, who commanded a battalion of Continental light infantry. However, Hamilton protested, saying that he was the senior officer. Washington concurred with Hamilton and gave him command of the attack.[52][f]
 At 6:30 pm, gunfire announced the diversionary attack on the Fusiliers redoubt.[53] At other places in the line, movements were made as if preparing for an assault on Yorktown itself, which caused the British to panic.[53] With bayonets fixed, the Americans marched towards Redoubt No. 10. Hamilton sent Lieutenant Colonel John Laurens around to the rear of the redoubt to prevent the British from escaping.[54] The Americans reached the redoubt and began chopping through the British wooden defenses with their axes. A British sentry called a challenge, and then fired at the Americans.[54] The Americans responded by charging with their bayonets towards the redoubt. They hacked through the abatis, crossed a ditch and climbed the parapet into the redoubt.[55] The Americans forced their way into the redoubt, falling into giant shell holes created by the preparatory bombardment. The British fire was heavy, but the Americans overwhelmed them.[55] Someone in the front shouted, ""Rush on boys! The fort's ours!"" The British threw hand grenades at the Americans with little effect.[55] Men in the trench stood on the shoulders of their comrades to climb into the redoubt. The bayonet fight cleared the British from the redoubt and almost the entire garrison was captured, including the commander of the redoubt, Major Campbell.[56] In the assault, the Americans lost 9 dead and 25 wounded.[56]
 The French assault began at the same time, but they were halted by the abatis, which was undamaged by the artillery fire.[56] The French began to hack at the abatis and a Hessian sentry came out and asked who was there. When there was no response, the sentry opened fire as did other Hessians on the parapet.[57] The French soldiers fired back, and then charged the redoubt. The Germans charged the Frenchmen climbing over the walls but the French fired a volley, driving them back.[57] The Hessians then took a defensive position behind some barrels but threw down their arms and surrendered when the French prepared a bayonet charge.[57]
 With the capture of Redoubts Nos. 9 and 10, Washington was able to have his artillery shell the town from three directions and the allies moved some of their artillery into the redoubts.[58][59] On October 15, Cornwallis turned all of his guns onto the nearest allied position. He then ordered a storming party of 350 British troops under the command of Colonel Robert Abercromby to attack the allied lines and spike the American and French cannon (i.e., plug the touch hole with an iron spike).[60] The allies were sleeping and unprepared. As the British charged Abercromby shouted ""Push on my brave boys, and skin the bastards!""[59] The British party spiked several cannons in the parallel and then spiked the guns on an unfinished redoubt.[61] A French party came and drove them out of the allied lines and back to Yorktown. The British had been able to spike six guns, but by the morning they were all repaired.[61] The bombardment resumed with the American and French troops engaged in competition to see who could do the most damage to the enemy defenses.[59]
 On the morning of October 16, more allied guns were in line and the fire intensified.[61] In desperation, Cornwallis attempted to evacuate his troops across the York River to Gloucester Point.[59] At Gloucester Point, the troops might be able to break through the allied lines and escape into Virginia and then march to New York.[62] One wave of boats made it across, but a squall hit when they returned to take more soldiers, making the evacuation impossible.[63]
 The fire on Yorktown from the allies was heavier than ever as new artillery pieces joined the line.[64] Cornwallis talked with his officers that day and they agreed that their situation was hopeless.[65]
 On the morning of October 17, a drummer appeared, followed by an officer waving a white handkerchief.[66] The bombardment ceased, and the officer was blindfolded and led behind the French and American lines. Negotiations began at the Moore House on October 18 between Lieutenant Colonel Thomas Dundas and Major Alexander Ross (who represented the British) and Lieutenant Colonel Laurens (who represented the Americans) and Marquis de Noailles (who represented the French). To make sure that nothing fell apart between the French and Americans at the last minute, Washington ordered that the French be given an equal share in every step of the surrender process. At 2:00 pm the allied army entered the British positions, with the French on the left and the Americans on the right.[66]
 The British had asked for the traditional honors of war, which would allow the army to march out with flags flying, bayonets fixed, and the band playing an American or French tune as a tribute to the victors. However, Washington firmly refused to grant the British the honors that they had denied the defeated American army the year before at the siege of Charleston.[67] Consequently, the British and Hessian troops marched with flags furled and muskets shouldered, while the band was forced to play ""a British or German march.""[g] American history books recount the legend that the British band played ""The World Turn'd Upside Down"", but the story may be apocryphal.[69][70]
 Cornwallis refused to attend the surrender ceremony, claiming that he had an illness. Instead, Brigadier General Charles O'Hara led the British army onto the field. O'Hara first attempted to surrender to Rochambeau, who shook his head and pointed to Washington. O'Hara then offered his sword to Washington, who also refused and motioned to Major General Benjamin Lincoln, his second-in-command. The surrender finally took place when Lincoln accepted the sword of Cornwallis' deputy.[71][72][73]
 The British soldiers marched out and laid down their arms in between the French and American armies, while many civilians watched.[74] At this time, the troops on the other side of the river in Gloucester also surrendered.[75] The British soldiers had been issued new uniforms hours before the surrender and until prevented by General O'Hara some threw down their muskets with the apparent intention of smashing them. Others wept or appeared to be drunk.[76] In all, 8,000 soldiers, 214 artillery pieces, thousands of muskets, 24 transport ships, wagons, and horses were captured.[72]
 Malaria was endemic in the marshlands of eastern Virginia during the time, and Cornwallis's army suffered greatly from the disease; he estimated during the surrender that half of his army was unable to fight as a result. The Continental Army enjoyed an advantage, in that most of their members had grown up with malaria, and hence had acquired resistance to the disease. As malaria has a month-long incubation period, most of the French soldiers had not begun to exhibit symptoms before the surrender.[77][78]
 The articles of capitulation, outlining the terms and conditions of surrender for officers, soldiers, military supplies, and personal property, were signed on October 19, 1781.[66] Signatories included Washington, Rochambeau, the Comte de Barras (on behalf of the French Navy), Cornwallis, and Captain Thomas Symonds (the senior Royal Navy officer present).[79] Cornwallis' men were declared prisoners of war and promised good treatment in American camps, and officers were permitted to return home after taking their parole.[66]
 Articles of Capitulation, Yorktown
 George Washington refused to accept the Tenth Article of the Yorktown Articles of Capitulation, which granted immunity to provincials, and Cornwallis failed to make any effort to press the matter. ""The outcry against the Tenth Article was vociferous and immediate, as Americans on both sides of the Atlantic proclaimed their sense of betrayal.""[81]
 Following the surrender, the American and French officers entertained the British officers to dinner. The British officers were ""overwhelmed"" by the civility their erstwhile foes extended to them, with some French officers offering ""profuse"" sympathies for the defeat, as one British officer, Captain Samuel Graham, commented. Equally, the French aide to Rochambeau, Cromot du Bourg, noted the coolness of the British officers, particularly O'Hara, considering the defeat they had endured.[82]
 Five days after the battle ended, on October 24, 1781, the British fleet sent by Clinton to rescue the British army arrived. The fleet picked up several provincials who had escaped on October 18, and they informed Admiral Thomas Graves that they believed Cornwallis had surrendered.[83] Graves picked up several more provincials along the coast, and they confirmed this fact. Graves sighted the French Fleet, but chose to leave because he was outnumbered by nine ships, and thus he sent the fleet back to New York.[84]
 On October 25, Washington issued an order which stipulated that all fugitive slaves who had joined the British were to be rounded up by the Continental Army and placed under the supervision of armed guards in fortified positions on both sides of the York River until arrangements could be made to return them to their enslavers. Historian Gregory J. W. Urwin describes Washington's action as ""[converting] his faithful Continentals—the men credited with winning American independence—into an army of slave catchers.""[85]
 After the British surrender, Washington sent Tench Tilghman to report the victory to Congress.[86] After a difficult journey, he arrived in Philadelphia, which celebrated for several days.  The British Prime Minister, Lord North, is reported to have exclaimed ""Oh God, it's all over"" when told of the defeat.[87] Three months after the battle, a motion to end ""further prosecution of offensive warfare on the continent of North America""—effectively a no confidence motion—passed in the British House of Commons. Lord North and his government resigned.
 Washington moved his army to New Windsor, New York[88] where they remained stationed until the Treaty of Paris was signed on September 3, 1783, formally ending the war.[89] Although the peace treaty did not happen for two years following the end of the battle, the Yorktown campaign proved to be decisive; there was no significant battle or campaign on the North American mainland after the Battle of Yorktown and in March 1782, ""the British Parliament had agreed to cease hostilities.""[90]
 On October 19, 1881, an elaborate ceremony took place to honor the battle's centennial. U.S. naval vessels floated on Chesapeake Bay, and
special markers highlighted where Washington and Lafayette's siege guns were placed. President Chester Arthur, sworn in only thirty days before, following James Garfield's death, made his first public speech as president. Also present were descendants of Lafayette, Rochambeau, de Grasse, and Steuben. To close the ceremony, Arthur gave an order to salute the British flag.[91]
 There is a belief that General Cornwallis's sword, surrendered by Charles O'Hara after the battle, is to this day on display at the White House. However, U.S. National Park Service historian Jerome Green, in his 2005 history of the siege, The Guns of Independence, concurs with the 1881 centennial account by Johnston, noting simply that when Brigadier General O'Hara presented the sword to Major General Lincoln, he held it for a moment and immediately returned it to O'Hara.[92]
 The American Battlefield Trust and its partners have preserved 49 battlefield acres outside of the national park as of mid-2023.[93]
 The siege of Yorktown is also known in some German historiographies as ""die deutsche Schlacht"" (""the German battle""), because Germans played significant roles in all three armies, accounting for roughly one third of all forces involved.  According to one estimate more than 2,500 German soldiers served at Yorktown with each of the British and French armies, and more than 3,000 German Americans were in Washington's army.[94]
 Four Army National Guard units (113th Inf,[95] 116th Inf,[96] 175th Inf,[97] and 198th Sig Bn[98]) and one active Regular Army Field Artillery battalion (1–5th FA)[99] are derived from American units that participated in the Battle of Yorktown. There are thirty current U.S. Army units with lineages that go back to the colonial era.
 Five days after the British surrendered, Congress passed a resolution agreeing to erect a structure dedicated to commemorating those who participated in the battle.[100] Construction of the monument was delayed, however, as the Confederation government had several other financial obligations that were considered to be of a more urgent nature.[100] In 1834, the citizens of Yorktown asked Congress for the monument to be constructed, and then followed up once again in 1836, but still no action was taken. The desirability of the project was recognized in 1876 ""when a memorial from the Common Council of Fredericksburg, Virginia was before Congress.""[100]
 The project was postponed once again until the battle's centennial sparked renewed enthusiasm in the resolution and prompted the government to begin building the monument in 1881 amid national support.[100] The crowning figure was set on August 12, 1884; the structure was officially reported in a communication as complete on January 5, 1885, and currently resides within Colonial National Historical Park.[100] The artists commissioned by the Secretary of War for the monument project included Mr. R. M. Hunt (Chairman) and Mr. J. Q. A. Ward (Architect) of New York and Mr. Henry Van Brunt (Sculptor) of Boston.[101][102]
 A four-day celebration to commemorate the 150th anniversary of the siege took place in Yorktown on October 16–19, 1931. It was presided over by the Governor of Virginia John Garland Pollard and attended by then President Herbert Hoover along with French representatives. The event included the official dedication of the Colonial National Historical Park, which also includes Historic Jamestown.[103][104]  President Ronald Reagan visited Yorktown in 1981 for the bicentennial celebration.
"
Treaty of Paris (1783),https://en.wikipedia.org/wiki/Treaty_of_Paris_(1783),"
 The Treaty of Paris, signed in Paris by representatives of King George III of Great Britain and representatives of the United States on September 3, 1783, officially ended the American Revolutionary War and recognized the Thirteen Colonies, which had been part of colonial British America, to be free, sovereign and independent states.
 The treaty set the boundaries between British North America, later called Canada, and the United States, on lines the British labeled as ""exceedingly generous"",[2] although exact boundary definitions in the far-northwest and to the south continued to be subject to some controversy. Details included fishing rights and restoration of property and prisoners of war.
 This treaty and the separate peace treaties between Great Britain and the nations that supported the American cause, including France, Spain, and the Dutch Republic are known collectively as the Peace of Paris.[3][4] Only Article 1 of the treaty, which acknowledges the United States' existence as free, sovereign, and independent states, remains in force.[5]
 Peace negotiations began in Paris in April 1782, following the victory of George Washington and the Continental Army in the American Revolutionary War. The negotiations continued through the summer of 1782. Representing the United States were Benjamin Franklin, John Jay, Henry Laurens, and John Adams. Representing the Kingdom of Great Britain and King George III were David Hartley and Richard Oswald.
 The treaty was drafted on November 30, 1782,[a] and signed at the Hôtel d'York at present-day 56 Rue Jacob in Paris on September 3, 1783, by Adams, Franklin, Jay, and Hartley.[6]
 In September 1782, French Foreign Minister Vergennes proposed a solution to deadlocked negotiations between the United States and the British, which was rejected by the United States. France was exhausted by the war, and all parties sought peace, except for Spain, which insisted on continuing the Revolutionary War until it could capture Gibraltar from the British. Vergennes developed treaty terms under which Spain would forego holding Gibraltar and the United States would be granted independence, but it would be confined to the area east of the Appalachian Mountains. Britain would keep the area north of the Ohio River, which was part of the Province of Quebec. In the area south of that would be an independent Indian barrier state, under Spanish control.[7]
 The American delegation perceived that they could obtain a better treaty in negotiating directly with the British in London. John Jay promptly told the British that he was willing to negotiate directly with them and to bypass France and Spain, and British Prime Minister Lord Shelburne agreed. In charge of the British negotiations, some of which took place in his study at Lansdowne House, now a bar in the Lansdowne Club, Shelburne now saw a chance to split the United States from France and to establish the new nation as a valuable economic partner.[8] The terms were that the United States would gain all of the area east of the Mississippi River, north of present-day Florida, and south of present-day Canada. The northern boundary would be almost the same as it is today.[9]
 The United States would gain fishing rights off Nova Scotia's coasts and agreed to allow British merchants and Loyalists to try to recover their property. The treaty was highly favorable for the United States and deliberately so from the British point of view. Shelburne foresaw highly profitable two-way trade between Britain and the rapidly-growing United States, which came to pass.[10]
 Great Britain also signed separate agreements with France and Spain, and provisionally with the Netherlands.[11] In the treaty with Spain, the territories of East and West Florida were ceded to Spain without a clear northern boundary, which resulted in a territorial dispute resolved by the Treaty of Madrid in 1795. Spain also received the island of Menorca, but the Bahamas, Grenada, and Montserrat, which had been captured by the French and Spaniards, were returned to Britain. The treaty with France was mostly about exchanges of captured territory. France's only net gains were the island of Tobago, and Senegal in Africa, but it also reinforced earlier treaties, guaranteeing fishing rights off Newfoundland. Dutch possessions in the East Indies, captured in 1781, were returned by Britain to the Netherlands in exchange for trading privileges in the Dutch East Indies by a treaty, which was not finalized until 1784.[12]
 The Congress of the Confederation, operating as the legislative body of the newly established United States, ratified the Treaty of Paris on January 14, 1784, in Annapolis, Maryland, in the Old Senate Chamber of the Maryland State House.[13] Copies were sent back to Europe for ratification by the other parties involved, the first reaching France in March 1784. British ratification occurred on April 9, 1784, and the ratified versions were exchanged in Paris on May 12, 1784.[14]
 The treaty and the separate peace treaties between Great Britain and the three colonial powers that supported the American cause, France, Spain, and the Dutch Republic, are known collectively as the Peace of Paris.[3][4] Only Article 1 of the treaty, which acknowledges the United States' existence as free sovereign and independent states, remains in force.[5] The U.S. borders changed in later years, which is a major reason specific articles of the treaty were superseded.[citation needed]
 Preamble. Declares the treaty to be ""in the Name of the Most Holy and Undivided Trinity"" followed by a reference to the divine providence,[15] states the bona fides of the signatories, and declares the intention of both parties to ""forget all past misunderstandings and differences"" and ""secure to both perpetual peace and harmony.""
 Eschatocol. ""Done at Paris, this third day of September in the year of our Lord, one thousand seven hundred and eighty-three.""
 Historians have often commented that the treaty was very generous to the United States in terms of greatly enlarged boundaries. Historians such as Alvord, Harlow, and Ritcheson have emphasized that British generosity was based on a statesmanlike vision of close economic ties between Britain and the United States. The concession of the vast trans-Appalachian region was designed to facilitate the growth of the American population and to create lucrative markets for British merchants without any military or administrative costs to Britain.[8] The point was that the United States would become a major trading partner. As French Foreign Minister Vergennes later put it, ""The English buy peace rather than make it.""[2] Vermont was included within the boundaries because the state of New York insisted that Vermont was a part of New York although Vermont was then under a government that considered Vermont not to be a part of the United States.[19]
 Privileges that the Americans had received from Britain automatically when they had colonial status, including protection from pirates in the Mediterranean Sea were lost. Individual states ignored federal recommendations, under Article 5, to restore confiscated Loyalist property and Article 6, which provided for confiscating Loyalist property for ""unpaid debts"". The Commonwealth of Virginia defied Article 4 and maintained laws against payment of debts to British creditors. Several Loyalists attempted to file for a return for their property in the US legal system after the American Revolutionary War, but most were unsuccessful.[20]
 The actual geography of North America turned out not to match the details used in the treaty. The treaty specified a southern boundary for the United States, but the separate Anglo-Spanish agreement did not specify a northern boundary for Florida. The Spanish government assumed that the boundary was the same as in the 1763 agreement by which it had first given its territory in Florida to Great Britain. While the West Florida Controversy continued, Spain used its new control of Florida to block American access to the Mississippi, in defiance of Article 8.[21] To the north, the treaty stated that the boundary of the United States extended from the ""most northwesternmost point"" of the Lake of the Woods in present-day Minnesota, Manitoba, and Ontario, directly westward until it reached the Mississippi River. However, the Mississippi does not extend that far northward, and the line going west from the Lake of the Woods never intersects the river. Additionally, the Treaty of Paris did not explain how the new border would function in terms of controlling the movement of people and trade between British North America and the United States. The American diplomats' expectation of negotiating a commercial treaty with Great Britain to resolve some of the unfinished business of the Treaty of Paris failed to materialize in 1784. Despite government agreements for British evacuation of northern forts, Britain continued to occupy the forts. Meanwhile, the British were dissatisfied with the American harassment of Loyalists.[22][23] The United States would thus wait until 1794 to negotiate its first commercial agreement with the British Empire, the Jay Treaty.[24]
 Great Britain violated the treaty stipulation that it would relinquish control of forts in United States territory ""with all convenient speed"". British troops remained stationed at six forts in the Great Lakes region and at two at the north end of Lake Champlain. The British also built an additional fort in present-day Ohio in 1794, during the Northwest Indian War. They justified their treaty violations during the unstable and extremely tense time that existed in the area following the Revolutionary War, and in the failure of the newly established federal government of the United States to fulfill commitments made to compensate loyalists for British losses, forcing the British to liquidate various assets in the region.[25] All of the posts were relinquished peacefully through diplomatic means as a result of the Jay Treaty:
"
Constitutional Convention (United States),https://en.wikipedia.org/wiki/Constitutional_Convention_(United_States),"

 The Constitutional Convention took place in Philadelphia from May 25 to September 17, 1787.[1] Although the convention was intended to revise the league of states and first system of government under the Articles of Confederation,[2] the intention from the outset of many of its proponents, chief among them James Madison of Virginia and Alexander Hamilton of New York, was to create a new frame of government rather than fix the existing one. The delegates elected George Washington of Virginia, former commanding general of the Continental Army in the late American Revolutionary War (1775–1783) and proponent of a stronger national government, to become President of the convention. The result of the convention was the creation of the Constitution of the United States, placing the Convention among the most significant events in American history.
 The convention took place in the old Pennsylvania State House, now known as Independence Hall, in Philadelphia. At the time, the convention was not referred to as a constitutional convention. It was contemporarily known as the Federal Convention,[3] the Philadelphia Convention,[3] or the Grand Convention at Philadelphia.[4][5] Nor did most of the delegates arrive intending to draft a new constitution. Many assumed that the purpose of the convention was to discuss and draft improvements to the existing Articles of Confederation, and would not have agreed to participate otherwise. Once the convention began, however, most of the delegates – though not all – came to agree in general terms that the goal would be a new system of government, not simply a revised version of the Articles of Confederation.
 Several broad outlines were proposed and debated, most notably Madison's Virginia Plan and William Paterson's New Jersey Plan. The Virginia Plan was selected as the basis for the new government, and the delegates quickly reached consensus on a general blueprint of a federal government which has three branches (legislative, executive, and judicial) along with the basic role of each branch. However, disagreement over the specific design and powers of the branches delayed progress for weeks and threatened the success of the convention. The most contentious disputes involved the legislature, specifically the composition and election procedures for the Senate as the upper legislative house of a bicameral Congress, and whether proportional representation was to be defined by a state's geography or by its population. The role of the executive was also hotly debated, including the key issues of whether to divide the executive power among three people or vest the power in a single chief executive to be called the President; how a president would be elected; the length of a presidential term and the number of allowable terms; what offenses should be impeachable; and whether judges should be chosen by the legislature or the executive. Finally, slavery was also a contentious issue, with the delegates debating the insertion of a fugitive slave clause; whether to allow the abolition of the slave trade; and whether slaves were to be counted in proportional representation. Most of the time during the convention was spent on deciding these issues.
 Progress was slow until mid-July, when the Connecticut Compromise resolved enough lingering arguments for a draft written by the Committee of Detail to gain acceptance. Though more modifications and compromises were made over the following weeks, most of this draft can be found in the finished version of the Constitution. After several more issues were debated and resolved, the Committee of Style produced the final version in early September. It was voted on by the delegates, inscribed on parchment by Jacob Shallus with engraving for printing, and signed by 39 of 55 delegates on September 17, 1787. The completed proposed Constitution was printed in several copies for review which began the debates and ratification process. Soon after it was also printed in newspapers for public review.
 During the American Revolution, the 13 American states replaced their colonial governments with republican constitutions based on the principle of separation of powers, organizing government into legislative, executive, and judicial branches. These revolutionary constitutions endorsed legislative supremacy by placing most power in the legislature, which was viewed as most representative of the people, including power traditionally considered as belonging to the executive and judicial branches. State governors lacked significant authority, and state courts and judges were under the control of the legislative branch at the time.[6]
 The Second Continental Congress adopted the Declaration of Independence in 1776, and the 13 states created a permanent alliance to coordinate American efforts to win the Revolutionary War. This alliance, the United States, was governed according to the Articles of Confederation, which was more of a treaty between independent countries than a national constitution.[7] The Articles were adopted by the Second Continental Congress in 1777 but not ratified by all states until 1781.[8]
 Under the Articles, the United States was essentially a federation of independent republics, with the Articles guaranteeing state sovereignty and independence. The Confederation was governed by the Congress of the Confederation, a unicameral legislature whose members were chosen by their state legislatures and where each state was entitled to one vote.[9] Congress's powers were limited to waging war and directing foreign affairs. It could not levy taxes or tariffs, and it could only request money from the states and could not force delinquent states to pay.[10] Since the Articles could only be amended by a unanimous vote of the states, each state had effective veto power over any proposed change.[11] A super majority (nine of thirteen state delegations) was required for Congress to pass major legislation such as declaring war, making treaties, or borrowing money.[12] The Confederation had no executive or judicial branches, which meant the Confederation government lacked effective means to enforce its own laws and treaties against state non-compliance.[13] It soon became evident to nearly all that the Confederation government, as originally organized, was inadequate for managing the various problems confronting the United States.[11]
 Once the crucial task of winning the war had passed, states began to look to their own interests rather than the country's. By the mid-1780s, states were refusing to provide Congress with funding, which meant the government could not meet the interest on its foreign debt, pay the soldiers stationed along the Ohio River, or defend American navigation rights on the Mississippi River against Spanish interference.[14] In 1782, Rhode Island vetoed an amendment that would have allowed Congress to levy taxes on imports to pay off federal debts. A second attempt was made to approve a federal impost in 1785; however, this time it was New York which disapproved.[15]
 The Confederation Congress also lacked the power to regulate foreign and interstate commerce. Britain, France and Spain imposed restrictions on American ships and products, while the U.S. was unable to coordinate retaliatory trade policies. When Massachusetts and Pennsylvania placed reciprocal duties on British trade, neighboring states such as Connecticut and Delaware established free ports to gain an economic advantage. Some states even began applying customs duties against the trade of neighboring states.[16] In 1784, Congress proposed an amendment to give it powers over foreign trade; however, it failed to receive unanimous approval by the states.[17]
 Many upper-class Americans complained that state constitutions were too democratic and, as a result, legislators were more concerned with maintaining popular approval than doing what was best for the nation. The most pressing example was the way state legislatures responded to calls for economic relief. Many people were unable to pay taxes and debts due to a post-war economic depression that was exacerbated by a scarcity of gold and silver coins. States responded by issuing paper currency, which often depreciated in value, and by making it easier to defer tax and debt payments. These policies favored debtors at the expense of creditors, and it was proposed that Congress be given power to prevent such populist laws.[18]
 When the government of Massachusetts refused to enact similar relief legislation, rural farmers resorted to violence in Shays' Rebellion (1786–1787). This rebellion was led by a former Revolutionary War captain, Daniel Shays, a small farmer with tax debts, who had never received payment for his service in the Continental Army. The rebellion took months for Massachusetts to put down, and some desired a federal army that would be able to put down such insurrections.[19]
 These and other issues greatly worried many of the Founders that the Union as it existed up to that point was in danger of breaking apart.[20][21] In September 1786, delegates from five states met at the Annapolis Convention and invited all states to a larger convention to be held in Philadelphia in 1787. On February 21, 1787, the Confederation Congress endorsed this convention ""for the sole and express purpose of revising the Articles of Confederation"".[22] The Congress only endorsed the convention on the grounds it would produce a report upon which necessary changes to the Articles could be introduced and ratified. Rhode Island was the only state that refused to send delegates, and it was the last state to ratify the Constitution in May 1790.[23]
 Originally planned to begin on May 14, the convention had to be postponed when very few of the selected delegates were present on that day due to the difficulty of travel in the late 18th century. On May 14, only delegates from Virginia and Pennsylvania were present.[24] It was not until May 25 that a quorum of seven states was secured and the convention could begin inside the Pennsylvania State House.[24] New Hampshire delegates would not join the convention until July 23, more than halfway through the proceedings.[25]
 Among the first things that the Convention did were to choose a presiding officer, unanimously electing George Washington to be the president of the convention.[26] The convention then adopted rules drafted by a committee whose members were George Wythe (chairman), Charles Pinckney, and Alexander Hamilton.[27] Each state delegation received a single vote either for or against a proposal in accordance with the majority opinion of the state's delegates.[28] This rule increased the power of the smaller states.[29]
 When a state's delegates divided evenly on a motion, the state did not cast a vote. Throughout the convention, delegates would regularly come and go. Only 30 to 40 delegates were present on a typical day, and each state had its own quorum requirements. Maryland and Connecticut allowed a single delegate to cast its vote. New York required all three of its delegates to be present. If too few of a state's delegates were in attendance, the state did not cast a vote. After two of New York's three delegates, John Lansing Jr. and Robert Yates, abandoned the convention on July 10 with no intention of returning, New York was unable to vote on any further proposals, although Alexander Hamilton would continue to occasionally speak during the debates.[30][28][29]
 The rules allowed delegates to demand reconsideration of any decision previously voted on. This enabled the delegates to take straw votes to measure the strength of controversial proposals and to change their minds as they worked for consensus.[31] It was also agreed that the discussions and votes would be kept secret until the conclusion of the meeting.[32] Despite the sweltering summer heat, the windows of the meeting hall were nailed shut to keep the proceedings a secret from the public.[33] Although William Jackson was elected as secretary, his records were brief and included very little detail. Madison's Notes of Debates in the Federal Convention of 1787 remain the most complete record of the convention, along with notes kept by Yates through mid-July.[34][35] Due to the pledge to secrecy, Madison's account was not published until after his death in 1836, while Yates's notes on the convention's first two months were published in 1821.[36][37]
 James Madison of Virginia arrived in Philadelphia eleven days early and determined to set the convention's agenda.[38] Before the convention, Madison studied republics and confederacies throughout history, such as ancient Greece and contemporary Switzerland.[39] In April 1787, he drafted a document titled, ""Vices of the Political System of the United States,"" which systematically evaluated the American political system and offered solutions for its weaknesses.[40] Due to his advance preparation, Madison's blueprint for constitutional revision became the starting point for the convention's deliberations.[41]
 Madison believed the solution to America's problems was to be found in a strong central government.[39] Congress needed compulsory taxation authority as well as power to regulate foreign and interstate commerce.[38] To prevent state interference with the federal government's authority, Madison believed there needed to be a way to enforce the federal supremacy, such as an explicit right of Congress to use force against non-compliant states and the creation of a federal court system. Madison also believed the method of representation in Congress had to change. Since under Madison's plan, Congress would exercise authority over citizens directly—not simply through the states—representation ought to be apportioned by population, with more populous states having more votes in Congress.[42]
 Madison was also concerned with preventing a tyranny of the majority. The government needed to be neutral between the various factions or interest groups that divided society—creditors and debtors, rich and poor, or farmers, merchants and manufacturers. Madison believed that a single faction could more easily control the government within a state but would have a more difficult time dominating a national government comprising many different interest groups. The government could be designed to further insulate officeholders from the pressures of a majority faction. To protect both national authority and minority rights, Madison believed Congress should be granted veto power over state laws.[43]
 While waiting for the convention to formally begin, Madison sketched out his initial proposal, which became known as the Virginia Plan and reflected his views as a strong nationalist. The Virginia and Pennsylvania delegates agreed with Madison's plan and formed what came to be the predominant coalition within the convention.[44] The plan was modeled on the state governments and was written in the form of fifteen resolutions outlining basic principles. It lacked the system of checks and balances that would become central to the US Constitution.[45] It called for a supreme national government and was a radical departure from the Articles of Confederation.[46] On May 29, Edmund Randolph, the governor of Virginia, presented the Virginia Plan to the convention.[47]
 The same day, Charles Pinckney of South Carolina introduced his own plan that also greatly increased the power of the national government; however, the supporters of the Virginia Plan ensured that it, rather than Pinckney's plan, received the most consideration.[48] Many of Pinckney's ideas did appear in the final draft of the Constitution. His plan called for a bicameral legislature made up of a House of Delegates and a Senate. The popularly elected House would elect senators who would serve for four-year terms and represent one of four regions. The national legislature would have veto power over state laws. The legislature would elect a chief executive called a president. The president and his cabinet would have veto power over legislation. The plan also included a national judiciary.[49]
 On May 30, the Convention agreed, at the request of Gouverneur Morris, ""that a national government ought to be established consisting of a supreme Legislative, Executive and Judiciary"".[50] This was the convention's first move towards going beyond its mandate merely to amend the Articles of Confederation and instead produce an entirely new government.[51] Once it had agreed to the idea of a supreme national government, the convention began debating specific parts of the Virginia Plan.
 The Virginia Plan called for the unicameral Confederation Congress to be replaced with a bicameral Congress. This would be a truly national legislature with power to make laws ""in all cases to which the separate states are incompetent.""[52] It would also be able to veto state laws. Representation in both houses of Congress would be apportioned according either to quotas of contribution (a state's wealth as reflected in the taxes it paid) or the size of each state's non-slave population. The lower house of Congress would be directly elected by the people, while the upper house would be elected by the lower house from candidates nominated by state legislatures.[53]
 Immediately after agreeing to form a supreme national government, the delegates turned to the Virginia Plan's proposal for proportional representation in Congress.[54] Virginia, Pennsylvania and Massachusetts, the most populous states, were unhappy with the one-vote-per-state rule in the Confederation Congress because they could be outvoted by the smaller states despite representing more than half of the nation's population.[55] Nevertheless, the delegates were divided over the best way to apportion representatives. Quotas of contribution appealed to southern delegates because they would include slave property, but Rufus King of Massachusetts highlighted the impractical side of such a scheme. If the national government did not impose direct taxes (which, for the next century, it rarely did), he noted, representatives could not be assigned. Calculating such quotas would also be difficult due to lack of reliable data. Basing representation on the number of ""free inhabitants"" was unpopular with delegates from the South, where forty percent of the population was enslaved.[56] In addition, the small states were opposed to any change that decreased their own influence. Delaware's delegation threatened to leave the Convention if proportional representation replaced equal representation, so debate on apportionment was postponed.[57]
 On June 9, William Paterson of New Jersey reminded the delegates that they were sent to Philadelphia to revise the Articles of Confederation, not to establish a national government. While he agreed that the Confederation Congress needed new powers, including the power to coerce the states, he was adamant that a confederation required equal representation for states.[58] James Madison records his words as follows:[59]
 On May 31, the delegates discussed the structure of Congress and how its members would be selected. The division of the legislature into an upper and lower house was familiar and had wide support. The British Parliament had an elected House of Commons and a hereditary House of Lords. All the states had bicameral legislatures except for Pennsylvania.[61] The delegates quickly agreed that each house of Congress should be able to originate bills. They also agreed that the new Congress would have all the legislative powers of the Confederation Congress and veto power over state laws.[62]
 There was some opposition to the popular election of the lower house or House of Representatives. Elbridge Gerry of Massachusetts and Roger Sherman of Connecticut feared the people were too easily misled by demagogues and that popular election could lead to mob rule and anarchy. Pierce Butler of South Carolina believed that only wealthy men of property could be trusted with political power. The majority of the convention, however, supported popular election.[63] George Mason of Virginia said the lower house was ""to be the grand depository of the democratic principle of the government.""[64]
 There was general agreement that the upper house or Senate should be smaller and more selective than the lower house. Its members should be gentlemen drawn from the most intelligent and virtuous among the citizenry.[65] Experience had convinced the delegates that such an upper house was necessary to tame the excesses of the democratically elected lower house.[61] The Virginia Plan's method of selecting the Senate was more controversial. Members concerned with preserving state power wanted state legislatures to select senators, while James Wilson of Pennsylvania proposed direct election by the people.[66] It was not until June 7 that the delegates unanimously decided that state legislatures would choose senators.[67]
 On the question of proportional representation, the three large states still faced opposition from the eight small states. James Wilson realized that the large states needed the support of the Deep South states of Georgia and the Carolinas. For these southern delegates, the main priority was protection of slavery.[68] Working with John Rutledge of South Carolina, Wilson, along with Charles Pinckney of South Carolina and Roger Sherman of Connecticut, proposed the Three-Fifths Compromise on June 11. This resolution apportioned seats in the House of Representatives based on a state's free population plus three-fifths of its slave population. Nine states voted in favor, with only New Jersey and Delaware against.[69] This compromise would give the South at least a dozen additional congressmen and electoral college votes.[70] That same day, the large-state/slave-state alliance also succeeded in applying the three-fifths ratio to Senate seats (though this was later overturned).[71]
 As English law had typically recognized government as having two separate functions—law making embodied in the legislature and law executing embodied in the king and his courts—the division of the legislature from the executive and judiciary was a natural and uncontested point.[26] Even so, the form the executive should take, its powers and its selection would be sources of constant dispute through the summer of 1787.[72] At the time, few nations had nonhereditary executives that could serve as models. The Dutch Republic was led by a stadtholder, but this office was usually inherited by members of the House of Orange. The Swiss Confederacy had no single leader, and the elective monarchies of the Holy Roman Empire and Polish–Lithuanian Commonwealth were viewed as corrupt.[73]
 As a result of their colonial experience, Americans distrusted a strong chief executive. Under the Articles of Confederation, the closest thing to an executive was the Committee of the States, which was empowered to transact government business while Congress was in recess. However, this body was largely inactive. The revolutionary state constitutions made the governors subordinate to the legislatures, denying them executive veto power over legislation. Without veto power, governors were unable to block legislation that threatened minority rights.[74] States chose governors in different ways. Many state constitutions empowered legislatures to select them, but several allowed direct election by the people. In Pennsylvania, the people elected an executive council and the legislature appointed one of its members to be chief executive.[73]
 The Virginia Plan proposed a national executive chosen by Congress. It would have power to execute national laws and be vested with the power to make war and treaties.[75] Whether the executive would be a single person or a group of people was not defined.[76] The executive together with a ""convenient number"" of federal judges would form a Council of Revision with the power to veto any act of Congress. This veto could be overridden by an unspecified number of votes in both houses of Congress.[75]
 James Wilson feared that the Virginia Plan made the executive too dependent on Congress. He argued that there should be a single, unitary executive. Members of a multiple executive would most likely be chosen from different regions and represent regional interests. In Wilson's view, only a single executive could represent the entire nation while giving ""energy, dispatch, and responsibility"" to the government.[77]
 Wilson used his understanding of civic virtue as defined by the Scottish Enlightenment to help design the presidency. The challenge was to design a properly constituted executive that was fit for a republic and based on civic virtue by the general citizenry. He spoke 56 times calling for a chief executive who would be energetic, independent, and accountable. He believed that the moderate level of class conflict in American society produced a level of sociability and inter-class friendships that could make the presidency the symbolic leader of the entire American people. Wilson did not consider the possibility of bitterly polarized political parties. He saw popular sovereignty as the cement that held America together linking the interests of the people and of the presidential administration. Wilson envisioned a president who would be a man of the people and who embodied the national responsibility for the public good and provided transparency and accountability by being a highly visible national leader, as opposed to numerous largely anonymous congressmen.[78][79][80]
 On June 1, Wilson proposed that ""the Executive consist of a single person."" This motion was seconded by Charles Pinckney, whose plan called for a single executive and specifically named this official a ""president"".[77] Edmund Randolph agreed with Wilson that the executive needed ""vigor"", but he disapproved of a unitary executive, which he feared was ""the foetus of monarchy"".[81] Randolph and George Mason led the opposition against a unitary executive, but most delegates agreed with Wilson. The prospect that George Washington would be the first president may have allowed the proponents of a unitary executive to accumulate a large coalition. Wilson's motion for a single executive passed on June 4.[82] Initially, the convention set the executive's term of office to seven years, but this would be revisited.[83]
 Wilson also argued that the executive should be directly elected by the people. Only through direct election could the executive be independent of both Congress and the states.[84] This view was unpopular. A few delegates such as Roger Sherman, Elbridge Gerry, and Pierce Butler opposed the direct election of the executive because they considered the people too easily manipulated. However, most delegates were not questioning the intelligence of the voters; rather, what concerned them was the slowness by which information spread in the late 18th century. Due to a lack of information, delegates feared that the average voter would be too ignorant about the candidates to make an informed decision.[85]
 Sherman proposed an arrangement similar to a parliamentary system in which the executive should be appointed by and directly accountable to the legislature.[81] A majority of delegates favored the president's election by Congress for a seven-year term, though there was concern that this would give the legislature too much power. Southern delegates supported selection by state legislatures, but this was opposed by nationalists such as Madison who feared that such a president would become a power broker between different states' interests rather than a symbol of national unity. Eventually however, on June 2 the convention proposed and agreed to the national legislature choosing the executive for a single 7-year term, a proposal which of course later changed. Realizing that direct election was impossible, Wilson proposed what would become the electoral college—the states would be divided into districts in which voters would choose electors who would then elect the president. This would preserve the separation of powers and keep the state legislatures out of the selection process. Initially, however, this scheme received little support.[86]
 The issue of the executive was one of the last major issues to be resolved. Resolution was achieved by adjustment to the electoral college proposal. At the time, before the formation of modern political parties, there was widespread concern that candidates would routinely fail to secure a majority of electors in the electoral college. The method of resolving this problem, therefore, was a contested issue. Most thought that the House of Representatives should then choose the president since it most closely reflected the will of the people. This caused dissension among delegates from smaller states, who realized that this would put their states at a disadvantage. To resolve this dispute, the Convention agreed that the House would elect the president if no candidate had an electoral college majority, but that each state delegation would vote as a bloc, rather than individually.[83]
 The Virginia Plan made no provision for removing the executive. On June 2, John Dickinson of Delaware proposed that the president be removed from office by Congress at the request of a majority of state legislatures. Madison and Wilson opposed this state interference in the national executive branch. Sherman argued that Congress should be able to remove the president for any reason in what was essentially a vote of no-confidence. George Mason worried that would make the president a ""mere creature of the legislature"" and violate the separation of powers. Dickinson's motion was rejected, but in the aftermath of the vote there was still no consensus over how an unfit president should be removed from office.[87]
 On June 4, the delegates debated the Council of Revision. Wilson and Alexander Hamilton of New York disagreed with the mixing of executive and judicial branches. They wanted the president to have an absolute veto to guarantee his independence from the legislative branch. Remembering how colonial governors used their veto to ""extort money"" from the legislature, Benjamin Franklin of Pennsylvania opposed giving the president an absolute veto. Gerry proposed that a two-thirds majority in both houses of Congress be able to overrule any veto of the Council of Revision. This was amended to replace the council with the president alone, but Madison insisted on retaining a Council of Revision and consideration of the veto power was postponed.[88]
 In the English tradition, judges were seen as agents of the king and his court who represented him throughout his realm. Madison believed that in the American states, this direct link between state executives and judges was a source of corruption through patronage, and thought the link had to be severed between the two, thus creating the ""third branch"" of the judiciary which had been without any direct precedent before this point.[89][page needed]
 On June 4, delegates unanimously agreed to a national judiciary ""of one supreme tribunal and one or more inferior tribunals"". The delegates disagreed on how federal judges should be chosen. The Virginia Plan called for the national legislature to appoint judges. James Wilson wanted the president to appoint judges to increase the power of that office.[90]
 On June 13, the revised report on the Virginia Plan was issued. This report summarized the decisions made by the delegates in the first two weeks of the convention. It was agreed that a ""national judiciary be established, to consist of one supreme tribunal"". Congress would have the power to create and appoint inferior courts. Judges were to hold office ""during good behavior"", and the Senate would appoint them.[91]
 The small state delegates were alarmed at the plan taking shape: a supreme national government that could override state laws and proportional representation in both houses of Congress.[92] William Paterson and other delegates from New Jersey, Connecticut, Maryland and New York created an alternative plan that consisted of several amendments to the Articles of Confederation. Under the New Jersey Plan, the Confederation Congress would remain unicameral with each state having one vote. Congress would be allowed to levy tariffs and other taxes as well as regulate trade and commerce. Congress would elect a plural ""federal executive"" whose members would serve a single term and could be removed by Congress at the request of a majority of state governors. There would also be a federal judiciary to apply US law. Federal judges would serve for life and be appointed by the executives. Laws enacted by Congress would take precedence over state laws. This plan was introduced on June 15.[93][94][49]
 On June 18, Alexander Hamilton of New York presented his own plan that was at odds with both the Virginia and New Jersey plans. It called for the constitution to be modeled on the British government. The bicameral legislature included a lower house called the Assembly elected by the people for three year terms. The people would choose electors who would elect the members of a Senate who served for life. Electors would also choose a single executive called the governor who would also serve for life. The governor would have an absolute veto over bills. There would also be a national judiciary whose members would serve for life. Hamilton called for the abolition of the states (or at least their reduction to sub-jurisdictions with limited powers). Some scholars have suggested that Hamilton presented this radical plan to help secure passage of the Virginia Plan by making it seem moderate by comparison. The plan was so out of step with political reality that it was not even debated, and Hamilton would be troubled for years by accusations that he was a monarchist.[95][49]
 On June 19, the delegates voted on the New Jersey Plan. With the support of the slave states and Connecticut, the large states defeated the plan by a 7–3 margin. Maryland's delegation was divided, so it did not vote.[96] This did not end the debate over representation. Rather, the delegates found themselves in a stalemate that lasted into July.
 On several occasions, the Connecticut delegation, including Roger Sherman, Oliver Ellsworth, and William Samuel Johnson, proposed a compromise that the House would have proportional representation and the Senate equal representation.[97] A version of this compromise had originally been crafted and proposed by Sherman on June 11. He agreed with Madison that the Senate should be composed of the wisest and most virtuous citizens, but he also saw its role as defending the rights and interests of the states.[98] James Madison recorded Sherman's June 11 speech as follows:[99]
 On June 29, Johnson made a similar point: ""that in one branch, the people ought to be represented; in the other, the states.""[100] Neither side was ready yet to embrace the concept of divided sovereignty between the states and a federal government, however.[101] The distrust between large and small state delegates had reached a high point, exemplified by comments made on June 30 by Gunning Bedford Jr. As reported by Robert Yates, Bedford stated:[102]
 As the convention was entering its second full month of deliberations, it was decided that further consideration of the prickly question of how to apportion representatives in the national legislature should be referred to a committee composed of one delegate from each of the eleven states present at that time at the convention. The members of this ""Grand Committee,"" as it has come to be known, included William Paterson of New Jersey, Robert Yates of New York, Luther Martin of Maryland, Gunning Bedford Jr. of Delaware, Oliver Ellsworth of Connecticut, Abraham Baldwin of Georgia, Elbridge Gerry of Massachusetts, George Mason of Virginia, William Davie of North Carolina, John Rutledge of South Carolina and Benjamin Franklin of Pennsylvania.[103] The committee's composition heavily favored the smaller states, as even the large state delegates tended to be more moderate.[104]
 While the Convention took a three-day recess in observance of the Fourth of July holiday, the Grand Committee began its work.[104] Franklin proposed and the committee adopted a compromise similar to the Connecticut plan. Membership in the House would be apportioned by population, with members elected from districts of forty thousand people. Each state would have an equal vote in the Senate. To gain large state support, however, Franklin proposed that the House of Representatives have exclusive power to originate bills concerned with raising money or government salaries (this would become the Origination Clause).[105]
 The committee presented its report on July 5, but the compromise was not immediately adopted by the convention. For the next eleven days, the Convention stalled as delegates attempted to gain as many votes for their states as possible.[106] On July 6, a five-man committee was appointed to allocate specific numbers of representatives to each state. It called for a 56-member House of Representatives and used ""[t]he number of blacks and whites with some regard to supposed wealth"" as a basis of allocating representatives to each state. The Northern states had 30 representatives while the Southern states had 26. Delegates from non-slave states objected to counting slaves as they could not vote.[107][108]
 On July 9, a new committee was chosen to reconsider the allocation of representatives. This time there were eleven members, one from each state. It recommended a 65-member House with allocation of representatives based on the number of free inhabitants and three-fifths of slaves. Under this new scheme, Northern states had 35 representatives and the South had 30. Southern delegates protested the North's greater representation and argued that their growing populations had been underestimated. The Committee of Eleven's report was approved, but the divergent interests of the Northern and Southern states remained obstacles to reaching consensus.[108]
 On July 10, Edmund Randolph called for a regular census on which to base future reallocation of House seats.[109] During the debate on the census, South Carolina delegates Pierce Butler and Charles Cotesworth Pinckney sought to replace the three-fifths ratio with a full count of the slave population. They argued that slave property contributed to the wealth of the Southern states and as such should be used in calculating representation. This irritated Northern delegates already reluctant to support the three-fifths compromise. James Wilson, one of the authors of the three-fifths compromise, asked, ""Are slaves to be admitted as Citizens? Then why are they not admitted on an equality with White Citizens? Are they admitted as property? Then why is not other property admitted into the computation?""[110]
 After fierce debate, the delegates voted to apportion representation and direct taxation based on all free inhabitants and three-fifths of the slave population. This formula would apply to the existing states as well as any states created in the future. The first census would occur six years after the new federal government began operations and every ten years afterwards.[111]
 On July 14, John Rutledge and James Wilson attempted to secure proportional representation in the Senate. Charles Pinckney proposed a form of semi-proportional representation in which the smaller states would gain more representation than under a completely proportional system. This proposal was defeated.[112]
 In a close vote on July 16, the convention adopted the Connecticut Compromise (also known as the Great Compromise) as recommended by the Grand Committee.[113] On July 23, the convention decided that each state should have two senators rather than three. It rejected a proposal by Luther Martin of Maryland that senators from the same state cast a single joint vote, which was the practice in the Confederation Congress. Martin believed this was necessary if the Senate was to represent the interests of the states. Instead, the convention gave senators individual voting power.[114] This accomplished the nationalist goal of preventing state governments from having a direct say in Congress's choice to make national laws.[115] The final document was thus a mixture of Madison's original ""national"" constitution and the desired ""federal"" Constitution that many of the delegates sought.[116]
 On July 17, the delegates worked to define the powers of Congress. The Virginia Plan asserted the supremacy of the national government, giving Congress authority ""to legislate in all cases to which the separate States are incompetent"" and stating that congressional legislation would take precedence over conflicting state laws. In a motion introduced by Gunning Bedford, the Convention approved this provision with only South Carolina and Georgia voting against. Four small states—Connecticut, New Jersey, Delaware and Maryland—accepted the expansion of congressional power. Later in life, Madison explained that this was a result of the Great Compromise. Once the small states were assured they would be represented in the new government, they ""exceeded all others in zeal"" for a strong national government.[117]
 The Virginia Plan also gave Congress veto power over state laws. Madison believed this provision was crucial to prevent the states from engaging in irresponsible behavior, such as had occurred under the Confederation government. Gouverneur Morris feared the congressional veto would alienate states that might otherwise support the Constitution. Luther Martin argued that it would be too impractical and time-consuming, asking ""Shall the laws of the states be sent up to the general legislature before they shall be permitted to operate?""[118]
 The Convention rejected the congressional veto. In its place, Martin proposed language taken from the New Jersey Plan that was unanimously approved by the convention: ""that the Legislative acts of the US made by virtue and pursuance of the articles of Union, and all treaties made and ratified under the authority of the US shall be the supreme law of the respective States . . . and that the . . . States shall be bound thereby in their decisions"".[119]
 In June, the delegates voted to let Congress appoint the executive, but there remained concerns that this would make the executive branch subservient to the legislature. On July 17, the Convention returned to the topic. Direct election by the people was defeated by a nine to one vote. Luther Martin then proposed an amended version of James Wilson's idea for an electoral college, first introduced in June. Wilson had proposed that people vote for electors who would then select the president. Martin's version called for state legislatures to choose electors, but this was also defeated.[120] Later, on July 19, Elbridge Gerry unsuccessfully proposed that governors choose electors, a policy that would have increased state influence over the presidency.[121]
 After reaffirming Congressional selection, the delegates voted to allow the president to serve multiple terms, a reversal of their earlier decision to limit the president to serving a single, seven–year term. James McClurg of Virginia went further and proposed that the president serve a lifelong term ""during good behavior"". McClurg believed this would protect the independence of the executive branch, but this was rejected for being too close to monarchy.[122]
 The Convention decided that the method of removing an unfit president would be legislative impeachment. At the time, impeachment was used by the British Parliament to depose the king's ministers (see Impeachment in the United Kingdom).[123]
 Needing a break from discussing the presidency, the delegates once again considered the judicial branch on July 18. They were still divided over the method of appointment. Half of the Convention wanted the Senate to choose judges, while the other half wanted the president to do it. Luther Martin supported Senate appointment because he thought that body's members would defend the interests of the individual states.[124]
 Nathaniel Gorham suggested a compromise—appointment by the president with the ""advice and consent of the Senate"". While the meaning of ""advice and consent"" was still undefined, the proposal gained some support. On July 21, Madison offered an alternative compromise—the president would appoint judges but the Senate could veto an appointment by a two-thirds majority. This proposal would have made it very hard for the Senate to block judicial appointments. Madison's proposal failed to garner support, and the delegates ended by reaffirming that the Senate would appoint judges.[125]
 On July 21, Wilson and Madison tried unsuccessfully to revive Madison's Council of Revision proposal. While judges had a role in reviewing the constitutionality of laws, argued Gorham, mixing the policy judgments of the president with the legal judgments of a court would violate separation of powers. John Rutledge agreed, saying ""judges ought never to give their opinion on a law till it comes before them"".[126]
 The delegates recognized that a major flaw with the Articles of Confederation was that any constitutional amendment required unanimous approval of the states. On July 23, the convention endorsed the need for a different way of amending the Constitution, but it was not prepared to vote on specifics.[127]
 It also discussed how the completed Constitution would become law. Oliver Ellsworth and William Paterson argued that state legislatures should ratify the Constitution to align with precedent under the Articles of Confederation and because the legislatures represented the will of the people. Nathaniel Gorham argued that state legislators would reject the Constitution to protect their own power. George Mason believed that state legislatures lacked the authority to ratify the new Constitution because they were creations of the state constitutions.[128]
 Mason argued that only the people acting through specially called state conventions could authorize a new government. Madison agreed with Mason. He considered the Articles of Confederation to be a mere treaty among the states, but a true constitution could only be adopted by the people themselves. By a vote of nine to one, the delegates voted to submit the Constitution to state ratifying conventions.[128]
 The Convention adjourned from July 26 to August 6 to await the report of the Committee of Detail, which was to produce a first draft of the Constitution. It was chaired by John Rutledge, with the other members including Edmund Randolph, Oliver Ellsworth, James Wilson, and Nathaniel Gorham.
 Though the committee did not record minutes of its proceedings, three key surviving documents offer clues to the committee's handiwork: an outline by Randolph with edits by Rutledge, extensive notes and a second draft by Wilson, also with Rutledge's edits, and the committee's final report to the convention.[129]: 168  From this evidence it is thought that the committee used the original Virginia Plan, the decisions of the convention on modifications to that plan, and other sources, such as the Articles of Confederation, provisions of the state constitutions, and even Charles Pinckney's plan, to produce the first full draft,[130][129]: 165  which author David O. Stewart has called a ""remarkable copy-and-paste job.""[129]: 165 
 Randolph adopted two rules in preparing his initial outline: that the Constitution should only include essential principles, avoiding minor provisions that would change over time, and that it should be stated in simple and precise language.[131]
 Much of what was included in the committee's report consisted of numerous details that the convention had never discussed but which the committee correctly viewed as uncontroversial and unlikely to be challenged; and as such, much of the committee's proposal would ultimately be incorporated into the final version of the Constitution without debate.[129]: 169  Examples of these details included the Speech and Debate Clause, which grants members of Congress immunity for comments made in their jobs, and the rules for organizing the House of Representatives and the Senate.
 However, Rutledge, himself a former state governor, was determined that while the new national government should be stronger than the Confederation government had been, the national government's power over the states should not be limitless; and at Rutledge's urging, the committee went beyond what the convention had proposed. As Stewart describes it, the committee ""hijacked"" and remade the Constitution, altering critical agreements the Convention delegates had already made, enhancing the powers of the states at the expense of the national government, and adding several far-reaching provisions that the convention had never discussed.[129]: 165 
 The first major change, insisted on by Rutledge, was meant to sharply curtail the essentially unlimited powers to legislate ""in all cases for the general interests of the Union"" that the Convention only two weeks earlier had agreed to grant the Congress. Rutledge and Randolph worried that the broad powers implied in the language agreed on by the convention would have given the national government too much power at the expense of the states. In Randolph's outline the committee replaced that language with a list of 18 specific ""enumerated"" powers, many adopted from the Articles of Confederation, that would strictly limit the Congress's authority to measures such as imposing taxes, making treaties, going to war, and establishing post offices.[132][129]: 170–71  Rutledge, however, was not able to convince all the members of the committee to accept the change. Over the course of a series of drafts, a catchall provision (the ""Necessary and Proper Clause"") was eventually added, most likely by Wilson, a nationalist little concerned with the sovereignty of individual states, giving the Congress the broad power ""to make all Laws that shall be necessary and proper for carrying into execution the foregoing powers, and all other powers vested by this Constitution in the government of the United States, or in any department or officer thereof.""[133][129]: 171–72  Another revision of Wilson's draft also placed eight specific limits on the states, such as barring them from independently entering into treaties and from printing their own money, providing a certain degree of balance to the limits on the national government intended by Rutledge's list of enumerated powers.[134][129]: 172  In addition, Wilson's draft modified the language of the Supremacy Clause adopted by the convention, to ensure that national law would take precedence over inconsistent state laws.[129]: 172 
 These changes set the final balance between the national and state governments that would be entered into the final document, as the Convention never challenged this dual-sovereignty between nation and state that had been fashioned by Rutledge and Wilson.[129]: 172 
 Another set of radical changes introduced by the Committee of Detail proved far more contentious when the committee's report was presented to the convention. On the day the convention had agreed to appoint the committee, Southerner Charles Cotesworth Pinckney of South Carolina, had warned of dire consequences should the committee fail to include protections for slavery in the Southern states, or allow for taxing of Southern agricultural exports.[135][129]: 173  In response to Pinckney and his fellow Southern delegates, the committee had included three provisions that explicitly restricted the Congress's authority in ways favorable to Southern interests. The proposed language would bar the Congress from ever interfering with the slave trade. It would also prohibit taxation of exports, and would require that any legislation concerning regulation of foreign commerce through tariffs or quotas (that is, any laws akin to England's ""Navigation Acts"") pass only with two-thirds majorities of both houses of Congress. While much of the rest of the committee's report would be accepted without serious challenge on the Convention floor, these last three proposals provoked outrage from Northern delegates and slavery opponents.[136][129]: 173–74 
 The final report of the committee, which became the first draft of the Constitution, was the first workable constitutional plan, as Madison's Virginia Plan had simply been an outline of goals and a broad structure. Even after it issued this report, the committee continued to meet off and on until early September.
 Another month of discussion and relatively minor refinement followed, during which several attempts were made to alter the Rutledge draft, though few were successful. Some wanted to add property qualifications for people to hold office, while others wanted to prevent the national government from issuing paper money.[129]: 187  Madison in particular wanted to push the Constitution back in the direction of his Virginia plan.
 One important change that did make it into the final version included the agreement between northern and southern delegates to empower Congress to end the slave trade starting in 1808. Southern and northern delegates also agreed to strengthen the Fugitive Slave Clause in exchange for removing a requirement that two-thirds of Congress agree on ""navigation acts"" (regulations of commerce between states and foreign governments). The two-thirds requirement was favored by southern delegates, who thought Congress might pass navigation acts that would be economically harmful to slaveholders.[129]: 196 
 Once the convention had finished amending the first draft from the Committee of Detail, a new set of unresolved questions were sent to several different committees for resolution. The Committee of Detail was considering several questions related to habeas corpus, freedom of the press, and an executive council to advise the president. Two committees addressed questions related to the slave trade and the assumption of war debts.
 A new committee was created, the Committee on Postponed Parts, to address other questions that had been postponed. Its members, such as Madison, were delegates who had shown a greater desire for compromise and were chosen for this reason as most in the Convention wanted to finish their work and go home.[129]: 207  The committee dealt with questions related to the taxes, war making, patents and copyrights, relations with indigenous tribes, and Franklin's compromise to require money bills to originate in the House. The biggest issue they addressed was the presidency, and the final compromise was written by Madison with the committee's input.[129]: 209  They adopted Wilson's earlier plan for choosing the president by an electoral college, and settled on the method of choosing the president if no candidate had an electoral college majority, which many such as Madison thought would be ""nineteen times out of twenty"".
 The committee also shortened the president's term from seven years to four years, freed the president to seek re-election after an initial term, and moved impeachment trials from the courts to the Senate. They also created the office of the vice president, whose only roles were to succeed a president unable to complete a term of office, to preside over the Senate, and to cast tie-breaking votes in the Senate. The committee transferred important powers from the Senate to the president, for example the power to make treaties and appoint ambassadors.[129]: 212  One controversial issue throughout much of the convention had been the length of the president's term, and whether the president was to be term limited. The problem had resulted from the understanding that the president would be chosen by Congress; the decision to have the president be chosen instead by an electoral college reduced the chance of the president becoming beholden to Congress, so a shorter term with eligibility for re-election became a viable option.
 Near the end of the convention, Gerry, Randolph, and Mason emerged as the main force of opposition. Their fears were increased as the Convention moved from Madison's vague Virginia Plan to the concrete plan of Rutledge's Committee of Detail.[129]: 235  Some have argued that Randolph's attacks on the Constitution were motivated by political ambition, in particular his anticipation of possibly facing rival Patrick Henry in a future election. The main objection of the three was the compromise that would allow Congress to pass ""navigation acts"" with a simple majority in exchange for strengthened slave provisions.[129]: 236  Among their other objections was an opposition to the office of vice president.
 Though most of their complaints did not result in changes, a couple did. Mason succeeded in adding ""high crimes and misdemeanors"" to the impeachment clause. Gerry also convinced the convention to include a second method for ratification of amendments. The report out of the Committee of Detail had included only one mechanism for constitutional amendment that required two-thirds of the states to ask Congress to convene a convention for consideration of amendments. Upon Gerry's urging, the Convention added back the Virginia Plan's original method whereby Congress would propose amendments that the states would then ratify.[129]: 238  All amendments to the Constitution, save the 21st amendment, have been made through this latter method.
 Despite their successes, these three dissenters grew increasingly unpopular as most other delegates wanted to bring the convention's business to an end and return home. As the convention was drawing to a conclusion, and delegates prepared to refer the Constitution to the Committee on Style to pen the final version, one delegate raised an objection over civil trials. He wanted to guarantee the right to a jury trial in civil matters, and Mason saw in this a larger opportunity. Mason told the Convention that the constitution should include a bill of rights, which he thought could be prepared in a few hours. Gerry agreed, though the rest of the committee overruled them. They wanted to go home, and thought this was nothing more than another delaying tactic.[129]: 241 
 Few at the time realized how important the issue would become, with the absence of a bill of rights becoming the main argument of the anti-Federalists against ratification. Most of the convention's delegates thought that states already protected individual rights, and that the Constitution did not authorize the national government to take away rights, so there was no need to include protections of rights. Once the Convention moved beyond this point, the delegates addressed a couple of last-minute issues. Importantly, they modified the language that required spending bills to originate in the House of Representatives and be flatly accepted or rejected, unmodified, by the Senate. The new language empowered the Senate to modify spending bills proposed by the House.[129]: 243 
 Once the final modifications were made, the Committee of Style and Arrangement was appointed ""to revise the style of and arrange the articles which had been agreed to by the house."" Unlike other committees, whose members were named so the committees included members from different regions, this final committee included no champions of the small states. Its members mostly supported a strong national government and were unsympathetic to calls for states' rights.[129]: 229–30  They were William Samuel Johnson (Connecticut), Alexander Hamilton (New York), Gouverneur Morris (Pennsylvania), James Madison (Virginia), and Rufus King (Massachusetts). On Wednesday, September 12, the report of the ""committee of style"" was ordered printed for the convenience of the delegates. For three days, the Convention compared this final version with the proceedings of the convention. The Constitution was then ordered engrossed on Saturday, September 15 by Jacob Shallus, and was submitted for signing on September 17. It made at least one important change to what the convention had agreed to; King wanted to prevent states from interfering in contracts. Although the Convention never took up the matter, his language was now inserted, creating the contract clause.[129]: 243 
 Gouverneur Morris is credited, both now and then, as the chief draftsman of the final document, including the stirring preamble. Not all the delegates were pleased with the results; thirteen left before the ceremony, and three of those remaining refused to sign: Edmund Randolph of Virginia, George Mason of Virginia, and Elbridge Gerry of Massachusetts. Mason demanded a Bill of Rights if he was to support the Constitution. The Bill of Rights was not included in the Constitution submitted to the states for ratification, but many states ratified the Constitution with the understanding that a bill of rights would soon follow.[138] Shortly before the document was to be signed, Gorham proposed to lower the size of congressional districts from 40,000 to 30,000 citizens. A similar measure had been proposed earlier, and failed by one vote. George Washington spoke up here, making his only substantive contribution to the text of the Constitution in supporting this move. The Convention adopted it without further debate. Gorham would sign the document, although he had openly doubted whether the United States would remain a single, unified nation for more than 150 years.[129]: 112  Ultimately, 39 of the 55 delegates who attended (74 had been chosen from 12 states) ended up signing, but it is likely that none were completely satisfied. Their views were summed up by Benjamin Franklin, who said, ""I confess that there are several parts of this Constitution which I do not at present approve, but I am not sure I shall never approve them. ... I doubt too whether any other Convention we can obtain, may be able to make a better Constitution. ... It therefore astonishes me, Sir, to find this system approaching so near to perfection as it does; and I think it will astonish our enemies ...""[139]
 Rhode Island never sent delegates, and two of New York's three delegates did not stay at the convention for long. Therefore, as George Washington stated, the document was executed by ""eleven states, and Colonel Hamilton.""[129]: 244  Washington signed the document first, and then moving by state delegation from north to south, as had been the custom throughout the convention, the delegates filed to the front of the room to sign their names.
 At the time the document was signed, Franklin gave a persuasive speech involving an anecdote on a sun that was painted on the back of Washington's Chippendale chair.[140] As recounted in Madison's notes:
 The Constitution was then submitted to the states for ratification, pursuant to its own Article VII.[142]
 Slavery was one of the most difficult issues confronting the delegates. Slavery was widespread in the states at the time of the convention.[129]: 68  At least a third of the convention's 55 delegates owned slaves, including all of the delegates from Virginia and South Carolina.[129]: 68–69  Slaves comprised approximately one-fifth of the population of the states,[143]: 139  and apart from northernmost New England, where slavery had largely been eliminated, slaves lived in all regions of the country.[143]: 132  However, more than 90% of the slaves[143]: 132  lived in the South, where approximately 1 in 3 families owned slaves (in the largest and wealthiest state, Virginia, that figure was nearly 1 in 2 families).[143]: 135  The entire agrarian economy of the South was based on slave labor, and the Southern delegates to the convention were unwilling to accept any proposal that they believed would threaten the institution.
 Whether slavery was to be regulated under the new Constitution was a matter of such intense conflict between the North and South that three Southern states, Georgia and the two Carolinas, refused to join the Union if slavery were not to be allowed. Delegates opposed to slavery were forced to yield in their demands that slavery be outlawed within the new nation. However, they continued to argue that the Constitution should prohibit the states from participating in the international slave trade, including in the importation of new slaves from Africa and the export of slaves to other countries. The Convention postponed making a final decision on the international slave trade until late in the deliberations because of the contentious nature of the issue. During the convention's late July recess, the Committee of Detail had inserted language that would prohibit the federal government from attempting to ban international slave trading and from imposing taxes on the purchase or sale of slaves. The convention could not agree on these provisions when the subject came up again in late August, so they referred the matter to an eleven-member committee for further discussion. This committee helped work out a compromise: Congress would have the power to ban the international slave trade, but not for another twenty years (that is, not until 1808). In exchange for this concession, the federal government's power to regulate foreign commerce would be strengthened by provisions that allowed for taxation of slave trades in the international market and that reduced the requirement for passage of navigation acts from two-thirds majorities of both houses of Congress to simple majorities.[144]
 Another contentious slavery-related question was whether slaves would be counted as part of the population in determining representation of the states in the Congress or would instead be considered property and thus would not be included. The original Virginia Plan contemplated a proportional representation based on the number of free inhabitants. However, the Committee of the Whole modified this in June to include three-fifths of ""other persons"", a euphemism for slaves.[145] While this benefited states with large slave population in terms of representation, delegates from these states argued that slaves should be considered property and not as persons if the new government would be levying taxes on the basis of population.[146][147] Despite the conflict, the Three-Fifths Compromise would be adopted by the convention as part of the Connecticut Compromise.[145]
 
 Fifty-five delegates attended sessions of the Constitutional Convention, all of whom are considered the Framers of the Constitution, though only 39 delegates actually signed.[148][149] The states had originally appointed 70 representatives to the convention, but a number of the appointees did not accept or could not attend, leaving 55 who ultimately crafted the document.[148]
 Almost all of the 55 Framers had taken part in the Revolution, with at least 29 having served in the Continental forces, most in positions of command.[150] All but two or three had served in colonial or state government during their careers.[151] The vast majority (about 75%) of the delegates were or had been members of the Confederation Congress, and many had been members of the Continental Congress during the Revolution.[129]: 25  Several had been state governors.[151][150] Only two delegates, Roger Sherman and Robert Morris, would sign all three of the nation's founding documents: Declaration of Independence, Articles of Confederation, and U.S. Constitution.[150]
 More than half of the delegates had trained as lawyers (several had been judges), although only about a quarter had practiced law as their principal occupation. Other delegates included merchants, manufacturers, shippers, land speculators, bankers, and financiers. Several were physicians and small farmers, and one was a minister.[152][150] Of the 25 who owned slaves, 16 depended on slave labor to run the plantations or other businesses that formed the mainstay of their income. Many of the delegates were landowners with substantial holdings, and most were comfortably wealthy.[153] George Washington and Robert Morris, for example, were among the wealthiest men in the entire country.[150]
 Their depth of knowledge and experience in self-government was remarkable. As Thomas Jefferson in Paris semi-seriously wrote to John Adams in London, ""It really is an assembly of demigods.""[154][155]
 (*) Did not sign the final draft of the U.S. Constitution. Randolph, Mason, and Gerry were the only three present in Philadelphia at the time who refused to sign.
 Several prominent Founders are notable for not participating in the Constitutional Convention. Thomas Jefferson was abroad, serving as the minister to France.[156] John Adams was in Britain, serving as minister to that country, but he wrote home to encourage the delegates. Patrick Henry refused to participate because he ""smelt a rat in Philadelphia, tending toward the monarchy."" Also absent were Samuel Adams, John Hancock and John Jay. Many of the states' older and more experienced leaders may have simply been too busy with the local affairs of their states to attend the convention,[151] which had originally been planned to strengthen the existing Articles of Confederation, not to write a constitution for a completely new national government.
 
.mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}39°56′56″N 75°09′00″W﻿ / ﻿39.94889°N 75.15000°W﻿ / 39.94889; -75.15000
"
Constitution of the United States,https://en.wikipedia.org/wiki/Constitution_of_the_United_States,"


 The Constitution of the United States is the supreme law of the United States of America.[3] It superseded the Articles of Confederation, the nation's first constitution, on March 4, 1789. Originally including seven articles, the Constitution delineates the frame of the federal government. The Constitution's first three articles embody the doctrine of the separation of powers, in which the federal government is divided into three branches: the legislative, consisting of the bicameral Congress (Article I); the executive, consisting of the president and subordinate officers (Article II); and the judicial, consisting of the Supreme Court and other federal courts (Article III). Article IV, Article V, and Article VI embody concepts of federalism, describing the rights and responsibilities of state governments, the states in relationship to the federal government, and the shared process of constitutional amendment. Article VII establishes the procedure subsequently used by the 13 states to ratify it. The Constitution of the United States is the oldest and longest-standing written and codified national constitution in force in the world.[4][a]
 The drafting of the Constitution, often referred to as its framing, was completed at the Constitutional Convention, which assembled at Independence Hall in Philadelphia between May 25 and September 17, 1787.[5] Delegates to the convention were chosen by the state legislatures of 12 of the 13 original states; Rhode Island refused to send delegates.[6] The convention's initial mandate was limited to amending the Articles of Confederation, which had proven highly ineffective in meeting the young nation's needs.[7] Almost immediately, however, delegates began considering measures to replace the Articles.[8] The first proposal discussed, introduced by delegates from Virginia, called for a bicameral (two-house) Congress that was to be elected on a proportional basis based on state population, an elected chief executive, and an appointed judicial branch.[9] An alternative to the Virginia Plan, known as the New Jersey Plan, also called for an elected executive but retained the legislative structure created by the Articles, a unicameral Congress where all states had one vote.[10]
 On June 19, 1787, delegates rejected the New Jersey Plan with three states voting in favor, seven against, and one divided. The plan's defeat led to a series of compromises centering primarily on two issues: slavery and proportional representation.[11][12] The first of these pitted Northern states, where slavery was slowly being abolished, against Southern states, whose agricultural economies depended on slave labor.[13] The issue of proportional representation was of similar concern to less populous states, which under the Articles had the same power as larger states.[14] To satisfy interests in the South, particularly in Georgia and South Carolina, the delegates agreed to protect the slave trade, that is, the importation of slaves, for 20 years.[15] Slavery was protected further by allowing states to count three-fifths of their slaves as part of their populations, for the purpose of representation in the federal government, and by requiring the return of escaped slaves to their owners, even if captured in states where slavery had been abolished.[16] Finally, the delegates adopted the Connecticut Compromise, which proposed a Congress with proportional representation in the lower house and equal representation in the upper house (the Senate) giving each state two senators.[17] While these compromises held the Union together and aided the Constitution's ratification, slavery continued for six more decades and the less populous states continue to have disproportional representation in the U.S. Senate and Electoral College.[18][12]
 Since the Constitution became operational in 1789, it has been amended 27 times.[19][20] The first ten amendments, known collectively as the Bill of Rights, offer specific protections of individual liberty and justice and place restrictions on the powers of government within the U.S. states.[21][22] The majority of the 17 later amendments expand individual civil rights protections. Others address issues related to federal authority or modify government processes and procedures. Amendments to the United States Constitution, unlike ones made to many constitutions worldwide, are appended to the document. The original U.S. Constitution[23] was handwritten on five pages of parchment by Jacob Shallus.[24]
The first permanent constitution,[b] it is interpreted, supplemented, and implemented by a large body of federal constitutional law and has influenced the constitutions of other nations.
 From September 5, 1774, to March 1, 1781, the Second Continental Congress, convened in Philadelphia in what today is called Independence Hall, functioned as the provisional government of the United States. Delegates to the First Continental Congress in 1774 and then the Second Continental Congress from 1775 to 1781 were chosen largely from the revolutionary committees of correspondence in various colonies rather than through the colonial governments of the Thirteen Colonies.[27]
 The Articles of Confederation and Perpetual Union was the first constitution of the United States.[28] The document was drafted by a committee appointed by the Second Continental Congress in mid-June 1777 and was adopted by the full Congress in mid-November of that year. Ratification by the 13 colonies took more than three years and was completed March 1, 1781. The Articles gave little power to the central government. While the Confederation Congress had some decision-making abilities, it lacked enforcement powers. The implementation of most decisions, including amendments to the Articles, required legislative approval by all 13 of the newly formed states.[29][30]
 Despite these limitations, based on the Congressional authority granted in Article 9, the league of states was considered as strong as any similar republican confederation ever formed.[31] The chief problem was, in the words of George Washington, ""no money.""[32] The Confederated Congress could print money, but it was worthless, and while the Congress could borrow money, it could not pay it back.[32] No state paid its share of taxes to support the government, and some paid nothing. A few states did meet the interest payments toward the national debt owed by their citizens, but nothing greater, and no interest was paid on debts owed foreign governments. By 1786, the United States was facing default on its outstanding debts.[32]
 Under the Articles, the United States had little ability to defend its sovereignty. Most of the troops in the nation's 625-man army were deployed facing non-threatening British forts on American soil. Soldiers were not being paid, some were deserting, and others were threatening mutiny.[33] Spain closed New Orleans to American commerce, despite the protests of U.S. officials. When Barbary pirates began seizing American ships of commerce, the Treasury had no funds to pay toward ransom. If a military crisis required action, the Congress had no credit or taxing power to finance a response.[32]
 Domestically, the Articles of Confederation was failing to bring unity to the diverse sentiments and interests of the various states. Although the Treaty of Paris in 1783 was signed between Britain and the U.S., and named each of the American states, various states proceeded to violate it. New York and South Carolina repeatedly prosecuted Loyalists for wartime activity and redistributed their lands.[32] Individual state legislatures independently laid embargoes, negotiated directly with foreign authorities, raised armies, and made war, all violating the letter and the spirit of the Articles.[citation needed]
 In September 1786, during an inter–state convention to discuss and develop a consensus about reversing the protectionist trade barriers that each state had erected, James Madison questioned whether the Articles of Confederation was a binding compact or even a viable government. Connecticut paid nothing and ""positively refused"" to pay U.S. assessments for two years.[34] A rumor at the time was that a seditious party of New York legislators had opened a conversation with the Viceroy of Canada. To the south, the British were said to be openly funding Creek Indian raids on Georgia, and the state was under martial law.[35] Additionally, during Shays' Rebellion (August 1786 – June 1787) in Massachusetts, Congress could provide no money to support an endangered constituent state. General Benjamin Lincoln was obliged to raise funds from Boston merchants to pay for a volunteer army.[36]
 Congress was paralyzed. It could do nothing significant without nine states, and some legislation required all 13. When a state produced only one member in attendance, its vote was not counted. If a state's delegation was evenly divided, its vote could not be counted towards the nine-count requirement.[37] The Congress of the Confederation had ""virtually ceased trying to govern.""[38] The vision of a respectable nation among nations seemed to be fading in the eyes of revolutionaries such as George Washington, Benjamin Franklin, and Rufus King. Their dream of a republic, a nation without hereditary rulers, with power derived from the people in frequent elections, was in doubt.[39][40]
 On February 21, 1787, the Confederation Congress called a convention of state delegates in Philadelphia to propose revisions to the Articles.[41] Unlike earlier attempts, the convention was not meant for new laws or piecemeal alterations, but for the ""sole and express purpose of revising the Articles of Confederation."" The convention was not limited to commerce; rather, it was intended to ""render the federal constitution adequate to the exigencies of government and the preservation of the Union."" The proposal might take effect when approved by Congress and the states.[42]
 On the appointed day, May 14, 1787, only the Virginia and Pennsylvania delegations were present, and the convention's opening meeting was postponed for lack of a quorum.[43] A quorum of seven states met on May 25, and deliberations began. Eventually 12 states were represented, with Rhode Island refusing to participate. Of the 74 delegates appointed by the states, 55 attended.[6] The delegates were generally convinced that an effective central government with a wide range of enforceable powers must replace the weaker Congress established by the Articles of Confederation.[citation needed]
 Two plans for structuring the federal government arose at the convention's outset:
 On May 31, the Convention devolved into the Committee of the Whole, charged with considering the Virginia Plan. On June 13, the Virginia resolutions in amended form were reported out of committee. The New Jersey Plan was put forward in response to the Virginia Plan.[citation needed]
 A Committee of Eleven, including one delegate from each state represented, met from July 2 to 16[44] to work out a compromise on the issue of representation in the federal legislature. All agreed to a republican form of government grounded in representing the people in the states. For the legislature, two issues were to be decided: how the votes were to be allocated among the states in the Congress, and how the representatives should be elected. In its report, now known as the Connecticut Compromise (or ""Great Compromise""), the committee proposed proportional representation for seats in the House of Representatives based on population (with the people voting for representatives), and equal representation for each State in the Senate (with each state's legislators generally choosing their respective senators), and that all money bills would originate in the House.[45]
 The Great Compromise ended the stalemate between patriots and nationalists, leading to numerous other compromises in a spirit of accommodation. There were sectional interests to be balanced by the Three-Fifths Compromise; reconciliation on Presidential term, powers, and method of selection; and jurisdiction of the federal judiciary.[citation needed]
 On July 24, a Committee of Detail, including John Rutledge (South Carolina), Edmund Randolph (Virginia), Nathaniel Gorham (Massachusetts), Oliver Ellsworth (Connecticut), and James Wilson (Pennsylvania), was elected to draft a detailed constitution reflective of the resolutions passed by the convention up to that point.[46] The Convention recessed from July 26 to August 6 to await the report of this ""Committee of Detail"". Overall, the report of the committee conformed to the resolutions adopted by the convention, adding some elements. A twenty-three article (plus preamble) constitution was presented.[47]
 From August 6 to September 10, the report of the committee of detail was discussed, section by section and clause by clause. Details were attended to, and further compromises were effected.[44][46] Toward the close of these discussions, on September 8, a Committee of Style and Arrangement, including Alexander Hamilton from New York, William Samuel Johnson from Connecticut, Rufus King from Massachusetts, James Madison from Virginia, and Gouverneur Morris from Pennsylvania, was appointed to distill a final draft constitution from the 23 approved articles.[46] The final draft, presented to the convention on September 12, contained seven articles, a preamble and a closing endorsement, of which Morris was the primary author.[6] The committee also presented a proposed letter to accompany the constitution when delivered to Congress.[48]
 The final document, engrossed by Jacob Shallus,[49] was taken up on Monday, September 17, at the convention's final session. Several of the delegates were disappointed in the result, a makeshift series of unfortunate compromises. Some delegates left before the ceremony and three others refused to sign. Of the thirty-nine signers, Benjamin Franklin summed up, addressing the convention: ""There are several parts of this Constitution which I do not at present approve, but I am not sure I shall never approve them."" He would accept the Constitution, ""because I expect no better and because I am not sure that it is not the best.""[50]
 The advocates of the Constitution were anxious to obtain unanimous support of all twelve states represented in the convention. Their accepted formula for the closing endorsement was ""Done in Convention, by the unanimous consent of the States present."" At the end of the convention, the proposal was agreed to by eleven state delegations and the lone remaining delegate from New York, Alexander Hamilton.[51]
 Within three days of its signing on September 17, 1787, the Constitution was submitted to the Congress of the Confederation, then sitting in New York City, the nation's temporary capital.[52][53][54] The document, originally intended as a revision of the Articles of Confederation, instead introduced a completely new form of government.[55][56][57] While members of Congress had the power to reject it, they voted unanimously on September 28 to forward the proposal to the thirteen states for their ratification.[58][59] Under the process outlined in Article VII of the proposed Constitution, the state legislatures were tasked with organizing ""Federal Conventions"" to ratify the document. This process ignored the amendment provision of the Articles of Confederation which required unanimous approval of all the states. Instead, Article VII called for ratification by just nine of the 13 states—a two-thirds majority.[60][29][61]
 Two factions soon emerged, one supporting the Constitution, the Federalists, and the other opposing it, the so-called Anti-Federalists.[62][63] Over the ensuing months, the proposal was debated, criticized, and expounded upon clause by clause. In the state of New York, at the time a hotbed of anti-Federalism, three delegates from the Philadelphia Convention who were also members of the Congress—Hamilton, Madison, and Jay—published a series of commentaries, now known as The Federalist Papers, in support of ratification.[64][65]
 Before year's end, three state legislatures voted in favor of ratification. Delaware was first, voting unanimously 30–0; Pennsylvania second, approving the measure 46–23;[66][67][68] and New Jersey third, also recording a unanimous vote.[69] As 1788 began, Connecticut and Georgia followed Delaware's lead with almost unanimous votes, but the outcome became less certain as leaders in key states such as Virginia, New York, and Massachusetts expressed concerns over the lack of protections for people's rights.[70][71][72][73] Fearing the prospect of defeat, the Federalists relented, promising that if the Constitution was adopted, amendments would be added to secure individual liberties.[74] With that, the anti-Federalists' position collapsed.[75]
 On June 21, 1788, New Hampshire became the ninth state to ratify. Three months later, on September 17, the Congress of the Confederation certified the ratification of eleven states, and passed resolutions setting dates for choosing the first senators and representatives, the first Wednesday of January (January 7, 1789); electing the first president, the first Wednesday of February (February 4); and officially starting the new government, the first Wednesday of March (March 4), when the first Congress would convene in New York City.[76] As its final act, the Congress of Confederation agreed to purchase 10 square miles from Maryland and Virginia for establishing a permanent capital.
 North Carolina waited to ratify the Constitution until after the Bill of Rights was passed by the new Congress, and Rhode Island's ratification would only come after a threatened trade embargo.[77][78]
 The U.S. Constitution was a federal one and was greatly influenced by the study of Magna Carta and other federations, both ancient and extant. The Due Process Clause of the Constitution was partly based on common law and on Magna Carta (1215), which had become a foundation of English liberty against arbitrary power wielded by a ruler.[79][80]  The idea of Separation of Powers inherent in the Constitution was largely inspired by eighteenth-century Enlightenment philosophers, such as Montesquieu and John Locke.[81]
 The influence of Montesquieu, Locke, Edward Coke and William Blackstone were evident at the Constitutional Convention. Prior to and during the framing and signing of the Constitution, Blackstone, Hume, Locke and Montesquieu were among the political philosophers most frequently referred to.[82] Historian Herbert W. Schneider held that the Scottish Enlightenment was ""probably the most potent single tradition in the American Enlightenment"" and the advancement of personal liberties.[83] Historian Jack P. Greene maintains that by 1776 the founders drew heavily upon Magna Carta and the later writings of ""Enlightenment rationalism"" and English common law. Historian Daniel Walker Howe notes that Benjamin Franklin greatly admired David Hume, an eighteenth-century Scottish philosopher, and had studied many of his works while at Edinburgh in 1760. Both embraced the idea that high-ranking public officials should receive no salary[84] and that the lower class was a better judge of character when it came to choosing their representatives.[85]
 In his Institutes of the Lawes of England, Coke interpreted Magna Carta protections and rights to apply not just to nobles, but to all British subjects. In writing the Virginia Charter of 1606, he enabled the King in Parliament to give those to be born in the colonies all rights and liberties as though they were born in England. William Blackstone's Commentaries on the Laws of England are considered the most influential books on law in the new republic.[82][86] Madison made frequent reference to Blackstone, Locke, and Montesquieu,[87] all of whom were among the most prominent political theorists of the late eighteenth century.[88]
 Following the Glorious Revolution of 1688, British political philosopher John Locke was a major influence,[89] expanding on the contract theory of government advanced by Thomas Hobbes, his contemporary.[90] Locke advanced the principle of consent of the governed in his Two Treatises of Government. Government's duty under a social contract among the sovereign people was to serve the people by protecting their rights. These basic rights were life, liberty, and property.[91]
 Montesquieu's influence on the framers is evident in Madison's Federalist No. 47 and Hamilton's Federalist No. 78. Jefferson, Adams, and Mason were known to read Montesquieu.[92] Supreme Court Justices, the ultimate interpreters of the constitution, have cited Montesquieu throughout the Court's history.[93] (See, e.g., Green v. Biddle, 21 U.S. 1, 1, 36 (1823).United States v. Wood, 39 U.S. 430, 438 (1840).Myers v. United States, 272 U.S. 52, 116 (1926).Nixon v. Administrator of General Services, 433 U.S. 425, 442 (1977).Bank Markazi v. Peterson, 136 U.S. 1310, 1330 (2016).) Montesquieu emphasized the need for balanced forces pushing against each other to prevent tyranny (reflecting the influence of Polybius's 2nd century BC treatise on the checks and balances of the Roman Republic). In his The Spirit of Law, Montesquieu maintained that the separation of state powers should be by its service to the people's liberty: legislative, executive and judicial,[94][95] while also emphasizing that the idea of separation had for its purpose the even distribution of authority among the several branches of government.[96]
 The English Bill of Rights (1689) was an inspiration for the American Bill of Rights. Both require jury trials, contain a right to keep and bear arms, prohibit excessive bail and forbid ""cruel and unusual punishments"".[97] Many liberties protected by state constitutions and the Virginia Declaration of Rights were incorporated into the Bill of Rights.[98] Upon the arrival of the American Revolution, many of the rights guaranteed by the Federal Bill of Rights were recognized as being inspired by English law.[97] A substantial body of thought had been developed from the literature of republicanism in the United States, typically demonstrated by the works of John Adams, who often quoted Blackstone and Montesquieu verbatim, and applied to the creation of state constitutions.[99]
 While the ideas of unalienable rights, the separation of powers and the structure of the Constitution were largely influenced by the European Enlightenment thinkers, like Montesquieu, John Locke and others,[82][100][101] Benjamin Franklin and Thomas Jefferson still had reservations about the existing forms of government in Europe.[102] In a speech at the Constitutional Convention Franklin stated, ""We have gone back to ancient history for models of Government, and examined different forms of those Republics ... And we have viewed modern States all round Europe but find none of their Constitutions suitable to our circumstances.""[103] Jefferson maintained, that most European governments were autocratic monarchies and not compatible with the egalitarian character of the American people. In a 1787 letter to John Rutledge, Jefferson asserted that ""The only condition on earth to be compared with [American government] ... is that of the Indians, where they still have less law than we.""[104]
 American Indian history scholars Donald Grinde and Bruce Johansen claim there is ""overwhelming evidence"" that Iroquois Confederacy political concepts and ideas influenced the U.S. Constitution,[105] and are considered to be the most outspoken supporters of the Iroquois thesis.[106] The idea as to the extent of that influence on the founding, however, varies among historians and has been questioned or criticized by various historians, including Samuel Payne,[107] William Starna, George Hamell,[108] and historian and archaeologist Philip Levy, who claims the evidence is largely coincidental and circumstantial.[109] The most outspoken critic, anthropologist Elisabeth Tooker, claimed the Iroquois influence thesis is largely the product of ""white interpretations of Indians"" and ""scholarly misapprehension"".[110][111]
 John Napoleon Brinton Hewitt, who was born on the Tuscarora Indian Reservation, and was an ethnologist at the Smithsonian Institution's Bureau of Ethnology is often cited by historians of Iroquois history. Hewitt, however, rejected the idea that the Iroquois League had a major influence on the Albany Plan of Union, Benjamin Franklin's plan to create a unified government for the Thirteen Colonies, which was rejected.[110]
 The Constitution includes four sections: an introductory paragraph titled Preamble, a list of seven Articles that define the government's framework, an untitled closing endorsement with the signatures of 39 framers, and 27 amendments that have been adopted under Article V (see below).
 The Preamble, the Constitution's introductory paragraph, lays out the purposes of the new government:[112]
 The opening words, ""We the People"", represented a new thought: the idea that the people and not the states were the source of the government's legitimacy.[113][114][115][116][117][118] Coined by Gouverneur Morris of Pennsylvania, who chaired the convention's Committee of Style, the phrase is considered an improvement on the section's original draft which followed the words We the People with a list of the 13 states.[119][112] In place of the names of the states Morris substituted ""of the United States"" and then listed the Constitution's six goals, none of which were mentioned originally.[120][121]
 The Constitution's main provisions include seven articles that define the basic framework of the federal government. Articles that have been amended still include the original text, although provisions repealed by amendments under Article V are usually bracketed or italicized to indicate they no longer apply. Despite these changes, the focus of each Article remains the same as when adopted in 1787.[citation needed]
 Article I describes the Congress, the legislative branch of the federal government. Section 1 reads, ""All legislative powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives."" The article establishes the manner of election and the qualifications of members of each body. Representatives must be at least 25 years old, be a citizen of the United States for seven years, and live in the state they represent. Senators must be at least 30 years old, be a citizen for nine years, and live in the state they represent.
 Article I, Section 8 enumerates the powers delegated to the legislature. Financially, Congress has the power to tax, borrow, pay debt and provide for the common defense and the general welfare; to regulate commerce, bankruptcies, and coin money. To regulate internal affairs, it has the power to regulate and govern military forces and militias, suppress insurrections and repel invasions. It is to provide for naturalization, standards of weights and measures, post offices and roads, and patents; to directly govern the federal district and cessions of land by the states for forts and arsenals. Internationally, Congress has the power to define and punish piracies and offenses against the Law of Nations, to declare war and make rules of war. The final Necessary and Proper Clause, also known as the Elastic Clause, expressly confers incidental powers upon Congress without the Articles' requirement for express delegation for each and every power. Article I, Section 9 lists eight specific limits on congressional power.
 The Supreme Court has sometimes broadly interpreted the Commerce Clause and the Necessary and Proper Clause in Article One to allow Congress to enact legislation that is neither expressly allowed by the enumerated powers nor expressly denied in the limitations on Congress. In McCulloch v. Maryland (1819), the Supreme Court read the Necessary and Proper Clause to permit the federal government to take action that would ""enable [it] to perform the high duties assigned to it [by the Constitution] in the manner most beneficial to the people,""[122] even if that action is not itself within the enumerated powers. Chief Justice Marshall clarified: ""Let the end be legitimate, let it be within the scope of the Constitution, and all means which are appropriate, which are plainly adapted to that end, which are not prohibited, but consist with the letter and spirit of the Constitution, are Constitutional.""[122]
 Article II describes the office, qualifications, and duties of the President of the United States and the Vice President. The President is head of the executive branch of the federal government, as well as the nation's head of state and head of government.
 Article two is modified by the 12th Amendment, which tacitly acknowledges political parties, and the 25th Amendment relating to office succession. The president is to receive only one compensation from the federal government. The inaugural oath is specified to preserve, protect and defend the Constitution.
 The president is the Commander in Chief of the United States Armed Forces, as well as of state militias when they are mobilized. The president makes treaties with the advice and consent of a two-thirds quorum of the Senate. To administer the federal government, the president commissions all the offices of the federal government as Congress directs; and may require the opinions of its principal officers and make ""recess appointments"" for vacancies that may happen during the recess of the Senate. The president ensures the laws are faithfully executed and may grant reprieves and pardons with the exception of Congressional impeachment. The president reports to Congress on the State of the Union, and by the Recommendation Clause, recommends ""necessary and expedient"" national measures. The president may convene and adjourn Congress under special circumstances.
 Section 4 provides for the removal of the president and other federal officers. The president is removed on impeachment for, and conviction of, treason, bribery, or other high crimes and misdemeanors.
 Article III describes the court system (the judicial branch), including the Supreme Court. The article describes the kinds of cases the court takes as original jurisdiction. Congress can create lower courts and an appeals process and enacts law defining crimes and punishments. Article Three also protects the right to trial by jury in all criminal cases, and defines the crime of treason.
 Section 1 vests the judicial power of the United States in federal courts and, with it, the authority to interpret and apply the law to a particular case. Also included is the power to punish, sentence, and direct future action to resolve conflicts. The Constitution outlines the U.S. judicial system. In the Judiciary Act of 1789, Congress began to fill in details. Currently, Title 28 of the U.S. Code[123] describes judicial powers and administration.
 As of the First Congress, the Supreme Court justices rode circuit to sit as panels to hear appeals from the district courts.[c] In 1891, Congress enacted a new system. District courts would have original jurisdiction. Intermediate appellate courts (circuit courts) with exclusive jurisdiction heard regional appeals before consideration by the Supreme Court. The Supreme Court holds discretionary jurisdiction, meaning that it does not have to hear every case that is brought to it.[123]
 To enforce judicial decisions, the Constitution grants federal courts both criminal contempt and civil contempt powers. Other implied powers include injunctive relief and the habeas corpus remedy. The Court may imprison for contumacy, bad-faith litigation, and failure to obey a writ of mandamus. Judicial power includes that granted by Acts of Congress for rules of law and punishment. Judicial power also extends to areas not covered by statute. Generally, federal courts cannot interrupt state court proceedings.[123]
 Clause 1 of Section 2 authorizes the federal courts to hear actual cases and controversies only. Their judicial power does not extend to cases that are hypothetical, or which are proscribed due to standing, mootness, or ripeness issues. Generally, a case or controversy requires the presence of adverse parties who have some interest genuinely at stake in the case.[d]
 Clause 2 of Section 2 provides that the Supreme Court has original jurisdiction in cases involving ambassadors, ministers, and consuls, for all cases respecting foreign nation-states,[124] and also in those controversies which are subject to federal judicial power because at least one state is a party. Cases arising under the laws of the United States and its treaties come under the jurisdiction of federal courts. Cases under international maritime law and conflicting land grants of different states come under federal courts. Cases between U.S. citizens in different states, and cases between U.S. citizens and foreign states and their citizens, come under federal jurisdiction. The trials will be in the state where the crime was committed.[123]
 No part of the Constitution expressly authorizes judicial review, but the Framers did contemplate the idea, and precedent has since established that the courts could exercise judicial review over the actions of Congress or the executive branch. Two conflicting federal laws are under ""pendent"" jurisdiction if one presents a strict constitutional issue. Federal court jurisdiction is rare when a state legislature enacts something as under federal jurisdiction.[e] To establish a federal system of national law, considerable effort goes into developing a spirit of comity between federal government and states. By the doctrine of 'Res judicata', federal courts give ""full faith and credit"" to State Courts.[f] The Supreme Court will decide Constitutional issues of state law only on a case-by-case basis, and only by strict Constitutional necessity, independent of state legislators' motives, their policy outcomes or its national wisdom.[g]
 Section 3 bars Congress from changing or modifying Federal law on treason by simple majority statute. This section also defines treason as an overt act of making war or materially helping those at war with the United States. Accusations must be corroborated by at least two witnesses. Congress is a political body, and political disagreements routinely encountered should never be considered as treason. This allows for nonviolent resistance to the government because opposition is not a life or death proposition. However, Congress does provide for other lesser subversive crimes, such as conspiracy.[h]
 Article IV outlines the relations among the states and between each state and the federal government. In addition, it provides for such matters as admitting new states and border changes between the states. For instance, it requires states to give ""full faith and credit"" to the public acts, records, and court proceedings of the other states. Congress is permitted to regulate the manner in which proof of such acts may be admitted. The ""privileges and immunities"" clause prohibits state governments from discriminating against citizens of other states in favor of resident citizens. For instance, in criminal sentencing, a state may not increase a penalty on the grounds that the convicted person is a non-resident.
 It also establishes extradition between the states, as well as laying down a legal basis for freedom of movement and travel among the states. Today, this provision is sometimes taken for granted, but in the days of the Articles of Confederation, crossing state lines was often arduous and costly. The Territorial Clause gives Congress the power to make rules for disposing of federal property and governing non-state territories of the United States. Finally, the fourth section of Article Four requires the United States to guarantee to each state a republican form of government and to protect them from invasion and violence.
 Article V outlines the process for amending the Constitution. Eight state constitutions in effect in 1787 included an amendment mechanism. Amendment-making power rested with the legislature in three of the states, and in the other five it was given to specially elected conventions. The Articles of Confederation provided that amendments were to be proposed by Congress and ratified by the unanimous vote of all 13 state legislatures. This proved to be a major flaw in the Articles, as it created an insurmountable obstacle to constitutional reform. The amendment process crafted during the Philadelphia Constitutional Convention was, according to The Federalist No. 43, designed to establish a balance between pliancy and rigidity:[125][better source needed]
 There are two steps in the amendment process. Proposals to amend the Constitution must be properly adopted and ratified before they change the Constitution. First, there are two procedures for adopting the language of a proposed amendment, either by (a) Congress, by two-thirds majority in both the Senate and the House of Representatives, or (b) national convention (which shall take place whenever two-thirds of the state legislatures collectively call for one). Second, there are two procedures for ratifying the proposed amendment, which requires three-fourths of the states' (presently 38 of 50) approval: (a) consent of the state legislatures, or (b) consent of state ratifying conventions. The ratification method is chosen by Congress for each amendment.[126] State ratifying conventions were used only once, for the Twenty-first Amendment.[127]
 Presently, the Archivist of the United States is charged with responsibility for administering the ratification process under the provisions of 1 U.S. Code § 106b. The Archivist submits the proposed amendment to the states for their consideration by sending a letter of notification to each Governor. Each Governor then formally submits the amendment to their state's legislature. When a state ratifies a proposed amendment, it sends the Archivist an original or certified copy of the state's action. Ratification documents are examined by the Office of the Federal Register for facial legal sufficiency and an authenticating signature.[128]
 Article Five ends by shielding certain clauses in the new frame of government from being amended. Article One, Section 9, Clause 1 prevents Congress from passing any law that would restrict the importation of slaves into the United States prior to 1808, plus the fourth clause from that same section, which reiterates the Constitutional rule that direct taxes must be apportioned according to state populations. These clauses were explicitly shielded from Constitutional amendment prior to 1808. On January 1, 1808, the first day it was permitted to do so, Congress approved legislation prohibiting the importation of slaves into the country. On February 3, 1913, with ratification of the Sixteenth Amendment, Congress gained the authority to levy an income tax without apportioning it among the states or basing it on the United States Census. The third textually entrenched provision is Article One, Section 3, Clauses 1, which provides for equal representation of the states in the Senate. The shield protecting this clause from the amendment process (""no state, without its consent, shall be deprived of its equal Suffrage in the Senate"") is less absolute but it is permanent.
 Article VI establishes that the Constitution and all federal laws and treaties made in accordance with it have supremacy over state laws, and that ""the judges in every state shall be bound thereby, any thing in the laws or constitutions of any state notwithstanding."" It validates national debt created under the Articles of Confederation and requires that all federal and state legislators, officers, and judges take oaths or affirmations to support the Constitution. This means that the states' constitutions and laws should not conflict with the laws of the federal constitution and that in case of a conflict, state judges are legally bound to honor the federal laws and constitution over those of any state. Article Six also states ""no religious Test shall ever be required as a Qualification to any Office or public Trust under the United States.""
 Article VII describes the process for establishing the proposed new frame of government. Anticipating that the influence of many state politicians would be Antifederalist, delegates to the Philadelphia Convention provided for ratification of the Constitution by popularly elected ratifying conventions in each state. The convention method also made it possible that judges, ministers and others ineligible to serve in state legislatures, could be elected to a convention. Suspecting that Rhode Island, at least, might not ratify, delegates decided that the Constitution would go into effect as soon as nine states (two-thirds rounded up) ratified.[129] Each of the remaining four states could then join the newly formed union by ratifying.[130]
 The signing of the United States Constitution occurred on September 17, 1787, when 39 delegates endorsed the constitution created during the convention. In addition to signatures, this closing endorsement, the Constitution's eschatocol, included a brief declaration that the delegates' work has been successfully completed and that those whose signatures appear on it subscribe to the final document. Included are a statement pronouncing the document's adoption by the states present, a formulaic dating of its adoption, and the delegates' signatures. Additionally, the convention's secretary, William Jackson, added a note to verify four amendments made by hand to the final document, and signed the note to authenticate its validity.[131]
 The language of the concluding endorsement, conceived by Gouverneur Morris and presented to the convention by Benjamin Franklin, was made intentionally ambiguous in hopes of winning over the votes of dissenting delegates. Advocates for the new frame of government, realizing the impending difficulty of obtaining the consent of the states needed to make it operational, were anxious to obtain the unanimous support of the delegations from each state. It was feared that many of the delegates would refuse to give their individual assent to the Constitution. Therefore, in order that the action of the convention would appear to be unanimous, the formula, Done in convention by the unanimous consent of the states present ... was devised.[132][better source needed]
 The document is dated: ""the Seventeenth Day of September in the Year of our Lord"" 1787, and ""of the Independence of the United States of America the Twelfth."" This two-fold epoch dating serves to place the Constitution in the context of the religious traditions of Western civilization and, at the same time, links it to the regime principles proclaimed in the Declaration of Independence. This dual reference can also be found in the Articles of Confederation and the Northwest Ordinance.[132][better source needed]
 The closing endorsement serves an authentication function only. It neither assigns powers to the federal government nor does it provide specific limitations on government action. It does, however, provide essential documentation of the Constitution's validity, a statement of ""This is what was agreed to."" It records who signed the Constitution, and when and where.[citation needed]
 The procedure for amending the Constitution is outlined in Article V (see above). The process is overseen by the archivist of the United States. Between 1949 and 1985, it was overseen by the administrator of General Services, and before that by the secretary of state.[128]
 Under Article Five, a proposal for an amendment must be adopted either by two-thirds of both houses of Congress or by a national convention that had been requested by two-thirds of the state legislatures.[128] Once the proposal has passed by either method, Congress must decide whether the proposed amendment is to be ratified by state legislatures or by state ratifying conventions. The proposed amendment along with the method of ratification is sent to the Office of the Federal Register, which copies it in slip law format and submits it to the states.[128] To date, the convention method of proposal has never been tried and the convention method of ratification has only been used once, for the Twenty-first Amendment.[126]
 A proposed amendment becomes an operative part of the Constitution as soon as it is ratified by three-fourths of the States (currently 38 of the 50 states). There is no further step. The text requires no additional action by Congress or anyone else after ratification by the required number of states.[133] Thus, when the Office of the Federal Register verifies that it has received the required number of authenticated ratification documents, it drafts a formal proclamation for the Archivist to certify that the amendment is valid and has become part of the nation's frame of government. This certification is published in the Federal Register and United States Statutes at Large and serves as official notice to Congress and to the nation that the ratification process has been successfully completed.[128]
 The Constitution has twenty-seven amendments. Structurally, the Constitution's original text and all prior amendments remain untouched. The precedent for this practice was set in 1789, when Congress considered and proposed the first several Constitutional amendments. Among these, Amendments 1–10 are collectively known as the Bill of Rights, and Amendments 13–15 are known as the Reconstruction Amendments. Excluding the Twenty-seventh Amendment, which was pending before the states for 202 years, 225 days, the longest pending amendment that was successfully ratified was the Twenty-second Amendment, which took 3 years, 343 days. The Twenty-sixth Amendment was ratified in the shortest time, 100 days. The average ratification time for the first twenty-six amendments was 1 year, 252 days; for all twenty-seven, 9 years, 48 days.
 The first ten Amendments introduced were referred to as the Bill of Rights which consists of 10 amendments that were added to the Constitution in 1791, as supporters of the Constitution had promised critics during the debates of 1788.[134]
 The First Amendment (1791) prohibits Congress from obstructing the exercise of certain individual freedoms: freedom of religion, freedom of speech, freedom of the press, freedom of assembly, and right to petition. Its Free Exercise Clause guarantees a person's right to hold whatever religious beliefs they want, and to freely exercise that belief, and its Establishment Clause prevents the federal government from creating an official national church or favoring one set of religious beliefs over another. The amendment guarantees an individual's right to express and to be exposed to a wide range of opinions and views. It was intended to ensure a free exchange of ideas, even unpopular ones. It also guarantees an individual's right to physically gather or associate with others in groups for economic, political or religious purposes. Additionally, it guarantees an individual's right to petition the government for a redress of grievances.[135]
 The Second Amendment (1791) protects the right of individuals[136][137] to keep and bear arms.[138][139][140][141] The Supreme Court has ruled that this right applies to individuals, not merely to collective militias. It has also held that the government may regulate or place some limits on the manufacture, ownership and sale of firearms or other weapons.[142][143] Requested by several states during the Constitutional ratification debates, the amendment reflected the lingering resentment over the widespread efforts of the British to confiscate the colonists' firearms at the outbreak of the Revolutionary War. Patrick Henry had rhetorically asked, shall we be stronger, ""when we are totally disarmed, and when a British Guard shall be stationed in every house?""[144]
 The Third Amendment (1791) prohibits the federal government from forcing individuals to provide lodging to soldiers in their homes during peacetime without their consent. Requested by several states during the Constitutional ratification debates, the amendment reflected the lingering resentment over the Quartering Acts passed by the British Parliament during the Revolutionary War, which had allowed British soldiers to take over private homes for their own use.[145]
 The Fourth Amendment (1791) protects people against unreasonable searches and seizures of either self or property by government officials. A search can mean everything from a frisking by a police officer or to a demand for a blood test to a search of an individual's home or car. A seizure occurs when the government takes control of an individual or something in the possession of the individual. Items that are seized often are used as evidence when the individual is charged with a crime. It also imposes certain limitations on police investigating a crime and prevents the use of illegally obtained evidence at trial.[146]
 The Fifth Amendment (1791) establishes the requirement that a trial for a major crime may commence only after an indictment has been handed down by a grand jury; protects individuals from double jeopardy, being tried and put in danger of being punished more than once for the same criminal act; prohibits punishment without due process of law, thus protecting individuals from being imprisoned without fair procedures; and provides that an accused person may not be compelled to reveal to the police, prosecutor, judge, or jury any information that might incriminate or be used against him or her in a court of law. Additionally, the Fifth Amendment also prohibits government from taking private property for public use without ""just compensation"", the basis of eminent domain in the United States.[147]
 The Sixth Amendment (1791) provides several protections and rights to an individual accused of a crime. The accused has the right to a fair and speedy trial by a local and impartial jury. Likewise, a person has the right to a public trial. This right protects defendants from secret proceedings that might encourage abuse of the justice system, and serves to keep the public informed. This amendment also guarantees a right to legal counsel if accused of a crime, guarantees that the accused may require witnesses to attend the trial and testify in the presence of the accused, and guarantees the accused a right to know the charges against them. In 1966, the Supreme Court ruled that, with the Fifth Amendment, this amendment requires what has become known as the Miranda warning.[148]
 The Seventh Amendment (1791) extends the right to a jury trial to federal civil cases, and inhibits courts from overturning a jury's findings of fact. Although the Seventh Amendment itself says that it is limited to ""suits at common law"", meaning cases that triggered the right to a jury under English law, the amendment has been found to apply in lawsuits that are similar to the old common law cases. For example, the right to a jury trial applies to cases brought under federal statutes that prohibit race or gender discrimination in housing or employment. Importantly, this amendment guarantees the right to a jury trial only in federal court, not in state court.[149]
 The Eighth Amendment (1791) protects people from having bail or fines set at an amount so high that it would be impossible for all but the richest defendants to pay, and also protects people from being subjected to cruel and unusual punishment. Although this phrase originally was intended to outlaw certain gruesome methods of punishment, it has been broadened over the years to protect against punishments that are grossly disproportionate to or too harsh for the particular crime. This provision has also been used to challenge prison conditions such as extremely unsanitary cells, overcrowding, insufficient medical care and deliberate failure by officials to protect inmates from one another.[150]
 The Ninth Amendment (1791) declares that individuals have other fundamental rights, in addition to those stated in the Constitution. During the Constitutional ratification debates, Anti-Federalists argued that a Bill of Rights should be added. The Federalists opposed it on grounds that a list would necessarily be incomplete but would be taken as explicit and exhaustive, thus enlarging the power of the federal government by implication. The Anti-Federalists persisted, and several state ratification conventions refused to ratify the Constitution without a more specific list of protections, so the First Congress added what became the Ninth Amendment as a compromise. Because the rights protected by the Ninth Amendment are not specified, they are referred to as ""unenumerated"". The Supreme Court has found that unenumerated rights include such important rights as the right to travel, the right to vote, the right to privacy, and the right to make important decisions about one's health care or body.[151]
 The Tenth Amendment (1791) was included in the Bill of Rights to further define the balance of power between the federal government and the states. The amendment states that the federal government has only those powers specifically granted by the Constitution. These powers include the power to declare war, to collect taxes, to regulate interstate business activities and others that are listed in the articles or in subsequent constitutional amendments. Any power not listed is, says the Tenth Amendment, left to the states or the people. While there is no specific list of what these ""reserved powers"" may be, the Supreme Court has ruled that laws affecting family relations, commerce within a state's own borders, abortion, and local law enforcement activities, are among those specifically reserved to the states or the people.[152][153]
 The Eleventh Amendment (1795) specifically prohibits federal courts from hearing cases in which a state is sued by an individual from another state or another country, thus extending to the states sovereign immunity protection from certain types of legal liability. Article Three, Section 2, Clause 1 has been affected by this amendment, which also overturned the Supreme Court's decision in Chisholm v. Georgia (1793)[154][155]
 The Sixteenth Amendment (1913) removed existing Constitutional constraints that limited the power of Congress to lay and collect taxes on income. Specifically, the apportionment constraints delineated in Article 1, Section 9, Clause 4 have been removed by this amendment, which also overturned an 1895 Supreme Court decision, in Pollock v. Farmers' Loan & Trust Co., that declared an unapportioned federal income tax on rents, dividends, and interest unconstitutional. This amendment has become the basis for all subsequent federal income tax legislation and has greatly expanded the scope of federal taxing and spending in the years since.[156]
 The Eighteenth Amendment (1919) prohibited the making, transporting, and selling of alcoholic beverages nationwide. It also authorized Congress to enact legislation enforcing this prohibition. Adopted at the urging of a national temperance movement, proponents believed that the use of alcohol was reckless and destructive and that prohibition would reduce crime and corruption, solve social problems, decrease the need for welfare and prisons, and improve the health of all Americans. During prohibition, it is estimated that alcohol consumption and alcohol related deaths declined dramatically. But prohibition had other, more negative consequences. The amendment drove the lucrative alcohol business underground, giving rise to a large and pervasive black market. In addition, prohibition encouraged disrespect for the law and strengthened organized crime. Prohibition came to an end in 1933, when this amendment was repealed.[157]
 The Twenty-first Amendment (1933) repealed the Eighteenth Amendment and returned the regulation of alcohol to the states. Each state sets its own rules for the sale and importation of alcohol, including the drinking age. Because a federal law provides federal funds to states that prohibit the sale of alcohol to minors under the age of twenty-one, all fifty states have set their drinking age there. Rules about how alcohol is sold vary greatly from state to state.[158]
 The Thirteenth Amendment (1865) abolished slavery and involuntary servitude, except as punishment for a crime, and authorized Congress to enforce abolition. Though millions of slaves had been declared free by the 1863 Emancipation Proclamation, their post Civil War status was unclear, as was the status of other millions.[159] Congress intended the Thirteenth Amendment to be a proclamation of freedom for all slaves throughout the nation and to take the question of emancipation away from politics. This amendment rendered inoperative or moot several of the original parts of the constitution.[160]
 The Fourteenth Amendment (1868) granted United States citizenship to former slaves and to all persons ""subject to U.S. jurisdiction."" It also contained three new limits on state power: a state shall not violate a citizen's privileges or immunities; shall not deprive any person of life, liberty, or property without due process of law; and must guarantee all persons equal protection of the laws. These limitations dramatically expanded the protections of the Constitution. This amendment, according to the Supreme Court's Doctrine of Incorporation, makes most provisions of the Bill of Rights applicable to state and local governments as well. It superseded the mode of apportionment of representatives delineated in Article 1, Section 2, Clause 3, and also overturned the Supreme Court's decision in Dred Scott v. Sandford (1857).[161]
 The Fifteenth Amendment (1870) prohibits the use of race, color, or previous condition of servitude in determining which citizens may vote. The last of three post Civil War Reconstruction Amendments, it sought to abolish one of the key vestiges of slavery and to advance the civil rights and liberties of former slaves.[162]
 The Nineteenth Amendment (1920) prohibits the government from denying women the right to vote on the same terms as men. Prior to the amendment's adoption, only a few states permitted women to vote and to hold office.[163]
 The Twenty-third Amendment (1961) extends the right to vote in presidential elections to citizens residing in the District of Columbia by granting the District electors in the Electoral College, as if it were a state. When first established as the nation's capital in 1800, the District of Columbia's five thousand residents had neither a local government, nor the right to vote in federal elections. By 1960 the population of the District had grown to over 760,000.[164]
 The Twenty-fourth Amendment (1964) prohibits a poll tax for voting. Although passage of the Thirteenth, Fourteenth, and Fifteenth Amendments helped remove many of the discriminatory laws left over from slavery, they did not eliminate all forms of discrimination. Along with literacy tests and durational residency requirements, poll taxes were used to keep low-income (primarily African American) citizens from participating in elections. The Supreme Court has since struck down these discriminatory measures.[165]
 The Twenty-sixth Amendment (1971) prohibits the government from denying the right of United States citizens, eighteen years of age or older, to vote on account of age. The drive to lower the voting age was driven in large part by the broader student activism movement protesting the Vietnam War. It gained strength following the Supreme Court's decision in Oregon v. Mitchell (1970).[166]
 The Twelfth Amendment (1804) modifies the way the Electoral College chooses the president and vice president. It stipulates that each elector must cast a distinct vote for president and vice president, instead of two votes for president. It also suggests that the president and vice president should not be from the same state. Article II, Section 1, Clause 3 is superseded by this amendment, which also extends the eligibility requirements to become president to the vice president.[167]
 The Seventeenth Amendment (1913) modifies the way senators are elected. It stipulates that senators are to be elected by direct popular vote. The amendment supersedes Article 1, Section 3, Clauses 1 and 2, under which the two senators from each state were elected by the state legislature. It also allows state legislatures to permit their governors to make temporary appointments until a special election can be held.[168]
 The Twentieth Amendment (1933) changes the date on which a new president, Vice President and Congress take office, thus shortening the time between Election Day and the beginning of Presidential, Vice Presidential and Congressional terms.[169] Originally, the Constitution provided that the annual meeting was to be on the first Monday in December unless otherwise provided by law. This meant that, when a new Congress was elected in November, it did not come into office until the following March, with a ""lame duck"" Congress convening in the interim. By moving the beginning of the president's new term from March 4 to January 20 (and in the case of Congress, to January 3), proponents hoped to put an end to lame duck sessions, while allowing for a speedier transition for the new administration and legislators.[170]
 The Twenty-second Amendment (1951) limits an elected president to two terms in office, a total of eight years. However, under some circumstances it is possible for an individual to serve more than eight years. Although nothing in the original frame of government limited how many presidential terms one could serve, the nation's first president, George Washington, declined to run for a third term, suggesting that two terms of four years were enough for any president. This precedent remained an unwritten rule of the presidency until broken by Franklin D. Roosevelt, who was elected to a third term as president 1940 and in 1944 to a fourth.[171]
 The Twenty-fifth Amendment (1967) clarifies what happens upon the death, removal, or resignation of the President or Vice President and how the Presidency is temporarily filled if the President becomes disabled and cannot fulfill the responsibilities of the office. It supersedes the ambiguous succession rule established in Article II, Section 1, Clause 6. A concrete plan of succession has been needed on multiple occasions since 1789. However, for nearly 20% of U.S. history, there has been no vice president in office who can assume the presidency.[172]
 The Twenty-seventh Amendment (1992) prevents members of Congress from granting themselves pay raises during the current session. Rather, any raises that are adopted must take effect during the next session of Congress. Its proponents believed that Federal legislators would be more likely to be cautious about increasing congressional pay if they have no personal stake in the vote. Article One, section 6, Clause 1 has been affected by this amendment, which remained pending for over two centuries as it contained no time limit for ratification.[173]
 Collectively, members of the House and Senate propose around 150 amendments during each two-year term of Congress.[174] Most however, never get out of the Congressional committees in which they are proposed, and only a fraction of those approved in committee receive sufficient support to win Congressional approval and actually enter the constitutional ratification process.[citation needed]
 Six amendments approved by Congress and proposed to the states for consideration have not been ratified by the required number of states to become part of the Constitution. Four of these are technically still pending, as Congress did not set a time limit (see also Coleman v. Miller) for their ratification. The other two are no longer pending, as both had a time limit attached and in both cases the time period set for their ratification expired.[citation needed]
 The way the Constitution is understood is influenced by court decisions, especially those of the Supreme Court. These decisions are referred to as precedents. Judicial review is the power of the Court to examine federal legislation, federal executive, and all state branches of government, to decide their constitutionality, and to strike them down if found unconstitutional.[citation needed]
 Judicial review includes the power of the Court to explain the meaning of the Constitution as it applies to particular cases. Over the years, Court decisions on issues ranging from governmental regulation of radio and television to the rights of the accused in criminal cases have changed the way many constitutional clauses are interpreted, without amendment to the actual text of the Constitution.[citation needed]
 Legislation passed to implement the Constitution, or to adapt those implementations to changing conditions, broadens and, in subtle ways, changes the meanings given to the words of the Constitution. Up to a point, the rules and regulations of the many federal executive agencies have a similar effect. If an action of Congress or the agencies is challenged, however, the court system ultimately decides whether these actions are permissible under the Constitution.[citation needed]
 Courts established by the Constitution can regulate government under the Constitution, the supreme law of the land.[j] First, they have jurisdiction over actions by an officer of government and state law. Second, federal courts may rule on whether coordinate branches of national government conform to the Constitution. Until the twentieth century, the Supreme Court of the United States may have been the only high tribunal in the world to use a court for constitutional interpretation of fundamental law, others generally depending on their national legislature.[185]
 The basic theory of American judicial review is summarized by constitutional legal scholars and historians as follows: the written Constitution is fundamental law within the states. It can change only by extraordinary legislative process of national proposal, then state ratification. The powers of all departments are limited to enumerated grants found in the Constitution. Courts are expected (a) to enforce provisions of the Constitution as the supreme law of the land, and (b) to refuse to enforce anything in conflict with it.[186]
 As to judicial review and the Congress, the first proposals by Madison (Virginia) and Wilson (Pennsylvania) called for a supreme court veto over national legislation. In this it resembled the system in New York, where the Constitution of 1777 called for a ""Council of Revision"" by the governor and justices of the state supreme court. The council would review and veto any passed legislation; violating the spirit of the Constitution before it went into effect. The nationalist's proposal in convention was defeated three times and replaced by a presidential veto with congressional over-ride. Judicial review relies on the jurisdictional authority in Article III, and the Supremacy Clause.[187]
 The justification for judicial review is to be explicitly found in the open ratifications held in the states and reported in their newspapers. John Marshall in Virginia, James Wilson in Pennsylvania and Oliver Ellsworth of Connecticut all argued for Supreme Court judicial review of acts of state legislature. In Federalist No. 78, Alexander Hamilton advocated the doctrine of a written document held as a superior enactment of the people. ""A limited constitution can be preserved in practice no other way"" than through courts which can declare void any legislation contrary to the Constitution. The preservation of the people's authority over legislatures rests ""particularly with judges.""[188][k]
 The Supreme Court was initially made up of jurists who had been intimately connected with the framing of the Constitution and the establishment of its government as law. John Jay (New York), a co-author of The Federalist Papers, served as chief justice for the first six years. The second chief justice, John Rutledge (South Carolina), was appointed by Washington in 1795 as a recess appointment, but was not confirmed by the Senate. Resigning later that year, he was succeeded in 1796 by the third chief justice, Oliver Ellsworth (Connecticut).[190] Both Rutledge and Ellsworth were delegates to the Constitutional Convention. John Marshall (Virginia), the fourth chief justice, had served in the Virginia Ratification Convention in 1788. His 34 years of service on the Court would see some of the most important rulings to help establish the nation the Constitution had begun. Other early members of the Supreme Court who had been delegates to the Constitutional Convention included James Wilson (Pennsylvania) for ten years, and John Blair Jr. (Virginia) for five years.[citation needed]
 When John Marshall followed Oliver Ellsworth as chief justice of the Supreme Court in 1801, the federal judiciary had been established by the Judiciary Act, but there were few cases, and less prestige. ""The fate of judicial review was in the hands of the Supreme Court itself."" Review of state legislation and appeals from state supreme courts was understood. But the Court's life, jurisdiction over state legislation was limited. The Marshall Court's landmark Barron v. Baltimore held that the Bill of Rights restricted only the federal government, and not the states.[188]
 In the landmark Marbury v. Madison case, the Supreme Court asserted its authority of judicial review over Acts of Congress. Its findings were that Marbury and the others had a right to their commissions as judges in the District of Columbia. Marshall, writing the opinion for the majority, announced his discovered conflict between Section 13 of the Judiciary Act of 1789 and Article III.[l][191][m] In this case, both the Constitution and the statutory law applied to the particulars at the same time. ""The very essence of judicial duty"" according to Marshall was to determine which of the two conflicting rules should govern. The Constitution enumerates powers of the judiciary to extend to cases arising ""under the Constitution"". Further, justices take a Constitutional oath to uphold it as ""Supreme law of the land.""[192] Therefore, since the United States government as created by the Constitution is a limited government, the federal courts were required to choose the Constitution over congressional law if there were deemed to be a conflict.[citation needed]
 ""This argument has been ratified by time and by practice ...""[n][o] The Supreme Court did not declare another act of Congress unconstitutional until the controversial Dred Scott decision in 1857, held after the voided Missouri Compromise statute had already been repealed. In the eighty years following the Civil War to World War II, the Court voided congressional statutes in 77 cases, on average almost one a year.[194]
 A crisis arose when, in 1935 and 1936, the Supreme Court handed down twelve decisions voiding acts of Congress relating to the New Deal. President Franklin D. Roosevelt then responded with his abortive ""court packing plan"". Other proposals have suggested a Court super-majority to overturn Congressional legislation, or a constitutional amendment to require that the justices retire at a specified age by law. To date, the Supreme Court's power of judicial review has persisted.[189]
 The power of judicial review could not have been preserved long in a democracy unless it had been ""wielded with a reasonable measure of judicial restraint, and with some attention, as Mr. Dooley said, to the election returns."" Indeed, the Supreme Court has developed a system of doctrine and practice that self-limit its power of judicial review.[195]
 The Court controls almost all of its business by choosing what cases to consider, writs of certiorari. In this way, it can avoid opinions on embarrassing or difficult cases. The Supreme Court limits itself by defining what is a ""justiciable question"". First, the Court is fairly consistent in refusing to make any ""advisory opinions"" in advance of actual cases.[p] Second, ""friendly suits"" between those of the same legal interest are not considered. Third, the Court requires a ""personal interest"", not one generally held, and a legally protected right must be immediately threatened by government action. Cases are not taken up if the litigant has no standing to sue. Simply having the money to sue and being injured by government action are not enough.[195]
 These three procedural ways of dismissing cases have led critics to charge that the Supreme Court delays decisions by unduly insisting on technicalities in their ""standards of litigability"". They say cases are left unconsidered which are in the public interest, with genuine controversy, and resulting from good faith action. ""The Supreme Court is not only a court of law but a court of justice.""[196]
 The Supreme Court balances several pressures to maintain its roles in national government. It seeks to be a co-equal branch of government, but its decrees must be enforceable. The Court seeks to minimize situations where it asserts itself superior to either president or Congress, but federal officers must be held accountable. The Supreme Court assumes power to declare acts of Congress as unconstitutional but it self-limits its passing on constitutional questions.[197] But the Court's guidance on basic problems of life and governance in a democracy is most effective when American political life reinforces its rulings.[198]
 Justice Brandeis summarized four general guidelines that the Supreme Court uses to avoid constitutional decisions relating to Congress:[q] The Court will not anticipate a question of constitutional law nor decide open questions unless a case decision requires it. If it does, a rule of constitutional law is formulated only as the precise facts in the case require. The Court will choose statutes or general law for the basis of its decision if it can without constitutional grounds. If it does, the Court will choose a constitutional construction of an act of Congress, even if its constitutionality is seriously in doubt.[197]
 Likewise with the executive department, Edwin Corwin observed that the Court does sometimes rebuff presidential pretensions, but it more often tries to rationalize them. Against Congress, an act is merely ""disallowed"". In the executive case, exercising judicial review produces ""some change in the external world"" beyond the ordinary judicial sphere.[199] The ""political question"" doctrine especially applies to questions which present a difficult enforcement issue. Chief Justice Charles Evans Hughes addressed the Court's limitation when political process allowed future policy change, but a judicial ruling would ""attribute finality"". Political questions lack ""satisfactory criteria for a judicial determination.""[200]
 John Marshall recognized that the president holds ""important political powers"" which as executive privilege allows great discretion. This doctrine was applied in Court rulings on President Grant's duty to enforce the law during Reconstruction. It extends to the sphere of foreign affairs. Justice Robert Jackson explained, foreign affairs are inherently political, ""wholly confided by our Constitution to the political departments of the government ... [and] not subject to judicial intrusion or inquiry.""[201]
 Critics of the Court object in two principal ways to self-restraint in judicial review, deferring as it does as a matter of doctrine to acts of Congress and presidential actions.
 Supreme Courts under the leadership of subsequent chief justices have also used judicial review to interpret the Constitution among individuals, states and federal branches. Notable contributions were made by the Chase Court, the Taft Court, the Warren Court, and the Rehnquist Court.[citation needed]
 Salmon P. Chase was a Lincoln appointee, serving as chief justice from 1864 to 1873. His career encompassed service as a U.S. senator and Governor of Ohio. He coined the slogan, ""Free soil, free Labor, free men."" One of Lincoln's ""team of rivals"", he was appointed Secretary of Treasury during the Civil War, issuing ""greenbacks"". Partly to appease the Radical Republicans, Lincoln appointed him chief justice upon the death of Roger B. Taney.
 In one of his first official acts, Chase admitted John Rock, the first African American to practice before the Supreme Court. The Chase Court is famous for Texas v. White, which asserted a permanent Union of indestructible states. Veazie Bank v. Fenno upheld the Civil War tax on state banknotes. Hepburn v. Griswold found parts of the Legal Tender Acts unconstitutional, though it was reversed under a late Supreme Court majority.
 William Howard Taft was a Harding appointment to chief justice from 1921 to 1930. A Progressive Republican from Ohio, he was a one-term President.
 As chief justice, he advocated the Judiciary Act of 1925 that brought the Federal District Courts under the administrative jurisdiction of the Supreme Court. Taft successfully sought the expansion of Court jurisdiction over non-states such as District of Columbia and Territories of Alaska and Hawaii.
 In 1925, the Taft Court issued a ruling overturning a Marshall Court ruling on the Bill of Rights. In Gitlow v. New York, the Court established the doctrine of ""incorporation"", which applied the Bill of Rights to the states. Important cases included the Board of Trade of City of Chicago v. Olsen, which upheld Congressional regulation of commerce. Olmstead v. United States allowed exclusion of evidence obtained without a warrant based on application of the 14th Amendment proscription against unreasonable searches. Wisconsin v. Illinois ruled the equitable power of the United States can impose positive action on a state to prevent its inaction from damaging another state.
 Earl Warren was an Eisenhower nominee, chief justice from 1953 to 1969. Warren's Republican career in the law reached from county prosecutor, California state attorney general, and three consecutive terms as governor. His programs stressed progressive efficiency, expanding state education, re-integrating returning veterans, infrastructure, and highway construction.
 In 1954, the Warren Court overturned a landmark Fuller Court ruling on the Fourteenth Amendment interpreting racial segregation as permissible in government and commerce providing ""separate but equal"" services. Warren built a coalition of justices after 1962 that developed the idea of natural rights as guaranteed in the Constitution. Brown v. Board of Education banned segregation in public schools. Baker v. Carr and Reynolds v. Sims established Court ordered ""one-man-one-vote"". Bill of Rights Amendments were incorporated into the states. Due process was expanded in Gideon v. Wainwright and Miranda v. Arizona. First Amendment rights were addressed in Griswold v. Connecticut concerning privacy, and Engel v. Vitale relative to free speech.
 William Rehnquist was a Reagan-appointed chief justice, serving from 1986 to 2005. While he would concur with overthrowing a state supreme court's decision, as in Bush v. Gore, he built a coalition of Justices after 1994 that developed the idea of federalism as provided for in the Tenth Amendment. In the hands of the Supreme Court, the Constitution and its amendments were to restrain Congress, as in City of Boerne v. Flores.
 Nevertheless, the Rehnquist Court was noted in the contemporary ""culture wars"" for overturning state laws relating to privacy, prohibiting late-term abortions in Stenberg v. Carhart, prohibiting sodomy in Lawrence v. Texas, or ruling so as to protect free speech in Texas v. Johnson or affirmative action in Grutter v. Bollinger.
 There is a viewpoint that some Americans have come to see the documents of the Constitution, along with the Declaration of Independence and the Bill of Rights, as being a cornerstone of a type of civil religion. Some commentators depict the multi-ethnic, multi-sectarian United States as held together by political orthodoxy, in contrast with a nation-state of people having more ""natural"" ties.[203][204]
 The United States Constitution has been a notable model for governance worldwide, especially through the 1970s. Its international influence is found in similarities in phrasing and borrowed passages in other constitutions, as well as in the principles of the rule of law, separation of powers, and recognition of individual rights.[citation needed]
 The American experience of fundamental law with amendments and judicial review has motivated constitutionalists at times when they were considering the possibilities for their nation's future.[205] It informed Abraham Lincoln during the American Civil War,[v] his contemporary and ally Benito Juárez of Mexico,[w] and the second generation of 19th-century constitutional nationalists, José Rizal of the Philippines[x] and Sun Yat-sen of China.[y] The framers of the Australian constitution integrated federal ideas from the U.S. and other constitutions.[211]
 Since the 1980s, the influence of the United States Constitution has been waning as other countries have created new constitutions or updated older constitutions, a process which Sanford Levinson believes to be more difficult in the United States than in any other country.[212][213][214]
 The United States Constitution has faced various criticisms since its inception in 1787.
 The Constitution did not originally define who was eligible to vote, allowing each state to determine who was eligible. In the early history of the U.S., most states allowed only white male adult property owners to vote; the notable exception was New Jersey, where women were able to vote on the same basis as men.[215][216][217] Until the Reconstruction Amendments were adopted between 1865 and 1870, the five years immediately following the American Civil War, the Constitution did not abolish slavery, nor give citizenship and voting rights to former slaves.[218] These amendments did not include a specific prohibition on discrimination in voting on the basis of sex; it took another amendment—the Nineteenth, ratified in 1920—for the Constitution to prohibit any United States citizen from being denied the right to vote on the basis of sex.[219]
 According to a 2012 study by David Law and Mila Versteeg published in the New York University Law Review, the U.S. Constitution guarantees relatively few rights compared to the constitutions of other countries and contains fewer than half (26 of 60) of the provisions listed in the average bill of rights. It is also one of the few in the world today that still features the right to keep and bear arms; the other two being the constitutions of Guatemala and Mexico.[213][214]
 Sanford Levinson wrote in 2006 that it has been the most difficult constitution in the world to amend since the fall of Yugoslavia.[212][220] Levitsky and Ziblatt argue that the US Constitution is the most difficult in the world to amend, and that this helps explain why the US still has so many undemocratic institutions that most or all other democracies have reformed, directly allowing significant democratic backsliding in the United States.[221]
 In 1937, the U.S. Post Office, at the prompting of President Franklin Delano Roosevelt, an avid stamp collector himself, released a commemorative postage stamp celebrating the 150th anniversary of the signing of the U.S. Constitution. The engraving on this issue is after an 1856 painting by Junius Brutus Stearns of Washington and shows delegates signing the Constitution at the 1787 Convention.[222] The following year another commemorative stamp was issued celebrating the 150th anniversary of the ratification of the Constitution.[223] In 1987 the U.S. Government minted a 1987 silver dollar in celebration of the 200th anniversary of the signing of the Constitution.[224][225]
"
United States Electoral College,https://en.wikipedia.org/wiki/United_States_Electoral_College,"

 In the United States, the Electoral College is the group of presidential electors that is formed every four years during the presidential election for the sole purpose of voting for the president and vice president. This process is described in Article Two of the Constitution.[1] The number of electoral votes exercised by each state is equal to that state's congressional delegation which is the number of Senators (two) plus the number of Representatives for that state. Each state appoints electors using legal procedures determined by its legislature. Federal office holders, including senators and representatives, cannot be electors. Additionally, the Twenty-third Amendment granted the federal District of Columbia three electors (bringing the total number from 535 to 538). A simple majority of electoral votes (270 or more) is required to elect the president and vice president. If no candidate achieves a majority, a contingent election is held by the House of Representatives, to elect the president, and by the Senate, to elect the vice president.
 The states and the District of Columbia hold a statewide or district-wide popular vote on Election Day in November to choose electors based upon how they have pledged to vote for president and vice president, with some state laws prohibiting faithless electors. All states except Maine and Nebraska use a party block voting, or general ticket method, to choose their electors, meaning all their electors go to one winning ticket. Maine and Nebraska choose one elector per congressional district and two electors for the ticket with the highest statewide vote. The electors meet and vote in December, and the inaugurations of the president and vice president take place in January.
 The merit of the electoral college system has been a matter of ongoing debate in the United States since its inception at the Constitutional Convention in 1787, becoming more controversial by the latter years of the 19th century, up to the present day.[2][3] More resolutions have been submitted to amend the Electoral College mechanism than any other part of the constitution.[4] An amendment that would have abolished the system was approved by the House in 1969, but failed to move past the Senate.[5]
 Supporters argue that it requires presidential candidates to have broad appeal across the country to win, while critics argue that it is not representative of the popular will of the nation.[a] Winner-take-all systems, especially with representation not proportional to population, do not align with the principle of ""one person, one vote"".[b][9] Critics object to the inequity that, due to the distribution of electors, individual citizens in states with smaller populations have more voting power than those in larger states. Because the number of electors each state appoints is equal to the size of its congressional delegation, each state is entitled to at least three electors regardless of its population, and the apportionment of the statutorily fixed number of the rest is only roughly proportional. This allocation has contributed to runners-up of the nationwide popular vote being elected president in 1824, 1876, 1888, 2000, and 2016.[10][11] In addition, faithless electors may not vote in accord with their pledge.[12][c] A further objection is that swing states receive the most attention from candidates.[14] By the end of the 20th century, electoral colleges had been abandoned by all other democracies around the world in favor of direct elections for an executive president.[15][16]:215
 Article II, Section 1, Clause 2 of the United States Constitution directs each state to appoint a number of electors equal to that state's congressional delegation (the number of members of the House of Representatives plus two senators). The same clause empowers each state legislature to determine the manner by which that state's electors are chosen but prohibits federal office holders from being named electors. Following the national presidential election day on Tuesday after the first Monday in November,[17] each state, and the federal district, selects its electors according to its laws. After a popular election, the states identify and record their appointed electors in a Certificate of Ascertainment, and those appointed electors then meet in their respective jurisdictions and produce a Certificate of Vote for their candidate; both certificates are then sent to Congress to be opened and counted.[18]
 In 48 of the 50 states, state laws mandate that the winner of the plurality of the statewide popular vote receives all of that state's electoral votes.[19] In Maine and Nebraska, two electoral votes are assigned in this manner, while the remaining electoral votes are allocated based on the plurality of votes in each of their congressional districts.[20] The federal district, Washington, D.C., allocates its 3 electoral votes to the winner of its single district election. States generally require electors to pledge to vote for that state's winning ticket; to prevent electors from being faithless electors, most states have adopted various laws to enforce the electors' pledge.[21]
 The electors of each state meet in their respective state capital on the first Tuesday after the second Wednesday of December, between December 14 and 20, to cast their votes.[19][22] The results are sent to and counted by the Congress, where they are tabulated in the first week of January before a joint meeting of the Senate and the House of Representatives, presided over by the current vice president, as president of the Senate.[19][23]
 Should a majority of votes not be cast for a candidate, a contingent election takes place: the House holds a presidential election session, where one vote is cast by each of the fifty states. The Senate is responsible for electing the vice president, with each senator having one vote.[24] The elected president and vice president are inaugurated on January 20.
 Since 1964, there have been 538 electors. States select 535 of the electors, this number matches the aggregate total of their congressional delegations.[25][26][27] The additional three electors come from the Twenty-third Amendment, ratified in 1961, providing that the district established pursuant to Article I, Section 8, Clause 17 as the seat of the federal government (namely, Washington, D.C.) is entitled to the same number of electors as the least populous state.[28] In practice, that results in Washington D.C. being entitled to three electors.[29][30]
 The Electoral College was officially selected as the means of electing president towards the end of the Constitutional Convention, due to pressure from slave states wanting to increase their voting power, since they could count slaves as 3/5 of a person when allocating electors, and by small states who increased their power given the minimum of three electors per state.[31] The compromise was reached after other proposals, including a direct election for president (as proposed by Hamilton among others), failed to get traction among slave states.[31] Steven Levitsky and Daniel Ziblatt describe it as ""not a product of constitutional theory or farsighted design. Rather, it was adopted by default, after all other alternatives had been rejected.""[31]
 In 1787, the Constitutional Convention used the Virginia Plan as the basis for discussions, as the Virginia proposal was the first. The Virginia Plan called for Congress to elect the president.[32][33] Delegates from a majority of states agreed to this mode of election. After being debated, delegates came to oppose nomination by Congress for the reason that it could violate the separation of powers. James Wilson then made a motion for electors for the purpose of choosing the president.[34][35]
 Later in the convention, a committee formed to work out various details. They included the mode of election of the president, including final recommendations for the electors, a group of people apportioned among the states in the same numbers as their representatives in Congress (the formula for which had been resolved in lengthy debates resulting in the Connecticut Compromise and Three-Fifths Compromise), but chosen by each state ""in such manner as its Legislature may direct"". Committee member Gouverneur Morris explained the reasons for the change. Among others, there were fears of ""intrigue"" if the president were chosen by a small group of men who met together regularly, as well as concerns for the independence of the president if he were elected by Congress.[36][37]
 Once the Electoral College had been decided on, several delegates (Mason, Butler, Morris, Wilson, and Madison) openly recognized its ability to protect the election process from cabal, corruption, intrigue, and faction. Some delegates, including James Wilson and James Madison, preferred popular election of the executive.[38][39] Madison acknowledged that while a popular vote would be ideal, it would be difficult to get consensus on the proposal given the prevalence of slavery in the South:
 The convention approved the committee's Electoral College proposal, with minor modifications, on September 4, 1787.[41][42] Delegates from states with smaller populations or limited land area, such as Connecticut, New Jersey, and Maryland, generally favored the Electoral College with some consideration for states.[43][non-primary source needed] At the compromise providing for a runoff among the top five candidates, the small states supposed that the House of Representatives, with each state delegation casting one vote, would decide most elections.[44]
 In The Federalist Papers, James Madison explained his views on the selection of the president and the Constitution. In Federalist No. 39, Madison argued that the Constitution was designed to be a mixture of state-based and population-based government. Congress would have two houses: the state-based Senate and the population-based House of Representatives. Meanwhile, the president would be elected by a mixture of the two modes.[45]
 Alexander Hamilton in Federalist No. 68, published on March 12, 1788, laid out what he believed were the key advantages to the Electoral College. The electors come directly from the people and them alone, for that purpose only, and for that time only. This avoided a party-run legislature or a permanent body that could be influenced by foreign interests before each election.[46][non-primary source needed] Hamilton explained that the election was to take place among all the states, so no corruption in any state could taint ""the great body of the people"" in their selection. The choice was to be made by a majority of the Electoral College, as majority rule is critical to the principles of republican government. Hamilton argued that electors meeting in the state capitals were able to have information unavailable to the general public, in a time before telecommunications. Hamilton also argued that since no federal officeholder could be an elector, none of the electors would be beholden to any presidential candidate.[46]
 Another consideration was that the decision would be made without ""tumult and disorder"", as it would be a broad-based one made simultaneously in various locales where the decision makers could deliberate reasonably, not in one place where decision makers could be threatened or intimidated. If the Electoral College did not achieve a decisive majority, then the House of Representatives was to choose the president from among the top five candidates,[47][citation needed] ensuring selection of a presiding officer administering the laws would have both ability and good character. Hamilton was also concerned about somebody unqualified but with a talent for ""low intrigue, and the little arts of popularity"" attaining high office.[46]
 In the Federalist No. 10, James Madison argued against ""an interested and overbearing majority"" and the ""mischiefs of faction"" in an electoral system. He defined a faction as ""a number of citizens whether amounting to a majority or minority of the whole, who are united and actuated by some common impulse of passion, or of interest, adverse to the rights of other citizens, or to the permanent and aggregate interests of the community."" A republican government (i.e., representative democracy, as opposed to direct democracy) combined with the principles of federalism (with distribution of voter rights and separation of government powers), would countervail against factions. Madison further postulated in the Federalist No. 10 that the greater the population and expanse of the Republic, the more difficulty factions would face in organizing due to such issues as sectionalism.[48]
 Although the United States Constitution refers to ""Electors"" and ""electors"", neither the phrase ""Electoral College"" nor any other name is used to describe the electors collectively. It was not until the early 19th century that the name ""Electoral College"" came into general usage as the collective designation for the electors selected to cast votes for president and vice president. The phrase was first written into federal law in 1845, and today the term appears in 3 U.S.C. § 4, in the section heading and in the text as ""college of electors"".[49]
 Article II, Section 1, Clause 3 of the Constitution provided the original plan by which the electors voted for president. Under the original plan, each elector cast two votes for president; electors did not vote for vice president. Whoever received a majority of votes from the electors would become president, with the person receiving the second most votes becoming vice president.
 According to Stanley Chang, the original plan of the Electoral College was based upon several assumptions and anticipations of the Framers of the Constitution:[50]
 Election expert, William C. Kimberling, reflected on the original intent as follows:
 According to Supreme Court justice Robert H. Jackson, in a dissenting opinion, the original intention of the framers was that the electors would not feel bound to support any particular candidate, but would vote their conscience, free of external pressure.
 In support for his view, Justice Jackson cited Federalist No. 68:
 Philip J. VanFossen of Purdue University explains that the original purpose of the electors was not to reflect the will of the citizens, but rather to ""serve as a check on a public who might be easily misled.""[56]
 Randall Calvert, the Eagleton Professor of Public Affairs and Political Science at Washington University in St. Louis, stated, ""At the framing the more important consideration was that electors, expected to be more knowledgeable and responsible, would actually do the choosing.""[57]
 Constitutional expert Michael Signer explained that the electoral college was designed ""to provide a mechanism where intelligent, thoughtful and statesmanlike leaders could deliberate on the winner of the popular vote and, if necessary, choose another candidate who would not put Constitutional values and practices at risk.""[58] Robert Schlesinger, writing for U.S. News and World Report, similarly stated, ""The original conception of the Electoral College, in other words, was a body of men who could serve as a check on the uninformed mass electorate.""[59]
 In spite of Hamilton's assertion that electors were to be chosen by mass election, initially, state legislatures chose the electors in most of the states.[60] States progressively changed to selection by popular election. In 1824, there were six states in which electors were still legislatively appointed. By 1832, only South Carolina had not transitioned. Since 1864 (with the sole exception of newly admitted Colorado in 1876 for logistical reasons),  electors in every state have been chosen based on a popular election held on Election Day.[25] The popular election for electors means the president and vice president are in effect chosen through indirect election by the citizens.[61]
 The framers of the Constitution did not anticipate political parties.[62] Indeed George Washington's Farewell Address in 1796 included an urgent appeal to avert such parties. Neither did the framers anticipate candidates ""running"" for president. Within just a few years of the ratification of the Constitution, however, both phenomena became permanent features of the political landscape of the United States.[citation needed]
 The emergence of political parties and nationally coordinated election campaigns soon complicated matters in the elections of 1796 and 1800. In 1796, Federalist Party candidate John Adams won the presidential election. Finishing in second place was Democratic-Republican Party candidate Thomas Jefferson, the Federalists' opponent, who became the vice president. This resulted in the president and vice president being of different political parties.[citation needed]
 In 1800, the Democratic-Republican Party again nominated Jefferson for president and also again nominated Aaron Burr for vice president. After the electors voted, Jefferson and Burr were tied with one another with 73 electoral votes each. Since ballots did not distinguish between votes for president and votes for vice president, every ballot cast for Burr technically counted as a vote for him to become president, despite Jefferson clearly being his party's first choice. Lacking a clear winner by constitutional standards, the election had to be decided by the House of Representatives pursuant to the Constitution's contingency election provision.[citation needed]
 Having already lost the presidential contest, Federalist Party representatives in the lame duck House session seized upon the opportunity to embarrass their opposition by attempting to elect Burr over Jefferson. The House deadlocked for 35 ballots as neither candidate received the necessary majority vote of the state delegations in the House (The votes of nine states were needed for a conclusive election.). On the 36th ballot, Delaware's lone Representative, James A. Bayard, made it known that he intended to break the impasse for fear that failure to do so could endanger the future of the Union. Bayard and other Federalists from South Carolina, Maryland, and Vermont abstained, breaking the deadlock and giving Jefferson a majority.[63]
 Responding to the problems from those elections, Congress proposed on December 9, 1803, and three-fourths of the states ratified by June 15, 1804, the Twelfth Amendment. Starting with the 1804 election, the amendment requires electors to cast separate ballots for president and vice president, replacing the system outlined in Article II, Section 1, Clause 3.[citation needed]
 Some Founding Fathers hoped that each elector would be elected by the citizens of a district[64] and that elector was to be free to analyze and deliberate regarding who is best suited to be president.[65]
 
In Federalist No. 68 Alexander Hamilton described the Founding Fathers' view of how electors would be chosen:  However, when electors were pledged to vote for a specific candidate, the slate of electors chosen by the state were no longer free agents, independent thinkers, or deliberative representatives. They became, as Justice Robert H. Jackson wrote, ""voluntary party lackeys and intellectual non-entities.""[67] According to Hamilton, writing in 1788, the selection of the president should be ""made by men most capable of analyzing the qualities adapted to the station [of president].""[66]
 Hamilton stated that the electors were to analyze the list of potential presidents and select the best one. He also used the term ""deliberate."" In a 2020 opinion of the U.S. Supreme Court, the court additionally cited John Jay's view that the electors' choices would reflect ""discretion and discernment.""[68]
Reflecting on this original intention, a U.S. Senate report in 1826 critiqued the evolution of the system:
 In 1833, Supreme Court Justice Joseph Story detailed how badly from the framers' intention the Electoral Process had been ""subverted"":
 Story observed that if an elector does what the framers of the Constitution expected him to do, he would be considered immoral:
 Article II, Section 1, Clause 2 of the Constitution states:
 According to Hamilton, Madison and others, the original intent was that this would take place district by district.[71][72][73] The district plan was last carried out in Michigan in 1892.[74] For example, in Massachusetts in 1820, the rule stated ""the people shall vote by ballot, on which shall be designated who is voted for as an Elector for the district.""[75][76] In other words, the name of a candidate for president was not on the ballot. Instead, citizens voted for their local elector.
 Some state leaders began to adopt the strategy that the favorite partisan presidential candidate among the people in their state would have a much better chance if all of the electors selected by their state were sure to vote the same way—a ""general ticket"" of electors pledged to a party candidate.[77] Once one state took that strategy, the others felt compelled to follow suit in order to compete for the strongest influence on the election.[77]
 
When James Madison and Alexander Hamilton, two of the most important architects of the Electoral College, saw this strategy being taken by some states, they protested strongly.[71][72][78] Madison said that when the Constitution was written, all of its authors assumed individual electors would be elected in their districts, and it was inconceivable that a ""general ticket"" of electors dictated by a state would supplant the concept. Madison wrote to George Hay:  Each state government was free to have its own plan for selecting its electors, and the Constitution does not explicitly require states to popularly elect their electors. However, Federalist No. 68, insofar as it reflects the intent of the founders, states that Electors will be ""selected by their fellow-citizens from the general mass,"" and with regard to choosing Electors, ""they [the framers] have referred it in the first instance to an immediate act of the people of America."" Several methods for selecting electors are described below.
 Madison and Hamilton were so upset by the trend to ""general tickets"" that they advocated a constitutional amendment to prevent anything other than the district plan. Hamilton drafted an amendment to the Constitution mandating the district plan for selecting electors.[80][non-primary source needed] Hamilton's untimely death in a duel with Aaron Burr in 1804 prevented him from advancing his proposed reforms any further. ""[T]he election of Presidential Electors by districts, is an amendment very proper to be brought forward,"" Madison told George Hay in 1823.[79][non-primary source needed]
 Madison also drafted a constitutional amendment that would ensure the original ""district"" plan of the framers.[81][non-primary source needed] Jefferson agreed with Hamilton and Madison saying, ""all agree that an election by districts would be the best.""[74][non-primary source needed] Jefferson explained to Madison's correspondent why he was doubtful of the amendment being ratified: ""the states are now so numerous that I despair of ever seeing another amendment of the constitution.""[82][non-primary source needed]
 In 1789, the at-large popular vote, the winner-take-all method, began with Pennsylvania and Maryland. Massachusetts, Virginia and Delaware used a district plan by popular vote, and state legislatures chose in the five other states participating in the election (Connecticut, Georgia, New Hampshire, New Jersey, and South Carolina).[83][failed verification][non-primary source needed] New York, North Carolina and Rhode Island did not participate in the election. New York's legislature deadlocked over the method of choosing electors and abstained;[84] North Carolina and Rhode Island had not yet ratified the Constitution.[85]
 By 1800, Virginia and Rhode Island voted at large; Kentucky, Maryland, and North Carolina voted popularly by district; and eleven states voted by state legislature. Beginning in 1804 there was a definite trend towards the winner-take-all system for statewide popular vote.[86][non-primary source needed]
 By 1832, only South Carolina legislatively chose its electors, and it abandoned the method after 1860.[86][non-primary source needed] Maryland was the only state using a district plan, and from 1836 district plans fell out of use until the 20th century, though Michigan used a district plan for 1892 only. States using popular vote by district have included ten states from all regions of the country.[87][non-primary source needed]
 Since 1836, statewide winner-take-all popular voting for electors has been the almost universal practice.[88][non-primary source needed] Currently, Maine (since 1972) and Nebraska (since 1992) use a district plan, with two at-large electors assigned to support the winner of the statewide popular vote.[89][non-primary source needed]
 Since the mid-19th century, when all electors have been popularly chosen, the Electoral College has elected the candidate who received the most (though not necessarily a majority) popular votes nationwide, except in four elections: 1876, 1888, 2000, and 2016. A case has also been made that it happened in 1960. In 1824, when there were six states in which electors were legislatively appointed, rather than popularly elected, the true national popular vote is uncertain. The electors in 1824 failed to select a winning candidate, so the matter was decided by the House of Representatives.[90][better source needed]
 After the initial estimates agreed to in the original Constitution, Congressional and Electoral College reapportionment was made according to a decennial census to reflect population changes, modified by counting three-fifths of slaves. On this basis after the first census, the Electoral College still gave the free men of slave-owning states (but never slaves) extra power (Electors) based on a count of these disenfranchised people, in the choice of the U.S. president.[91]
 At the Constitutional Convention, the college composition, in theory, amounted to 49 votes for northern states (in the process of abolishing slavery) and 42 for slave-holding states (including Delaware). In the event, the first (i.e. 1788) presidential election lacked votes and electors for unratified Rhode Island (3) and North Carolina (7) and for New York (8) which reported too late; the Northern majority was 38 to 35.[92][non-primary source needed] For the next two decades, the three-fifths clause led to electors of free-soil Northern states numbering 8% and 11% more than Southern states. The latter had, in the compromise, relinquished counting two-fifths of their slaves and, after 1810, were outnumbered by 15.4% to 23.2%.[93]
 While House members for Southern states were boosted by an average of .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄3,[94] a free-soil majority in the college maintained over this early republic and Antebellum period.[95] Scholars conclude that the three-fifths clause had low impact on sectional proportions and factional strength, until denying the North a pronounced supermajority, as to the Northern, federal initiative to abolish slavery. The seats that the South gained from such ""slave bonus"" were quite evenly distributed between the parties. In the First Party System (1795–1823), the Jefferson Republicans gained 1.1 percent more adherents from the slave bonus, while the Federalists lost the same proportion. At the Second Party System (1823–1837) the emerging Jacksonians gained just 0.7% more seats, versus the opposition loss of 1.6%.[96]
 The three-fifths slave-count rule is associated with three or four outcomes, 1792–1860:
 The first ""Jeffersonian"" and ""Jacksonian"" victories were of great importance as they ushered in sustained party majorities of several Congresses and presidential party eras.[100]
 Besides the Constitution prohibiting Congress from regulating foreign or domestic slave trade before 1808 and a duty on states to return escaped ""persons held to service"",[101][non-primary source needed] legal scholar Akhil Reed Amar argues that the college was originally advocated by slaveholders as a bulwark to prop up slavery. In the Congressional apportionment provided in the text of the Constitution with its Three-Fifths Compromise estimate, ""Virginia emerged as the big winner [with] more than a quarter of the [votes] needed to win an election in the first round [for Washington's first presidential election in 1788]."" Following the 1790 United States census, the most populous state was Virginia, with 39.1% slaves, or 292,315 counted three-fifths, to yield a calculated number of 175,389 for congressional apportionment.[102][non-primary source needed]
 ""The ""free"" state of Pennsylvania had 10% more free persons than Virginia but got 20% fewer electoral votes.""[103] Pennsylvania split eight to seven for Jefferson, favoring Jefferson with a majority of 53% in a state with 0.1% slave population.[104][non-primary source needed] Historian Eric Foner agrees the Constitution's Three-Fifths Compromise gave protection to slavery.[105]
 Supporters of the College have provided many counterarguments to the charges that it defended slavery. Abraham Lincoln, the president who helped abolish slavery, won a College majority in 1860 despite winning 39.8% of citizen's votes.[106] This, however, was a clear plurality of a popular vote divided among four main candidates.
 Benner notes that Jefferson's first margin of victory would have been wider had the entire slave population been counted on a per capita basis.[107] He also notes that some of the most vociferous critics of a national popular vote at the constitutional convention were delegates from free states, including Gouverneur Morris of Pennsylvania, who declared that such a system would lead to a ""great evil of cabal and corruption,"" and Elbridge Gerry of Massachusetts, who called a national popular vote ""radically vicious"".[107]
 Delegates Oliver Ellsworth and Roger Sherman of Connecticut, a state which had adopted a gradual emancipation law three years earlier, also criticized a national popular vote.[107] Of like view was Charles Cotesworth Pinckney, a member of Adams' Federalist Party, presidential candidate in 1800. He hailed from South Carolina and was a slaveholder.[107] In 1824, Andrew Jackson, a slaveholder from Tennessee, was similarly defeated by John Quincy Adams, a strong critic of slavery.[107]
 Section 2 of the Fourteenth Amendment requires a state's representation in the House of Representatives to be reduced if the state denies the right to vote to any male citizen aged 21 or older, unless on the basis of ""participation in rebellion, or other crime"". The reduction is to be proportionate to such people denied a vote. This amendment refers to ""the right to vote at any election for the choice of electors for President and Vice President of the United States"" (among other elections). It is the only part of the Constitution currently alluding to electors being selected by popular vote.
 On May 8, 1866, during a debate on the Fourteenth Amendment, Thaddeus Stevens, the leader of the Republicans in the House of Representatives, delivered a speech on the amendment's intent. Regarding Section 2, he said:[108]
 Federal law (2 U.S.C. § 6) implements Section 2's mandate.
 Article II, Section 1, Clause 4 of the Constitution authorizes Congress to fix the day on which the electors shall vote, which must be the same day throughout the United States. And both Article II, Section 1, Clause 3 and the Twelfth Amendment that replaced it specifies that ""the President of the Senate shall, in the presence of the Senate and House of Representatives, open all the certificates and the votes shall then be counted.""
 In 1887, Congress passed the Electoral Count Act, now codified in Title 3, Chapter 1 of the United States Code, establishing specific procedures for the counting of the electoral votes. The law was passed in response to the disputed 1876 presidential election, in which several states submitted competing slates of electors. Among its provisions, the law established deadlines that the states must meet when selecting their electors, resolving disputes, and when they must cast their electoral votes.[23][110]
 From 1948 to 2022, the date fixed by Congress for the meeting of the Electoral College was ""on the first Monday after the second Wednesday in December next following their appointment"".[111] As of 2022, with the passing of ""S.4573 - Electoral Count Reform and Presidential Transition Improvement Act of 2022"", this was changed to be ""on the first Tuesday after the second Wednesday in December next following their appointment"".[22]
 Article II, Section 1, Clause 2, disqualifies all elected and appointed federal officials from being electors. The Office of the Federal Register is charged with administering the Electoral College.[112]
 After the vote, each state sends to Congress a certified record of their electoral votes, called the Certificate of Vote. These certificates are opened during a joint session of Congress, held on January 6[113][non-primary source needed] unless another date is specified by law, and read aloud by the incumbent vice president, acting in his capacity as president of the Senate. If any person receives an absolute majority of electoral votes, that person is declared the winner.[114][non-primary source needed] If there is a tie, or if no candidate for either or both offices receives an absolute majority, then choice falls to Congress in a procedure known as a contingent election.
 Even though the aggregate national popular vote is calculated by state officials, media organizations, and the Federal Election Commission, the people only indirectly elect the president and vice president. The president and vice president of the United States are elected by the Electoral College, which consists of 538 electors from the fifty states and Washington, D.C. Electors are selected state-by-state, as determined by the laws of each state. Since the 1824 election, the majority of states have chosen their presidential electors based on winner-take-all results in the statewide popular vote on Election Day.[115]
 As of 2020[update], Maine and Nebraska are exceptions as both use the congressional district method, Maine since 1972 and in Nebraska since 1992.[116] In most states, the popular vote ballots list the names of the presidential and vice presidential candidates (who run on a ticket). The slate of electors that represent the winning ticket will vote for those two offices. Electors are nominated by the party and, usually, they vote for the ticket to which are promised.[117][non-primary source needed]
 Many states require an elector to vote for the candidate to which the elector is pledged, but some ""faithless electors"" have voted for other candidates or refrained from voting. A candidate must receive an absolute majority of electoral votes (currently 270) to win the presidency or the vice presidency. If no candidate receives a majority in the election for president or vice president, the election is determined via a contingency procedure established by the Twelfth Amendment. In such a situation, the House chooses one of the top three presidential electoral vote winners as the president, while the Senate chooses one of the top two vice presidential electoral vote winners as vice president.
 A state's number of electors equals the number of representatives plus two electors for the senators the state has in the United States Congress.[118][119] Each state is entitled to at least one representative, the remaining number of representatives per state is apportioned based on their respective populations, determined every ten years by the United States census. In summary, 153 electors are divided equally among the states and the District of Columbia (3 each), and the remaining 385 are assigned by an apportionment among states.[120][non-primary source needed]
 Under the Twenty-third Amendment, Washington, D.C., is allocated as many electors as it would have if it were a state but no more electors than the least populous state. Because the least populous state (Wyoming, in the 2020 census) has three electors, D.C. cannot have more than three electors. Even if D.C. were a state, its population would entitle it to only three electors. Based on its population per electoral vote, D.C. has the third highest per capita Electoral College representation, after Wyoming and Vermont.[121][non-primary source needed]
 Currently, there are 538 electors, based on 435 representatives, 100 senators from the fifty states and three electors from Washington, D.C. The six states with the most electors are California (54), Texas (40), Florida (30), New York (28), Illinois (19), and Pennsylvania (19). The District of Columbia and the six least populous states—Alaska, Delaware, North Dakota, South Dakota, Vermont, and Wyoming—have three electors each.[122][non-primary source needed]
 The custom of allowing recognized political parties to select a slate of prospective electors developed early. In contemporary practice, each presidential-vice presidential ticket has an associated slate of potential electors. Then on Election Day, the voters select a ticket and thereby select the associated electors.[25]
 Candidates for elector are nominated by state chapters of nationally oriented political parties in the months prior to Election Day. In some states, the electors are nominated by voters in primaries the same way other presidential candidates are nominated. In some states, such as Oklahoma, Virginia, and North Carolina, electors are nominated in party conventions. In Pennsylvania, the campaign committee of each candidate names their respective electoral college candidates, an attempt to discourage faithless electors. Varying by state, electors may also be elected by state legislatures or appointed by the parties themselves.[123][unreliable fringe source?]
 Article II, Section 1, Clause 2 of the Constitution requires each state legislature to determine how electors for the state are to be chosen, but it disqualifies any person holding an Office of Trust or Profit under the United States, from being an elector.[124] Under Section 3 of the Fourteenth Amendment, any person who has sworn an oath to support the United States Constitution in order to hold either a state or federal office, and later rebelled against the United States directly or by giving assistance to those doing so, is disqualified from being an elector. Congress may remove this disqualification by a two-thirds vote in each house.
 All states currently choose presidential electors by popular vote. As of 2020, eight states[d] name the electors on the ballot. Mostly, the ""short ballot"" is used. The short ballot displays the names of the candidates for president and vice president, rather than the names of prospective electors.[125] Some states support voting for write-in candidates. Those that do may require pre-registration of write-in candidacy, with designation of electors being done at that time.[126][127] Since 1992, all but two states have followed the winner takes all method of allocating electors by which every person named on the slate for the ticket winning the statewide popular vote are named as presidential electors.[128][129]
 Maine and Nebraska are the only states not using this method.[116] In those states, the winner of the popular vote in each of its congressional districts is awarded one elector, and the winner of the statewide vote is then awarded the state's remaining two electors.[128][130] This method has been used in Maine since 1972 and in Nebraska since 1992. The Supreme Court previously upheld the power for a state to choose electors on the basis of congressional districts, holding that states possess plenary power to decide how electors are appointed in McPherson v. Blacker, 146 U.S. 1 (1892).
 The Tuesday following the first Monday in November has been fixed as the day for holding federal elections, called the Election Day.[131] After the election, each state prepares seven Certificates of Ascertainment, each listing the candidates for president and vice president, their pledged electors, and the total votes each candidacy received.[132][non-primary source needed] One certificate is sent, as soon after Election Day as practicable, to the National Archivist in Washington. The Certificates of Ascertainment are mandated to carry the state seal and the signature of the governor, or mayor of D.C.[133][non-primary source needed]
 The Electoral College never meets as one body. Electors meet in their respective state capitals (electors for the District of Columbia meet within the District) on the same day (set by Congress as the Tuesday after the second Wednesday in December) at which time they cast their electoral votes on separate ballots for president and vice president.[134][135][136][non-primary source needed][22]
 Although procedures in each state vary slightly, the electors generally follow a similar series of steps, and the Congress has constitutional authority to regulate the procedures the states follow.[citation needed] The meeting is opened by the election certification official—often that state's secretary of state or equivalent—who reads the certificate of ascertainment. This document sets forth who was chosen to cast the electoral votes. The attendance of the electors is taken and any vacancies are noted in writing. The next step is the selection of a president or chairman of the meeting, sometimes also with a vice chairman. The electors sometimes choose a secretary, often not an elector, to take the minutes of the meeting. In many states, political officials give short speeches at this point in the proceedings.[non-primary source needed]
 When the time for balloting arrives, the electors choose one or two people to act as tellers. Some states provide for the placing in nomination of a candidate to receive the electoral votes (the candidate for president of the political party of the electors). Each elector submits a written ballot with the name of a candidate for president. Ballot formats vary between the states: in New Jersey for example, the electors cast ballots by checking the name of the candidate on a pre-printed card. In North Carolina, the electors write the name of the candidate on a blank card. The tellers count the ballots and announce the result. The next step is the casting of the vote for vice president, which follows a similar pattern.[non-primary source needed]
 Under the Electoral Count Act (updated and codified in 3 U.S.C. § 9), each state's electors must complete six certificates of vote. Each Certificate of Vote (or Certificate of the Vote) must be signed by all of the electors and a certificate of ascertainment must be attached to each of the certificates of vote. Each Certificate of Vote must include the names of those who received an electoral vote for either the office of president or of vice president. The electors certify the Certificates of Vote, and copies of the certificates are then sent in the following fashion:[137][non-primary source needed]
 A staff member of the president of the Senate collects the certificates of vote as they arrive and prepares them for the joint session of the Congress. The certificates are arranged—unopened—in alphabetical order and placed in two special mahogany boxes. Alabama through Missouri (including the District of Columbia) are placed in one box and Montana through Wyoming are placed in the other box.[138]
 Before 1950, the Secretary of State's office oversaw the certifications. Since then, the Office of Federal Register in the Archivist's office reviews them to make sure the documents sent to the archive and Congress match, and that all formalities have been followed, sometimes requiring states to correct the documents.[112]
 An elector votes for each office, but at least one of these votes (president or vice president) must be cast for a person who is not a resident of the same state as that elector.[139] A ""faithless elector"" is one who does not cast an electoral vote for the candidate of the party for whom that elector pledged to vote. Faithless electors are comparatively rare because electors are generally chosen among those who are already personally committed to a party and party's candidate.[140]
 Thirty-three states plus the District of Columbia have laws against faithless electors,[141] which were first enforced after the 2016 election, where ten electors voted or attempted to vote contrary to their pledges. Faithless electors have never changed the outcome of a U.S. election for president. Altogether, 23,529 electors have taken part in the Electoral College as of the 2016 election. Only 165 electors have cast votes for someone other than their party's nominee. Of that group, 71 did so because the nominee had died – 63 Democratic Party electors in 1872, when presidential nominee Horace Greeley died; and eight Republican Party electors in 1912, when vice presidential nominee James S. Sherman died.[142]
 While faithless electors have never changed the outcome of any presidential election, there are two occasions where the vice presidential election has been influenced by faithless electors:
 Some constitutional scholars argued that state restrictions would be struck down if challenged based on Article II and the Twelfth Amendment.[145] However, the United States Supreme Court has consistently ruled that state restrictions are allowed under the Constitution. In Ray v. Blair, 343 U.S. 214 (1952), the court ruled in favor of state laws requiring electors to pledge to vote for the winning candidate, as well as removing electors who refuse to pledge. As stated in the ruling, electors are acting as a functionary of the state, not the federal government. In Chiafalo v. Washington, 591 U.S. ___ (2020), and a related case, the court held that electors must vote in accord with their state's laws.[146][147] Faithless electors also may face censure from their political party, as they are usually chosen based on their perceived party loyalty.[148]
 The Twelfth Amendment mandates Congress assemble in joint session to count the electoral votes and declare the winners of the election.[149] The session is ordinarily required to take place on January 6 in the calendar year immediately following the meetings of the presidential electors.[150] Since the Twentieth Amendment, the newly elected joint Congress declares the winner of the election. All elections before 1936 were determined by the outgoing House.
 The Office of the Federal Register is charged with administering the Electoral College.[112] The meeting is held at 1 p.m. in the chamber of the U.S. House of Representatives.[150] The sitting vice president is expected to preside, but in several cases the president pro tempore of the Senate has chaired the proceedings. The vice president and the speaker of the House sit at the podium, with the vice president sitting to the right of the speaker of the House. Senate pages bring in two mahogany boxes containing each state's certified vote and place them on tables in front of the senators and representatives. Each house appoints two tellers to count the vote, normally one member of each political party. Relevant portions of the certificate of vote are read for each state, in alphabetical order.
 Before an amendment to the law in 2022, members of Congress could object to any state's vote count, provided objection is presented in writing and is signed by at least one member of each house of Congress. In 2022, the number of members required to make an objection was raised to one-fifth of each house. An appropriately made objection is followed by the suspension of the joint session and by separate debates and votes in each house of Congress. After both houses deliberate on the objection, the joint session is resumed.
 A state's certificate of vote can be rejected only if both houses of Congress vote to accept the objection via a simple majority,[151] meaning the votes from the state in question are not counted. Individual votes can also be rejected, and are also not counted.
 If there are no objections or all objections are overruled, the presiding officer simply includes a state's votes, as declared in the certificate of vote, in the official tally.
 After the certificates from all states are read and the respective votes are counted, the presiding officer simply announces the final state of the vote. This announcement concludes the joint session and formalizes the recognition of the president-elect and of the vice president-elect. The senators then depart from the House chamber. The final tally is printed in the Senate and House journals.
 Objections to the electoral vote count are rarely raised, although it has occurred a few times.
 If no candidate for president receives an absolute majority of the electoral votes (since 1964, 270 of the 538 electoral votes), then the Twelfth Amendment requires the House of Representatives to go into session immediately to choose a president. In this event, the House of Representatives is limited to choosing from among the three candidates who received the most electoral votes for president. Each state delegation votes en bloc—each delegation having a single vote. The District of Columbia does not get to vote.
 A candidate must receive an absolute majority of state delegation votes (i.e., from 1959, which is the last time a new state was admitted to the union, a minimum of 26 votes) in order for that candidate to become the president-elect. Delegations from at least two thirds of all the states must be present for voting to take place. The House continues balloting until it elects a president.
 The House of Representatives has been required to choose the president only twice: in 1801 under Article II, Section 1, Clause 3; and in 1825 under the Twelfth Amendment.
 If no candidate for vice president receives an absolute majority of electoral votes, then the Senate must go into session to choose a vice president. The Senate is limited to choosing from the two candidates who received the most electoral votes for vice president. Normally this would mean two candidates, one less than the number of candidates available in the House vote.
 However, the text is written in such a way that all candidates with the most and second-most electoral votes are eligible for the Senate election—this number could theoretically be larger than two. The Senate votes in the normal manner in this case (i.e., ballots are individually cast by each senator, not by state delegations). Two-thirds of the senators must be present for voting to take place.
 The Twelfth Amendment states a ""majority of the whole number"" of senators, currently 51 of 100, is necessary for election.[157] The language requiring an absolute majority of Senate votes precludes the sitting vice president from breaking any tie that might occur,[158] although some academics and journalists have speculated to the contrary.[159]
 The only time the Senate chose the vice president was in 1837. In that instance, the Senate adopted an alphabetical roll call and voting aloud. The rules further stated, ""[I]f a majority of the number of senators shall vote for either the said Richard M. Johnson or Francis Granger, he shall be declared by the presiding officer of the Senate constitutionally elected Vice President of the United States""; the Senate chose Johnson.[160]
 Section 3 of the Twentieth Amendment specifies that if the House of Representatives has not chosen a president-elect in time for the inauguration (noon EST on January 20), then the vice president-elect becomes acting president until the House selects a president. Section 3 also specifies that Congress may statutorily provide for who will be acting president if there is neither a president-elect nor a vice president-elect in time for the inauguration. Under the Presidential Succession Act of 1947, the Speaker of the House would become acting president until either the House selects a president or the Senate selects a vice president. Neither of these situations has ever arisen to this day.
 In Federalist No. 68, Alexander Hamilton argued that one concern that led the Constitutional Convention to create the Electoral College was to ensure peaceful transitions of power and continuity of government during transitions between presidential administrations.[161][e] While recognizing that the question had not been presented in the case, the U.S. Supreme Court stated in the majority opinion in Chiafalo v. Washington (2020) that ""nothing in this opinion should be taken to permit the States to bind electors to a deceased candidate"" after noting that more than one-third of the cumulative faithless elector votes in U.S. presidential elections history were cast during the 1872 presidential election when Liberal Republican Party and Democratic Party nominee Horace Greeley died after the polls were held and vote tabulations were completed by the states but before the Electoral College cast its ballots, and acknowledging the petitioners concern about the potential turmoil that the death of a presidential candidate between Election Day and the Electoral College meetings could cause.[162][163]
 In 1872, Greeley carried the popular vote in 6 states (Georgia, Kentucky, Maryland, Missouri, Tennessee, and Texas) and had 66 electoral votes pledged to him. After his death on November 29, 1872, 63 of the electors pledged to him voted faithlessly, while 3 votes (from Georgia) that remained pledged to him were rejected at the Electoral College vote count on February 12, 1873, on the grounds that he had died.[164][165] Greeley's running mate, B. Gratz Brown, still received the 3 electoral votes from Georgia for vice president that were rejected for Greeley. This brought Brown's number of electoral votes for vice president to 47 since he still received all 28 electoral votes from Maryland, Tennessee, and Texas, and 16 other electoral votes from Georgia, Kentucky, and Missouri in total. The other 19 electors from the latter states voted faithlessly for vice president.[166]
 During the presidential transition following the 1860 presidential election, Abraham Lincoln had to arrive in Washington, D.C. in disguise and on an altered train schedule after the Pinkerton National Detective Agency found evidence that suggested a secessionist plot to assassinate Lincoln would be attempted in Baltimore.[167][168] During the presidential transition following the 1928 presidential election, an Argentine anarchist group plotted to assassinate Herbert Hoover while Hoover was traveling through Central and South America and crossing the Andes from Chile by train. The plotters were arrested before the attempt was made.[169][170]
 During the presidential transition following the 1932 presidential election, Giuseppe Zangara attempted to assassinate Franklin D. Roosevelt by gunshot while Roosevelt was giving an impromptu speech in a car in Miami, but instead killed Chicago Mayor Anton Cermak, who was a passenger in the car, and wounded 5 bystanders.[171][172] During the presidential transition following the 1960 presidential election, Richard Paul Pavlick plotted to assassinate John F. Kennedy while Kennedy was vacationing in Palm Beach, Florida, by detonating a dynamite-laden car where Kennedy was staying. Pavlick delayed his attempt and was arrested and committed to a mental hospital.[173][174][175][176]
 During the presidential transition following the 2008 presidential election, Barack Obama was targeted in separate security incidents by an assassination plot and a death threat,[177][178] after an assassination plot in Denver during the 2008 Democratic National Convention and an assassination plot in Tennessee during the election were prevented.[179][180]
 During the presidential transition following the 2020 presidential election, as a result of former president Donald Trump's false insistence that he had won the election, the General Services Administration did not declare Biden the winner until November 23.[181] The subsequent attack on the United States Capitol on January 6 caused delays in the counting of electoral votes to certify Joe Biden's victory in the 2020 election, but was ultimately unsuccessful in preventing the count from occurring.[182]
 Ratified in 1933, Section 3 of the 20th Amendment requires that if a president-elect dies before Inauguration Day, that the vice president-elect becomes the president.[183][184] Akhil Amar has noted that the explicit text of the 20th Amendment does not specify when the candidates of the winning presidential ticket officially become the president-elect and vice president-elect, and that the text of Article II, Section I and the 12th Amendment suggests that candidates for president and vice president are only formally elected upon the Electoral College vote count.[185] Conversely, a 2020 report issued by the Congressional Research Service (CRS), stated that the balance of scholarly opinion has concluded that the winning presidential ticket is formally elected as soon as the majority of the electoral votes they receive are cast, according to the 1932 House committee report on the 20th Amendment.[183]
 If a vacancy on a presidential ticket occurs before Election Day—as in 1912 when Republican nominee for Vice President James S. Sherman died less than a week before the election and was replaced by Nicholas Murray Butler at the Electoral College meetings, and in 1972 when Democratic nominee for Vice President Thomas Eagleton withdrew his nomination less than three weeks after the Democratic National Convention and was replaced by Sargent Shriver—the internal rules of the political parties apply for filling vacancies.[186] If a vacancy on a presidential ticket occurs between Election Day and the Electoral College meetings, the 2020 CRS report notes that most legal commentators have suggested that political parties would still follow their internal rules for filling the vacancies.[187] However, in 1872, the Democratic National Committee did not meet to name a replacement for Horace Greeley,[164] and the 2020 CRS report notes that presidential electors may argue that they are permitted to vote faithlessly if a vacancy occurs between Election Day and the Electoral College meetings since they were pledged to vote for a specific candidate.[187]
 Under the Presidential Succession Clause of Article II, Section I, Congress is delegated the power to ""by Law provide for the Case of Removal, Death, Resignation or Inability, both of the President and Vice President, declaring what Officer shall then act as President, and such Officer shall act accordingly, until the Disability be removed, or a President shall be elected.""[188][f][g] Pursuant to the Presidential Succession Clause, the 2nd United States Congress passed the Presidential Succession Act of 1792 that required a special election by the Electoral College in the case of a dual vacancy in the presidency and vice presidency.[192][193] Despite vacancies in the Vice Presidency from 1792 to 1886,[h] the special election requirement would be repealed with the rest of the Presidential Succession Act of 1792 by the 49th United States Congress in passing the Presidential Succession Act of 1886.[196][197]
 In a special message to the 80th United States Congress calling for revisions to the Presidential Succession Act of 1886, President Harry S. Truman proposed restoring special elections for dual vacancies in the Presidency and Vice Presidency. While most of Truman's proposal was included in the final version of the Presidential Succession Act of 1947, the restoration of special elections for dual vacancies was not.[198][199] Along with six other recommendations related to presidential succession,[200] the Continuity of Government Commission recommended restoring special elections for president in the event of a dual vacancy in the presidency and vice presidency due to a catastrophic terrorist attack or nuclear strike, in part because all members of the presidential line of succession live and work in Washington, D.C.[201]
 Under the 12th Amendment, presidential electors are still required to meet and cast their ballots for president and vice president within their respective states.[202] The CRS noted in a separate 2020 report that members of the presidential line of succession, after the vice president, only become an acting president under the Presidential Succession Clause and Section 3 of the 20th Amendment, rather than fully succeeding to the presidency.[203]
 Source: Presidential Elections 1789–2000 at Psephos (Adam Carr's Election Archive)
Note: In 1788, 1792, 1796, and 1800, each elector cast two votes for president.
 Before the advent of the ""short ballot"" in the early 20th century (as described in Selection process) the most common means of electing the presidential electors was through the general ticket. The general ticket is quite similar to the current system and is often confused with it. In the general ticket, voters cast ballots for individuals running for presidential elector. In the short ballot, voters cast ballots for an entire slate of electors.
 In the general ticket, the state canvass would report the number of votes cast for each candidate for elector, a complicated process in states like New York with multiple positions to fill. Both the general ticket and the short ballot are often considered at-large or winner-takes-all voting. The short ballot was adopted by the various states at different times. It was adopted for use by North Carolina and Ohio in 1932. Alabama was still using the general ticket as late as 1960 and was one of the last states to switch to the short ballot.
 The question of the extent to which state constitutions may constrain the legislature's choice of a method of choosing electors has been touched on in two U.S. Supreme Court cases. In McPherson v. Blacker, 146 U.S. 1 (1892), the Court cited Article II, Section 1, Clause 2 which states that a state's electors are selected ""in such manner as the legislature thereof may direct"" and wrote these words ""operat[e] as a limitation upon the state in respect of any attempt to circumscribe the legislative power"".
 In Bush v. Palm Beach County Canvassing Board, 531 U.S. 70 (2000), a Florida Supreme Court decision was vacated (not reversed) based on McPherson. On the other hand, three dissenting justices in Bush v. Gore, 531 U.S. 98 (2000), wrote: ""[N]othing in Article II of the Federal Constitution frees the state legislature from the constraints in the State Constitution that created it.""[207]
 In the earliest presidential elections, state legislative choice was the most common method of choosing electors. A majority of the state legislatures selected presidential electors in both 1792 (9 of 15) and 1800 (10 of 16), and half of them did so in 1812.[208] Even in the 1824 election, a quarter of state legislatures (6 of 24) chose electors. In that election, Andrew Jackson lost in spite of having a plurality of both the popular vote and the number of electoral votes representing them.[209] Yet, as six states did not hold a popular election for their electoral votes, the full expression of the popular vote nationally cannot be known.[209]
 Some state legislatures simply chose electors. Other states used a hybrid method in which state legislatures chose from a group of electors elected by popular vote.[210] By 1828, with the rise of Jacksonian democracy, only Delaware and South Carolina used legislative choice.[209] Delaware ended its practice the following election (1832). South Carolina continued using the method until it seceded from the Union in December 1860.[209] South Carolina used the popular vote for the first time in the 1868 election.[211]
 Excluding South Carolina, legislative appointment was used in only four situations after 1832:
 Legislative appointment was brandished as a possibility in the 2000 election. Had the recount continued, the Florida legislature was prepared to appoint the Republican slate of electors to avoid missing the federal safe-harbor deadline for choosing electors.[213]
 The Constitution gives each state legislature the power to decide how its state's electors are chosen[209] and it can be easier and cheaper for a state legislature to simply appoint a slate of electors than to create a legislative framework for holding elections to determine the electors. As noted above, the two situations in which legislative choice has been used since the Civil War have both been because there was not enough time or money to prepare for an election. However, appointment by state legislature can have negative consequences: bicameral legislatures can deadlock more easily than the electorate. This is precisely what happened to New York in 1789 when the legislature failed to appoint any electors.[214]
 Another method used early in U.S. history was to divide the state into electoral districts. By this method, voters in each district would cast their ballots for the electors they supported and the winner in each district would become the elector. This was similar to how states are currently separated into congressional districts. The difference stems from the fact that every state always had two more electoral districts than congressional districts. As with congressional districts, this method is vulnerable to gerrymandering.
 There are two versions of the congressional district method: one has been implemented in Maine and Nebraska; another was used in New York in 1828 and proposed for use in Virginia. Under the implemented method, electors are awarded the way seats in Congress are awarded. One electoral vote goes per the plurality of the popular votes of each congressional district (for the U.S. House Of Representatives), and two per the statewide popular vote. This may result in greater proportionality. But it can give results similar to the winner-takes-all states, as in 1992, when George H. W. Bush won all five of Nebraska's electoral votes with a clear plurality on 47% of the vote; in a truly proportional system, he would have received three and Bill Clinton and Ross Perot each would have received one.[215]
 In 2013, the Virginia proposal was tabled. Like the other congressional district methods, this would have distributed the electoral votes based on the popular vote winner within each of Virginia's 11 congressional districts; the two statewide electoral votes would be awarded based on which candidate won the most congressional districts.[216] A similar method was used in New York in 1828: the two at large electors were elected by the electors selected in districts.
 A congressional district method is more likely to arise than other alternatives to the winner-takes-whole-state method, in view of the main two parties' resistance to scrap first-past-the-post. State legislation is sufficient to use this method.[217][non-primary source needed] Advocates of the method believe the system encourages higher voter turnout or incentivizes candidates, to visit and appeal to some states deemed safe, overall, for one party.[218]
 Winner-take-all systems ignore thousands of votes. In Democratic California there are Republican districts, in Republican Texas there are Democratic districts. Because candidates have an incentive to campaign in competitive districts, with a district plan, candidates have an incentive to actively campaign in over thirty states versus about seven ""swing"" states.[219][220] Opponents of the system argue that candidates might only spend time in certain battleground districts instead of the entire state and cases of gerrymandering could become exacerbated as political parties attempt to draw as many safe districts as they can.[221]
 Unlike simple congressional district comparisons, the district plan popular vote bonus in the 2008 election would have given Obama 56% of the Electoral College versus the 68% he did win; it ""would have more closely approximated the percentage of the popular vote won [53%]"".[222] However, the district plan would have given Obama 49% of the Electoral College in 2012, and would have given Romney a win in the Electoral College even though Obama won the popular vote by nearly 4% (51.1–47.2) over Romney.[223]
 Of the 44 multi-district states whose 517 electoral votes are amenable to the method, only Maine (4 EV) and Nebraska (5 EV) apply it.[224][225] Maine began using the congressional district method in the election of 1972. Nebraska has used the congressional district method since the election of 1992.[226][227] Michigan used the system for the 1892 presidential election,[215][228][229] and several other states used various forms of the district plan before 1840: Virginia, Delaware, Maryland, Kentucky, North Carolina, Massachusetts, Illinois, Maine, Missouri, and New York.[230]
 The congressional district method allows a state the chance to split its electoral votes between multiple candidates. Prior to 2008, Nebraska had never split its electoral votes, while Maine had only done so once under its previous district plan in the 1828 election.[215][231][232] Nebraska split its electoral votes for the first time in 2008, giving John McCain its statewide electors and those of two congressional districts, while Barack Obama won the electoral vote of Nebraska's 2nd congressional district, centered on the state's largest city, Omaha.[233] Following the 2008 split, some Nebraska Republicans made efforts to discard the congressional district method and return to the winner-takes-all system.[234] In January 2010, a bill was introduced in the Nebraska legislature to revert to a winner-take-all system;[235] the bill died in committee in March 2011.[236] Republicans had passed bills in 1995 and 1997 to do the same, which were vetoed by Democratic Governor Ben Nelson.[234]
 More recently, Maine split its electoral votes for the first time under the congressional district method in 2016. Hillary Clinton won its two statewide electors and its 1st congressional district, which covers the state's southwestern coastal region and its largest city of Portland, while Donald Trump won the electoral vote of Maine's 2nd congressional district, which takes in the remainder of the state and is much larger by area. In the 2020 election, both Nebraska and Maine split their electoral votes, following the same pattern of congressional district differences that were seen in 2008 and 2016 respectively: Nebraska's 2nd congressional district voted for Democrat Joe Biden while the remainder of the state voted for Republican Donald Trump; and Maine's 2nd congressional district voted for Trump while the remainder of the state voted for Biden.[237]
 In 2010, Republicans in Pennsylvania, who controlled both houses of the legislature as well as the governorship, put forward a plan to change the state's winner-takes-all system to a congressional district method system. Pennsylvania had voted for the Democratic candidate in the five previous presidential elections, so this was seen an attempt to take away Democratic electoral votes. Democrat Barack Obama won Pennsylvania in 2008 with 55% of its vote. The district plan would have awarded him 11 of its 21 electoral votes, a 52.4% which was much closer to the popular vote percentage.[238][239] The plan later lost support.[240] Other Republicans, including Michigan state representative Pete Lund,[241] RNC Chairman Reince Priebus, and Wisconsin Governor Scott Walker, have floated similar ideas.[242][243]
 In a proportional system, electors would be selected in proportion to the votes cast for their candidate or party, rather than being selected by the statewide plurality vote.[244]
 Gary Bugh's research of congressional debates over proposed constitutional amendments to abolish the Electoral College reveals reform opponents have often appealed to tradition and the preference for indirect elections, whereas reform advocates often champion a more egalitarian one person, one vote system.[246] Electoral colleges have been scrapped by all other democracies around the world in favor of direct elections for an executive president.[247][15]
 Critics argue that the Electoral College is less democratic than a national direct popular vote and is subject to manipulation because of faithless electors;[12][13] that the system is antithetical to a democracy that strives for a standard of ""one person, one vote"";[9] and there can be elections where one candidate wins the national popular vote but another wins the electoral vote, as in the 2000 and 2016 elections.[11] Individual citizens in less populated states with 5% of the Electoral College have proportionately more voting power than those in more populous states,[248] and candidates can win by focusing on just a few ""swing states"".[14][249]
 21st century polling data shows that a majority of Americans consistently favor having a direct popular vote for presidential elections. The popularity of the Electoral College has hovered between 35% and 44%.[245][250][k]
 Opponents of the Electoral College claim such outcomes do not logically follow the normative concept of how a democratic system should function. One view is the Electoral College violates the principle of political equality, since presidential elections are not decided by the one-person one-vote principle.[251]
 While many assume the national popular vote observed under the Electoral College system would reflect the popular vote observed under a National Popular Vote system, supporters contend that is not necessarily the case as each electoral institution produces different incentives for, and strategy choices by, presidential campaigns.[252][253]
 The elections of 1876, 1888, 2000, and 2016 produced an Electoral College winner who did not receive at least a plurality of the nationwide popular vote.[251] In 1824, there were six states in which electors were legislatively appointed, rather than popularly elected, so it is uncertain what the national popular vote would have been if all presidential electors had been popularly elected. When no presidential candidate received a majority of electoral votes in 1824, the election was decided by the House of Representatives and so could be considered distinct from the latter four elections in which all of the states had popular selection of electors.[254] The true national popular vote was also uncertain in the 1960 election, and the plurality for the winner depends on how votes for Alabama electors are allocated.[255]
 Elections where the popular vote and electoral college results differed
 *These popular vote tallies are partial because several of the states still used their legislature to choose electors not a popular vote. In both elections a tied electoral college threw the contest over to Congress to decide.
 The Electoral College encourages political campaigners to focus on a few so-called swing states while ignoring the rest of the country. Populous states in which pre-election poll results show no clear favorite are inundated with campaign visits, saturation television advertising, get-out-the-vote efforts by party organizers, and debates, while four out of five voters in the national election are ""absolutely ignored"", according to one assessment.[257] Since most states use a winner-takes-all arrangement in which the candidate with the most votes in that state receives all of the state's electoral votes, there is a clear incentive to focus almost exclusively on only a few key undecided states.[251]
 Each state gets a minimum of three electoral votes, regardless of population, which has increasingly given low-population states more electors per voter (or more voting power).[258][259] For example, an electoral vote represents nearly four times as many people in California as in Wyoming.[258][260] On average, voters in the ten least populated states have 2.5 more electors per person compared with voters in the ten most populous states.[258]
 In 1968, John F. Banzhaf III developed the Banzhaf power index (BPI) which argued that a voter in the state of New York had, on average, 3.3 times as much voting power in presidential elections as the average voter outside New York.[261] Mark Livingston used a similar method and estimated that individual voters in the largest state, based on the 1990 census, had 3.3 times more individual power to choose a president than voters of Montana.[262][better source needed]
 However, others argue that Banzhaf's method ignores the demographic makeup of the states and treats votes like independent coin-flips. Critics of Banzhaf's method say empirically based models used to analyze the Electoral College have consistently found that sparsely populated states benefit from having their resident's votes count for more than the votes of those residing in the more populous states.[263]
 Except in closely fought swing states, voter turnout does not affect the election results due to entrenched political party domination in most states. The Electoral College decreases the advantage a political party or campaign might gain for encouraging voters to turn out, except in those swing states.[264] If the presidential election were decided by a national popular vote, in contrast, campaigns and parties would have a strong incentive to work to increase turnout everywhere.[265]
 Individuals would similarly have a stronger incentive to persuade their friends and neighbors to turn out to vote. The differences in turnout between swing states and non-swing states under the current electoral college system suggest that replacing the Electoral College with direct election by popular vote would likely increase turnout and participation significantly.[264]
 According to this criticism, the electoral college reduces elections to a mere count of electors for a particular state, and, as a result, it obscures any voting problems within a particular state. For example, if a particular state blocks some groups from voting, perhaps by voter suppression methods such as imposing reading tests, poll taxes, registration requirements, or legally disfranchising specific groups (like women or people of color), then voting inside that state would be reduced, but as the state's electoral count would be the same, disenfranchisement has no effect on its overall electoral power. Critics contend that such disenfranchisement is not penalized by the Electoral College.
 A related argument is the Electoral College may have a dampening effect on voter turnout: there is no incentive for states to reach out to more of its citizens to include them in elections because the state's electoral count remains fixed in any event. According to this view, if elections were by popular vote, then states would be motivated to include more citizens in elections since the state would then have more political clout nationally. Critics contend the electoral college system insulates states from negative publicity as well as possible federal penalties for disenfranchising subgroups of citizens.
 Legal scholars Akhil Amar and Vikram Amar have argued that the original Electoral College compromise was enacted partially because it enabled Southern states to disenfranchise their slave populations.[266] It permitted Southern states to disfranchise large numbers of slaves while allowing these states to maintain political clout and prevent Northern dominance within the federation by using the Three-Fifths Compromise. They noted that James Madison believed the question of counting slaves had presented a serious challenge, but that ""the substitution of electors obviated this difficulty and seemed on the whole to be liable to the fewest objections.""[267] Akhil and Vikram Amar added:
 After the Thirteenth Amendment abolished slavery, white voters in Southern states benefited from elimination of the Three-Fifths Compromise because with all former slaves counted as one person, instead of 3/5, Southern states increased their share of electors in the Electoral College. Southern states also enacted laws that restricted access to voting by former slaves, thereby increasing the electoral weight of votes by southern whites.[268]
 Minorities tend to be disproportionately located in noncompetitive states, reducing their impact on the overall election and over-representing white voters who have tended to live in the swing states that decide elections.[269][270]
 Roughly four million Americans in Puerto Rico, the Northern Mariana Islands, the U.S. Virgin Islands, American Samoa, and Guam, do not have a vote in presidential elections.[29][258] Only U.S. states (per Article II, Section 1, Clause 2) and Washington, D.C. (per the Twenty-third Amendment) are entitled to electors. Various scholars consequently conclude that the U.S. national-electoral process is not fully democratic.[271][272] Guam has held non-binding straw polls for president since the 1980s to draw attention to this fact.[273][274] The Democratic and Republican parties, as well as other third parties, have, however, made it possible for people in U.S. territories to vote in party presidential primaries.[275][276]
 In practice, the winner-take-all manner of allocating a state's electors generally decreases the importance of minor parties.[277]
 For many years early in the nation's history, up until the Jacksonian Era (1830s), many states appointed their electors by a vote of the state legislature, and proponents argue that, in the end, the election of the president must still come down to the decisions of each state, or the federal nature of the United States will give way to a single massive, centralized government, to the detriment of the States.[278]
 In his 2007 book A More Perfect Constitution, Professor Larry Sabato preferred allocating the electoral college (and Senate seats) in stricter proportion to population while keeping the Electoral College for the benefit of lightly populated swing states and to strengthen the role of the states in federalism.[279][278]
 Willamette University College of Law professor Norman R. Williams has argued that the Constitutional Convention delegates chose the Electoral College to choose the president largely in reaction to the experience during the Confederation period where state governors were often chosen by state legislatures and wanting the new federal government to have an executive branch that was effectively independent of the legislative branch.[280] For example, Alexander Hamilton argued that the Electoral College would prevent, sinister bias, foreign interference and domestic intrigue in presidential elections by not permitting members of Congress or any other officer of the United States to serve as electors.[281]
 More resolutions have been submitted to amend the U.S. Electoral College mechanism than any other part of the constitution.[4] Since 1800, over 700 proposals to reform or eliminate the system have been introduced in Congress. Proponents of these proposals argued that the electoral college system does not provide for direct democratic election, affords less-populous states an advantage, and allows a candidate to win the presidency without winning the most votes. None of these proposals has received the approval of two thirds of Congress and three fourths of the states required to amend the Constitution.[282] Ziblatt and Levitsky argue that America has by far the most difficult constitution to amend, which is why reform efforts have stalled in America.[283]
 The closest the United States has come to abolishing the Electoral College occurred during the 91st Congress (1969–1971).[284] The 1968 election resulted in Richard Nixon receiving 301 electoral votes (56% of electors), Hubert Humphrey 191 (35.5%), and George Wallace 46 (8.5%) with 13.5% of the popular vote. However, Nixon had received only 511,944 more popular votes than Humphrey, 43.5% to 42.9%, less than 1% of the national total.[285][non-primary source needed]
 Representative Emanuel Celler (D–New York), chairman of the House Judiciary Committee, responded to public concerns over the disparity between the popular vote and electoral vote by introducing House Joint Resolution 681, a proposed Constitutional amendment that would have replaced the Electoral College with a simpler plurality system based on the national popular vote. With this system, the pair of candidates (running for president and vice-president) who had received the highest number of votes would win the presidency and vice presidency provided they won at least 40% of the national popular vote. If no pair received 40% of the popular vote, a runoff election would be held in which the choice of president and vice president would be made from the two pairs of persons who had received the highest number of votes in the first election.[286]
 On April 29, 1969, the House Judiciary Committee voted 28 to 6 to approve the proposal.[287] Debate on the proposal before the full House of Representatives ended on September 11, 1969[288] and was eventually passed with bipartisan support on September 18, 1969, by a vote of 339 to 70.[289]
 On September 30, 1969, President Nixon gave his endorsement for adoption of the proposal, encouraging the Senate to pass its version of the proposal, which had been sponsored as Senate Joint Resolution 1 by Senator Birch Bayh (D–Indiana).[290]
 On October 8, 1969, the New York Times reported that 30 state legislatures were ""either certain or likely to approve a constitutional amendment embodying the direct election plan if it passes its final Congressional test in the Senate."" Ratification of 38 state legislatures would have been needed for adoption. The paper also reported that six other states had yet to state a preference, six were leaning toward opposition, and eight were solidly opposed.[291]
 On August 14, 1970, the Senate Judiciary Committee sent its report advocating passage of the proposal to the full Senate. The Judiciary Committee had approved the proposal by a vote of 11 to 6. The six members who opposed the plan, Democratic senators James Eastland of Mississippi, John Little McClellan of Arkansas, and Sam Ervin of North Carolina, along with Republican senators Roman Hruska of Nebraska, Hiram Fong of Hawaii, and Strom Thurmond of South Carolina, all argued that although the present system had potential loopholes, it had worked well throughout the years. Senator Bayh indicated that supporters of the measure were about a dozen votes shy from the 67 needed for the proposal to pass the full Senate.[292] He called upon President Nixon to attempt to persuade undecided Republican senators to support the proposal.[293] However, Nixon, while not reneging on his previous endorsement, chose not to make any further personal appeals to back the proposal.[294]
 On September 8, 1970, the Senate commenced openly debating the proposal,[295] and the proposal was quickly filibustered. The lead objectors to the proposal were mostly Southern senators and conservatives from small states, both Democrats and Republicans, who argued that abolishing the Electoral College would reduce their states' political influence.[294] On September 17, 1970, a motion for cloture, which would have ended the filibuster, received 54 votes to 36 for cloture,[294] failing to receive the then-required two-thirds majority of senators voting.[296] [non-primary source needed] A second motion for cloture on September 29, 1970, also failed, by 53 to 34. Thereafter, the Senate majority leader, Mike Mansfield of Montana, moved to lay the proposal aside so the Senate could attend to other business.[297] However, the proposal was never considered again and died when the 91st Congress ended on January 3, 1971.
 
On March 22, 1977, President Jimmy Carter wrote a letter of reform to Congress that also included his expression of abolishing the Electoral College. The letter read in part: President Carter's proposed program for the reform of the Electoral College was very liberal for a modern president during this time, and in some aspects of the package, it went beyond original expectations.[299]
Newspapers like The New York Times saw President Carter's proposal at that time as ""a modest surprise"" because of the indication of Carter that he would be interested in only eliminating the electors but retaining the electoral vote system in a modified form.[299]
 Newspaper reaction to Carter's proposal ranged from some editorials praising the proposal to other editorials, like that in the Chicago Tribune, criticizing the president for proposing the end of the Electoral College.[300]
 In a letter to The New York Times, Representative Jonathan B. Bingham (D-New York) highlighted the danger of the ""flawed, outdated mechanism of the Electoral College"" by underscoring how a shift of fewer than 10,000 votes in two key states would have led to President Gerald Ford winning the 1976 election despite Jimmy Carter's nationwide 1.7 million-vote margin.[301]
 Since January 3, 2019, joint resolutions have been made proposing constitutional amendments that would replace the Electoral College with the popular election of the president and vice president.[302][303] Unlike the Bayh–Celler amendment, with its 40% threshold for election, these proposals do not require a candidate to achieve a certain percentage of votes to be elected.[304][305][306][non-primary source needed]
 As of April 2024, seventeen states plus the District of Columbia have joined the National Popular Vote Interstate Compact.[307][308][better source needed] Those joining the compact will, acting together if and when reflecting a majority of electors (at least 270), pledge their electors to the winner of the national popular vote. The compact applies Article II, Section 1, Clause 2 of the Constitution, which gives each state legislature the plenary power to determine how it chooses electors.
 Some scholars have suggested that Article I, Section 10, Clause 3 of the Constitution requires congressional consent before the compact could be enforceable;[309] thus, any attempted implementation of the compact without congressional consent could face court challenges to its constitutionality. Others have suggested that the compact's legality was strengthened by Chiafalo v. Washington, in which the Supreme Court upheld the power of states to enforce electors' pledges.[310][311]
 The eighteen adherents of the compact have 209 electors, which is 77% of the 270 required for it to take effect, or be considered justiciable.[307][better source needed]
 It has been argued by the advocacy group Equal Citizens that the Equal Protection Clause of the Fourteenth Amendment to the United States Constitution bars the winner-takes-all apportionment of electors by the states. According to this argument, the votes of the losing party are discarded entirely, thereby leading to an unequal position between different voters in the same state.[312] Lawsuits have been filed to this end in California, Massachusetts, Texas and South Carolina, though all have been unsuccessful.[312]
"
Thomas Jefferson,https://en.wikipedia.org/wiki/Thomas_Jefferson,"


 Thomas Jefferson (April 13 [O.S. April 2], 1743 – July 4, 1826) was an American Founding Father who served as the third president of the United States from 1801 to 1809.[6] He was the primary author of the Declaration of Independence. Following the American Revolutionary War and before becoming president in 1801, Jefferson was the nation's first U.S. secretary of state under George Washington and then the nation's second vice president under John Adams. Jefferson was a leading proponent of democracy, republicanism, and natural rights, and he produced formative documents and decisions at the state, national, and international levels. 
 Jefferson was born into the Colony of Virginia's planter class, dependent on slave labor. During the American Revolution, Jefferson represented Virginia at the Second Continental Congress. His advocacy for individual rights, including freedom of thought, speech, and religion, helped shape the ideological foundations of the revolution and inspired the Thirteen Colonies' fight for independence, culminating in the establishment of the United States as a free and sovereign nation.[7][8] He served as the second governor of revolutionary Virginia from 1779 to 1781. In 1785, Congress appointed Jefferson U.S. minister to France, where he served from 1785 to 1789. President Washington then appointed Jefferson the nation's first secretary of state, where he served from 1790 to 1793. During this time, in the early 1790s, Jefferson and political ally James Madison organized the Democratic-Republican Party to oppose the Federalist Party during the formation of the nation's First Party System. Jefferson and Federalist John Adams became both personal friends and political rivals. In the 1796 U.S. presidential election between the two, Jefferson came in second, which made him Adams' vice president under the electoral laws of the time. Four years later, in the 1800 presidential election, Jefferson again challenged Adams and won the presidency. In 1804, Jefferson was reelected overwhelmingly to a second term.
 As president, Jefferson assertively defended the nation's shipping and trade interests against Barbary pirates and aggressive British trade policies, promoted a western expansionist policy with the Louisiana Purchase, which doubled the nation's geographic size, and was able to reduce military forces and expenditures following successful negotiations with France. In his second presidential term, Jefferson was beset by difficulties at home, including the trial of his former vice president Aaron Burr. In 1807, Jefferson implemented the Embargo Act to defend the nation's industries from British threats to U.S. shipping, limiting foreign trade and stimulating the birth of the American manufacturing industry.
 Jefferson is ranked by both scholars and in public opinion among the upper tier of U.S. presidents. Presidential scholars and historians praise Jefferson's public achievements, including his advocacy of religious freedom and tolerance, his peaceful acquisition of the Louisiana Territory from France, and his leadership in supporting the Lewis and Clark Expedition. They acknowledge the fact of his lifelong ownership of large numbers of slaves and give differing interpretations of his views on and relationship with slavery.[9]
 Jefferson was born on April 13, 1743 (April 2, 1743, Old Style, Julian calendar), at the family's Shadwell Plantation in the British Colony of Virginia, the third of ten children.[10] He was of English and possibly Welsh descent, and was born a British subject.[11] His father, Peter Jefferson, was a planter and surveyor who died when Jefferson was fourteen; his mother was Jane Randolph.[b] Peter Jefferson moved his family to Tuckahoe Plantation in 1745 on the death of William Randolph III, the plantation's owner and Jefferson's friend, who in his will had named Peter guardian of Randolph's children. The Jeffersons returned to Shadwell before October 1753.[13]
 Peter died in 1757, and his estate was divided between his sons Thomas and Randolph.[14] John Harvie Sr. became 13-year-old Thomas' guardian.[15] Thomas inherited approximately 5,000 acres (2,000 ha; 7.8 sq mi), which included Monticello, and he assumed full legal authority over the property at age 21.[16]
 Jefferson began his education together with the Randolph children at Tuckahoe under tutors.[17] Thomas' father Peter, who was self-taught and regretted not having a formal education, entered Thomas into an English school at age five. In 1752, at age nine, he attended a local school run by a Scottish Presbyterian minister and also began studying the natural world, which he grew to love. At this time he began studying Latin, Greek, and French, while learning to ride horses as well. Thomas also read books from his father's modest library.[18] He was taught from 1758 to 1760 by the Reverend James Maury near Gordonsville, Virginia, where he studied history, science, and the classics while boarding with Maury's family.[18][19] Jefferson came to know various American Indians, including the Cherokee chief Ostenaco, who often stopped at Shadwell to visit on their way to Williamsburg to trade.[20][21] In Williamsburg, the young Jefferson met and came to admire Patrick Henry, eight years his senior, and shared a common interest in the playing of the violin.[22]
 Jefferson entered the College of William & Mary in Williamsburg, Virginia, in 1761, at the age of eighteen, and studied mathematics, metaphysics, and philosophy with William Small. Under Small's tutelage, Jefferson encountered the ideas of the British Empiricists, including John Locke, Francis Bacon, and Isaac Newton. Small introduced Jefferson to George Wythe and Francis Fauquier. Small, Wythe, and Fauquier recognized Jefferson as a man of exceptional ability and included him in their inner circle, where he became a regular member of their Friday dinner parties. Jefferson later wrote that, while there, he ""heard more common good sense, more rational & philosophical conversations than in all the rest of my life"".[23]
 During his first year at the college, Jefferson spent considerable time attending parties and dancing and was not very frugal with his expenditures; in his second year, regretting that he had squandered away time and money in his first year, he committed to studying fifteen hours a day.[24] While at William & Mary, Jefferson became a member of the Flat Hat Club.[25]
 Jefferson concluded his formal studies in April 1762.[26] He read the law under Wythe's tutelage while working as a law clerk in his office.[27] Jefferson was well-read in a broad variety of subjects, which, along with law and philosophy, included history, natural law, natural religion, ethics, and several areas in science, including agriculture. During his years of study under the watchful eye of Wythe, Jefferson authored a Commonplace Book, a survey of his extensive readings.[28] Wythe was so impressed with Jefferson that he later bequeathed his entire library to him.[29]
 On July 20, 1765, Jefferson's sister Martha married his close friend and college companion Dabney Carr, which was greatly pleasing to Jefferson. In October of that year, however, Jefferson mourned his sister Jane's unexpected death at age 25; he wrote a farewell epitaph for her in Latin.[30]
 Jefferson treasured his books and amassed three sizable libraries in his lifetime. He began assembling his first library, which grew to 200 volumes, in his youth. It included books inherited from his father and left to him by Wythe.[31] In 1770, however, Jefferson's first library was destroyed in a fire at his Shadwell home. His second library replenished the first. It grew to 1,250 titles by 1773, and to nearly 6,500 volumes by 1814.[32] Jefferson organized his books into three broad categories corresponding with elements of the human mind: memory, reason, and imagination.[33] After British forces burnt the Library of Congress during the 1814 Burning of Washington, Jefferson sold his second library to the U.S. government for $23,950, hoping to help jumpstart the Library of Congress's rebuilding. Jefferson used a portion of the proceeds to pay off some of his large debt. However, Jefferson soon resumed collecting what amounted to his third personal library, writing to John Adams, ""I cannot live without books.""[34][35] By the time of his death a decade later, the library had grown to nearly 2,000 volumes.[36]
 Jefferson was admitted to the Virginia bar in 1767, and lived with his mother at Shadwell.[37] He represented Albemarle County in the Virginia House of Burgesses from 1769 until 1775.[38] He pursued reforms to slavery, including writing and sponsoring legislation in 1769 to strip power from the royal governor and courts, instead providing masters of slaves with the discretion to emancipate them. Jefferson persuaded his cousin Richard Bland to spearhead the legislation's passage, but it faced strong opposition in a state whose economy was largely agrarian.[39]
 Jefferson took seven cases of freedom-seeking enslaved people[40] and waived his fee for one he claimed should be freed before the minimum statutory age for emancipation.[41] Jefferson invoked natural law, arguing ""everyone comes into the world with a right to his own person and using it at his own will ... This is what is called personal liberty, and is given him by the author of nature, because it is necessary for his own sustenance."" The judge cut him off and ruled against his client. As a consolation, Jefferson gave his client some money, which was conceivably used to aid his escape shortly thereafter.[41] However, Jefferson's underlying intellectual argument that all people were entitled by their creator to what he labeled a ""natural right"" to liberty is one he would later incorporate as he set about authoring the Declaration of Independence.[42] He also took on 68 cases for the General Court of Virginia in 1767, in addition to three notable cases: Howell v. Netherland (1770), Bolling v. Bolling (1771), and Blair v. Blair (1772).[43]
 Jefferson wrote a resolution calling for a ""Day of Fasting and Prayer"" and a boycott of all British goods in protest of the British Parliament passing the Intolerable Acts in 1774. Jefferson's resolution was later expanded into A Summary View of the Rights of British America, in which he argued that people have the right to govern themselves.[44]
 In 1768, Jefferson began constructing his primary residence, Monticello, whose name in Italian means ""Little Mountain"", on a hilltop overlooking his 5,000-acre (20 km2; 7.8 sq mi) plantation.[c] He spent most of his adult life designing Monticello as an architect and was quoted as saying, ""Architecture is my delight, and putting up, and pulling down, one of my favorite amusements.""[46] Construction was done mostly by local masons and carpenters, assisted by Jefferson's slaves.[47] He moved into the South Pavilion in 1770. Turning Monticello into a neoclassical masterpiece in the Palladian style was his perennial project.[48]
 On January 1, 1772, Jefferson married his third cousin[49] Martha Wayles Skelton, the 23-year-old widow of Bathurst Skelton.[50][51] She was a frequent hostess for Jefferson and managed the large household. Biographer Dumas Malone described the marriage as the happiest period of Jefferson's life.[52] Martha read widely, did fine needlework, and was a skilled pianist; Jefferson often accompanied her on the violin or cello.[53] During their ten years of marriage, Martha bore six children: Martha ""Patsy"" (1772–1836); Jane Randolph (1774–1775); an unnamed son who lived for only a few weeks in 1777; Mary ""Polly"" (1778–1804); Lucy Elizabeth (1780–1781); and another Lucy Elizabeth (1782–1784).[54][d] Only Martha and Mary survived to adulthood.[57] Martha's father John Wayles died in 1773, and the couple inherited 135 enslaved people, 11,000 acres (45 km2; 17 sq mi), and the estate's debts. The debts took Jefferson years to satisfy, contributing to his financial problems.[50]
 Martha later suffered from ill health, including diabetes, and frequent childbirth weakened her. Her mother had died young, and Martha lived with two stepmothers as a girl. A few months after the birth of her last child, she died on September 6, 1782, with Jefferson at her bedside. Shortly before her death, Martha made Jefferson promise never to marry again, telling him that she could not bear to have another mother raise her children.[58] Jefferson was grief-stricken by her death, relentlessly pacing back and forth. He emerged after three weeks, taking long rambling rides on secluded roads with his daughter Martha, by her description ""a solitary witness to many a violent burst of grief"".[57][59]
 After serving as U.S. Secretary of State from 1790 to 1793 during Washington's presidency, Jefferson returned to Monticello and initiated a remodeling based on architectural concepts he had learned and acquired in Europe. The work continued throughout most of his presidency and was completed in 1809.[60][61]
 Jefferson was the primary author of the Declaration of Independence.[63] At age 33, he was one of the youngest delegates to the Second Continental Congress beginning in 1775 at the outbreak of the American Revolutionary War, where a formal declaration of independence from Britain was overwhelmingly favored.[64] Jefferson was inspired by the Enlightenment ideals of the sanctity of the individual, and the writings of Locke and Montesquieu.[65]
 Jefferson sought out John Adams, a Continental Congress delegate from Massachusetts and an emerging leader in the Congress.[66] They became close friends, and Adams supported Jefferson's appointment to the Committee of Five, charged by the Congress with authoring a declaration of independence. The five chosen were Adams, Jefferson, Benjamin Franklin, Robert R. Livingston, and Roger Sherman. The committee initially thought that Adams should write the document, but Adams persuaded the committee to choose Jefferson. His choice was due to Jefferson being a Virginian, popular, and being considered a good writer by Adams.[e]
 Jefferson consulted with his fellow committee members, but mostly wrote the Declaration of Independence in isolation between June 11 and 28, 1776, in a home he was renting at 700 Market Street in Center City Philadelphia.[62] Jefferson drew considerably on his proposed draft of the Virginia Constitution, George Mason's draft of the Virginia Declaration of Rights, and other sources.[68] Other committee members made some changes, and a final draft was presented to Congress on June 28, 1776.[69]
 The declaration was introduced on Friday, June 28, and Congress began debate over its contents on Monday, July 1,[69] resulting in the removal of roughly a fourth of Jefferson's original draft.[70][71] Jefferson resented the changes, but he did not speak publicly about the revisions.[f] On July 4, 1776, the Congress ratified the Declaration, and delegates signed it on August 2; in so doing, the delegates were knowingly committing an act of high treason against The Crown, which was deemed the most serious criminal offense and was punishable by torture and death.[73]
 Jefferson's preamble is regarded as an enduring statement on individual and human rights, and the phrase ""all men are created equal"" has been called ""one of the best-known sentences in the English language"". The Declaration of Independence, historian Joseph Ellis wrote in 2008, represents ""the most potent and consequential words in American history"".[71][74]
 At the start of the Revolution, Colonel Jefferson was named commander of the Albemarle County Militia on September 26, 1775.[75] He was then elected to the Virginia House of Delegates for Albemarle County in September 1776, when finalizing the state constitution was a priority.[76][77]
For nearly three years, he assisted with the constitution and was especially proud of his Bill for Establishing Religious Freedom, which prohibited state support of religious institutions or enforcement of religious doctrine.[78] The bill failed to pass, as did his legislation to disestablish the Anglican Church, but both were later revived by James Madison.[79]
 In 1778, Jefferson was given the task of revising the state's laws. He drafted 126 bills in three years, including laws to streamline the judicial system. He proposed statutes that provided for general education, which he considered the basis of ""republican government"".[76] Jefferson also was concerned that Virginia's powerful landed gentry were becoming a hereditary aristocracy and took the lead in abolishing what he called ""feudal and unnatural distinctions.""[80] He targeted laws such as entail and primogeniture by which a deceased landowner's oldest son was vested with all land ownership and power.[80][g]
 Jefferson was elected governor for one-year terms in 1779 and 1780.[82] He transferred the state capital from Williamsburg to Richmond, and introduced additional measures for public education, religious freedom, and inheritance.[83]
 During General Benedict Arnold's 1781 invasion of Virginia, Jefferson escaped Richmond just ahead of the British forces, which razed the city.[84][85] He sent emergency dispatches to Colonel Sampson Mathews and other commanders in an attempt to repel Arnold's efforts.[86][87] General Charles Cornwallis that spring dispatched a cavalry force led by Banastre Tarleton to capture Jefferson and members of the Assembly at Monticello, but Jack Jouett of the Virginia militia thwarted the British plan. Jefferson escaped to Poplar Forest, his plantation to the west.[88] When the General Assembly reconvened in June 1781, it conducted an inquiry into Jefferson's actions which eventually concluded that Jefferson had acted with honor—but he was not re-elected.[89]
 In April of the same year, his daughter Lucy died at age one. A second daughter of that name was born the following year, but she died at age two.[90]
 In 1782, Jefferson refused a partnership offer by North Carolina Governor Abner Nash, in a profiteering scheme involving the sale of confiscated Loyalist lands.[91] Unlike some Founders, Jefferson was content with his Monticello estate and the land he owned in the vicinity of Virginia's Shenandoah Valley. Jefferson thought of Monticello as an intellectual gathering place for his friends James Madison and James Monroe.[92]
 In 1780, Jefferson received from French diplomat François Barbé-Marbois a letter of inquiry into the geography, history, and government of Virginia, as part of a study of the United States. Jefferson organized his responses in a book, Notes on the State of Virginia (1785).[93] He compiled the book over five years, including reviews of scientific knowledge, Virginia's history, politics, laws, culture, and geography.[94] The book explores what constitutes a good society, using Virginia as an exemplar. Jefferson included extensive data about the state's natural resources and economy and wrote at length about slavery and miscegenation; he articulated his belief that blacks and whites could not live together as free people in one society because of justified resentments of the enslaved.[95] He also wrote of his views on the American Indians, equating them to European settlers.[96][97]
 Notes was first published in 1785 in French and appeared in English in 1787.[98] Biographer George Tucker considered the work ""surprising in the extent of the information which a single individual had been thus far able to acquire, as to the physical features of the state"";[99] Merrill D. Peterson described it as an accomplishment for which all Americans should be grateful.[100]
 Jefferson was appointed a Virginia delegate to the Congress of the Confederation organized following the peace treaty with Great Britain in 1783. He was a member of the committee setting foreign exchange rates and recommended an American currency based on the decimal system that was adopted.[101] He advised the formation of the Committee of the States to fill the power vacuum when Congress was in recess.[102] The committee met when Congress adjourned, but disagreements rendered it dysfunctional.[103]
 In the Congress's 1783–1784 session, Jefferson acted as chairman of committees to establish a viable system of government for the new Republic and to propose a policy for settlement of the western territories. He was the principal author of the Land Ordinance of 1784, whereby Virginia ceded to the national government the vast area that it claimed northwest of the Ohio River. He insisted that this territory should not be used as colonial territory by any of the thirteen states, but that it should be divided into sections that could become states. He plotted borders for nine new states in their initial stages and wrote an ordinance banning slavery in all the nation's territories. Congress made extensive revisions and rejected the ban on slavery.[104][105] The provisions banning slavery, known as the ""Jefferson Proviso"", were modified and implemented three years later in the Northwest Ordinance of 1787 and became the law for the entire Northwest Territory.[104]
 On May 7, 1784, Jefferson was appointed by the Congress of the Confederation[h] to join Benjamin Franklin and John Adams in Paris as Minister Plenipotentiary for Negotiating Treaties of Amity and Commerce with Great Britain and other countries.[106][i] With his young daughter Patsy and two servants, he departed in July 1784, arriving in Paris the next month.[108][109] Jefferson had Patsy educated at the Pentemont Abbey. Less than a year later he was assigned the additional duty of succeeding Franklin as Minister to France. French foreign minister Count de Vergennes commented, ""You replace Monsieur Franklin, I hear."" Jefferson replied, ""I succeed. No man can replace him.""[110] During his five years in Paris, Jefferson played a leading role in shaping U.S. foreign policy.[111]
 In 1786, he met and fell in love with Maria Cosway, an accomplished—and married—Italian-English musician of 27. She returned to Great Britain after six weeks, but they maintained a lifelong correspondence.[112]
 During the summer of 1786, Jefferson arrived in London to meet with John Adams, the US Ambassador to Britain. Adams had official access to George III and arranged a meeting between Jefferson and the king. Jefferson later described the king's reception of the men as ""ungracious."" According to Adams's grandson, George III turned his back on both in a gesture of public insult. Jefferson returned to France in August.[113]
 Jefferson sent for his youngest surviving child, nine-year-old Polly, in June 1787. She was accompanied by a young slave from Monticello, Sally Hemings. Jefferson had taken her older brother, James Hemings, to Paris as part of his domestic staff and had him trained in French cuisine.[114] According to Sally's son, Madison Hemings, the 16-year-old Sally and Jefferson began a sexual relationship in Paris, where she became pregnant.[115] The son indicated Hemings agreed to return to the United States only after Jefferson promised to free her children when they came of age.[115]
 While in France, Jefferson became a regular companion of the Marquis de Lafayette, a French hero of the American Revolution, and Jefferson used his influence to procure trade agreements with France.[116][117] As the French Revolution began, he allowed his Paris residence, the Hôtel de Langeac, to be used for meetings by Lafayette and other republicans. He was in Paris during the storming of the Bastille and consulted with Lafayette while the latter drafted the Declaration of the Rights of Man and of the Citizen.[118] Jefferson often found his mail opened by postmasters, so he invented his own enciphering device, the ""Wheel Cipher""; he wrote important communications in code for the rest of his career.[119][j] Unable to attend the 1787 Constitution Convention, Jefferson supported the Constitution but desired the addition of the promised Bill of Rights.[120] Jefferson left Paris for America in September 1789.[121] He remained a firm supporter of the French Revolution while opposing its more violent elements.[122]
 Soon after returning from France, Jefferson accepted President Washington's invitation to serve as Secretary of State.[123] Pressing issues at this time were the national debt and the permanent location of the capital. He opposed a national debt, preferring that each state retire its own, in contrast to Secretary of the Treasury Alexander Hamilton, who desired consolidation of states' debts by the federal government.[124] Hamilton also had bold plans to establish national credit and a national bank, but Jefferson strenuously opposed this and attempted to undermine his agenda, which nearly led Washington to dismiss him from the cabinet. He later left the cabinet voluntarily.[125]
 The second major issue was the capital's permanent location. Hamilton favored a capital close to the major commercial centers of the Northeast, while Washington, Jefferson, and other agrarians wanted it further south.[126] After lengthy deadlock, the Compromise of 1790 was struck, permanently locating the capital on the Potomac River, and the federal government assumed the war debts of all original 13 states.[126]
 Jefferson's goals were to decrease American dependence on British commerce and to expand commercial trade with France. He sought to weaken Spanish colonialism of the trans-Appalachian West and British control in the North, believing this would aid in the pacification of Native Americans.[127]
 Jefferson and political protegé Congressman James Madison founded the National Gazette in 1791, along with author Phillip Freneau, to counter Hamilton's Federalist policies, which Hamilton was promoting through the influential Federalist newspaper the Gazette of the United States. The National Gazette made particular criticism of the policies promoted by Hamilton, often through anonymous essays signed by the pen name Brutus at Jefferson's urging, which were written by Madison.[128] In Spring 1791, Jefferson and Madison took a vacation to Vermont; Jefferson had been suffering from migraines and was tiring of the in-fighting with Hamilton.[129]
 In May 1792, Jefferson became alarmed at the political rivalries taking shape; he wrote to Washington, imploring him to run for reelection that year as a unifying influence.[130] He urged the president to rally the citizenry to a party that would defend democracy against the corrupting influence of banks and monied interests, as espoused by the Federalists. Historians recognize this letter as the earliest delineation of Democratic-Republican Party principles.[131] Jefferson, Madison, and other Democratic-Republican organizers favored states' rights and local control and opposed the federal concentration of power, whereas Hamilton sought more power for the federal government.[132]
 Jefferson supported France against Britain when the two nations fought in 1793, though his arguments in the Cabinet were undercut by French Revolutionary envoy Edmond-Charles Genêt's open scorn for Washington.[133] In his discussions with British Minister George Hammond, he tried in vain to persuade the British to vacate their posts in the Northwest and to compensate the U.S. for enslaved people whom the British had freed at the end of the war. Jefferson sought a return to private life, and resigned from the cabinet position in December 1793; he may also have wanted to bolster his political influence from outside the administration.[134]
 After the Washington administration negotiated the Jay Treaty with Britain in 1794, Jefferson saw a cause around which to rally his party and organized a national opposition from Monticello.[135] The treaty, designed by Hamilton, aimed to reduce tensions and increase trade. Jefferson warned that it would increase British influence and subvert republicanism, calling it ""the boldest act [Hamilton and Jay] ever ventured on to undermine the government"".[136] The Treaty passed, but it expired in 1805 during Jefferson's presidential administration and was not renewed. Jefferson continued his pro-France stance; during the violence of the Reign of Terror, he declined to disavow the revolution: ""To back away from France would be to undermine the cause of republicanism in America.""[137]
 In the presidential campaign of 1796, Jefferson lost the electoral college vote to Federalist John Adams 71–68. He did, however, receive the second-highest number of votes and, under the electoral laws at the time, was elected as vice president. As presiding officer of the Senate, he assumed a more passive role than his predecessor John Adams. He allowed the Senate to freely conduct debates and confined his participation to procedural issues, which he called an ""honorable and easy"" role.[138] Jefferson had previously studied parliamentary law and procedure for 40 years, making him quite qualified to serve as presiding officer. In 1800, he published his assembled notes on Senate procedure as A Manual of Parliamentary Practice.[139] He cast only three tie-breaking votes in the Senate.
 In four confidential talks with French consul Joseph Létombe in the spring of 1797, Jefferson attacked Adams and predicted that his rival would serve only one term. He also encouraged France to invade England, and advised Létombe to stall any American envoys sent to Paris.[140] This toughened the tone that the French government adopted toward the Adams administration. After Adams's initial peace envoys were rebuffed, Jefferson and his supporters lobbied for the release of papers related to the incident, called the XYZ Affair after the letters used to disguise the identities of the French officials involved.[141] However, the tactic backfired when it was revealed that French officials had demanded bribes, rallying public support against France. The U.S. began an undeclared naval war with France known as the Quasi-War.[142]
 During the Adams presidency, the Federalists rebuilt the military, levied new taxes, and enacted the Alien and Sedition Acts. Jefferson believed these laws were intended to suppress Democratic-Republicans, rather than prosecute enemy aliens, and considered them unconstitutional.[143] To rally opposition, he and James Madison anonymously wrote the Kentucky and Virginia Resolutions, declaring that the federal government had no right to exercise powers not specifically delegated to it by the states.[144] The resolutions followed the ""interposition"" approach of Madison, that states may shield their citizens from federal laws that they deem unconstitutional. Jefferson advocated nullification, allowing states to invalidate federal laws altogether.[145][k] He warned that, ""unless arrested at the threshold"", the Alien and Sedition Acts would ""drive these states into revolution and blood"".[147]
 Historian Ron Chernow claims that ""the theoretical damage of the Kentucky and Virginia Resolutions was deep and lasting, and was a recipe for disunion"", contributing to the American Civil War as well as later events.[148] Washington was so appalled by the resolutions that he told Patrick Henry that, if ""systematically and pertinaciously pursued"", the resolutions would ""dissolve the union or produce coercion.""[149] Jefferson had always admired Washington's leadership skills but felt that his Federalist party was leading the country in the wrong direction. He decided not to attend Washington's funeral in 1799 because of acute differences with him while serving as secretary of state.[150]
 Jefferson contended for president once more against John Adams in 1800. Adams' campaign was weakened by unpopular taxes and vicious Federalist infighting over his actions in the Quasi-War.[151] Democratic-Republicans pointed to the Alien and Sedition Acts and accused the Federalists of being secret pro-Britain monarchists, while Federalists charged that Jefferson was a godless libertine beholden to the French.[152] Historian Joyce Appleby said the election was ""one of the most acrimonious in the annals of American history"".[153]
 The Democratic-Republicans ultimately won more electoral college votes, due in part to the electors that resulted from the addition of three-fifths of the South's slaves to the population calculation under the Three-Fifths Compromise.[154] Jefferson and his vice-presidential candidate Aaron Burr unexpectedly received an equal total. Because of the tie, the election was decided by the Federalist-dominated House of Representatives.[155][l] Hamilton lobbied Federalist representatives on Jefferson's behalf, believing him a lesser political evil than Burr. On February 17, 1801, after thirty-six ballots, the House elected Jefferson president and Burr vice president.[156]
 The win was marked by Democratic-Republican celebrations throughout the country.[157] Some of Jefferson's opponents argued that he owed his victory to the South's inflated number of electors.[158] Others alleged that Jefferson secured James Asheton Bayard's tie-breaking electoral vote by guaranteeing the retention of various Federalist posts in the government.[156] Jefferson disputed the allegation, and the historical record is inconclusive.[159]
 The transition proceeded smoothly, marking a watershed in American history. As historian Gordon S. Wood writes, ""it was one of the first popular elections in modern history that resulted in the peaceful transfer of power from one 'party' to another.""[156]
 Jefferson was sworn in as president by Chief Justice John Marshall at the new Capitol in Washington, D.C., on March 4, 1801. His inauguration was not attended by outgoing President Adams. In contrast to his two predecessors, Jefferson exhibited a dislike of formal etiquette. Plainly dressed, he chose to walk alongside friends to the Capitol from his nearby boardinghouse that day instead of arriving by carriage.[160] His inaugural address struck a note of reconciliation and commitment to democratic ideology, declaring, ""We have been called by different names brethren of the same principle. We are all Republicans, we are all Federalists.""[161][162] Ideologically, he stressed ""equal and exact justice to all men"", minority rights, and freedom of speech, religion, and press.[163] He said that a free and republican government was ""the strongest government on earth.""[163] He nominated moderate Republicans to his cabinet: James Madison as secretary of state, Henry Dearborn as secretary of war, Levi Lincoln as attorney general, and Robert Smith as secretary of the navy.[162]
 Widowed since 1782, Jefferson first relied on his two daughters to serve as his official hostesses.[164] In late May 1801, he asked Dolley Madison, wife of his long-time friend James Madison, to be the permanent White House hostess. She was also in charge of the completion of the White House mansion. Dolley served as White House hostess for the rest of Jefferson's two terms and then for another eight years as First Lady while her husband was president.[164]
 Jefferson's first challenge as president was shrinking the $83 million national debt.[165] He began dismantling Hamilton's Federalist fiscal system with help from the secretary of the treasury, Albert Gallatin.[162] Gallatin devised a plan to eliminate the national debt in sixteen years by extensive annual appropriations and reduction in taxes.[166] The administration eliminated the whiskey excise and other taxes after closing ""unnecessary offices"" and cutting ""useless establishments and expenses"".[167][168]
 Jefferson believed that the First Bank of the United States represented a ""most deadly hostility"" to republican government.[166] He wanted to dismantle the bank before its charter expired in 1811, but was dissuaded by Gallatin.[169] Gallatin argued that the national bank was a useful financial institution and set out to expand its operations.[170] Jefferson looked to other corners to address the growing national debt.[170] He shrank the Navy, for example, deeming it unnecessary in peacetime, and incorporated a fleet of inexpensive gunboats intended only for local defense to avoid provocation against foreign powers.[167] After two terms, he had lowered the national debt from $83 million to $57 million.[171]
 Jefferson pardoned several of those imprisoned under the Alien and Sedition Acts.[172] Congressional Republicans repealed the Judiciary Act of 1801, which removed nearly all of Adams's ""midnight judges"". A subsequent appointment battle led to the Supreme Court's landmark decision in Marbury v. Madison, asserting judicial review over executive branch actions.[173] Jefferson appointed three Supreme Court justices: William Johnson (1804), Henry Brockholst Livingston (1807), and Thomas Todd (1807).[174]
 Jefferson strongly felt the need for a national military university, producing an officer engineering corps for a national defense based on the advancement of the sciences, rather than having to rely on foreign sources.[175] He signed the Military Peace Establishment Act on March 16, 1802, founding the United States Military Academy at West Point. The act documented a new set of laws and limits for the military. Jefferson was also hoping to bring reform to the Executive branch, replacing Federalists and active opponents throughout the officer corps to promote Republican values.[176]
 Jefferson took great interest in the Library of Congress, which had been established in 1800. He often recommended books to acquire. In 1802, Congress authorized Jefferson to name the first Librarian of Congress, and formed a committee to establish library regulations. Congress also granted both the president and vice president the right to use the library.[177]
 American merchant ships had been protected from Barbary Coast pirates by the Royal Navy when the states were British colonies.[178] After independence, however, pirates often captured U.S. merchant ships, pillaged cargoes, and enslaved or held crew members for ransom. Jefferson had opposed paying tribute to the Barbary States since 1785. In 1801, he authorized a U.S. Navy fleet under Commodore Richard Dale to make a show of force in the Mediterranean, the first American naval squadron to cross the Atlantic.[179] Following the fleet's first engagement, he successfully asked Congress for a declaration of war.[179] The ""First Barbary War"" was the first foreign war fought by the U.S.[180]
 Pasha of Tripoli Yusuf Karamanli captured the USS Philadelphia, so Jefferson authorized William Eaton, the U.S. Consul to Tunis, to lead a force to restore the pasha's older brother to the throne.[181] The American navy forced Tunis and Algiers into breaking their alliance with Tripoli. Jefferson ordered five separate naval bombardments of Tripoli, leading the pasha to sign a treaty that restored peace in the Mediterranean.[182] This victory proved only temporary, but according to Wood, ""many Americans celebrated it as a vindication of their policy of spreading free trade around the world and as a great victory for liberty over tyranny.""[183]
 Spain ceded ownership of the Louisiana territory in 1800 to France. Jefferson was concerned that Napoleon's interests in the vast territory would threaten the security of the continent and Mississippi River shipping. He wrote that the cession ""works most sorely on the U.S. It completely reverses all the political relations of the U.S.""[184] In 1802, he instructed James Monroe and Robert R. Livingston to negotiate the purchase of New Orleans and adjacent coastal areas.[185] In early 1803, Jefferson offered Napoleon nearly $10 million for 40,000 square miles (100,000 square kilometres) of tropical territory.[186]
 Napoleon realized that French military control was impractical over such a vast remote territory, and he was in dire need of funds for his wars on the home front. In early April 1803, he unexpectedly made negotiators a counter-offer to sell 827,987 square miles (2,144,480 square kilometres) of French territory for $15 million (~$371 million in 2023), doubling the size of the United States.[186] U.S. negotiators accepted the offer and signed the treaty on April 30, 1803.[171] Word of the unexpected purchase did not reach Jefferson until July 3, 1803.[171] He unknowingly acquired the most fertile tract of land of its size on Earth, making the new country self-sufficient in food and other resources. The sale also significantly curtailed European presence in North America, removing obstacles to U.S. westward expansion.[187]
 Most thought that this was an exceptional opportunity, despite Republican reservations about the Constitutional authority of the federal government to acquire land.[188] Jefferson initially thought that a Constitutional amendment was necessary to purchase and govern the new territory; but he later changed his mind, fearing that this would give cause to oppose the purchase, and urged a speedy debate and ratification.[189] On October 20, 1803, the Senate ratified the purchase treaty by a vote of 24–7.[190] Jefferson personally was humble about acquiring the Louisiana Territory, but he resented complainers who called the vast domain a ""howling wilderness"".[191]
 After the purchase, Jefferson preserved the region's Spanish legal code and instituted a gradual approach to integrating settlers into American democracy. He believed that a period of the federal rule would be necessary while Louisianans adjusted to their new nation.[192][m] Historians have differed in their assessments regarding the constitutional implications of the sale,[194] but they typically hail the Louisiana acquisition as a major accomplishment. Frederick Jackson Turner called the purchase the most formative event in American history.[187]
 Jefferson anticipated further westward settlements due to the Louisiana Purchase and arranged for the exploration and mapping of the uncharted territory. He sought to establish a U.S. claim ahead of competing European interests and to find the rumored Northwest Passage.[195] Jefferson and others were influenced by exploration accounts of Le Page du Pratz in Louisiana (1763) and James Cook in the Pacific (1784),[196] and they persuaded Congress in 1804 to fund an expedition to explore and map the newly acquired territory to the Pacific Ocean.[197]
 Jefferson appointed secretary Meriwether Lewis and acquaintance William Clark to lead the Corps of Discovery (1803–1806).[198] In the months leading up to the expedition, Jefferson tutored Lewis in the sciences of mapping, botany, natural history, mineralogy, and astronomy and navigation, giving him unlimited access to his library at Monticello, which included the largest collection of books in the world on the subject of the geography and natural history of the North American continent, along with an impressive collection of maps.[199]
 The expedition lasted from May 1804 to September 1806 and obtained a wealth of scientific and geographic knowledge, including knowledge of many Indian tribes.[200]
 Jefferson organized three other western expeditions: the William Dunbar and George Hunter Expedition on the Ouachita River (1804–1805), the Thomas Freeman and Peter Custis Expedition (1806) on the Red River, and the Zebulon Pike Expedition (1806–1807) into the Rocky Mountains and the Southwest. All three produced valuable information about the American frontier.[201] This interest also motivated Jefferson to meet the Prussian explorer Alexander von Humboldt several times in June 1804, inquiring into Humboldt's knowledge of New Spain's natural resources, economic prospects, and demographic development.[202]
 Jefferson refuted the contemporary notion that Indians were inferior and maintained that they were equal in body and mind to people of European descent,[203] although he believed them to be inferior in terms of culture and technology.[204] As governor of Virginia during the Revolutionary War, Jefferson recommended moving the Cherokee and Shawnee tribes, who had allied with the British, to west of the Mississippi River. But when he took office as president, he quickly took measures to avert another major conflict, as American and Indian societies were in collision and the British were inciting Indian tribes from Canada.[205][206] In Georgia, he stipulated that the state would release its legal claims for lands to its west in exchange for military support in expelling the Cherokee from Georgia. This facilitated his policy of western expansion, to ""advance compactly as we multiply"".[207]
 In keeping with his Enlightenment thinking, President Jefferson adopted an assimilation policy toward American Indians known as his ""civilization program"" which included securing peaceful U.S.–Indian treaty alliances and encouraging agriculture. Jefferson advocated that Indian tribes should make federal purchases by credit holding their lands as collateral. Various tribes accepted Jefferson's policies, including the Shawnees led by Black Hoof, the Muscogee, and the Cherokee. However, some Shawnees, led by Tecumseh, broke off from Black Hoof, and opposed Jefferson's assimilation policies.[208]
 Historian Bernard Sheehan argues that Jefferson believed that assimilation was best for American Indians, and next-best was removal to the west; he felt that the worst outcome of the conflict would be their attacking the whites.[206] Jefferson told U.S. Secretary of War Henry Dearborn, who then oversaw Indian affairs, ""If we are constrained to lift the hatchet against any tribe, we will never lay it down until that tribe is exterminated or driven beyond the Mississippi.""[209] Miller agrees that Jefferson believed that Indians should assimilate to American customs and agriculture. Historians such as Peter S. Onuf and Merrill D. Peterson argue that Jefferson's actual Indian policies did little to promote assimilation and were a pretext to seize lands.[210]
 Jefferson was nominated for reelection by the Republican party, with George Clinton replacing Burr as his running mate.[211] The Federalist party ran Charles Cotesworth Pinckney of South Carolina, John Adams's vice-presidential candidate in the 1800 election. The Jefferson-Clinton ticket won overwhelmingly in the electoral college vote, by 162 to 14, promoting their achievement of a strong economy, lower taxes, and the Louisiana Purchase.[211]
 In March 1806, a split developed in the Republican party, led by fellow Virginian and former Republican ally John Randolph, who viciously accused President Jefferson on the floor of the House of moving too far in the Federalist direction. In so doing, Randolph permanently set himself apart politically from Jefferson. Jefferson and Madison had backed resolutions to limit or ban British imports in retaliation for British seizures of American shipping. Also, in 1808, Jefferson was the first president to propose a broad Federal plan to build roads and canals across several states, asking for $20 million, further alarming Randolph and believers of limited government.[212]
 Jefferson's popularity further suffered in his second term due to his response to wars in Europe. Positive relations with Britain had diminished, due partly to the antipathy between Jefferson and British diplomat Anthony Merry. After Napoleon's decisive victory at the Battle of Austerlitz in 1805, Napoleon became more aggressive in his negotiations over trading rights, which American efforts failed to counter. Jefferson then led the enactment of the Embargo Act of 1807, directed at both France and Britain. This triggered economic chaos in the U.S. and was strongly criticized, resulting in Jefferson having to abandon the policy a year later.[213]
 During the revolutionary era, the states abolished the international slave trade, but South Carolina reopened it. In his annual message of December 1806, Jefferson denounced the ""violations of human rights"" attending the international slave trade, calling on the newly elected Congress to criminalize it immediately. In 1807, Congress passed the Act Prohibiting Importation of Slaves, which Jefferson signed.[214][215] The act established severe punishment against the international slave trade, although it did not address the issue domestically.[216]
 In Haiti, Jefferson's neutrality had allowed arms to enable the slave independence movement during its Revolution, and blocked attempts to assist Napoleon, who was defeated there in 1803.[217] But his administration refused official recognition of the country during his second term, in deference to southern complaints about the racial violence against slave-holders; it was eventually extended to Haiti in 1862.[218]
 Following the 1801 electoral deadlock, Jefferson's relationship with his vice president, Aaron Burr, rapidly eroded. Jefferson suspected Burr of seeking the presidency for himself, while Burr was angered by Jefferson's refusal to appoint some of his supporters to federal office. Burr was dropped from the Democratic-Republican ticket in 1804 in favor of charismatic George Clinton.
 The same year, Burr was soundly defeated in his bid to be elected New York governor. During the campaign, Alexander Hamilton publicly made callous remarks regarding Burr's moral character.[219] Burr challenged Hamilton to a duel, mortally wounding him on July 11, 1804. Burr was indicted for Hamilton's murder in New York and New Jersey, causing him to flee to Georgia, although he remained president of the Senate during Supreme Court Justice Samuel Chase's impeachment trial.[220] Both indictments quietly died and Burr was not prosecuted.[221] Also during the election, certain New England separatists approached Burr, desiring a New England federation and intimating that he would be their leader.[222] However, nothing came of the plot, since Burr had lost the election and his reputation was ruined after killing Hamilton.[222] In August 1804, Burr contacted British Minister Anthony Merry offering to cede U.S. western territory in return for money and British ships.[223]
 After leaving office in April 1805, Burr traveled west and conspired with Louisiana Territory governor James Wilkinson, beginning a large-scale recruitment for a military expedition.[224] Other plotters included Ohio Senator John Smith and Irishman Harman Blennerhassett.[224] Burr discussed seizing control of Mexico or Spanish Florida, or forming a secessionist state in New Orleans or the Western U.S.; historians remain unclear as to his true goal.[225][n] In the fall of 1806, Burr launched a military flotilla carrying about 60 men down the Ohio River. Wilkinson renounced the plot and reported Burr's expedition to Jefferson, who ordered Burr's arrest.[224][227][228] On February 13, 1807, Burr was captured in Louisiana and sent to Virginia to be tried for treason.[223]
 Burr's 1807 conspiracy trial became a national issue.[229] Jefferson attempted to preemptively influence the verdict by telling Congress that Burr's guilt was ""beyond question"", but the case came before his longtime political foe John Marshall, who dismissed the treason charge. Burr's legal team subpoenaed Jefferson, but Jefferson refused to testify, making the first argument for executive privilege. Instead, Jefferson provided relevant legal documents.[230] After a three-month trial, the jury found Burr not guilty, while Jefferson denounced his acquittal.[228][231][o][232] Jefferson subsequently removed Wilkinson as territorial governor but retained him in the U.S. military. Historian James N. Banner criticized Jefferson for continuing to trust Wilkinson, a ""faithless plotter"".[228]
 Commanding General James Wilkinson was a holdover of the Washington and Adams administrations. In 1804, Wilkinson received 12,000 pesos from the Spanish for information on American boundary plans.[233] Wilkinson also received advances on his salary and payments on claims submitted to Secretary of War Henry Dearborn. This damaging information apparently was unknown to Jefferson. In 1805, Jefferson trusted Wilkinson and appointed him Louisiana Territory governor, admiring Wilkinson's work ethic.
 In January 1806, Jefferson received information from Kentucky U.S. Attorney Joseph Davies that Wilkinson was on the Spanish payroll. Jefferson took no action against Wilkinson, since there was not then significant evidence against him.[234] An investigation by the U.S. House of Representatives in December 1807 exonerated Wilkinson.[235] In 1808, a military court looked into the allegations against Wilkinson but also found a lack of evidence. Jefferson retained Wilkinson in the U.S. Army.[236] Evidence found in Spanish archives in the 20th century proved Wilkinson was on the Spanish payroll.[233]
 In the aftermath of the Louisiana Purchase, Jefferson attempted to annex West Florida from Spain. In his annual message to Congress, on December 3, 1805, Jefferson railed against Spain over Florida border depredations.[237][238] A few days later Jefferson secretly requested a two-million-dollar expenditure to purchase Florida. Floor leader John Randolph opposed annexation, was upset over Jefferson's secrecy on the matter, and believed the money would end up going to Napoleon.[238][239] The Two Million Dollar bill passed only after Jefferson successfully maneuvered to replace Randolph with Barnabas Bidwell as floor leader.[238][239] This aroused suspicion of Jefferson and charges of undue executive influence over Congress. Jefferson signed the bill into law in February 1806. Six weeks later the law was made public. The two million dollars was to be given to France as payment, in turn, to put pressure on Spain to permit the annexation of Florida by the United States. France, however, refused the offer and Florida remained under Spanish control.[240][238] The failed venture damaged Jefferson's reputation among his supporters.[241][238]
 Starting in 1806, the Royal Navy began stopping American merchantmen to search for deserters from the British navy; approximately 6,000 sailors were impressed  into the Royal Navy this way, leading to deep anger and resentment among the U.S. public. In 1806, Jefferson issued a call for a boycott of British goods; on April 18, Congress passed the Non-Importation Acts, but they were never enforced. Later that year, Jefferson asked James Monroe and William Pinkney to negotiate an end to foreign interference with American merchant shipping, though relations with Britain showed no signs of improving. The Monroe–Pinkney Treaty was finalized but lacked any provisions regarding the issue of impressment, and Jefferson refused to submit it to the Senate for ratification.[242]
 The British warship HMS Leopard encountered the USS Chesapeake off the Virginia coast in June 1807; Leopard fired at Chesapeake after the latter refused to allow for a search for deserters before removing four deserters from the ship.[243] Jefferson issued a proclamation banning British warships from U.S. waters. He presumed unilateral authority to call on the states to prepare 100,000 militia and ordered the purchase of arms, ammunition, and supplies, writing, ""The laws of necessity, of self-preservation, of saving our country when in danger, are of higher obligation [than strict observance of written laws]"". The USS Revenge was dispatched to demand an explanation from the British government, and Jefferson called for a special session of Congress in October to enact an embargo or alternatively to consider war.[244]
 In December 1807, news arrived that Napoleon had extended the Berlin Decree, globally banning British imports. The Royal Navy, meanwhile continued to impress sailors from American merchant ships. However, Congress had no appetite to prepare the U.S. for war; Jefferson asked for and received the Embargo Act, an alternative that allowed the U.S. more time to build up defensive works, militias, and naval forces. Meacham argued that the Embargo Act was a projection of power that surpassed the Alien and Sedition Acts, and R. B. Bernstein said that Jefferson ""was pursuing policies resembling those he had cited in 1776 as grounds for independence and revolution"".[245]
 In November 1807, Jefferson, for several days, met with his cabinet to discuss the deteriorating foreign situation.[246] Secretary of State James Madison supported the embargo,[247] while Treasury Secretary Gallatin opposed it, due to its indefinite time frame and the risk to the policy of American neutrality.[248] The U.S. economy suffered, criticism grew, and opponents began evading the embargo. Instead of retreating, Jefferson sent federal agents to secretly track down smugglers and violators.[249] Three acts were passed in Congress during 1807 and 1808, called the Supplementary, the Additional, and the Enforcement acts.[243] The government could not prevent American vessels from trading with the European belligerents once they had left American ports, although the embargo triggered a devastating decline in exports.[243]
 In December 1807, Jefferson announced his intention not to seek a third term. He turned his attention increasingly to Monticello during the last year of his presidency, giving Madison and Gallatin almost total control of affairs.[250] Shortly before leaving office in March 1809, Jefferson signed the repeal of the Embargo. In its place, the Non-Intercourse Act was passed, but it proved no more effective.[243] The day before Madison was inaugurated as his successor, Jefferson said that he felt like ""a prisoner, released from his chains"".[251]
 After his presidency, Jefferson remained influential and continued to correspond with many of the country's leaders (including his two protégées, Madison and Monroe, who succeeded him as president); the Monroe Doctrine strongly resembles solicited advice that Jefferson gave to Monroe in 1823.[252][253]
 Jefferson envisioned a university free of church influences where students could specialize in new areas not offered at other colleges. He believed that education engendered a stable society, which should provide publicly funded schools accessible based solely on ability.[254] He initially proposed his university in a letter to Joseph Priestley in 1800[255] and, in 1819, founded the University of Virginia. He organized the state legislative campaign for its charter and, with the assistance of Edmund Bacon, purchased the location. He was the principal designer of the buildings, planned the university's curriculum, and served as the first rector upon its opening in 1825.[256]
 Jefferson was a strong disciple of Greek and Roman architectural styles, which he believed to be most representative of American democracy. Each academic unit, called a pavilion, was designed with a two-story temple front, while the library ""Rotunda"" was modeled on the Roman Pantheon. Jefferson referred to the university's grounds as the ""Academical Village"", and he reflected his educational ideas in its layout. The ten pavilions included classrooms and faculty residences; they formed a quadrangle and were connected by colonnades, behind which stood the student rooms. Gardens and vegetable plots were placed behind the pavilions and were surrounded by serpentine walls, affirming the importance of the agrarian lifestyle.[257] The university had a library rather than a church at its center, emphasizing its secular nature—controversial at the time.[258]
 When Jefferson died in 1826, James Madison replaced him as rector.[259] Jefferson bequeathed most of his reconstructed library of almost 2,000 volumes to the university.[260] Only one other ex-president has founded a university; Millard Fillmore founded the University at Buffalo in 1846.[261]
 Jefferson and John Adams became good friends in the first decades of their political careers, serving together in the Continental Congress in the 1770s and in Europe in the 1780s. The Federalist/Republican split of the 1790s divided them, however, and Adams felt betrayed by Jefferson's sponsorship of partisan attacks, such as those of James Callender. Jefferson was angered by Adams' appointment of ""midnight judges"".[262] The two men did not communicate directly for more than a decade after Jefferson succeeded Adams as president.[263] A brief correspondence took place between Abigail Adams and Jefferson after Jefferson's daughter Polly died in 1804, in an attempt at reconciliation unknown to Adams. However, an exchange of letters resumed open hostilities between Adams and Jefferson.[262]
 As early as 1809, Benjamin Rush began to prod the two through correspondence to re-establish contact.[262] In 1812, Adams wrote a short New Year's greeting to Jefferson, prompted earlier by Rush, to which Jefferson warmly responded. This initial correspondence began what historian David McCullough calls ""one of the most extraordinary correspondences in American history"".[264] Over the next 14 years, Jefferson and Adams exchanged 158 letters discussing their political differences, justifying their respective roles in events, and debating the revolution's import to the world.[265]
 When Adams died on July 4, 1826, the 50th anniversary of the Declaration of Independence, his last words were an acknowledgment of his longtime friend and rival. ""Thomas Jefferson survives"", Adams said, unaware that Jefferson had died a few hours earlier.[266][267][268]
 In 1821, at the age of 77, Jefferson began writing his Autobiography of Thomas Jefferson: 1743–1790, in which he said he sought to ""state some recollections of dates and facts concerning myself"".[269] He focused on the struggles and achievements he experienced until July 29, 1790, where the narrative stopped short.[270] He excluded his youth, emphasizing the revolutionary era. He related that his ancestors came from Wales to America in the early 17th century and settled in the western frontier of the Virginia colony, which influenced his zeal for individual and state rights. Jefferson described his father as uneducated, but with a ""strong mind and sound judgement"". He also addressed his enrollment in the College of William & Mary and his election to the Continental Congress in Philadelphia in 1775.[269]
 He expressed opposition to the idea of a privileged aristocracy made up of large landowning families partial to the King, and instead promoted ""the aristocracy of virtue and talent, which nature has wisely provided for the direction of the interests of society, & scattered with equal hand through all its conditions, was deemed essential to a well-ordered republic"".[269] The work is primarily concerned with the Declaration and reforming the government of Virginia. He used notes, letters, and documents to tell many of the stories. He suggested that this history was so rich that his personal affairs were better overlooked, but he incorporated a self-analysis using the Declaration and other patriotism.[271]
 Thomas Jefferson was a philhellene, lover of Greek culture, who sympathized with the Greek War of Independence.[272][273] He has been described as the most influential of the Founding Fathers who supported the Greek cause,[273][274] viewing it as similar to the American Revolution.[275] By 1823, Jefferson was exchanging ideas with Greek scholar Adamantios Korais.[273] Jefferson advised Korais on building the political system of Greece by using classical liberalism and examples from the American governmental system, ultimately prescribing a government akin to that of a U.S. state.[276] He also suggested the application of a classical education system for the newly founded First Hellenic Republic.[277] Jefferson's philosophical instructions were welcomed by the Greek people.[277] Korais became one of the designers of the Greek constitution and urged his associates to study Jefferson's works and other literature from the American Revolution.[277]
 In the summer of 1824, the Marquis de Lafayette accepted an invitation from President James Monroe to visit the country. Jefferson and Lafayette had not seen each other since 1789. After visits to New York, New England, and Washington, Lafayette arrived at Monticello on November 4.[256]
 Jefferson's grandson Randolph was present and recorded the reunion: ""As they approached each other, their uncertain gait quickened itself into a shuffling run, and exclaiming, 'Ah Jefferson!' 'Ah Lafayette!', they burst into tears as they fell into each other's arms."" Jefferson and Lafayette then retired to the house to reminisce.[278] The next morning Jefferson, Lafayette, and James Madison attended a tour and banquet at the University of Virginia. Jefferson had someone else read a speech he had prepared for Lafayette, as his voice was weak and could not carry. This was his last public presentation. After an 11-day visit, Lafayette bid Jefferson goodbye and departed Monticello.[279]
 Jefferson's approximately $100,000 of debt weighed heavily on his mind in his final months,[citation needed] as it became increasingly clear that he would have little to leave to his heirs. In February 1826, he successfully applied to the General Assembly to hold a public lottery as a fundraiser.[280] His health began to deteriorate in July 1825, due to a combination of rheumatism from arm and wrist injuries, and intestinal and urinary disorders.[256] By June 1826, he was confined to bed.[280] On July 3, overcome by fever, Jefferson declined an invitation to attend an anniversary celebration of the Declaration in Washington.[281]
 
During his last hours, he was accompanied by family members and friends. Jefferson died on July 4, 1826, at 12:50 p.m. at age 83, on the 50th anniversary of the adoption of the Declaration of Independence. In the moments prior to his death, Jefferson instructed his treating physician, ""No, doctor, nothing more"", refusing laudanum. But his final significant words were, ""Is it the Fourth?"" or ""This is the Fourth"".[282] When John Adams died later that same day, his last words were ""Thomas Jefferson survives"", though Adams was unaware that Jefferson had died several hours before.[283][284][285][286] The sitting president was Adams's son, John Quincy Adams, and he called the coincidence of their deaths on the nation's anniversary ""visible and palpable remarks of Divine Favor"".[287]
 Shortly after Jefferson died, attendants found a gold locket on a chain around his neck, containing a small faded blue ribbon around a lock of his wife Martha's hair.[288]
 Jefferson was interred at Monticello, under an epitaph that he wrote:
 In his advanced years, Jefferson became increasingly concerned that people would understand the principles in the Declaration of Independence, and the people responsible for writing it, and he continually defended himself as its author. He considered the document one of his greatest life achievements, in addition to authoring the Statute of Virginia for Religious Freedom and founding the University of Virginia. Absent from his epitaph were his political roles, including his presidency.[290]
 Jefferson died deeply in debt, and was unable to pass on his estate freely to his heirs.[291] He gave instructions in his will for disposal of his assets,[292] including the freeing of Sally Hemings's children;[293] but his estate, possessions, and slaves were sold at public auctions starting in 1827.[294] In 1831, Monticello was sold by Martha Jefferson Randolph and the other heirs.[295]
 Jefferson subscribed to the political ideals expounded by John Locke, Francis Bacon, and Isaac Newton, whom he considered the three greatest men who ever lived.[296][297] He was also influenced by the writings of Gibbon, Hume, Robertson, Bolingbroke, Montesquieu, and Voltaire.[298] Jefferson thought that the independent yeoman and agrarian life were ideals of republican virtues. He distrusted cities and financiers, favored decentralized government power, and believed that the tyranny that had plagued the common man in Europe was due to corrupt political establishments and monarchies. He supported efforts to disestablish the Church of England,[299] wrote the Virginia Statute for Religious Freedom, and he pressed for a wall of separation between church and state.[300] The Republicans under Jefferson were strongly influenced by the 18th-century British Whig Party, which believed in limited government.[301] His Democratic-Republican Party became dominant in early American politics, and his views became known as Jeffersonian democracy.[302][303]
 Jefferson wrote letters and speeches prolifically; these show him to be well-read in the philosophical literature of his day and of antiquity. Nevertheless, some scholars do not take Jefferson seriously as a philosopher mainly because he did not produce a formal work on philosophy. However, he has been described as one of the most outstanding philosophical figures of his time because his work provided the theoretical background to, and the substance of, the social and political events of the revolutionary years and the development of the American Constitution in the 1770s and 1780s.[304] Jefferson continued to attend to more theoretical questions of natural philosophy and subsequently left behind a rich philosophical legacy in the form of presidential messages, letters, and public papers.[305]
 Jefferson described himself as an Epicurean and, although he adopted the Stoic belief in intuition and found comfort in the Stoic emphasis on the patient endurance of misfortune, he rejected most aspects of Stoicism with the notable exception of Epictetus' works.[close paraphrasing][306][307] He rejected the Stoics' doctrine of a separable soul and their fatalism, and was angered by their misrepresentation of Epicureanism as mere hedonism.[close paraphrasing][307] Jefferson knew Epicurean philosophy from original sources, but also mentioned Pierre Gassendi's Syntagma philosophicum as influencing his ideas on Epicureanism.[308]
 According to Jefferson's philosophy, citizens have ""certain inalienable rights"" and ""rightful liberty is unobstructed action according to our will, within limits drawn around us by the equal rights of others.""[309][310]  A staunch advocate of the jury system, he proclaimed in 1801, ""I consider [trial by jury] as the only anchor yet imagined by man, by which a government can be held to the principles of its constitution.""[311] Jeffersonian government not only prohibited individuals in society from infringing on the liberty of others, but also restrained itself from diminishing individual liberty as a protection against tyranny of the majority.[312] Initially, Jefferson favored restricted voting to those who could actually have the free exercise of their reason by escaping any corrupting dependence on others. He advocated enfranchising a majority of Virginians, seeking to expand suffrage to include ""yeoman farmers"" who owned their own land while excluding tenant farmers, city day laborers, vagrants, most American Indians, and women.[313]
 He was convinced that individual liberties were the fruit of political equality, which was threatened by the arbitrary government.[314] Excesses of democracy in his view were caused by institutional corruption rather than human nature. He was less suspicious of a working democracy than many contemporaries.[313] As president, Jefferson feared that the federal system enacted by Washington and Adams had encouraged corrupting patronage and dependence. He tried to restore a balance between the state and federal governments more nearly reflecting the Articles of Confederation, seeking to reinforce state prerogatives where his party was in the majority.[313]
 According to Stanford Scholar Jack Rakove, ""[w]hen Jefferson wrote 'all men are created equal' in the preamble to the Declaration, he was not talking about individual equality. What he really meant was that the American colonists, as a people, had the same rights of self-government as other peoples, and hence could declare independence, create new governments and assume their 'separate and equal station' among other nations.""[310] Jefferson's famous mantra later became a statement ""of individual equality that everyone and every member of a deprived group could claim for himself or herself.""[310] Historian Henry Wiencek has noted Jefferson included slaves when he penned ""all men are created equal"" in the Declaration. As early as 1774, Jefferson had supported ending domestic slavery, and making slaves citizens.[315] Later, writing in Notes (1781), Jefferson supported gradual emancipation of slaves, to be sent away from the U.S. to an unspecified place. The former slaves would be replaced by white immigrant workers.[316] In 1792, Jefferson calculated that he was making a 4 percent profit every year on the birth of black children. After this he wrote that slavery presented an investment strategy for the future. Historian Brion Davis writes that Jefferson's emancipation efforts virtually ceased.[317]
 Jefferson was steeped in the Whig tradition of the oppressed majority set against a repeatedly unresponsive court party in the Parliament. He justified small outbreaks of rebellion as necessary to get monarchial regimes to amend oppressive measures compromising popular liberties. In a republican regime ruled by the majority, he acknowledged ""it will often be exercised when wrong"".[318] But ""the remedy is to set them right as to facts, pardon and pacify them.""[319] As Jefferson saw his party triumph in two terms of his presidency and launch into a third term under James Madison, his view of the U.S. as a continental republic and an ""empire of liberty"" grew more upbeat. On departing the presidency, he described America as ""trusted with the destines of this solitary republic of the world, the only monument of human rights, and the sole depository of the sacred fire of freedom and self-government"".[320]
 Jefferson was a supporter of American expansionism, writing in 1801 that ""it is impossible not to look forward to distant times when our rapid multiplication will expand itself beyond those limits, and cover the whole northern, if not the southern continent.""[321]
 Jefferson considered democracy to be the expression of society and promoted national self-determination, cultural uniformity, and education of all males of the commonwealth.[322] He supported public education and a free press as essential components of a democratic nation.[323]
 After resigning as secretary of state in 1795, Jefferson focused on the electoral bases of the Republicans and Federalists. The ""Republican"" classification for which he advocated included ""the entire body of landholders"" everywhere and ""the body of laborers"" without land.[324] Republicans united behind Jefferson as vice president, with the election of 1796 expanding democracy nationwide at grassroots levels.[325] Jefferson promoted Republican candidates for local offices.[326]
 Beginning with Jefferson's electioneering for the ""revolution of 1800"", his political efforts were based on egalitarian appeals.[327] In his later years, he referred to the 1800 election ""as real a revolution in the principles of our government as that of '76 was in its form"", one ""not effected indeed by the sword ... but by the ... suffrage of the people"".[328] Voter participation grew during Jefferson's presidency, increasing to ""unimaginable levels"" compared to the Federalist Era, with turnout of about 67,000 in 1800 rising to about 143,000 in 1804.[329]
 At the onset of the American Revolution, Jefferson accepted William Blackstone's argument that property ownership would sufficiently empower voters' independent judgement, but he sought to further expand suffrage by land distribution to the poor.[330] In the heat of the Revolutionary Era and afterward, several states expanded voter eligibility from landed gentry to all propertied male, tax-paying citizens with Jefferson's support.[331] In retirement, he gradually became critical of his home state for violating ""the principle of equal political rights""—the social right of universal male suffrage.[332] He sought a ""general suffrage"" of all taxpayers and militia-men, and equal representation by population in the General Assembly to correct preferential treatment of the slave-holding regions.[333]
 Baptized in his youth, Jefferson became a governing member of his local Episcopal Church in Charlottesville, which he later attended with his daughters.[334] Jefferson, however, spurned Biblical views of Christianity.[335] Influenced by Deist authors during his college years, Jefferson abandoned orthodox Christianity after his review of New Testament teachings.[336][337] Jefferson has sometimes been portrayed as a follower of the liberal religious strand of Deism that values reason over revelation.[338] Nonetheless, in 1803, Jefferson asserted, ""I am Christian, in the only sense in which [Jesus] wished any one to be"".[215]
 Jefferson later defined being a Christian as one who followed the simple teachings of Jesus. Influenced by Joseph Priestley,[338] Jefferson selected New Testament passages of Jesus' teachings into a private work he called The Life and Morals of Jesus of Nazareth, known today as the Jefferson Bible, which was never published during his lifetime.[339][340] Jefferson believed that Jesus' message had been obscured and corrupted by Paul the Apostle, the Gospel writers and Protestant reformers.[338] Peterson states that Jefferson was a theist ""whose God was the Creator of the universe ... all the evidences of nature testified to His perfection; and man could rely on the harmony and beneficence of His work"".[341] In a letter to John Adams, Jefferson wrote that what he believed was genuinely Christ's, found in the Gospels, was ""as easily distinguishable as diamonds in a dunghill"".[335] By omitting miracles and the resurrection, Jefferson made the figure of Jesus more compatible with a worldview based on reason.[335]
 Jefferson was firmly anticlerical, writing in ""every age, the priest has been hostile to liberty ... they have perverted the purest religion ever preached to man into mystery and jargon.""[342] The full letter to Horatio Spatford can be read at the National Archives.[343] Jefferson once supported banning clergy from public office but later relented.[344] In 1777, he drafted the Virginia Statute for Religious Freedom. Ratified in 1786, it made compelling attendance or contributions to any state-sanctioned religious establishment illegal and declared that men ""shall be free to profess ... their opinions in matters of religion"".[345] The Statute is one of only three accomplishments he chose for his epitaph.[346][347] Early in 1802, Jefferson wrote to the Danbury Connecticut Baptist Association that ""religion is a matter which lies solely between Man and his God"". He interpreted the First Amendment as having built ""a wall of separation between Church and State"".[348] The phrase 'Separation of Church and State' has been cited several times by the Supreme Court in its interpretation of the Establishment Clause.[349]
 Jefferson donated to the American Bible Society, saying the Four Evangelists delivered a ""pure and sublime system of morality"" to humanity. He thought Americans would rationally create ""Apiarian"" religion, extracting the best traditions of every denomination.[350] He contributed generously to several local denominations near Monticello.[351] Acknowledging organized religion would always be factored into political life, he encouraged reason over supernatural revelation to make inquiries into religion. He believed in a creator god, an afterlife, and the sum of religion as loving God and neighbors. But he also controversially rejected fundamental Christian beliefs, denying the conventional Christian Trinity, Jesus's divinity as the Son of God and miracles, the Resurrection of Christ, atonement from sin, and original sin.[340][352][353] Jefferson believed that original sin was a gross injustice.[340]
 Jefferson's unorthodox religious beliefs became an important issue in the 1800 presidential election.[354] Federalists attacked him as an atheist. As president, Jefferson countered the accusations by praising religion in his inaugural address and attending services at the Capitol.[354]
 In October 1765, while Jefferson was still a law student he bought a copy of the Quran from the year 1734.[355] He had the Quran shipped from England to Williamsburg, Virginia.[356] He was interested in comparative religions. Keith Ellison was sworn in on Jefferson's copy of the Quran.[357]
 Jefferson distrusted government banks and opposed public borrowing, which he thought created long-term debt, bred monopolies, and invited dangerous speculation as opposed to productive labor.[358] In one letter to Madison, he argued each generation should curtail all debt within 19 years, and not impose a long-term debt on subsequent generations.[359]
 In 1791, President Washington asked Jefferson, then secretary of state, and Hamilton, the secretary of the treasury, if the Congress had the authority to create a national bank. While Hamilton believed so, Jefferson and Madison thought a national bank would ignore the needs of individuals and farmers, and would violate the Tenth Amendment by assuming powers not granted to the federal government by the states.[360] Hamilton successfully argued that the implied powers given to the federal government in the Constitution supported the creation of a national bank, among other federal actions.
 Jefferson used agrarian resistance to banks and speculators as the first defining principle of an opposition party, recruiting candidates for Congress on the issue as early as 1792.[361] As president, Jefferson was persuaded by Secretary of the Treasury Albert Gallatin to leave the bank intact but sought to restrain its influence.[362][p]
 Scholars give radically differing interpretations on Jefferson's views and relationship with slavery.[9] Opinions range from ""emancipationists"" who view him as an early proto-abolitionist, who subsequently made pragmatic compromises with the slave power to preserve the union; to ""revisionists"", who argue that he in fact entrenched the institution in American society; with people also having more nuanced opinions, who either argue that Jefferson held inconsistent views on the institution throughout his lifetime or that both interpretations are too overly simplistic.[9]
 Jefferson lived in a planter economy largely dependent upon slavery, and as a wealthy landholder, used slave labor for his household, plantation, and workshops. He first recorded his slaveholding in 1774, when he counted 41 enslaved people.[364] Over his lifetime he enslaved about 600 people; he inherited about 175 people while most of the remainder were people born on his plantations.[365] Jefferson purchased some slaves in order to reunite their families. He sold approximately 110 people for economic reasons, primarily slaves from his outlying farms.[365][366] In 1784, when the number of people he enslaved likely was approximately 200, he began to divest himself of many slaves, and by 1794 he had divested himself of 161 individuals.[367][q]
 Approximately 100 slaves lived at Monticello at any given time. In 1817, the plantation recorded its largest slave population of 140 individuals.[368]
 Jefferson once said, ""My first wish is that the labourers may be well treated"".[365] Jefferson did not work his slaves on Sundays and Christmas and he allowed them more personal time during the winter months.[369] Some scholars doubt Jefferson's benevolence,[370] noting cases of excessive slave whippings in his absence. His nail factory was staffed only by enslaved children. Many of the enslaved boys became tradesmen. Burwell Colbert, who started his working life as a child in Monticello's Nailery, was later promoted to the supervisory position of butler.[371]
 Jefferson felt slavery was harmful to both slave and master but had reservations about releasing slaves from captivity, and advocated for gradual emancipation.[372][373][374] In 1779, he proposed gradual voluntary training and resettlement to the Virginia legislature, and three years later drafted legislation allowing slaveholders to free their own slaves.[69] In his draft of the Declaration of Independence, he included a section, stricken by other Southern delegates, criticizing King George III for supposedly forcing slavery onto the colonies.[375] In 1784, Jefferson proposed the abolition of slavery in all western U.S. territories, limiting slave importation to 15 years.[376] Congress, however, failed to pass his proposal by one vote.[376] In 1787, Congress passed the Northwest Ordinance, a partial victory for Jefferson that terminated slavery in the Northwest Territory. Jefferson freed his slave Robert Hemings in 1794 and he freed his cook slave James Hemings in 1796.[377] Jefferson freed his runaway slave Harriet Hemings in 1822. Upon his death in 1826, Jefferson freed five male Hemings slaves in his will.[378]
 During his presidency, Jefferson allowed the diffusion of slavery into the Louisiana Territory hoping to prevent slave uprisings in Virginia and to prevent South Carolina secession.[379] In 1804, in a compromise, Jefferson and Congress banned domestic slave trafficking for one year into the Louisiana Territory.[380] In 1806 he officially called for anti-slavery legislation terminating the import or export of slaves. Congress passed the law in 1807.[372][381][382]
 In 1819, Jefferson strongly opposed a Missouri statehood application amendment that banned domestic slave importation and freed slaves at the age of 25 on grounds it would destroy the union.[383] In Notes on the State of Virginia, he created controversy by calling slavery a moral evil for which the nation would ultimately have to account to God.[384] Jefferson wrote of his ""suspicion"" that Black people were mentally and physically inferior to Whites, but argued that they nonetheless had innate human rights.[372][385][386] He therefore supported colonization plans that would transport freed slaves to another country, such as Liberia or Sierra Leone, though he recognized the impracticability of such proposals.[387] According to Eric Foner, ""In 1824 Jefferson proposed that the federal government purchase and deport 'the increase of each year' (that is, children), so that the slave population would age and eventually disappear.""[388]
 During his presidency, Jefferson was for the most part publicly silent on the issue of slavery and emancipation,[389] as the Congressional debate over slavery and its extension caused a dangerous north–south rift among the states, with talk of a northern confederacy in New England.[390][r] The violent attacks on white slave owners during the Haitian Revolution due to injustices under slavery supported Jefferson's fears of a race war, increasing his reservations about promoting emancipation.[372][391] After numerous attempts and failures to bring about emancipation,[392] Jefferson wrote privately in an 1805 letter to William A. Burwell, ""I have long since given up the expectation of any early provision for the extinguishment of slavery among us."" That same year he also related this idea to George Logan, writing, ""I have most carefully avoided every public act or manifestation on that subject.""[393]
 Claims that Jefferson fathered children with his slave Sally Hemings after his wife's death have been debated since 1802. In that year James T. Callender, after being denied a position as postmaster, alleged Jefferson had taken Hemings as a concubine and fathered several children with her.[394] In 1998, a panel of researchers conducted a Y-DNA study of living descendants of Jefferson's uncle, Field, and of a descendant of Hemings's son, Eston Hemings. The results showed a match with the male Jefferson line.[395][396] Subsequently, the Thomas Jefferson Foundation (TJF) formed a nine-member research team of historians to assess the matter.[396] The TJF report concluded that ""the DNA study ... indicates a high probability that Thomas Jefferson fathered Eston Hemings"".[396][397][s] The TJF also concluded that Jefferson likely fathered all of Hemings's children listed at Monticello.[396][t]
 In July 2017, the TJF announced that archeological excavations at Monticello had revealed what they believe to have been Sally Hemings's quarters, adjacent to Jefferson's bedroom.[399][400] Since the results of the DNA tests were made public, the consensus among most historians has been that Jefferson had a sexual relationship with Sally Hemings and that he was the father of her son Eston Hemings.[401]
 Still, a minority of scholars maintain the evidence is insufficient to prove Jefferson's paternity conclusively. Based on DNA and other evidence, they note the possibility that additional Jefferson males, including his brother Randolph Jefferson and any one of Randolph's four sons, or his cousin, could have fathered Sally Hemings's children.[402] In 2002, historian Merrill Peterson said: ""in the absence of direct documentary evidence either proving or refuting the allegation, nothing conclusive can be said about Jefferson's relations with Sally Hemings.""[403] Concerning the 1998 DNA study, Peterson said that ""the results of the DNA testing of Jefferson and Hemings descendants provided support for the idea that Jefferson was the father of at least one of Sally Hemings's children"".[403]
 After Jefferson's death in 1826, although not formally manumitted, Sally Hemings was allowed by Jefferson's daughter Martha to live in Charlottesville as a free woman with her two sons until her death in 1835.[404][u] The Monticello Association refused to allow Sally Hemings' descendants the right of burial at Monticello.[406]
 Jefferson was a farmer, obsessed with new crops, soil conditions, garden designs, and scientific agricultural techniques. His main cash crop was tobacco, but its price was usually low and it was rarely profitable. He tried to achieve self-sufficiency with wheat, vegetables, flax, corn, hogs, sheep, poultry, and cattle to supply his family, slaves, and employees, but he lived perpetually beyond his means[407] and was always in debt.[408] Jefferson also planted two vineyards at Monticello and hoped to grow Vitis vinifera, the European wine grape species, to make wine, but the crop failed. His efforts were nonetheless an important contribution to the development of American viticulture.[409]
 Jefferson mastered architecture through self-study. His primary authority was Andrea Palladio's 1570 The Four Books of Architecture, which outlines the principles of classical design.[410] Jefferson helped popularize the Neo-Palladian style in the United States utilizing designs for the Virginia State Capitol, the University of Virginia, Monticello, and others.[411] It has been speculated that he was inspired by the Château de Rastignac in south-west France—the plans of which he saw during his ambassadorship—to convince the architect of the White House to modify the South Portico to resemble the château.[412]
 In the field of archaeology, in 1784, Jefferson, using the trench method, started excavating several Native American burial mounds in Virginia. His excavations were prompted by the ""Moundbuilders"" question and his careful methods allowed him to witness the stratigraphic layout, the various human remains and other artifacts inside the mound. The evidence present at the site granted him enough insight to admit that he saw no reason why the ancestors of the present-day Native Americans could not have raised those mounds.[413]
 He was interested in birds and wine, and was a noted gourmet.[414] As a naturalist, he was fascinated by the Natural Bridge geological formation, and in 1774 successfully acquired the Bridge by a grant from George III.[415]
 Jefferson was a member of the American Philosophical Society for 35 years, beginning in 1780. Through the society he advanced the sciences and Enlightenment ideals, emphasizing that knowledge of science reinforced and extended freedom.[416] His Notes on the State of Virginia was written in part as a contribution to the society.[417] He became the society's third president on March 3, 1797, a few months after he was elected Vice President of the United States.[417][418] In accepting, Jefferson stated: ""I feel no qualification for this distinguished post but a sincere zeal for all the objects of our institution and an ardent desire to see knowledge so disseminated through the mass of mankind that it may at length reach even the extremes of society, beggars and kings.""[416]
 On March 10, 1797, Thomas Jefferson gave a lecture, later published as a paper in 1799, which reported on the skeletal remains of an extinct large sloth, which he named Megalonyx, unearthed by saltpeter workers from a cave in what is now Monroe County, West Virginia.[419][420] Jefferson is considered to be a pioneer of scientific paleontology research in North America.[421]
 Jefferson served as APS president for the next eighteen years, including through both terms of his presidency.[417] He introduced Meriwether Lewis to the society, where various scientists tutored him in preparation for the Lewis and Clark Expedition.[417][422] He resigned on January 20, 1815, but remained active through correspondence.[423]
 Jefferson had a lifelong interest in linguistics, and could speak, read, and write in a number of languages, including French, Greek, Italian, and German. In his early years, he excelled in classical languages.[424][425] Jefferson later came to regard Greek as the ""perfect language"" as expressed in its laws and philosophy.[426] While attending the College of William & Mary, he taught himself Italian.[427] Here Jefferson first became familiar with the Anglo-Saxon language, studying it in a linguistic and philosophical capacity. He owned 17 volumes of Anglo-Saxon texts and grammar and later wrote an essay on the Anglo-Saxon language.[424] Jefferson claimed to have taught himself Spanish during his nineteen-day journey to France, using only a grammar guide and a copy of Don Quixote.[428]
 Linguistics played a significant role in how Jefferson modeled and expressed political and philosophical ideas. He believed that the study of ancient languages was essential in understanding the roots of modern language.[429] Jefferson criticized language purists and supported the introduction of neologisms to English, foreseeing the emergence of ""an American dialect"". He described the Académie Française, a body designated to regulate the French language, as an ""endeavor to arrest the progress of their language"".[430] 
 He collected and understood a number of American Indian vocabularies and instructed Lewis and Clark to record and collect various Indian languages during their Expedition.[431] When Jefferson moved from Washington after his presidency, he took 50 Native American vocabulary lists back to Monticello along with the rest of his possessions. Somewhere along the journey, a thief stole the heavy chest, thinking it was full of valuables, but its contents were dumped into the James River when the thief discovered it was only filled with papers. Thirty years of collecting were lost, with only a few fragments rescued from the muddy banks of the river.[432]
 Jefferson was not an outstanding orator and preferred to communicate through writing or remain silent if possible. Instead of delivering his State of the Union addresses himself, Jefferson wrote the annual messages and sent a representative to read them aloud in Congress. This started a tradition that continued until 1913 when President Woodrow Wilson chose to deliver his own State of the Union address.[433]
 Jefferson invented many small practical devices and improved contemporary inventions, including a revolving book-stand and a ""Great Clock"" powered by the gravitational pull on cannonballs. He improved the pedometer, the polygraph (a device for duplicating writing),[434] and the moldboard plow, an idea he never patented and gave to posterity.[435] Jefferson can also be credited as the creator of the swivel chair, the first of which he created and used to write much of the Declaration of Independence.[436] He first opposed patents but later supported them. In 1790–1793, as Secretary of State, he was the ex officio head of the three-person patent review board. He drafted reforms of US patent law which led to him being relieved of this duty in 1793, and also drastically changed the patent system.[437]
 As Minister to France, Jefferson was impressed by the military standardization program known as the Système Gribeauval, and initiated a program as president to develop interchangeable parts for firearms. For his inventiveness and ingenuity, he received an honorary Doctor of Law degree from Harvard University.[438]
 Jefferson is seen as an icon of individual liberty, democracy, and republicanism, hailed as the author of the Declaration of Independence, an architect of the American Revolution, and a renaissance man who promoted science and scholarship.[439] The participatory democracy and expanded suffrage he championed defined his era and became a standard for later generations.[440] Meacham opined that Jefferson was the most influential figure of the democratic republic in its first half-century, succeeded by presidential adherents James Madison, James Monroe, Andrew Jackson, and Martin Van Buren.[441] The Siena Research Institute poll of presidential scholars, begun in 1982, has consistently ranked Jefferson as one of the five best U.S. presidents,[442] and a 2015 Brookings Institution poll of American Political Science Association members ranked him as the fifth greatest president.[443]
 Jefferson has been memorialized with buildings, sculptures, postage, and currency. In the 1920s, Jefferson, together with George Washington, Theodore Roosevelt, and Abraham Lincoln, was chosen by sculptor Gutzon Borglum and approved by President Calvin Coolidge to be depicted in a stone national memorial at Mount Rushmore in the Black Hills of South Dakota.[444]
 The Jefferson Memorial was dedicated in Washington, D.C., in 1943, on the 200th anniversary of Jefferson's birth. The interior of the memorial includes a 19-foot (6 m) statue of Jefferson by Rudulph Evans and engravings of passages from Jefferson's writings. Most prominent among these passages are the words inscribed around the Jefferson Memorial: ""I have sworn upon the altar of God eternal hostility against every form of tyranny over the mind of man"", a quote from Jefferson's September 23, 1800, letter to Benjamin Rush.[445]
 In October 2021, in response to lobbying, the New York City Public Design Commission voted unanimously to remove the plaster model of the statue of Jefferson that currently stands in the United States Capitol rotunda from the chamber of the New York City Council, where it had been for more than a century, due to him fathering children with people he enslaved.[446] The statue was taken down the next month.[447]
"
Alexander Hamilton,https://en.wikipedia.org/wiki/Alexander_Hamilton,"



 Alexander Hamilton (January 11, 1755 or 1757[a] – July 12, 1804) was an American military officer, statesman, and Founding Father who served as the first U.S. secretary of the treasury from 1789 to 1795 during George Washington's presidency.
 Born out of wedlock in Charlestown, Nevis, Hamilton was orphaned as a child and taken in by a prosperous merchant. He pursued his education in New York City where, despite his young age, he was a prolific and widely read pamphleteer advocating for the American revolutionary cause, though an anonymous one. He then served as an artillery officer in the American Revolutionary War, where he saw military action against the British in the New York and New Jersey campaign, served for years as an aide to General George Washington, and helped secure American victory at the climactic Siege of Yorktown. After the Revolutionary War, Hamilton served as a delegate from New York to the Congress of the Confederation in Philadelphia. He resigned to practice law and founded the Bank of New York. In 1786, Hamilton led the Annapolis Convention to discuss issues that arose under the Articles of Confederation. The next year he was a delegate to the Philadelphia Convention that drafted the Constitution of the United States, which he then helped ratify by writing 51 of the 85 installments of The Federalist Papers.
 As a trusted member of President Washington's first cabinet, Hamilton served as the first U.S. secretary of the treasury. He envisioned a central government led by an energetic executive, a strong national defense, and a more diversified economy that significantly expanded industry. He successfully argued that the implied powers of the Constitution provided the legal authority to fund the national debt, assume the states' debts, and create the First Bank of the United States, which was funded by a tariff on imports and a whiskey tax. He opposed American entanglement with the succession of unstable French Revolutionary governments and advocated in support of the Jay Treaty under which the U.S. resumed friendly trade relations with the British Empire. He also persuaded Congress to establish the Revenue Cutter Service. Hamilton's views became the basis for the Federalist Party, which was opposed by the Democratic-Republican Party led by Thomas Jefferson. Hamilton and other Federalists supported the Haitian Revolution, and Hamilton helped draft the constitution of Haiti.
 After resigning as Secretary of the Treasury, Hamilton resumed his legal and business activities. He was a leader in the abolition of the international slave trade. In the Quasi-War, Hamilton called for mobilization against France, and President John Adams appointed him major general. The army, however, did not see combat. Outraged by Adams' response to the crisis, Hamilton opposed his reelection campaign. Jefferson and Aaron Burr tied for the presidency in the electoral college and, despite philosophical differences, Hamilton endorsed Jefferson over Burr, whom he found unprincipled. When Burr ran for governor of New York in 1804, Hamilton again campaigned against him, arguing that he was unworthy. Taking offense, Burr challenged Hamilton to a pistol duel, taking place in Weehawken, New Jersey, on July 11, 1804. Hamilton was fatally wounded, and then was immediately transported in a delirious state back across the Hudson River to the home of William Bayard Jr. in Greenwich Village, New York for medical attention, but succumbed to his wounds the following day.
 Scholars generally regard Hamilton as an astute and intellectually brilliant administrator, politician, and financier who was sometimes impetuous. His ideas are credited with laying the foundation for American finance and government. British historian Paul Johnson stated that Hamilton was a ""genius—the only one of the Founding Fathers fully entitled to that accolade—and he had the elusive, indefinable characteristics of genius.""[6] 
 Hamilton was born on January 11, 1755 or 1757,[a] in Charlestown, the capital of Nevis in the British Leeward Islands, where he spent the early part of his childhood. Hamilton and his older brother, James Jr.,[7] were born out of wedlock to Rachel Lavien (née Faucette),[b] a married woman of half-British and half-French Huguenot descent,[c][16] and James A. Hamilton, a Scotsman who was the fourth son of Alexander Hamilton, the laird of Grange, Ayrshire.[17]
 Rachel Lavien had married on Saint Croix[18] but left her husband and first son in 1750, traveling to Saint Kitts where she met James Hamilton.[18] Hamilton and Lavien moved together to Nevis, her birthplace, where she had inherited a seaside lot in town from her father.[2] While their mother was living, Alexander and James Jr. received individual tutoring[2] and classes in a private school led by a Jewish headmistress.[19] Alexander supplemented his education with a family library of 34 books.[20]
 James Hamilton later abandoned Rachel Lavien and their two sons, ostensively to ""spar[e] [her] a charge of bigamy...after finding out that her first husband intend[ed] to divorce her under Danish law on grounds of adultery and desertion.""[17] Lavien then moved with their two children to Saint Croix, where she supported them by managing a small store in Christiansted. Both his mother and Hamilton contracted yellow fever, and it killed her on February 19, 1768, leaving him effectively orphaned.[21] His mother’s death may have had a severe emotional impact on Hamilton.[22] In probate court, Lavien's ""first husband seized her estate""[17] and obtained the few valuables that she had owned, including some household silver. Many items were auctioned off, but a friend purchased the family's books and returned them to Hamilton.[23]
 The brothers were briefly taken in by their cousin Peter Lytton. However, Lytton took his own life in July 1769, leaving his property to his mistress and their son, and the propertyless Hamilton brothers were subsequently separated.[23] James Jr. apprenticed with a local carpenter, while Alexander was given a home by Thomas Stevens, a merchant from Nevis.[24]
 Hamilton became a clerk at Beekman and Cruger, a local import-export firm that traded with the Province of New York and New England.[25] Though still a teenager, Hamilton proved capable enough as a trader to be left in charge of the firm for five months in 1771 while the owner was at sea.[26] He remained an avid reader, and later developed an interest in writing and a life outside Saint Croix. He wrote a detailed letter to his father regarding a hurricane that devastated Christiansted on August 30, 1772.[27] The Presbyterian Reverend Hugh Knox, a tutor and mentor to Hamilton, submitted the letter for publication in the Royal Danish-American Gazette. Biographer Ron Chernow found the letter astounding because ""for all its bombastic excesses, it does seem wondrous [that a] self-educated clerk could write with such verve and gusto"" and that a teenage boy produced an apocalyptic ""fire-and-brimstone sermon"" viewing the hurricane as a ""divine rebuke to human vanity and pomposity.""[28] The essay impressed community leaders, who collected a fund to send Hamilton to the North American colonies for his education.[29]
 In October 1772, Hamilton arrived by ship in Boston and proceeded to New York City, where he took lodgings with the Irish-born Hercules Mulligan, brother of a trader known to Hamilton's benefactors, who assisted Hamilton in selling cargo that was used to pay for his education and support.[30][31] Later that year, in preparation for college, Hamilton began to fill gaps in his education at the Elizabethtown Academy, a preparatory school run by Francis Barber in Elizabeth, New Jersey. While there, he came under the influence of William Livingston, a local leading intellectual and revolutionary with whom he lived for a time.[32][33][34]
 Hamilton entered Mulligan's alma mater King's College in New York City (now Columbia University) as a private student in the autumn of 1773, while again boarding with Mulligan until officially matriculating in May 1774.[35] His college roommate and lifelong friend Robert Troup spoke glowingly of Hamilton's clarity in concisely explaining the patriots' case against the British in what is credited as Hamilton's first public appearance on July 6, 1773.[36] As King's College students, Hamilton, Troup, and four other undergraduates formed an unnamed literary society that is regarded as a precursor of the Philolexian Society.[37][38]
 In 1774, Church of England clergyman Samuel Seabury in New York published a series of pamphlets promoting the Loyalist cause and Hamilton responded anonymously to it, with his first published political writings, A Full Vindication of the Measures of Congress and The Farmer Refuted. Seabury essentially tried to provoke fear in the colonies with an objective of preventing the colonies from uniting against the British.[39] Hamilton published two additional pieces attacking the Quebec Act,[40] and may have also authored the 15 anonymous installments of ""The Monitor"" for Holt's New York Journal.[41] Hamilton was a supporter of the Revolutionary cause before the war began, although he did not approve of mob reprisals against Loyalists. On May 10, 1775, Hamilton won credit for saving his college's president, Loyalist Myles Cooper, from an angry mob by speaking to the crowd long enough to allow Cooper to escape.[42] Hamilton was forced to discontinue his studies before graduating when the college closed its doors during the British occupation of New York City.[43]
 In 1775, after the first engagement of American patriot troops with the British at Lexington and Concord, Hamilton and other King's College students joined a New York volunteer militia company called the Corsicans, whose name reflected the Corsican Republic that was suppressed six years earlier and young American patriots regarded as a political model to be emulated.[44]
 Hamilton drilled with the company before classes in the graveyard of nearby St. Paul's Chapel. He studied military history and tactics on his own and was soon recommended for promotion.[45] Under fire from HMS Asia, and coordinating with Hercules Mulligan and the Sons of Liberty, he led his newly renamed unit the ""Hearts of Oak"" on a successful raid for British cannons in the Battery. The seizure of the cannons resulted in the unit being re-designated an artillery company.[46]: 13 
 Through his connections with influential New York patriots, including Alexander McDougall and John Jay, Hamilton raised the New York Provincial Company of Artillery of 60 men in 1776, and was elected captain.[47] The company took part in the campaign of 1776 in and around New York City; as rearguard of the Continental Army's retreat up Manhattan, serving at the Battle of Harlem Heights shortly after, and at the Battle of White Plains a month later. At the Battle of Trenton, the company was stationed at the high point of Trenton at the intersection of present-day Warren and Broad streets to keep the Hessians pinned in their Trenton barracks.[48][49]
 Hamilton participated in the Battle of Princeton on January 3, 1777. After an initial setback, Washington rallied the Continental Army troops and led them in a successful charge against the British forces. After making a brief stand, the British fell back, some leaving Princeton, and others taking up refuge in Nassau Hall. Hamilton transported three cannons to the hall, and had them fire upon the building as others rushed the front door and broke it down. The British subsequently put a white flag outside one of the windows;[49] 194 British soldiers walked out of the building and laid down their arms, ending the battle in an American victory.[50]
 Hamilton was invited to become an aide to Continental Army general William Alexander, Lord Stirling, and another general, perhaps Nathanael Greene or Alexander McDougall.[51] He declined these invitations, believing his best chance for improving his station in life was glory on the Revolutionary War's battlefields. Hamilton eventually received an invitation he felt he could not refuse: to serve as George Washington's aide with the rank of lieutenant colonel.[52] Washington believed that ""Aides de camp are persons in whom entire confidence must be placed and it requires men of abilities to execute the duties with propriety and dispatch.""[53]
 Hamilton served four years as Washington's chief staff aide. He handled letters to the Continental Congress, state governors, and the most powerful generals of the Continental Army. He drafted many of Washington's orders and letters under Washington's direction, and he eventually issued orders on Washington's behalf over his own signature.[54] Hamilton was involved in a wide variety of high-level duties, including intelligence, diplomacy, and negotiation with senior army officers as Washington's emissary.[55][56]
 While stationed at the army's winter headquarters in Morristown, New Jersey from December 1779 to March 1780, Hamilton met Elizabeth Schuyler, a daughter of General Philip Schuyler and Catherine Van Rensselaer. They married on December 14, 1780, at the Schuyler Mansion in Albany, New York.[57] They had eight children, Philip,[58] Angelica, Alexander, James,[59] John, William, Eliza, and another Philip.[60]
 During the Revolutionary War, Hamilton became the close friend of several fellow officers. His letters to the Marquis de Lafayette[61] and to John Laurens, employing the sentimental literary conventions of the late 18th century and alluding to Greek history and mythology,[62] have been read by Jonathan Ned Katz as revelatory of a homosocial or even homosexual relationship.[63] Biographer Gregory D. Massey amongst others, by contrast, dismisses all such speculation as unsubstantiated, describing their friendship as purely platonic camaraderie instead and placing their correspondence in the context of the flowery diction of the time.[64]
 While on Washington's staff, Hamilton long sought command and a return to active combat. As the war drew nearer to an end, he knew that opportunities for military glory were diminishing. On February 15, 1781, Hamilton was reprimanded by Washington after a minor misunderstanding. Although Washington quickly tried to mend their relationship, Hamilton insisted on leaving his staff.[65] He officially left in March, and settled with his new wife Elizabeth Schuyler close to Washington's headquarters. He continued to repeatedly ask Washington and others for a field command. Washington continued to demur, citing the need to appoint men of higher rank. This continued until early July 1781, when Hamilton submitted a letter to Washington with his commission enclosed, ""thus tacitly threatening to resign if he didn't get his desired command.""[66]
 On July 31, Washington relented and assigned Hamilton as commander of a battalion of light infantry companies of the 1st and 2nd New York Regiments and two provisional companies from Connecticut.[67] In the planning for the assault on Yorktown, Hamilton was given command of three battalions, which were to fight in conjunction with the allied French troops in taking Redoubts No. 9 and No. 10 of the British fortifications at Yorktown. Hamilton and his battalions took Redoubt No. 10 with bayonets alone so as not to risk accidental gunfire and discovery in a nighttime action, as planned. The French also suffered heavy casualties and took Redoubt No. 9. These actions forced the British surrender of an entire army at Yorktown, marking the de facto end of the war, although small battles continued for two more years until the signing of the Treaty of Paris and the departure of the last British troops.[68][69]
 After Yorktown, Hamilton returned to New York City and resigned his commission in March 1782. He passed the bar in July after six months of self-directed education and, in October, was licensed to argue cases before the Supreme Court of New York.[70] He also accepted an offer from Robert Morris to become receiver of continental taxes for the New York state.[71] Hamilton was appointed in July 1782 to the Congress of the Confederation as a New York representative for the term beginning in November 1782.[72] Before his appointment to Congress in 1782, Hamilton was already sharing his criticisms of Congress. He expressed these criticisms in his letter to James Duane dated September 3, 1780: ""The fundamental defect is a want of power in Congress ... the confederation itself is defective and requires to be altered; it is neither fit for war, nor peace.""[73]
 While on Washington's staff, Hamilton had become frustrated with the decentralized nature of the wartime Continental Congress, particularly its dependence upon the states for voluntary financial support that was not often forthcoming. Under the Articles of Confederation, Congress had no power to collect taxes or to demand money from the states. This lack of a stable source of funding had made it difficult for the Continental Army both to obtain its necessary provisions and to pay its soldiers. During the war, and for some time after, Congress obtained what funds it could from subsidies from the King of France, European loans, and aid requested from the several states, which were often unable or unwilling to contribute.[74]
 An amendment to the Articles had been proposed by Thomas Burke, in February 1781, to give Congress the power to collect a five percent impost, or duty on all imports, but this required ratification by all states; securing its passage as law proved impossible after it was rejected by Rhode Island in November 1782. James Madison joined Hamilton in influencing Congress to send a delegation to persuade Rhode Island to change its mind. Their report recommending the delegation argued the national government needed not just some level of financial autonomy, but also the ability to make laws that superseded those of the individual states. Hamilton transmitted a letter arguing that Congress already had the power to tax, since it had the power to fix the sums due from the several states; but Virginia's rescission of its own ratification of this amendment ended the Rhode Island negotiations.[75][76]
 While Hamilton was in Congress, discontented soldiers began to pose a danger to the young United States. Most of the army was then posted at Newburgh, New York. Those in the army were funding much of their own supplies, and they had not been paid in eight months. Furthermore, after Valley Forge, the Continental officers had been promised in May 1778 a pension of half their pay when they were discharged.[77] By the early 1780s, due to the structure of the government under the Articles of Confederation, it had no power to tax to either raise revenue or pay its soldiers.[78] In 1782, after several months without pay, a group of officers organized to send a delegation to lobby Congress, led by Captain Alexander McDougall. The officers had three demands: the army's pay, their own pensions, and commutation of those pensions into a lump-sum payment if Congress were unable to afford the half-salary pensions for life. Congress rejected the proposal.[78]
 Several congressmen, including Hamilton, Robert Morris, and Gouverneur Morris, attempted to use the so-called Newburgh Conspiracy as leverage to secure support from the states and in Congress for funding of the national government. They encouraged MacDougall to continue his aggressive approach, implying unknown consequences if their demands were not met, and defeated proposals designed to end the crisis without establishing general taxation: that the states assume the debt to the army, or that an impost be established dedicated to the sole purpose of paying that debt.[79]
 Hamilton suggested using the Army's claims to prevail upon the states for the proposed national funding system.[80] The Morrises and Hamilton contacted General Henry Knox to suggest he and the officers defy civil authority, at least by not disbanding if the army were not satisfied. Hamilton wrote Washington to suggest that Hamilton covertly ""take direction"" of the officers' efforts to secure redress, to secure continental funding but keep the army within the limits of moderation.[81][82] Washington wrote Hamilton back, declining to introduce the army.[83] After the crisis had ended, Washington warned of the dangers of using the army as leverage to gain support for the national funding plan.[81][84]
 On March 15, Washington defused the Newburgh situation by addressing the officers personally.[79] Congress ordered the Army officially disbanded in April 1783. In the same month, Congress passed a new measure for a 25-year impost—which Hamilton voted against[85]—that again required the consent of all the states; it also approved a commutation of the officers' pensions to five years of full pay. Rhode Island again opposed these provisions, and Hamilton's robust assertions of national prerogatives in his previous letter were widely held to be excessive.[86]
 In June 1783, a different group of disgruntled soldiers from Lancaster, Pennsylvania, sent Congress a petition demanding their back pay. When they began to march toward Philadelphia, Congress charged Hamilton and two others with intercepting the mob.[81] Hamilton requested militia from Pennsylvania's Supreme Executive Council, but was turned down. Hamilton instructed Assistant Secretary of War William Jackson to intercept the men. Jackson was unsuccessful. The mob arrived in Philadelphia, and the soldiers proceeded to harangue Congress for their pay. Hamilton argued that Congress ought to adjourn to Princeton, New Jersey. Congress agreed, and relocated there.[87] Frustrated with the weakness of the national government, Hamilton while in Princeton, drafted a call to revise the Articles of Confederation. This resolution contained many features of the future Constitution of the United States, including a strong federal government with the ability to collect taxes and raise an army. It also included the separation of powers into the legislative, executive, and judicial branches.[87]
 Hamilton resigned from Congress in 1783.[88] When the British left New York in 1783, he practiced there in partnership with Richard Harison. He specialized in defending Tories and British subjects, as in Rutgers v. Waddington, in which he defeated a claim for damages done to a brewery by the Englishmen who held it during the military occupation of New York. He pleaded for the mayor's court to interpret state law consistent with the 1783 Treaty of Paris, which had ended the Revolutionary War.[89][46]: 64–69  In 1784, Hamilton founded the Bank of New York.[90]
 Long dissatisfied with the Articles of Confederation as too weak to be effective, Hamilton played a major leadership role at the 1786 Annapolis Convention. He drafted its resolution for a constitutional convention, and in doing so brought one step closer to reality his longtime desire to have a more effectual, more financially self-sufficient federal government.[91]
 As a member of the legislature of New York, Hamilton argued forcefully and at length in favor of a bill to recognize the sovereignty of the State of Vermont, against numerous objections to its constitutionality and policy. Consideration of the bill was deferred to a later date. From 1787 to 1789, Hamilton exchanged letters with Nathaniel Chipman, a lawyer representing Vermont. After the Constitution of the United States went into effect, Hamilton said, ""One of the first subjects of deliberation with the new Congress will be the independence of Kentucky, for which the southern states will be anxious. The northern will be glad to send a counterpoise in Vermont.""[92] Vermont was admitted to the Union in 1791.[93]
 In 1788, he was awarded a Master of Arts degree from his alma mater, the former King's College, now reconstituted as Columbia College.[94] It was during this post-war period that Hamilton served on the college's board of trustees, playing a part in the reopening of the college and placing it on firm financial footing.[95]
 In 1787, Hamilton served as assemblyman from New York County in the New York State Legislature and was chosen as a delegate at the Constitutional Convention in Philadelphia by his father-in-law Philip Schuyler.[96]: 191 [97] Even though Hamilton had been a leader in calling for a new Constitutional Convention, his direct influence at the Convention itself was quite limited. Governor George Clinton's faction in the New York legislature had chosen New York's other two delegates, John Lansing Jr. and Robert Yates, and both of them opposed Hamilton's goal of a strong national government.[98][99] Thus, whenever the other two members of the New York delegation were present, they decided New York's vote, to ensure that there were no major alterations to the Articles of Confederation.[96]: 195 
 Early in the convention, Hamilton made a speech proposing a president-for-life; it had no effect upon the deliberations of the convention. He proposed to have an elected president and elected senators who would serve for life, contingent upon ""good behavior"" and subject to removal for corruption or abuse; this idea contributed later to the hostile view of Hamilton as a monarchist sympathizer, held by James Madison.[100] According to Madison's notes, Hamilton said in regards to the executive, ""The English model was the only good one on this subject. The hereditary interest of the king was so interwoven with that of the nation, and his personal emoluments so great, that he was placed above the danger of being corrupted from abroad... Let one executive be appointed for life who dares execute his powers.""[101]
 Hamilton argued, ""And let me observe that an executive is less dangerous to the liberties of the people when in office during life than for seven years. It may be said this constitutes as an elective monarchy ... But by making the executive subject to impeachment, the term 'monarchy' cannot apply ...""[101] In his notes of the convention, Madison interpreted Hamilton's proposal as claiming power for the ""rich and well born"". Madison's perspective all but isolated Hamilton from his fellow delegates and others who felt they did not reflect the ideas of revolution and liberty.[102]
 During the convention, Hamilton constructed a draft for the Constitution based on the convention debates, but he never presented it. This draft had most of the features of the actual Constitution. In this draft, the Senate was to be elected in proportion to the population, being two-fifths the size of the House, and the president and senators were to be elected through complex multistage elections, in which chosen electors would elect smaller bodies of electors; they would hold office for life, but were removable for misconduct. The president would have an absolute veto. The Supreme Court was to have immediate jurisdiction over all lawsuits involving the United States, and state governors were to be appointed by the federal government.[103]
 At the end of the convention, Hamilton was still not content with the final Constitution, but signed it anyway as a vast improvement over the Articles of Confederation, and urged his fellow delegates to do so also.[104] Since the other two members of the New York delegation, Lansing and Yates, had already withdrawn, Hamilton was the only New York signer to the United States Constitution.[96]: 206  He then took a highly active part in the successful campaign for the document's ratification in New York in 1788, which was a crucial step in its national ratification. He first used the popularity of the Constitution by the masses to compel George Clinton to sign, but was unsuccessful. The state convention in Poughkeepsie in June 1788 pitted Hamilton, Jay, James Duane, Robert Livingston, and Richard Morris against the Clintonian faction led by Melancton Smith, Lansing, Yates, and Gilbert Livingston.[105]
 Clinton's faction wanted to amend the Constitution, while maintaining the state's right to secede if their attempts failed, and members of Hamilton's faction were against any conditional ratification, under the impression that New York would not be accepted into the Union. During the state convention, New Hampshire and Virginia becoming the ninth and tenth states to ratify the Constitution, respectively, had ensured any adjournment would not happen and a compromise would have to be reached.[105][106] Hamilton's arguments used for the ratifications were largely iterations of work from The Federalist Papers, and Smith eventually went for ratification, though it was more out of necessity than Hamilton's rhetoric.[106] The vote in the state convention was ratified 30 to 27, on July 26, 1788.[107]
 Hamilton recruited John Jay and James Madison to write The Federalist Papers, a series of essays, to defend the proposed Constitution. He made the largest contribution to that effort, writing 51 of the 85 essays published. Hamilton supervised the entire project, enlisted the participants, wrote the majority of the essays, and oversaw the publication. During the project, each person was responsible for their areas of expertise. Jay covered foreign relations. Madison covered the history of republics and confederacies, along with the anatomy of the new government. Hamilton covered the branches of government most pertinent to him: the executive and judicial branches, with some aspects of the Senate, as well as covering military matters and taxation.[108] The papers first appeared in The Independent Journal on October 27, 1787.[108]
 Hamilton wrote the first paper signed as Publius, and all of the subsequent papers were signed under the name.[96]: 210  Jay wrote the next four papers to elaborate on the confederation's weakness and the need for unity against foreign aggression and against splitting into rival confederacies, and, except for No. 64, was not further involved.[109][96]: 211  Hamilton's highlights included discussion that although republics have been culpable for disorders in the past, advances in the ""science of politics"" had fostered principles that ensured that those abuses could be prevented, such as the division of powers, legislative checks and balances, an independent judiciary, and legislators that were represented by electors (No. 7–9).[109] Hamilton also wrote an extensive defense of the constitution (No. 23–36), and discussed the Senate and executive and judicial branches (No. 65–85). Hamilton and Madison worked to describe the anarchic state of the confederation (No. 15–22), and the two have been described as not being significantly different in thought during this time period—in contrast to their stark opposition later in life.[109] Subtle differences appeared with the two when discussing the necessity of standing armies.[109]
 In 1789, Washington—who had become the first president of the United States—appointed Hamilton to be his cabinet's Secretary of the Treasury on the advice of Robert Morris, Washington's initial pick.[110] On September 11, 1789, Hamilton was nominated and confirmed in the Senate[111] and sworn in the same day as the first United States Secretary of the Treasury.[112]
 Before the adjournment of the House in September 1789, they requested Hamilton to make a report on suggestions to improve the public credit by January 1790.[113] Hamilton had written to Morris as early as 1781, that fixing the public credit will win their objective of independence.[113] The sources that Hamilton used ranged from Frenchmen such as Jacques Necker and Montesquieu to British writers such as Hume, Hobbes, and Malachy Postlethwayt.[114] While writing the report he also sought out suggestions from contemporaries such as John Witherspoon and Madison. Although they agreed on additional taxes such as distilleries and duties on imported liquors and land taxes, Madison feared that the securities from the government debt would fall into foreign hands.[115][96]: 244–245 
 In the report, Hamilton felt that the securities should be paid at full value to their legitimate owners, including those who took the financial risk of buying government bonds that most experts thought would never be redeemed. He argued that liberty and property security were inseparable, and that the government should honor the contracts, as they formed the basis of public and private morality. To Hamilton, the proper handling of the government debt would also allow America to borrow at affordable interest rates and would also be a stimulant to the economy.[114]
 Hamilton divided the debt into national and state, and further divided the national debt into foreign and domestic debt. While there was agreement on how to handle the foreign debt, especially with France, there was not with regards to the national debt held by domestic creditors. During the Revolutionary War, affluent citizens had invested in bonds, and war veterans had been paid with promissory notes and IOUs that plummeted in price during the Confederation. In response, the war veterans sold the securities to speculators for as little as fifteen to twenty cents on the dollar.[114][116]
 Hamilton felt the money from the bonds should not go to the soldiers who had shown little faith in the country's future, but the speculators that had bought the bonds from the soldiers. The process of attempting to track down the original bondholders along with the government showing discrimination among the classes of holders if the war veterans were to be compensated also weighed in as factors for Hamilton. As for the state debts, Hamilton suggested consolidating them with the national debt and label it as federal debt, for the sake of efficiency on a national scale.[114]
 The last portion of the report dealt with eliminating the debt by utilizing a sinking fund that would retire five percent of the debt annually until it was paid off. Due to the bonds being traded well below their face value, the purchases would benefit the government as the securities rose in price.[117]: 300  When the report was submitted to the House of Representatives, detractors soon began to speak against it. Some of the negative views expressed in the House were that the notion of programs that resembled British practice were wicked, and that the balance of power would be shifted away from the representatives to the executive branch. William Maclay suspected that several congressmen were involved in government securities, seeing Congress in an unholy league with New York speculators.[117]: 302  Congressman James Jackson also spoke against New York, with allegations of speculators attempting to swindle those who had not yet heard about Hamilton's report.[117]: 303 
 The involvement of those in Hamilton's circle such as Schuyler, William Duer, James Duane, Gouverneur Morris, and Rufus King as speculators was not favorable to those against the report, either, though Hamilton personally did not own or deal a share in the debt.[117]: 304 [96]: 250  Madison eventually spoke against it by February 1790. Although he was not against current holders of government debt to profit, he wanted the windfall to go to the original holders. Madison did not feel that the original holders had lost faith in the government but sold their securities out of desperation.[117]: 305  The compromise was seen as egregious to both Hamiltonians and their dissidents such as Maclay, and Madison's vote was defeated 36 votes to 13 on February 22.[117]: 305 [96]: 255 
 The fight for the national government to assume state debt was a longer issue and lasted over four months. During the period, the resources that Hamilton was to apply to the payment of state debts was requested by Alexander White, and was rejected due to Hamilton's not being able to prepare information by March 3, and was even postponed by his own supporters in spite of configuring a report the next day, which consisted of a series of additional duties to meet the interest on the state debts.[96]: 297–298  Duer resigned as Assistant Secretary of the Treasury, and the vote of assumption was voted down 31 votes to 29 on April 12.[96]: 258–259 
 During this period, Hamilton bypassed the rising issue of slavery in Congress, after Quakers petitioned for its abolition, returning to the issue the following year.[118]
 Another issue in which Hamilton played a role was the temporary location of the capital from New York City. Tench Coxe was sent to speak to Maclay to bargain about the capital being temporarily located to Philadelphia, as a single vote in the Senate was needed and five in the House for the bill to pass.[96]: 263  Thomas Jefferson wrote years afterward that Hamilton had a discussion with him, around this time period, about the capital of the United States being relocated to Virginia by means of a ""pill"" that ""would be peculiarly bitter to the Southern States, and that some concomitant measure should be adopted to sweeten it a little to them"".[96]: 263  The bill passed in the Senate on July 21 and in the House 34 votes to 28 on July 26, 1790.[96]: 263 
 Hamilton's Report on a National Bank was a projection from the first Report on the Public Credit. Although Hamilton had been forming ideas of a national bank as early as 1779,[96]: 268  he had gathered ideas in various ways over the past eleven years. These included theories from Adam Smith,[119] extensive studies on the Bank of England, the blunders of the Bank of North America and his experience in establishing the Bank of New York.[120] He also used American records from James Wilson, Pelatiah Webster, Gouverneur Morris, and from his assistant treasury secretary Tench Coxe.[120] He thought that this plan for a National Bank could help in any sort of financial crisis.[121]
 Hamilton suggested that Congress should charter the national bank with a capitalization of $10 million, one-fifth of which would be handled by the government. Since the government did not have the money, it would borrow the money from the bank itself, and repay the loan in ten even annual installments.[46]: 194  The rest was to be available to individual investors.[122] The bank was to be governed by a twenty-five-member board of directors that was to represent a large majority of the private shareholders, which Hamilton considered essential for his being under a private direction.[96]: 268  Hamilton's bank model had many similarities to that of the Bank of England, except Hamilton wanted to exclude the government from being involved in public debt, but provide a large, firm, and elastic money supply for the functioning of normal businesses and usual economic development, among other differences.[46]: 194–195  The tax revenue to initiate the bank was the same as he had previously proposed, increases on imported spirits: rum, liquor, and whiskey.[46]: 195–196 
 The bill passed through the Senate practically without a problem, but objections to the proposal increased by the time it reached the House of Representatives. It was generally held by critics that Hamilton was serving the interests of the Northeast by means of the bank,[123] and those of the agrarian lifestyle would not benefit from it.[96]: 270  Among those critics was James Jackson of Georgia, who also attempted to refute the report by quoting from The Federalist Papers.[96]: 270  Madison and Jefferson also opposed the bank bill. The potential of the capital not being moved to the Potomac if the bank was to have a firm establishment in Philadelphia was a more significant reason, and actions that Pennsylvania members of Congress took to keep the capital there made both men anxious.[46]: 199–200  The Whiskey Rebellion also showed how in other financial plans, there was a distance between the classes as the wealthy profited from the taxes.[124]
 Madison warned the Pennsylvania congress members that he would attack the bill as unconstitutional in the House, and followed up on his threat.[46]: 200  Madison argued his case of where the power of a bank could be established within the Constitution, but he failed to sway members of the House, and his authority on the constitution was questioned by a few members.[46]: 200–201  The bill eventually passed in an overwhelming fashion 39 to 20, on February 8, 1791.[96]: 271 
 Washington hesitated to sign the bill, as he received suggestions from Attorney General Edmund Randolph and Thomas Jefferson. Jefferson dismissed the Necessary and Proper Clause as reasoning for the creation of a national bank, stating that the enumerated powers ""can all be carried into execution without a bank.""[96]: 271–272  Along with Randolph and Jefferson's objections, Washington's involvement in the movement of the capital from Philadelphia is also thought to be a reason for his hesitation.[46]: 202–203  In response to the objection of the clause, Hamilton stated that ""Necessary often means no more than needful, requisite, incidental, useful, or conductive to"", and the bank was a ""convenient species of medium in which [taxes] are to be paid.""[96]: 272–273  Washington would eventually sign the bill into law.[96]: 272–273 
 In 1791, Hamilton submitted the Report on the Establishment of a Mint to the House of Representatives. Many of Hamilton's ideas for this report were from European economists, resolutions from the 1785 and 1786 Continental Congress meetings, and people such as Robert Morris, Gouverneur Morris and Thomas Jefferson.[46]: 197 [125]
 Because the most circulated coins in the United States at the time were Spanish currency, Hamilton proposed that minting a United States dollar weighing almost as much as the Spanish peso would be the simplest way to introduce a national currency.[126] Hamilton differed from European monetary policymakers in his desire to overprice gold relative to silver, on the grounds that the United States would always receive an influx of silver from the West Indies.[46]: 197  Despite his own preference for a monometallic gold standard,[127] he ultimately issued a bimetallic currency at a fixed 15:1 ratio of silver to gold.[46]: 197 [128][129]
 Hamilton proposed that the U.S. dollar should have fractional coins using decimals, rather than eighths like the Spanish coinage.[130] This innovation was originally suggested by Superintendent of Finance Robert Morris, with whom Hamilton corresponded after examining one of Morris's Nova Constellatio coins in 1783.[131] He also desired the minting of small value coins, such as silver ten-cent and copper cent and half-cent pieces, for reducing the cost of living for the poor.[46]: 198 [120] One of his main objectives was for the general public to become accustomed to handling money on a frequent basis.[46]: 198 
 By 1792, Hamilton's principles were adopted by Congress, resulting in the Coinage Act of 1792, and the creation of the mint. There was to be a ten-dollar gold Eagle coin, a silver dollar, and fractional money ranging from one-half to fifty cents.[127] The coining of silver and gold was issued by 1795.[127]
 Smuggling off American coasts was an issue before the Revolutionary War, and after the Revolution it was more problematic. Along with smuggling, lack of shipping control, pirating, and a revenue imbalance were also major problems.[132] In response, Hamilton proposed to Congress to enact a naval police force called revenue cutters in order to patrol the waters and assist the custom collectors with confiscating contraband.[133] This idea was also proposed to assist in tariff controlling, boosting the American economy, and promote the merchant marine.[132] It is thought that his experience obtained during his apprenticeship with Nicholas Kruger was influential in his decision-making.[134]
 Concerning some of the details of the System of Cutters,[135] Hamilton wanted the first ten cutters in different areas in the United States, from New England to Georgia.[133][136] Each of those cutters was to be armed with ten muskets and bayonets, twenty pistols, two chisels, one broad-ax and two lanterns. The fabric of the sails was to be domestically manufactured;[133] and provisions were made for the employees' food supply and etiquette when boarding ships.[133] Congress established the Revenue Cutter Service on August 4, 1790, which is viewed as the birth of the United States Coast Guard.[132]
 One of the principal sources of revenue Hamilton prevailed upon Congress to approve was an excise tax on whiskey. In his first Tariff Bill in January 1790, Hamilton proposed to raise the three million dollars needed to pay for government operating expenses and interest on domestic and foreign debts by means of an increase on duties on imported wines, distilled spirits, tea, coffee, and domestic spirits. It failed, with Congress complying with most recommendations excluding the excise tax on whiskey. The same year, Madison modified Hamilton's tariff to involve only imported duties; it was passed in September.[137]
In response of diversifying revenues, as three-fourths of revenue gathered was from commerce with Great Britain, Hamilton attempted once again during his Report on Public Credit when presenting it in 1790 to implement an excise tax on both imported and domestic spirits.[138][139] The taxation rate was graduated in proportion to the whiskey proof, and Hamilton intended to equalize the tax burden on imported spirits with imported and domestic liquor.[139] In lieu of the excise on production citizens could pay 60 cents by the gallon of dispensing capacity, along with an exemption on small stills used exclusively for domestic consumption.[139] He realized the loathing that the tax would receive in rural areas, but thought of the taxing of spirits more reasonable than land taxes.[138]
 Opposition initially came from Pennsylvania's House of Representatives protesting the tax. William Maclay had noted that not even the Pennsylvanian legislators had been able to enforce excise taxes in the western regions of the state.[138] Hamilton was aware of the potential difficulties and proposed inspectors the ability to search buildings that distillers were designated to store their spirits, and would be able to search suspected illegal storage facilities to confiscate contraband with a warrant.[140] Although the inspectors were not allowed to search houses and warehouses, they were to visit twice a day and file weekly reports in extensive detail.[138] Hamilton cautioned against expedited judicial means, and favored a jury trial with potential offenders.[140] As soon as 1791, locals began to shun or threaten inspectors, as they felt the inspection methods were intrusive.[138] Inspectors were also tarred and feathered, blindfolded, and whipped. Hamilton had attempted to appease the opposition with lowered tax rates, but it did not suffice.[141]
 Strong opposition to the whiskey tax by cottage producers in remote, rural regions erupted into the Whiskey Rebellion in 1794; in Western Pennsylvania and western Virginia, whiskey was the basic export product and was fundamental to the local economy. In response to the rebellion, believing compliance with the laws was vital to the establishment of federal authority, Hamilton accompanied to the rebellion's site President Washington, General Henry ""Light Horse Harry"" Lee, and more federal troops than were ever assembled in one place during the Revolution. This overwhelming display of force intimidated the leaders of the insurrection, ending the rebellion virtually without bloodshed.[142]
 Hamilton's next report was his Report on Manufactures. Although he was requested by Congress on January 15, 1790, for a report for manufacturing that would expand the United States' independence, the report was not submitted until December 5, 1791.[96]: 274, 277  In the report, Hamilton quoted from Wealth of Nations and used the French physiocrats as an example for rejecting agrarianism and the physiocratic theory, respectively.[46]: 233  Hamilton also refuted Smith's ideas of government noninterference, as it would have been detrimental for trade with other countries.[46]: 244  Hamilton also thought that the United States, being a primarily agrarian country, would be at a disadvantage in dealing with Europe.[143] In response to the agrarian detractors, Hamilton stated that the agriculturists' interest would be advanced by manufactures, and that agriculture was just as productive as manufacturing.[46]: 233 [96]: 276 
 Hamilton argued for industrial policy to support a modern manufacturing industry in the United States.[144][145] Among the ways that the government should assist manufacturing, Hamilton argued for government assistance to ""infant industries"" so they can achieve economies of scale, by levying protective duties on imported foreign goods that were also manufactured in the United States,[146] for withdrawing duties levied on raw materials needed for domestic manufacturing,[96]: 277 [146] and pecuniary boundaries.[96]: 277  He also encouraged immigration as a way to improve the American work force.[146][147] Congress shelved the report without much debate, except for Madison's objection to Hamilton's formulation of the general welfare clause, which Hamilton construed liberally as a legal basis for his extensive programs.[148]
 In 1791, Hamilton, along with Coxe and several entrepreneurs from New York City and Philadelphia formed the Society for the Establishment of Useful Manufactures, a private industrial corporation. In May 1792, the directors decided to examine the Great Falls of the Passaic River in New Jersey as a possible location for a manufacturing center. On July 4, 1792, the society directors met Philip Schuyler at Abraham Godwin's hotel on the Passaic River, where they led a tour prospecting the area for the national manufactory. It was originally suggested that they dig mile-long trenches and build the factories away from the falls, but Hamilton argued that it would be too costly and laborious.[149]
 The location at Great Falls of the Passaic River in New Jersey was selected due to access to raw materials, it being densely inhabited, and having access to water power from the falls of the Passaic.[46]: 231  The factory town was named Paterson after New Jersey's Governor William Paterson, who signed the charter.[46]: 232 [150] The profits were to derive from specific corporates rather than the benefits to be conferred to the nation and the citizens, which was unlike the report.[151] Hamilton also suggested the first stock to be offered at $500,000 and to eventually increase to $1 million, and welcomed state and federal government subscriptions alike.[96]: 280 [151] The company was never successful, with numerous shareholders reneged on stock payments and some going bankrupt. William Duer, the governor of the program, was sent to debtors' prison, where he died.[152] In spite of Hamilton's efforts to mend the disaster, the company folded.[150]
 When France and Britain went to war in early 1793, all four members of the Cabinet were consulted on what to do. They and Washington unanimously agreed to remain neutral, and to have the French ambassador who was raising privateers and mercenaries on American soil, Edmond-Charles Genêt, recalled.[153]: 336–341  However, in 1794, policy toward Britain became a major point of contention between the two parties. Hamilton and the Federalists wished for more trade with Britain, the largest trading partner of the newly formed United States. The Republicans saw monarchist Britain as the main threat to republicanism and proposed instead to start a trade war.[96]: 327–328 
 To avoid war, Washington sent Chief Justice John Jay to negotiate with the British, with Hamilton largely writing Jay's instructions. The result was a treaty denounced by the Republicans, but Hamilton mobilized support throughout the land.[154] The Jay Treaty passed the Senate in 1795 by exactly the required two-thirds majority. The treaty resolved issues remaining from the Revolution, averted war, and made possible ten years of peaceful trade between the United States and Britain.[153]: Ch 9  Historian George Herring notes the ""remarkable and fortuitous economic and diplomatic gains"" produced by the Treaty.[155]
 Several European states had formed the Second League of Armed Neutrality against incursions on their neutral rights; the cabinet was also consulted on whether the United States should join the alliance and decided not to. It kept that decision secret, but Hamilton revealed it in private to George Hammond, the British minister to the United States, without telling Jay or anyone else. His act remained unknown until Hammond's dispatches were read in the 1920s. This revelation may have had limited effect on the negotiations; Jay did threaten to join the League at one point, but the British had other reasons not to view the alliance as a serious threat.[153]: 411–412 [156]
 Hamilton's wife suffered a miscarriage[157] while he was absent during his armed repression of the Whiskey Rebellion.[158] In the wake of this, Hamilton tendered his resignation from office on December 1, 1794, giving Washington two months' notice,[159] Before leaving his post on January 31, 1795, Hamilton submitted the Report on a Plan for the Further Support of Public Credit to Congress to curb the debt problem. Hamilton grew dissatisfied with what he viewed as a lack of a comprehensive plan to fix the public debt. He wished to have new taxes passed with older ones made permanent and stated that any surplus from the excise tax on liquor would be pledged to lower public debt. His proposals were included in a bill by Congress within slightly over a month after his departure as treasury secretary.[160] Some months later, Hamilton resumed his law practice in New York to remain closer to his family.[161]
 Hamilton's vision was challenged by Virginia agrarians Thomas Jefferson and James Madison, who formed the Democratic-Republican Party. They favored strong state governments based in rural America and protected by state militias as opposed to a strong national government supported by a national army and navy. They denounced Hamilton as insufficiently devoted to republicanism, too friendly toward corrupt Britain and the monarchy in general, and too oriented toward cities, business and banking.[162]
 The two-party system began to emerge as political parties coalesced around competing interests. A congressional caucus, led by Madison, Jefferson, and William Branch Giles, began as an opposition group to Hamilton's financial programs. Hamilton and his allies began to call themselves the Federalists.[163][164]
 Hamilton assembled a nationwide coalition to garner support for the administration, including the expansive financial programs Hamilton had made administration policy and especially the president's policy of neutrality in the European war between Britain and France. Hamilton publicly denounced French minister Genêt, who commissioned American privateers and recruited Americans for private militias to attack British ships and colonial possessions of British allies. Eventually, even Jefferson joined Hamilton in seeking Genêt's recall.[165] If Hamilton's administrative republic was to succeed, Americans had to see themselves first as citizens of a nation and experience an administration that proved firm and demonstrated the concepts found within the Constitution.[166] The Federalists did impose some internal direct taxes, but they departed from most implications of Hamilton's administrative republic as risky.[167]
 The Republicans opposed banks and cities and favored the series of unstable revolutionary governments in France. They built their own national coalition to oppose the Federalists. Both sides gained the support of local political factions, and each side developed its own partisan newspapers. Noah Webster, John Fenno, and William Cobbett were energetic editors for the Federalists, while Benjamin Franklin Bache and Philip Freneau were fiery Republican editors. All of their newspapers were characterized by intense personal attacks, major exaggerations, and invented claims. In 1801, Hamilton established a daily newspaper, the New York Evening Post, and brought in William Coleman as its editor.[168] Hamilton's and Jefferson's incompatibility was heightened by the unavowed wish of each to be Washington's principal and most trusted advisor.[169]
 An additional partisan irritant to Hamilton was the 1791 United States Senate election in New York, which resulted in the election of Democratic-Republican candidate Aaron Burr over Federalist candidate Philip Schuyler, the incumbent and Hamilton's father-in-law. Hamilton blamed Burr personally for this outcome, and negative characterizations of Burr began to appear in his correspondence thereafter. The two men did work together from time to time thereafter on various projects, including Hamilton's army of 1798 and the Manhattan Water Company.[170]
 Hamilton's resignation as secretary of the treasury in 1795 did not remove him from public life. With the resumption of his law practice, he remained close to Washington as an advisor and friend. Hamilton influenced Washington in the composition of his farewell address by writing drafts for Washington to compare with the latter's draft, although when Washington contemplated retirement in 1792, he had consulted Madison for a draft that was used in a similar manner to Hamilton's.[171][172]
 In the election of 1796, under the Constitution as it stood then, each of the presidential electors had two votes, which they were to cast for different men from different states. The one who received the most votes would become president, the second-most, vice president. This system was not designed with the operation of parties in mind, as they had been thought disreputable and factious. The Federalists planned to deal with this by having all their electors vote for John Adams, then vice president, and all but a few for Thomas Pinckney.[173]
 Adams resented Hamilton's influence with Washington and considered him overambitious and scandalous in his private life; Hamilton compared Adams unfavorably with Washington and thought him too emotionally unstable to be president.[174] Hamilton took the election as an opportunity: he urged all the northern electors to vote for Adams and Pinckney, lest Jefferson get in; but he cooperated with Edward Rutledge to have South Carolina's electors vote for Jefferson and Pinckney. If all this worked, Pinckney would have more votes than Adams, Pinckney would become president, and Adams would remain vice president, but it did not work. The Federalists found out about it and northern Federalists voted for Adams but not for Pinckney, in sufficient numbers that Pinckney came in third and Jefferson became vice president.[175] Adams resented the intrigue since he felt his service to the nation was much more extensive than Pinckney's.[176]
 In the summer of 1797, Hamilton became the first major American politician publicly involved in a sex scandal.[177] Six years earlier, in the summer of 1791, 34-year-old Hamilton became involved in an affair with 23-year-old Maria Reynolds. According to Hamilton's account Maria approached him at his house in Philadelphia, claiming that her husband James Reynolds was abusive and had abandoned her, and she wished to return to her relatives in New York but lacked the means.[96]: 366–369  Hamilton recorded her address and subsequently delivered $30 personally to her boarding house, where she led him into her bedroom and ""Some conversation ensued from which it was quickly apparent that other than pecuniary consolation would be acceptable"". The two began an intermittent illicit affair that lasted approximately until June 1792.[178]
 Over the course of that year, while the affair was taking place, James Reynolds was well aware of his wife's infidelity, and likely orchestrated it from the beginning. He continually supported their relationship to extort blackmail money regularly from Hamilton. The common practice of the day for men of equal social standing was for the wronged husband to seek retribution in a duel, but Reynolds, of a lower social status and realizing how much Hamilton had to lose if his activity came into public view, resorted to extortion.[179] After an initial request of $1,000[180] to which Hamilton complied, Reynolds invited Hamilton to renew his visits to his wife ""as a friend""[181] only to extort forced ""loans"" after each visit that, most likely in collusion, Maria solicited with her letters. In the end, the blackmail payments totaled over $1,300 including the initial extortion.[96]: 369  Hamilton at this point may have been aware of both spouses being involved in the blackmail,[182] and he welcomed and strictly complied with James Reynolds' eventual request to end the affair.[178][183]
 In November 1792, James Reynolds and his associate Jacob Clingman were arrested for counterfeiting and speculating in Revolutionary War veterans' unpaid back wages. Clingman was released on bail and relayed information to Democratic-Republican congressman James Monroe that Reynolds had evidence incriminating Hamilton in illicit activity as Treasury Secretary. Monroe consulted with congressmen Muhlenberg and Venable on what actions to take and the congressmen confronted Hamilton on December 15, 1792.[178] Hamilton refuted the suspicions of financial speculation by exposing his affair with Maria and producing as evidence the letters by both of the Reynolds, proving that his payments to James Reynolds related to blackmail over his adultery, and not to treasury misconduct. The trio agreed on their honor to keep the documents privately with the utmost confidence.[96]: 366–369 
 Five years later however, in the summer of 1797, the ""notoriously scurrilous"" journalist James T. Callender published A History of the United States for the Year 1796.[46]: 334  The pamphlet contained accusations based on documents from the confrontation of December 15, 1792, taken out of context, that James Reynolds had been an agent of Hamilton. On July 5, 1797, Hamilton wrote to Monroe, Muhlenberg, and Venable, asking them to confirm that there was nothing that would damage the perception of his integrity while Secretary of Treasury. All but Monroe complied with Hamilton's request. Hamilton then published a 100-page booklet, later usually referred to as the Reynolds Pamphlet, and discussed the affair in indelicate detail for the time. Hamilton's wife Elizabeth eventually forgave him, but never forgave Monroe.[184] Although Hamilton faced ridicule from the Democratic-Republican faction, he maintained his availability for public service.[46]: 334–336 
 During the military build-up of the Quasi-War with France, and with the strong endorsement of Washington, Adams reluctantly appointed Hamilton a major general of the army. At Washington's insistence, Hamilton was made the senior major general, prompting Continental Army major general Henry Knox to decline the appointment to serve as Hamilton's junior, believing it would be degrading to serve beneath him.[185][186]
 Hamilton served as inspector general of the United States Army from July 18, 1798, to June 15, 1800. Because Washington was unwilling to leave Mount Vernon unless it were to command an army in the field, Hamilton was the de facto head of the army, to Adams's considerable displeasure. If full-scale war broke out with France, Hamilton argued that the army should conquer the North American colonies of France's ally, Spain, bordering the United States.[187] Hamilton was prepared to march the army through the Southern United States if necessary.[188]
 To fund this army, Hamilton wrote regularly to Oliver Wolcott Jr., his successor at the treasury, Representative William Loughton Smith, and U.S. senator Theodore Sedgwick. He urged them to pass a direct tax to fund the war. Smith resigned in July 1797, as Hamilton complained to him for slowness, and urged Wolcott to tax houses instead of land.[189] The eventual program included taxes on land, houses, and slaves, calculated at different rates in different states and requiring assessment of houses, and a stamp act like that of the British before the Revolution, though this time Americans were taxing themselves through their own representatives.[190] This provoked resistance in southeastern Pennsylvania nevertheless, led primarily by men such as John Fries who had marched with Washington against the Whiskey Rebellion.[191]
 Hamilton aided in all areas of the army's development, and after Washington's death he was by default the senior officer of the United States Army from December 14, 1799, to June 15, 1800. The army was to guard against invasion from France. Adams, however, derailed all plans for war by opening negotiations with France that led to peace.[192] There was no longer a direct threat for the army Hamilton was commanding to respond to.[193] Adams discovered that key members of his cabinet, namely Secretary of State Timothy Pickering and Secretary of War James McHenry, were more loyal to Hamilton than himself; Adams fired them in May 1800.[194]
 In November 1799, the Alien and Sedition Acts had left one Democratic-Republican newspaper functioning in New York City. When the last newspaper, the New Daily Advertiser, reprinted an article saying that Hamilton had attempted to purchase the Philadelphia Aurora to close it down, and said the purchase could have been funded by ""British secret service money"". Hamilton urged the New York Attorney General to prosecute the publisher for seditious libel, and the prosecution compelled the owner to close the paper.[195]
 In the 1800 election, Hamilton worked to defeat not only the Democratic-Republicans, but also his party's own nominee, John Adams.[96]: 392–399  Aaron Burr had won New York for Jefferson in May via the New York City legislative elections, as the legislature was to choose New York's electors; now Hamilton proposed a direct election, with carefully drawn districts where each district's voters would choose an elector—such that the Federalists would split the electoral vote of New York. Jay, who had resigned from the Supreme Court to be governor of New York, wrote on the back of a letter, ""Proposing a measure for party purposes which it would not become me to adopt,"" and declined to reply.[196]
 Adams was running this time with Charles Cotesworth Pinckney, the elder brother of former vice presidential candidate Thomas. Hamilton toured New England, again urging northern electors to hold firm for Pinckney in the renewed hope of making Pinckney president; and he again intrigued in South Carolina.[46]: 350–351  Hamilton's ideas involved coaxing middle-state Federalists to assert their non-support for Adams if there was no support for Pinckney and writing to more of the modest supports of Adams concerning his supposed misconduct while president.[46]: 350–351  Hamilton expected to see southern states such as the Carolinas cast their votes for Pinckney and Jefferson, and would result in the former being ahead of both Adams and Jefferson.[96]: 394–395 
 In accordance with the second of the aforementioned plans, and a recent personal rift with Adams,[46]: 351  Hamilton wrote a pamphlet called Letter from Alexander Hamilton, Concerning the Public Conduct and Character of John Adams, Esq. President of the United States that was highly critical of him, though it closed with a tepid endorsement.[96]: 396 
 Jefferson had beaten Adams, but both he and Aaron Burr had received 73 votes in the Electoral College. With Jefferson and Burr tied, the House of Representatives had to choose between the two men.[46]: 352 [96]: 399  Several Federalists who opposed Jefferson supported Burr, and for the first 35 ballots, Jefferson was denied a majority. Before the 36th ballot, Hamilton threw his weight behind Jefferson, supporting the arrangement reached by James A. Bayard of Delaware, in which five Federalist representatives from Maryland and Vermont abstained from voting, allowing those states' delegations to go for Jefferson, ending the impasse and electing Jefferson president rather than Burr.[46]: 350–351 
 Even though Hamilton did not like Jefferson and disagreed with him on many issues, he viewed Jefferson as the lesser of two evils. Hamilton spoke of Jefferson as being ""by far not so a dangerous man"" and of Burr as a ""mischievous enemy"" to the principal measure of the past administration.[197] It was for that reason, along with the fact that Burr was a northerner and not a Virginian, that many Federalist representatives voted for him.[198][contradictory]
 Hamilton wrote many letters to friends in Congress to convince the members to see otherwise.[46]: 352 [96]: 401  In the end, Burr would become vice president after losing to Jefferson.[199] However, according to several historians, the Federalists had rejected Hamilton's diatribe as reasons to not vote for Burr.[46]: 353 [96]: 401  In his book American Machiavelli: Alexander Hamilton and the Origins of US Foreign Policy, historian John Lamberton Harper stated Hamilton could have ""perhaps"" contributed ""to a degree"" in Burr's defeat.[200] Ron Chernow, alternatively, claimed that Hamilton ""squelched"" Burr's chance at becoming president.[201] When it became clear that Jefferson had developed his own concerns about Burr and would not support his return to the vice presidency,[199] Burr sought the New York governorship in 1804 with Federalist support, against the Jeffersonian Morgan Lewis, but was defeated by forces including Hamilton.[202]
 Soon after Lewis' gubernatorial victory, the Albany Register published Charles D. Cooper's letters, citing Hamilton's opposition to Burr and alleging that Hamilton had expressed ""a still more despicable opinion"" of the vice president at an upstate New York dinner party.[203][204] Cooper claimed that the letter was intercepted after relaying the information, but stated he was ""unusually cautious"" in recollecting the information from the dinner.[205]
 Burr, sensing an attack on his honor, and recovering from his defeat, demanded an apology in the form of a letter. Hamilton wrote a letter in response and ultimately refused because he could not recall the instance of insulting Burr. Hamilton would also have been accused of recanting Cooper's letter out of cowardice.[96]: 423–424  After a series of attempts to reconcile were to no avail, a duel was arranged through liaisons on June 27, 1804.[96]: 426 
 The concept of honor was fundamental to Hamilton's vision of himself and of the nation.[206] Historians have noted, as evidence of the importance that honor held in Hamilton's value system, that Hamilton had previously been a party to seven ""affairs of honor"" as a principal, and to three as an advisor or second.[207] Such affairs of honor were often concluded prior to reaching the final stage of a duel.[207]
 Before the duel, Hamilton wrote an explanation of his decision to participate while at the same time intending to ""throw away"" his shot.[208] His desire to be available for future political matters also played a factor.[203] A week before the duel, at an annual Independence Day dinner of the Society of the Cincinnati, both Hamilton and Burr were in attendance. Separate accounts confirm that Hamilton was uncharacteristically effusive while Burr was, by contrast, uncharacteristically withdrawn. Accounts also agree that Burr became roused when Hamilton, again uncharacteristically, sang a favorite song, which recent scholarship indicates that it was ""How Stands the Glass Around"", an anthem sung by military troops about fighting and dying in war.[209]
 The duel began at dawn on July 11, 1804, along the west bank of the Hudson River on a rocky ledge in Weehawken, New Jersey.[210] Both opponents were rowed over from Manhattan separately from different locations, as the spot was not accessible from the west due to the steepness of the adjoining cliffs. Coincidentally, the duel took place relatively close to the location of the duel that had ended the life of Hamilton's eldest son, Philip, three years earlier.[211] Lots were cast for the choice of position and which second should start the duel. Both were won by Hamilton's second, who chose the upper edge of the ledge for Hamilton facing the city to the east, toward the rising sun.[212] After the seconds had measured the paces Hamilton, according to both William P. Van Ness and Burr, raised his pistol ""as if to try the light"" and had to wear his glasses to prevent his vision from being obscured.[213] Hamilton also refused the more sensitive hairspring setting for the dueling pistols offered by Nathaniel Pendleton, and Burr was unaware of the option.[214]
 Vice President Burr shot Hamilton, delivering what proved to be a fatal wound. Hamilton's shot was said to have broken a tree branch directly above Burr's head.[173] Neither of the seconds, Pendleton nor Van Ness, could determine who fired first,[215] as each claimed that the other man had fired first.[214]
 Soon after, they measured and triangulated the shooting, but could not determine from which angle Hamilton had fired. Burr's shot hit Hamilton in the lower abdomen above his right hip. The bullet ricocheted off Hamilton's second or third false rib, fracturing it and causing considerable damage to his internal organs, particularly his liver and diaphragm, before becoming lodged in his first or second lumbar vertebra.[96]: 429 [216] The biographer Ron Chernow considers the circumstances to indicate that, after taking deliberate aim, Burr fired second,[217] while the biographer James Earnest Cooke suggests that Burr took careful aim and shot first, and Hamilton fired while falling, after being struck by Burr's bullet.[218]
 The paralyzed Hamilton was immediately attended by the same surgeon who tended Philip Hamilton, and ferried to the Greenwich Village boarding house of his friend William Bayard Jr., who had been waiting on the dock.[219] On his deathbed, Hamilton asked the Episcopal Bishop of New York, Benjamin Moore, to give him holy communion.[220] Moore initially declined to do so on the grounds that participating in a duel was a mortal sin and that Hamilton, although undoubtedly sincere in his faith, was not a member of the Episcopalian denomination.[221] After leaving, Moore was persuaded to return that afternoon by the urgent pleas of Hamilton's friends. Upon receiving Hamilton's solemn assurance that he repented for his part in the duel, Moore gave him communion.[221]
 After final visits from his family, friends, and considerable suffering for at least 31 hours, Hamilton died at two o'clock the following afternoon, July 12, 1804,[219][222] at Bayard's home just below the present Gansevoort Street.[223] The city fathers halted all business at noon two days later for Hamilton's funeral. The procession route of about two miles organized by the Society of the Cincinnati had so many participants of every class of citizen that it took hours to complete and was widely reported nationwide by newspapers.[224] Moore conducted the funeral service at Trinity Church.[220] Gouverneur Morris gave the eulogy and secretly established a fund to support his widow and children.[225] Hamilton was buried in the church's cemetery.[226]
 As a youth in the West Indies, Hamilton was an orthodox and conventional Presbyterian of the New Lights; he was mentored there by a former student of John Witherspoon, a moderate of the New School.[227] He wrote two or three hymns, which were published in the local newspaper.[228] Robert Troup, his college roommate, noted that Hamilton was ""in the habit of praying on his knees night and morning"".[229]: 10 
 According to Gordon Wood, Hamilton dropped his youthful religiosity during the Revolution and became ""a conventional liberal with theistic inclinations who was an irregular churchgoer at best""; however, he returned to religion in his last years.[230] Chernow wrote that Hamilton was nominally an Episcopalian, but:
 Stories were circulated that Hamilton had made two quips about God at the time of the Constitutional Convention in 1787.[232] During the French Revolution, he displayed a utilitarian approach to using religion for political ends, such as by maligning Jefferson as ""the atheist"", and insisting that Christianity and Jeffersonian democracy were incompatible.[232]: 316  After 1801, Hamilton further attested his belief in Christianity, proposing a Christian Constitutional Society in 1802 to take hold of ""some strong feeling of the mind"" to elect ""fit men"" to office, and advocating ""Christian welfare societies"" for the poor. After being shot, Hamilton spoke of his belief in God's mercy.[d]
 On his deathbed, Hamilton asked the Episcopal Bishop of New York, Benjamin Moore, to give him holy communion.[220] Moore initially declined to do so, on two grounds: that to participate in a duel was a mortal sin, and that Hamilton, although undoubtedly sincere in his faith, was not a member of the Episcopalian denomination.[233] After leaving, Moore was persuaded to return that afternoon by the urgent pleas of Hamilton's friends, and upon receiving Hamilton's solemn assurance that he never intended to shoot Burr and repented for his part in the duel, Moore gave him communion.[221] Bishop Moore returned the next morning, stayed with Hamilton for several hours until his death, and conducted the funeral service at Trinity Church.[220]
 Hamilton's birthplace had a large Jewish community, constituting one quarter of Charlestown's white population by the 1720s.[2] He came into contact with Jews on a regular basis; as a small boy, he was tutored by a Jewish schoolmistress, and had learned to recite the Ten Commandments in the original Hebrew.[229]
 Hamilton exhibited a degree of respect for Jews that was described by Chernow as ""a life-long reverence.""[234] He believed that Jewish achievement was a result of divine providence:
 Based primarily on the phonetic similarity of Lavien to a common Jewish surname, it has been suggested that Johann Lavien, the first husband of Hamilton's mother, was Jewish or of Jewish descent.[236] On this contested foundation, it was rumored that Hamilton himself was born Jewish, a claim that gained some popularity early in the 20th century,[237] and that was given serious consideration by one present-day historian.[238] The belief that Lavien was Jewish was popularized by Gertrude Atherton in her 1902 novel The Conqueror, a fictionalized biography of Hamilton which made the earliest known written assertion of the claim.[239][240] The consensus of mainstream scholars and historians who have addressed the underlying question of whether Lavien was Jewish, such as Ron Chernow, is that the assertion is not credible.[241]
 Hamilton's interpretations of the Constitution set forth in The Federalist Papers remain highly influential, as seen in scholarly studies and court decisions.[242] Although the Constitution was ambiguous as to the exact balance of power between national and state governments, Hamilton consistently took the side of greater federal power at the expense of the states.[243] As Secretary of the Treasury, Hamilton found himself in opposition to then Secretary of State Thomas Jefferson, who opposed establishing a de facto central bank. Hamilton justified the creation of this bank, and other federal powers, under Congress's constitutional authority to issue currency, regulate interstate commerce, and do anything else that would be ""necessary and proper"" to enact the provisions of the Constitution.[244]
 Jefferson, however, took a stricter view of the Constitution. Parsing the text carefully, he found no specific authorization for the establishment of a national bank. This controversy was eventually settled in McCulloch v. Maryland, which essentially adopted Hamilton's view, granting the federal government broad freedom to select the best means to execute its constitutionally enumerated powers, essentially confirming the doctrine of implied powers.[244] Nevertheless, the American Civil War and the Progressive Era demonstrated the sorts of crises and politics Hamilton's administrative republic sought to avoid.[245][how?]
 Hamilton's policies have had great influence on the development of the U.S. government. His constitutional interpretation, particularly of the Necessary and Proper Clause, set precedents for federal authority that are still used by the courts and are considered an authority on constitutional interpretation. French diplomat Charles Maurice de Talleyrand-Périgord, who spent 1794 in the United States, wrote, ""I consider Napoleon, Fox, and Hamilton the three greatest men of our epoch, and if I were forced to decide between the three, I would give without hesitation the first place to Hamilton,"" adding that Hamilton had intuited the problems of European conservatives.[246]
 Opinions of Hamilton run the gamut. Both Adams and Jefferson viewed him as unprincipled and dangerously aristocratic. Hamilton's reputation was mostly negative in the eras of Jeffersonian democracy and Jacksonian democracy. The older Jeffersonian view attacked Hamilton as a centralizer, sometimes to the point of accusations that he advocated monarchy.[247] By the Progressive Era, Herbert Croly, Henry Cabot Lodge, and Theodore Roosevelt praised his leadership of a strong government. Several Republicans in 19th and 20th centuries entered politics by writing laudatory biographies of Hamilton.[248]
 According to Princeton University historian Sean Wilentz, more recent views of Hamilton and his reputation have been favorable among scholars, who portray Hamilton as the visionary architect of the modern liberal capitalist economy and of a dynamic federal government headed by an energetic executive.[249] Conversely, modern scholars favoring Hamilton have portrayed Jefferson and his allies as naïve, dreamy idealists.[249]
 Hamilton is not known to have ever owned slaves, although members of his family did. At the time of her death, Hamilton's mother owned two slaves and wrote a will leaving them to her sons. However, due to their illegitimacy, Hamilton and his brother were held ineligible to inherit her property and never took ownership of the slaves.[250]: 17  Later, as a youth in Saint Croix, Hamilton worked for a company trading in commodities that included slaves.[250]: 17  Historians have discussed whether Hamilton personally owned slaves later in life.[251][252][253][254] Biographer Ron Chernow argued that while there is ""no definite proof"" that Hamilton personally owned slaves, ""oblique hints"" in Hamilton's papers suggest ""he and Eliza may have owned one or two household slaves.""[255] Hamilton occasionally handled slave transactions as the legal representative of his own family members, and his grandson, Allan McLane Hamilton, interpreted some of these journal entries as being purchases for himself.[256][257] In 1840, however, his son John maintained that his father ""never owned a slave; but on the contrary, having learned that a domestic whom he had hired was about to be sold by her master, he immediately purchased her freedom.""[258]
 By the time of Hamilton's early participation in the American Revolution, his abolitionist sensibilities had become evident. He was active during the Revolutionary War in trying to raise black troops for the army with the promise of freedom. In the 1780s and 1790s, Hamilton generally opposed pro-slavery southern interests, which he saw as hypocritical to the values of the revolution. In 1785, he joined his close associate John Jay in founding the New York Manumission Society, which successfully promoted the abolition of the international slave trade in New York City and passed a state law to end slavery in New York through a decades-long process of emancipation with a final end to slavery in the state on July 4, 1827.[250]
 At a time when most white leaders doubted the capacity of blacks, Hamilton believed slavery was morally wrong and wrote that ""their natural faculties are as good as ours.""[259] Unlike contemporaries such as Jefferson, who considered the removal of freed slaves to a western territory, West Indies, or Africa to be essential to any plan for emancipation, Hamilton pressed for abolition without such provisions.[250]: 22  Hamilton and other Federalists supported the Haitian Revolution, which had originated as a slave revolt.[250]: 23  His suggestions helped shape the Haitian constitution. In 1804, when Haiti became an independent state with a majority Black population, Hamilton urged closer economic and diplomatic ties.[250]: 23 
 Hamilton has been portrayed as the patron saint[260] of the American School economic philosophy that, according to one historian, later dominated American economic policy after 1861.[260] His ideas and work influenced the 19th-century German economist Friedrich List[261] and Abraham Lincoln's chief economic advisor, Henry Charles Carey.[262]
 As early as the fall of 1781, Hamilton firmly supported government intervention in favor of business after the manner of Jean-Baptiste Colbert.[263][264][265] In contrast to the British policy of international mercantilism, which he believed skewed benefits to colonial and imperial powers, Hamilton was a pioneering advocate of protectionism.[266] He is credited with the idea that industrialization would only be possible with tariffs to protect the ""infant industries"" of an emerging nation.[145]
 Political theorists credit Hamilton with the creation of the modern administrative state, citing his arguments in favor of a strong executive, linked to the support of the people, as the linchpin of an administrative republic.[267][268] The dominance of executive leadership in the formulation and carrying out of policy was, in his view, essential to resist the deterioration of a republican government.[269] Some scholars have raised similarities between Hamiltonian recommendations and the development of Meiji Japan as evidence of the global influence of Hamilton's theory.[270]
 Hamilton has appeared as a significant figure in popular works of historical fiction, including many that focused on other American political figures of his time. In comparison to other Founding Fathers, Hamilton attracted relatively little attention in American popular culture in the 20th century.[271]
 In 2015, he gained significant mainstream attention after the debut of the Broadway show Hamilton. Lin-Manuel Miranda played the title role and wrote the musical based on a biography by Ron Chernow. The musical was described by The New Yorker as ""an achievement of historical and cultural reimagining. In Miranda's telling, the headlong rise of one self-made immigrant becomes the story of America.""[272] The Off-Broadway production of Hamilton won the 2015 Drama Desk Award for Outstanding Musical as well as seven other Drama Desk Awards. In 2016, Hamilton received the Pulitzer Prize for Drama, and set a record with 16 Tony Award nominations,[273] of which the show won 11, including Best Musical.[274] During the presidency of Barack Obama, a plan to replace Hamilton on the ten-dollar bill was shelved due in part to the popularity of the musical.[275] On July 3, 2020, Disney+ released the movie Hamilton, an authorized film of the Broadway stage production performed by the original cast.[276]
"
French Revolution,https://en.wikipedia.org/wiki/French_Revolution,"


 The French Revolution (French: Révolution française [ʁevɔlysjɔ̃ fʁɑ̃sɛːz]) was a period of political and societal change in France that began with the Estates General of 1789, and ended with the coup of 18 Brumaire in November 1799 and the formation of the French Consulate. Many of its ideas are considered fundamental principles of liberal democracy,[1] while its values and institutions remain central to modern French political discourse.[2]
 The causes of the revolution were a combination of social, political, and economic factors which the ancien régime (""old regime"") proved unable to manage. A financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614. The representatives of the Third Estate broke away, and re-constituted themselves as a National Assembly in June. The Storming of the Bastille in Paris on 14 July was followed by a series of radical measures by the Assembly, among them the abolition of feudalism, state control over the Catholic Church, and a declaration of rights. The next three years were dominated by the struggle for political control, and military defeats following the outbreak of the French Revolutionary Wars in April 1792 led to an insurrection on 10 August. The monarchy was replaced by the French First Republic in September, and Louis XVI was executed in January 1793. 
 After another revolt in June 1793, the constitution was suspended, and adequate political power passed from the National Convention to the Committee of Public Safety, led by the Jacobins. About 16,000 people were executed in what was later referred to as Reign of Terror, which ended in July 1794. Weakened by external threats and internal opposition, the Republic was replaced in 1795 by the Directory, and four years later, in 1799, the Consulate seized power in a military coup led by Napoleon Bonaparte on 9 November. This event is generally seen as marking the end of the Revolutionary period.
 The Revolution resulted from multiple long-term and short-term factors, culminating in a social, economic, financial and political crisis in the late 1780s.[3][4][5] Combined with resistance to reform by the ruling elite, and indecisive policy by Louis XVI and his ministers, the result was a crisis the state was unable to manage.[6][7]
 Between 1715 and 1789, the French population grew from 21 to 28 million, 20% of whom lived in towns or cities, Paris alone having over 600,000 inhabitants.[8] This was accompanied by a tripling in the size of the middle class, which comprised almost 10% of the population by 1789.[9] Despite increases in overall prosperity, its benefits were largely restricted to the rentier and mercantile classes, while the living standards fell for wage labourers and peasant farmers who rented their land.[10][11] Economic recession from 1785, combined with bad harvests in 1787 and 1788, led to high unemployment and food prices, causing a financial and political crisis.[3][12][13][14]
 While the state also experienced a debt crisis, the level of debt itself was not high compared with Britain's.[15] A significant problem was that tax rates varied widely from one region to another, were often different from the official amounts, and collected inconsistently. Its complexity meant uncertainty over the amount contributed by any authorised tax caused resentment among all taxpayers.[16][a] Attempts to simplify the system were blocked by the regional Parlements which approved financial policy. The resulting impasse led to the calling of the Estates General of 1789, which became radicalised by the struggle for control of public finances.[18]
 Louis XVI was willing to consider reforms, but often backed down when faced with opposition from conservative elements within the nobility. Enlightenment critiques of social institutions were widely discussed among the educated French elite. At the same time, the American Revolution and the European revolts of the 1780s inspired public debate on issues such as patriotism, liberty, equality, and democracy. These shaped the response of the educated public to the crisis,[19] while scandals such as the Affair of the Diamond Necklace fuelled widespread anger at the court, nobility, and church officials.[20]
 France faced a series of budgetary crises during the 18th century, as revenues failed to keep pace with expenditure.[21][22] Although the economy grew solidly, the increase was not reflected in a proportional growth in taxes,[21] their collection being contracted to tax farmers who kept much of it as personal profit. As the nobility and Church benefited from many exemptions, the tax burden fell mainly on peasants.[23] Reform was difficult because new tax laws had to be registered with regional judicial bodies or parlements that were able to block them. The king could impose laws by decree, but this risked open conflict with the parlements, the nobility, and those subject to new taxes.[24]
 France primarily funded the Anglo-French War of 1778–1783 through loans. Following the peace, the monarchy borrowed heavily, culminating in a debt crisis. By 1788, half of state revenue was required to service its debt.[25] In 1786, the French finance minister, Calonne, proposed a package of reforms including a universal land tax, the abolition of grain controls and internal tariffs, and new provincial assemblies appointed by the king. The new taxes, however, were rejected, first by a hand-picked Assembly of Notables dominated by the nobility, then by the parlements when submitted by Calonne's successor Brienne. The notables and parlements argued that the proposed taxes could only be approved by an Estates-General, a representative body that had last met in 1614.[26]
 The conflict between the Crown and the parlements became a national political crisis. Both sides issued a series of public statements, the government arguing that it was combating privilege and the parlement defending the ancient rights of the nation. Public opinion was firmly on the side of the parlements, and riots broke out in several towns. Brienne's attempts to raise new loans failed, and on 8 August 1788, he announced that the king would summon an Estates-General to convene the following May. Brienne resigned and was replaced by Necker.[27]
 In September 1788, the Parlement of Paris ruled that the Estates-General should convene in the same form as in 1614, meaning that the three estates (the clergy, nobility, and Third Estate or ""commons"") would meet and vote separately, with votes counted by estate rather than by head. As a result, the clergy and nobility could combine to outvote the Third Estate despite representing less than 5% of the population.[28][29]
 Following the relaxation of censorship and laws against political clubs, a group of liberal nobles and middle class activists, known as the Society of Thirty, launched a campaign for the doubling of Third Estate representation and individual voting. The public debate saw an average of 25 new political pamphlets published a week from 25 September 1788.[30] The Abbé Sieyès issued influential pamphlets denouncing the privilege of the clergy and nobility, and arguing the Third Estate represented the nation and should sit alone as a National Assembly. Activists such as Mounier, Barnave and Robespierre organised regional meetings, petitions and literature in support of these demands.[31] In December, the king agreed to double the representation of the Third Estate, but left the question of counting votes for the Estates-General to decide.[32]
 The Estates-General contained three separate bodies, the First Estate representing 100,000 clergy, the Second the nobility, and the Third the ""commons"".[33] Since each met separately, and any proposals had to be approved by at least two, the First and Second Estates could outvote the Third despite representing less than 5% of the population.[28]
 Although the Catholic Church in France owned nearly 10% of all land, as well as receiving annual tithes paid by peasants,[34] three-quarters of the 303 clergy elected were parish priests, many of whom earned less than unskilled labourers and had more in common with their poor parishioners than with the bishops of the first estate.[35][36]
 The Second Estate elected 322 deputies, representing about 400,000 men and women, who owned about 25% of the land and collected seigneurial dues and rents from their tenants. Most delegates were town-dwelling members of the noblesse d'épée, or traditional aristocracy. Courtiers and representatives of the noblesse de robe (those who derived rank from judicial or administrative posts) were underrepresented.[37]
 Of the 610 deputies of the Third Estate, about two-thirds held legal qualifications and almost half were venal office holders. Less than 100 were in trade or industry and none were peasants or artisans.[38] To assist delegates, each region completed a list of grievances, known as Cahiers de doléances.[39] Tax inequality and seigneurial dues (feudal payments owed to landowners) headed the grievances in the cahiers de doleances for the estate.[40]
 On 5 May 1789, the Estates-General convened at Versailles. Necker outlined the state budget and reiterated the king's decision that each estate should decide on which matters it would agree to meet and vote in common with the other estates. On the following day, each estate was to separately verify the credentials of their representatives. The Third Estate, however, voted to invite the other estates to join them in verifying all the representatives of the Estates-General in common and to agree that votes should be counted by head. Fruitless negotiations lasted to 12 June when the Third Estate began verifying its own members. On 17 June, the Third Estate declared itself to be the National Assembly of France and that all existing taxes were illegal.[41] Within two days, more than 100 members of the clergy had joined them.[42]
 Shaken by this challenge to his authority, the king agreed to a reform package that he would announce at a Royal Session of the Estates-General. The Salle des États was closed to prepare for the joint session, but the members of the Estates-General were not informed in advance. On 20 June, when the members of the Third Estate found their meeting place closed, they moved to a nearby tennis court and swore not to disperse until a new constitution had been agreed.[43]
 At the Royal Session the king announced a series of tax and other reforms and stated that no new taxes or loans would be implemented without the consent of the Estates-General. However, he stated that the three estates were sacrosanct and it was up to each estate to agree to end their privileges and decide on which matters they would vote in common with the other estates. At the end of the session the Third Estate refused to leave the hall and reiterated their oath not to disperse until a constitution had been agreed. Over the next days more members of the clergy joined the National Assembly. On 27 June, faced with popular demonstrations and mutinies in his French Guards, Louis XVI capitulated. He commanded the members of the first and second estates to join the third in the National Assembly.[44]
 Even the limited reforms the king had announced went too far for Marie Antoinette and Louis' younger brother the Comte d'Artois. On their advice, Louis dismissed Necker again as chief minister on 11 July.[45] On 12 July, the Assembly went into a non-stop session after rumours circulated he was planning to use the Swiss Guards to force it to close. The news brought crowds of protestors into the streets, and soldiers of the elite Gardes Françaises regiment refused to disperse them.[46]
 On the 14th, many of these soldiers joined the mob in attacking the Bastille, a royal fortress with large stores of arms and ammunition. Its governor, Bernard-René de Launay, surrendered after several hours of fighting that cost the lives of 83 attackers. Taken to the Hôtel de Ville, he was executed, his head placed on a pike and paraded around the city; the fortress was then torn down in a remarkably short time. Although rumoured to hold many prisoners, the Bastille held only seven: four forgers, a lunatic, a failed assassin, and a deviant nobleman. Nevertheless, as a potent symbol of the Ancien Régime, its destruction was viewed as a triumph and Bastille Day is still celebrated every year.[47] In French culture, some see its fall as the start of the Revolution.[48]
 Alarmed by the prospect of losing control of the capital, Louis appointed the Marquis de Lafayette commander of the National Guard, with Jean-Sylvain Bailly as head of a new administrative structure known as the Commune. On 17 July, Louis visited Paris accompanied by 100 deputies, where he was greeted by Bailly and accepted a tricolore cockade to loud cheers. However, it was clear power had shifted from his court; he was welcomed as 'Louis XVI, father of the French and king of a free people.'[49]
 The short-lived unity enforced on the Assembly by a common threat quickly dissipated. Deputies argued over constitutional forms, while civil authority rapidly deteriorated. On 22 July, former Finance Minister Joseph Foullon and his son were lynched by a Parisian mob, and neither Bailly nor Lafayette could prevent it. In rural areas, wild rumours and paranoia resulted in the formation of militia and an agrarian insurrection known as la Grande Peur.[50] The breakdown of law and order and frequent attacks on aristocratic property led much of the nobility to flee abroad. These émigrés funded reactionary forces within France and urged foreign monarchs to back a counter-revolution.[51]
 In response, the Assembly published the August Decrees which abolished feudalism. Over 25% of French farmland was subject to feudal dues, providing the nobility with most of their income; these were now cancelled, along with church tithes. While their former tenants were supposed to pay them compensation, collecting it proved impossible, and the obligation was annulled in 1793.[52] Other decrees included equality before the law, opening public office to all, freedom of worship, and cancellation of special privileges held by provinces and towns.[53]
 With the suspension of the 13 regional parlements in November, the key institutional pillars of the old regime had all been abolished in less than four months. From its early stages, the Revolution therefore displayed signs of its radical nature; what remained unclear was the constitutional mechanism for turning intentions into practical applications.[54]
 On 9 July, the National Assembly appointed a committee to draft a constitution and statement of rights.[55] Twenty drafts were submitted, which were used by a sub-committee to create a Declaration of the Rights of Man and of the Citizen, with Mirabeau being the most prominent member.[56] The Declaration was approved by the Assembly and published on 26 August as a statement of principle.[57]
 The Assembly now concentrated on the constitution itself. Mounier and his monarchist supporters advocated a bicameral system, with an upper house appointed by the king, who would also have the right to appoint ministers and veto legislation. On 10 September, the majority of the Assembly, led by Sieyès and Talleyrand, voted in favour of a single body, and the following day approved a ""suspensive veto"" for the king, meaning Louis could delay implementation of a law, but not block it indefinitely. In October, the Assembly voted to restrict political rights, including voting rights, to ""active citizens"", defined as French males over the age of 25 who paid direct taxes equal to three days' labour. The remainder were designated ""passive citizens"", restricted to ""civil rights"", a distinction opposed by a significant minority, including the Jacobin clubs.[58][59] By mid-1790, the main elements of a constitutional monarchy were in place, although the constitution was not accepted by Louis until 1791.[60]
 Food shortages and the worsening economy caused frustration at the lack of progress, and led to popular unrest in Paris. This came to a head in late September 1789, when the Flanders Regiment arrived in Versailles to reinforce the royal bodyguard, and were welcomed with a formal banquet as was common practice. The radical press described this as a 'gluttonous orgy', and claimed the tricolour cockade had been abused, while the Assembly viewed their arrival as an attempt to intimidate them.[61]
 On 5 October, crowds of women assembled outside the Hôtel de Ville, agitating against high food prices and shortages.[62] These protests quickly turned political, and after seizing weapons stored at the Hôtel de Ville, some 7,000 of them marched on Versailles, where they entered the Assembly to present their demands. They were followed to Versailles by 15,000 members of the National Guard under Lafayette, who was virtually ""a prisoner of his own troops"".[63]
 When the National Guard arrived later that evening, Lafayette persuaded Louis the safety of his family required their relocation to Paris. Next morning, some of the protestors broke into the royal apartments, searching for Marie Antoinette, who escaped. They ransacked the palace, killing several guards. Order was eventually restored, and the royal family and Assembly left for Paris, escorted by the National Guard.[64] Louis had announced his acceptance of the August Decrees and the Declaration, and his official title changed from 'King of France' to 'King of the French'.[65]
 Historian John McManners argues ""in eighteenth-century France, throne and altar were commonly spoken of as in close alliance; their simultaneous collapse ... would one day provide the final proof of their interdependence."" One suggestion is that after a century of persecution, some French Protestants actively supported an anti-Catholic regime, a resentment fuelled by Enlightenment thinkers such as Voltaire.[66] Jean-Jacques Rousseau, considered a philosophical founder of the revolution,[67][68][69] wrote it was ""manifestly contrary to the law of nature... that a handful of people should gorge themselves with superfluities, while the hungry multitude goes in want of necessities.""[70]
 The Revolution caused a massive shift of power from the Catholic Church to the state; although the extent of religious belief has been questioned, elimination of tolerance for religious minorities meant by 1789 being French also meant being Catholic.[71] The church was the largest individual landowner in France, controlling nearly 10% of all estates and levied tithes, effectively a 10% tax on income, collected from peasant farmers in the form of crops. In return, it provided a minimal level of social support.[72]
 The August decrees abolished tithes, and on 2 November the Assembly confiscated all church property, the value of which was used to back a new paper currency known as assignats. In return, the state assumed responsibilities such as paying the clergy and caring for the poor, the sick and the orphaned.[73] On 13 February 1790, religious orders and monasteries were dissolved, while monks and nuns were encouraged to return to private life.[74]
 The Civil Constitution of the Clergy of 12 July 1790 made them employees of the state, as well as establishing rates of pay and a system for electing priests and bishops. Pope Pius VI and many French Catholics objected to this since it denied the authority of the Pope over the French Church. In October, thirty bishops wrote a declaration denouncing the law, further fuelling opposition.[75]
 When clergy were required to swear loyalty to the Civil Constitution in November 1790, it split the church between the 24% who complied, and the majority who refused.[76] This stiffened popular resistance against state interference, especially in traditionally Catholic areas such as Normandy, Brittany and the Vendée, where only a few priests took the oath and the civilian population turned against the revolution.[75] The result was state-led persecution of ""Refractory clergy"", many of whom were forced into exile, deported, or executed.[77]
 The period from October 1789 to spring 1791 is usually seen as one of relative tranquility, when some of the most important legislative reforms were enacted. However, conflict over the source of legitimate authority was more apparent in the provinces, where officers of the Ancien Régime had been swept away, but not yet replaced by new structures. This was less obvious in Paris, since the National Guard made it the best policed city in Europe, but disorder in the provinces inevitably affected members of the Assembly.[78]
 Centrists led by Sieyès, Lafayette, Mirabeau and Bailly created a majority by forging consensus with monarchiens like Mounier, and independents including Adrien Duport, Barnave and Alexandre Lameth. At one end of the political spectrum, reactionaries like Cazalès and Maury denounced the Revolution in all its forms, with radicals like Maximilien Robespierre at the other. He and Jean-Paul Marat opposed the criteria for ""active citizens"", gaining them substantial support among the Parisian proletariat, many of whom had been disenfranchised by the measure.[79]
 On 14 July 1790, celebrations were held throughout France commemorating the fall of the Bastille, with participants swearing an oath of fidelity to ""the nation, the law and the king."" The Fête de la Fédération in Paris was attended by the royal family, with Talleyrand performing a mass. Despite this show of unity, the Assembly was increasingly divided, while external players like the Paris Commune and National Guard competed for power. One of the most significant was the Jacobin club; originally a forum for general debate, by August 1790 it had over 150 members, split into different factions.[80]
 The Assembly continued to develop new institutions; in September 1790, the regional Parlements were abolished and their legal functions replaced by a new independent judiciary, with jury trials for criminal cases. However, moderate deputies were uneasy at popular demands for universal suffrage, labour unions and cheap bread, and over the winter of 1790 and 1791, they passed a series of measures intended to disarm popular radicalism. These included exclusion of poorer citizens from the National Guard, limits on use of petitions and posters, and the June 1791 Le Chapelier Law suppressing trade guilds and any form of worker organisation.[81]
 The traditional force for preserving law and order was the army, which was increasingly divided between officers, who largely came from the nobility, and ordinary soldiers. In August 1790, the loyalist General Bouillé suppressed a serious mutiny at Nancy; although congratulated by the Assembly, he was criticised by Jacobin radicals for the severity of his actions. Growing disorder meant many professional officers either left or became émigrés, further destabilising the institution.[82]
 Held in the Tuileries Palace under virtual house arrest, Louis XVI was urged by his brother and wife to re-assert his independence by taking refuge with Bouillé, who was based at Montmédy with 10,000 soldiers considered loyal to the Crown.[83] The royal family left the palace in disguise on the night of 20 June 1791; late the next day, Louis was recognised as he passed through Varennes, arrested and taken back to Paris. The attempted escape had a profound impact on public opinion; since it was clear Louis had been seeking refuge in Austria, the Assembly now demanded oaths of loyalty to the regime, and began preparing for war, while fear of 'spies and traitors' became pervasive.[84]
 Despite calls to replace the monarchy with a republic, Louis retained his position but was generally regarded with acute suspicion and forced to swear allegiance to the constitution. A new decree stated retracting this oath, making war upon the nation, or permitting anyone to do so in his name would be considered abdication. However, radicals led by Jacques Pierre Brissot prepared a petition demanding his deposition, and on 17 July, an immense crowd gathered in the Champ de Mars to sign. Led by Lafayette, the National Guard was ordered to ""preserve public order"" and responded to a barrage of stones by firing into the crowd, killing between 13 and 50 people.[85]
 The massacre badly damaged Lafayette's reputation: the authorities responded by closing radical clubs and newspapers, while their leaders went into exile or hiding, including Marat.[86] On 27 August, Emperor Leopold II and King Frederick William II of Prussia issued the Declaration of Pillnitz declaring their support for Louis and hinting at an invasion of France on his behalf. In reality, the meeting between Leopold and Frederick was primarily to discuss the Partitions of Poland; the Declaration was intended to satisfy Comte d'Artois and other French émigrés but the threat rallied popular support behind the regime.[87]
 Based on a motion proposed by Robespierre, existing deputies were barred from elections held in early September for the French Legislative Assembly. Although Robespierre himself was one of those excluded, his support in the clubs gave him a political power base not available to Lafayette and Bailly, who resigned respectively as head of the National Guard and the Paris Commune. The new laws were gathered together in the 1791 Constitution, and submitted to Louis XVI, who pledged to defend it ""from enemies at home and abroad"". On 30 September, the Constituent Assembly was dissolved, and the Legislative Assembly convened the next day.[88]
 The Legislative Assembly is often dismissed by historians as an ineffective body, compromised by divisions over the role of the monarchy, an issue exacerbated when Louis attempted to prevent or reverse limitations on his powers.[89] At the same time, restricting the vote to those who paid a minimal amount of tax disenfranchised a significant proportion of the 6 million Frenchmen over 25, while only 10% of those able to vote actually did so. Finally, poor harvests and rising food prices led to unrest among the urban class known as Sans-culottes, who saw the new regime as failing to meet their demands for bread and work.[90]
 This meant the new constitution was opposed by significant elements inside and outside the Assembly, itself split into three main groups. 264 members were affiliated with Barnave's Feuillants, constitutional monarchists who considered the Revolution had gone far enough, while another 136 were Jacobin leftists who supported a republic, led by Brissot and usually referred to as Brissotins.[91] The remaining 345 belonged to La Plaine, a centrist faction who switched votes depending on the issue, but many of whom shared doubts as to whether Louis was committed to the Revolution.[91] After he officially accepted the new Constitution, one recorded response was ""Vive le roi, s'il est de bon foi!"", or ""Long live the king – if he keeps his word"".[92]
 Although a minority in the Assembly, control of key committees allowed the Brissotins to provoke Louis into using his veto. They first managed to pass decrees confiscating émigré property and threatening them with the death penalty.[93] This was followed by measures against non-juring priests, whose opposition to the Civil Constitution led to a state of near civil war in southern France, which Barnave tried to defuse by relaxing the more punitive provisions. On 29 November, the Assembly approved a decree giving refractory clergy eight days to comply, or face charges of 'conspiracy against the nation', an act opposed even by Robespierre.[94] When Louis vetoed both, his opponents were able to portray him as opposed to reform in general.[95]
 Brissot accompanied this with a campaign for war against Austria and Prussia, often interpreted as a mixture of calculation and idealism. While exploiting popular anti-Austrianism, it reflected a genuine belief in exporting the values of political liberty and popular sovereignty.[96] Simultaneously, conservatives headed by Marie Antoinette also favoured war, seeing it as a way to regain control of the military, and restore royal authority. In December 1791, Louis made a speech in the Assembly giving foreign powers a month to disband the émigrés or face war, an act greeted with enthusiasm by supporters, but suspicion from opponents.[97]
 Barnave's inability to build a consensus in the Assembly resulted in the appointment of a new government, chiefly composed of Brissotins. On 20 April 1792, the French Revolutionary Wars began when French armies attacked Austrian and Prussian forces along their borders, before suffering a series of disastrous defeats. In an effort to mobilise popular support, the government ordered non-juring priests to swear the oath or be deported, dissolved the Constitutional Guard and replaced it with 20,000 fédérés; Louis agreed to disband the Guard, but vetoed the other two proposals, while Lafayette called on the Assembly to suppress the clubs.[98]
 Popular anger increased when details of the Brunswick Manifesto reached Paris on 1 August, threatening 'unforgettable vengeance' should any oppose the Allies in seeking to restore the power of the monarchy. On the morning of 10 August, a combined force of the Paris National Guard and provincial fédérés attacked the Tuileries Palace, killing many of the Swiss Guards protecting it.[99] Louis and his family took refuge with the Assembly and shortly after 11:00 am, the deputies present voted to 'temporarily relieve the king', effectively suspending the monarchy.[100]
 In late August, elections were held for the National Convention. New restrictions on the franchise meant the number of votes cast fell to 3.3 million, versus 4 million in 1791, while intimidation was widespread.[101] The Brissotins now split between moderate Girondins led by Brissot, and radical Montagnards, headed by Robespierre, Georges Danton and Jean-Paul Marat. While loyalties constantly shifted, voting patterns suggest roughly 160 of the 749 deputies can generally be categorised as Girondists, with another 200 Montagnards. The remainder were part of a centrist faction known as La Plaine, headed by Bertrand Barère, Pierre Joseph Cambon and Lazare Carnot.[102]
 In the September Massacres, between 1,100 and 1,600 prisoners held in Parisian jails were summarily executed, the vast majority being common criminals.[103] A response to the capture of Longwy and Verdun by Prussia, the perpetrators were largely National Guard members and fédérés on their way to the front. While responsibility is still disputed, even moderates expressed sympathy for the action, which soon spread to the provinces. One suggestion is that the killings stemmed from concern over growing lawlessness, rather than political ideology.[104]
 On 20 September, the French defeated the Prussians at the Battle of Valmy, in what was the first major victory by the army of France during the Revolutionary Wars. Emboldened by this, on 22 September the Convention replaced the monarchy with the French First Republic (1792–1804) and introduced a new calendar, with 1792 becoming ""Year One"".[105] The next few months were taken up with the trial of Citoyen Louis Capet, formerly Louis XVI. While evenly divided on the question of his guilt, members of the convention were increasingly influenced by radicals based within the Jacobin clubs and Paris Commune. The Brunswick Manifesto made it easy to portray Louis as a threat to the Revolution, especially when extracts from his personal correspondence showed him conspiring with Royalist exiles.[106]
 On 17 January 1793, Louis was sentenced to death for ""conspiracy against public liberty and general safety"". 361 deputies were in favour, 288 against, while another 72 voted to execute him, subject to delaying conditions. The sentence was carried out on 21 January on the Place de la Révolution, now the Place de la Concorde.[107] Conservatives across Europe now called for the destruction of revolutionary France, and in February the Convention responded by declaring war on Britain and the Dutch Republic. Together with Austria and Prussia, these two countries were later joined by Spain, Portugal, Naples, and Tuscany in the War of the First Coalition (1792–1797).[108]
 The Girondins hoped war would unite the people behind the government and provide an excuse for rising prices and food shortages, but found themselves the target of popular anger. Many left for the provinces. The first conscription measure or levée en masse on 24 February sparked riots in Paris and other regional centres. Already unsettled by changes imposed on the church, in March the traditionally conservative and royalist Vendée rose in revolt. On 18th, Dumouriez was defeated at Neerwinden and defected to the Austrians. Uprisings followed in Bordeaux, Lyon, Toulon, Marseille and Caen. The Republic seemed on the verge of collapse.[109]
 The crisis led to the creation on 6 April 1793 of the Committee of Public Safety, an executive committee accountable to the convention.[110] The Girondins made a fatal political error by indicting Marat before the Revolutionary Tribunal for allegedly directing the September massacres; he was quickly acquitted, further isolating the Girondins from the sans-culottes. When Jacques Hébert called for a popular revolt against the ""henchmen of Louis Capet"" on 24 May, he was arrested by the Commission of Twelve, a Girondin-dominated tribunal set up to expose 'plots'. In response to protests by the Commune, the Commission warned ""if by your incessant rebellions something befalls the representatives of the nation, Paris will be obliterated"".[109]
 Growing discontent allowed the clubs to mobilise against the Girondins. Backed by the Commune and elements of the National Guard, on 31 May they attempted to seize power in a coup. Although the coup failed, on 2 June the convention was surrounded by a crowd of up to 80,000, demanding cheap bread, unemployment pay and political reforms, including restriction of the vote to the sans-culottes, and the right to remove deputies at will.[111] Ten members of the commission and another twenty-nine members of the Girondin faction were arrested, and on 10 June, the Montagnards took over the Committee of Public Safety.[112]
 Meanwhile, a committee led by Robespierre's close ally Saint-Just was tasked with preparing a new Constitution. Completed in only eight days, it was ratified by the convention on 24 June, and contained radical reforms, including universal male suffrage. However, normal legal processes were suspended following the assassination of Marat on 13 July by the Girondist Charlotte Corday, which the Committee of Public Safety used as an excuse to take control. The 1793 Constitution was suspended indefinitely in October.[113]
 Key areas of focus for the new government included creating a new state ideology, economic regulation and winning the war.[114] They were helped by divisions among their internal opponents; while areas like the Vendée and Brittany wanted to restore the monarchy, most supported the Republic but opposed the regime in Paris. On 17 August, the Convention voted a second levée en masse; despite initial problems in equipping and supplying such large numbers, by mid-October Republican forces had re-taken Lyon, Marseille and Bordeaux, while defeating Coalition armies at Hondschoote and Wattignies.[115] The new class of military leaders included a young colonel named Napoleon Bonaparte, who was appointed commander of artillery at the siege of Toulon thanks to his friendship with Augustin Robespierre. His success in that role resulted in promotion to the Army of Italy in April 1794, and the beginning of his rise to military and political power.[116]
 Although intended to bolster revolutionary fervour, the Reign of Terror rapidly degenerated into the settlement of personal grievances. At the end of July, the Convention set price controls on a wide range of goods, with the death penalty for hoarders. On 9 September, 'revolutionary groups' were established to enforce these controls, while the Law of Suspects on 17th approved the arrest of suspected ""enemies of freedom"". This initiated what has become known as the ""Terror"". From September 1793 to July 1794, around 300,000 were arrested,[117] with some 16,600 people executed on charges of counter-revolutionary activity, while another 40,000 may have been summarily executed, or died awaiting trial.[118]
 Price controls made farmers reluctant to sell their produce in Parisian markets, and by early September, the city was suffering acute food shortages. At the same time, the war increased public debt, which the Assembly tried to finance by selling confiscated property. However, few would buy assets that might be repossessed by their former owners, a concern that could only be achieved by military victory. This meant the financial position worsened as threats to the Republic increased, while printing assignats to deal with the deficit further increased inflation.[119]
 On 10 October, the Convention recognised the Committee of Public Safety as the supreme Revolutionary Government, and suspended the Constitution until peace was achieved.[113] In mid-October, Marie Antoinette was convicted of a long list of crimes, and guillotined; two weeks later, the Girondist leaders arrested in June were also executed, along with Philippe Égalité. The ""Terror"" was not confined to Paris, with over 2,000 killed in Lyons after its recapture.[120]
 At Cholet on 17 October, the Republican army won a decisive victory over the Vendée rebels, and the survivors escaped into Brittany. Another defeat at Le Mans on 23 December ended the rebellion as a major threat, although the insurgency continued until 1796. The extent of the repression that followed has been debated by French historians since the mid-19th century.[121] Between November 1793 to February 1794, over 4,000 were drowned in the Loire at Nantes under the supervision of Jean-Baptiste Carrier. Historian Reynald Secher claims that as many as 117,000 died between 1793 and 1796. Although those numbers have been challenged, François Furet concluded it ""not only revealed massacre and destruction on an unprecedented scale, but a zeal so violent that it has bestowed as its legacy much of the region's identity.""[122] [b]
 At the height of the Terror, not even its supporters were immune from suspicion, leading to divisions within the Montagnard faction between radical Hébertists and moderates led by Danton.[c] Robespierre saw their dispute as de-stabilising the regime, and, as a deist, objected to the anti-religious policies advocated by the atheist Hébert, who was arrested and executed on 24 March with 19 of his colleagues, including Carrier.[126] To retain the loyalty of the remaining Hébertists, Danton was arrested and executed on 5 April with Camille Desmoulins, after a show trial that arguably did more damage to Robespierre than any other act in this period.[127]
 The Law of 22 Prairial (10 June) denied ""enemies of the people"" the right to defend themselves. Those arrested in the provinces were now sent to Paris for judgement; from March to July, executions in Paris increased from five to twenty-six a day.[128] Many Jacobins ridiculed the festival of the Cult of the Supreme Being on 8 June, a lavish and expensive ceremony led by Robespierre, who was also accused of circulating claims he was a second Messiah. Relaxation of price controls and rampant inflation caused increasing unrest among the sans-culottes, but the improved military situation reduced fears the Republic was in danger. Fearing their own survival depended on Robespierre's removal, on 29 June three members of the Committee of Public Safety openly accused him of being a dictator.[129]
 Robespierre responded by refusing to attend Committee meetings, allowing his opponents to build a coalition against him. In a speech made to the convention on 26 July, he claimed certain members were conspiring against the Republic, an almost certain death sentence if confirmed. When he refused to provide names, the session broke up in confusion. That evening he repeated these claims at the Jacobins club, where it was greeted with demands for execution of the 'traitors'. Fearing the consequences if they did not act first, his opponents attacked Robespierre and his allies in the Convention next day. When Robespierre attempted to speak, his voice failed, one deputy crying ""The blood of Danton chokes him!""[130]
 After the Convention authorised his arrest, he and his supporters took refuge in the Hotel de Ville, which was defended by elements of the National Guard. Other units loyal to the Convention stormed the building that evening and detained Robespierre, who severely injured himself attempting suicide. He was executed on 28 July with 19 colleagues, including Saint-Just and Georges Couthon, followed by 83 members of the Commune.[131] The Law of 22 Prairial was repealed, any surviving Girondists reinstated as deputies, and the Jacobin Club was closed and banned.[132]
 There are various interpretations of the Terror and the violence with which it was conducted. François Furet argues that the intense ideological commitment of the revolutionaries and their utopian goals required the extermination of any opposition.[133] A middle position suggests violence was not inevitable but the product of a series of complex internal events, exacerbated by war.[134]
 The bloodshed did not end with the death of Robespierre; Southern France saw a wave of revenge killings, directed against alleged Jacobins, Republican officials and Protestants. Although the victors of Thermidor asserted control over the Commune by executing their leaders, some of those closely involved in the ""Terror"" retained their positions. They included Paul Barras, later chief executive of the French Directory, and Joseph Fouché, director of the killings in Lyon who served as Minister of Police under the Directory, the Consulate and Empire.[135] Despite his links to Augustin Robespierre, military success in Italy meant Napoleon Bonaparte escaped censure.[136]
 The December 1794 Treaty of La Jaunaye ended the Chouannerie in western France by allowing freedom of worship and the return of non-juring priests.[137] This was accompanied by military success; in January 1795, French forces helped the Dutch Patriots set up the Batavian Republic, securing their northern border.[138] The war with Prussia was concluded in favour of France by the Peace of Basel in April 1795, while Spain made peace shortly thereafter.[139]
 However, the Republic still faced a crisis at home. Food shortages arising from a poor 1794 harvest were exacerbated in Northern France by the need to supply the army in Flanders, while the winter was the worst since 1709.[140] By April 1795, people were starving and the assignat was worth only 8% of its face value; in desperation, the Parisian poor rose again.[141] They were quickly dispersed and the main impact was another round of arrests, while Jacobin prisoners in Lyon were summarily executed.[142]
 A committee drafted a new constitution, approved by plebiscite on 23 September 1795 and put into place on 27th.[143] Largely designed by Pierre Daunou and Boissy d'Anglas, it established a bicameral legislature, intended to slow down the legislative process, ending the wild swings of policy under the previous unicameral systems. The Council of 500 was responsible for drafting legislation, which was reviewed and approved by the Council of Ancients, an upper house containing 250 men over the age of 40. Executive power was in the hands of five Directors, selected by the Council of Ancients from a list provided by the lower house, with a five-year mandate.[144]
 Deputies were chosen by indirect election, a total franchise of around 5 million voting in primaries for 30,000 electors, or 0.6% of the population. Since they were also subject to stringent property qualification, it guaranteed the return of conservative or moderate deputies. In addition, rather than dissolving the previous legislature as in 1791 and 1792, the so-called 'law of two-thirds' ruled only 150 new deputies would be elected each year. The remaining 600 Conventionnels kept their seats, a move intended to ensure stability.[145]
 Jacobin sympathisers viewed the Directory as a betrayal of the Revolution, while Bonapartists later justified Napoleon's coup by emphasising its corruption.[146] The regime also faced internal unrest, a weak economy, and an expensive war, while the Council of 500 could block legislation at will. Since the Directors had no power to call new elections, the only way to break a deadlock was rule by decree or use force. As a result, the Directory was characterised by ""chronic violence, ambivalent forms of justice, and repeated recourse to heavy-handed repression.""[147]
 Retention of the Conventionnels ensured the Thermidorians held a majority in the legislature and three of the five Directors, but they were increasingly challenged by the right. On 5 October, Convention troops led by Napoleon put down a royalist rising in Paris; when the first elections were held two weeks later, over 100 of the 150 new deputies were royalists of some sort.[148] The power of the Parisian sans-culottes had been broken by the suppression of the May 1795 revolt; relieved of pressure from below, the Jacobin clubs became supporters of the Directory, largely to prevent restoration of the monarchy.[149]
 Removal of price controls and a collapse in the value of the assignat led to inflation and soaring food prices. By April 1796, over 500,000 Parisians were unemployed, resulting in the May insurrection known as the Conspiracy of the Equals. Led by the revolutionary François-Noël Babeuf, their demands included immediate implementation of the 1793 Constitution, and a more equitable distribution of wealth. Despite support from sections of the military, the revolt was easily crushed, while Babeuf and other leaders were executed.[150] Nevertheless, by 1799 the economy had been stabilised, and important reforms made allowing steady expansion of French industry. Many of these remained in place for much of the 19th century.[151]
 Prior to 1797, three of the five Directors were firmly Republican; Barras, Révellière-Lépeaux and Jean-François Rewbell, as were around 40% of the legislature. The same percentage were broadly centrist or unaffiliated, along with two Directors, Étienne-François Letourneur and Lazare Carnot. Although only 20% were committed Royalists, many centrists supported the restoration of the exiled Louis XVIII of France in the belief this would bring peace.[152] The elections of May 1797 resulted in significant gains for the right, with Royalists Jean-Charles Pichegru elected President of the Council of 500, and Barthélemy appointed a Director.[153]
 With Royalists apparently on the verge of power, Republicans attempted a pre-emptive coup on 4 September. Using troops from Napoleon's Army of Italy under Pierre Augereau, the Council of 500 was forced to approve the arrest of Barthélemy, Pichegru and Carnot. The elections were annulled, sixty-three leading Royalists deported to French Guiana, and new laws passed against émigrés, Royalists and ultra-Jacobins. The removal of his conservative opponents opened the way for direct conflict between Barras, and those on the left.[154]
 Fighting continued despite general war weariness, and the 1798 elections saw a resurgence in Jacobin strength. Napoleon's invasion of Egypt in July 1798 confirmed European fears of French expansionism, and the War of the Second Coalition began in November. Without a majority in the legislature, the Directors relied on the army to enforce decrees, and extract revenue from conquered territories. Generals like Napoleon and Joubert were now central to the political process, while both the army and Directory became notorious for their corruption.[155]
 It has been suggested the Directory collapsed because by 1799, many 'preferred the uncertainties of authoritarian rule to the continuing ambiguities of parliamentary politics'.[156] The architect of its end was Sieyès, who when asked what he had done during the Terror allegedly answered ""I survived"". Nominated to the Directory, his first action was to remove Barras, with the help of allies including Talleyrand, and Napoleon's brother Lucien, President of the Council of 500.[157] On 9 November 1799, the Coup of 18 Brumaire replaced the five Directors with the French Consulate, which consisted of three members, Napoleon, Sieyès, and Roger Ducos. Most historians consider this the end point of the French Revolution.[158]
 The role of ideology in the Revolution is controversial with Jonathan Israel stating that the ""radical Enlightenment"" was the primary driving force of the Revolution.[159] Cobban, however, argues ""[t]he actions of the revolutionaries were most often prescribed by the need to find practical solutions to immediate problems, using the resources at hand, not by pre-conceived theories.""[160]
 The identification of ideologies is complicated by the profusion of revolutionary clubs, factions and publications, absence of formal political parties, and individual flexibility in the face of changing circumstances.[161] In addition, although the Declaration of the Rights of Man was a fundamental document for all revolutionary factions, its interpretation varied widely.[162]
 While all revolutionaries professed their devotion to liberty in principle, ""it appeared to mean whatever those in power wanted.""[163] For example, the liberties specified in the Rights of Man were limited by law when they might ""cause harm to others, or be abused"". Prior to 1792, Jacobins and others frequently opposed press restrictions on the grounds these violated a basic right.[164] However, the radical National Convention passed laws in September 1793 and July 1794 imposing the death penalty for offences such as ""disparaging the National Convention"", and ""misleading public opinion.""[165]
 While revolutionaries also endorsed the principle of equality, few advocated equality of wealth since property was also viewed as a right.[166] The National Assembly opposed equal political rights for women,[167] while the abolition of slavery in the colonies was delayed until February 1794 because it conflicted with the property rights of slave owners, and many feared it would disrupt trade.[168] Political equality for male citizens was another divisive issue, with the 1791 constitution limiting the right to vote and stand for office to males over 25 who met a property qualification, so-called ""active citizens"". This restriction was opposed by many activists, including Robespierre, the Jacobins, and Cordeliers.[169]
 The principle that sovereignty resided in the nation was a key concept of the Revolution.[170] However, Israel argues this obscures ideological differences over whether the will of the nation was best expressed through representative assemblies and constitutions, or direct action by revolutionary crowds, and popular assemblies such as the sections of the Paris commune.[171] Many considered constitutional monarchy as incompatible with the principle of popular sovereignty,[172] but prior to 1792, there was a strong bloc with an ideological commitment to such a system, based on the writings of Hobbes, Locke, Montesquieu and Voltaire.[173]
 Israel argues the nationalisation of church property and the establishment of the Constitutional Church reflected an ideological commitment to secularism, and a determination to undermine a bastion of old regime privilege.[174] While Cobban agrees the Constitutional Church was motivated by ideology, he sees its origins in the anti-clericalism of Voltaire and other Enlightenment figures.[175]
 Jacobins were hostile to formal political parties and factions which they saw as a threat to national unity and the general will, with ""political virtue"" and ""love of country"" key elements of their ideology.[176][177] They viewed the ideal revolutionary as selfless, sincere, free of political ambition, and devoted to the nation.[178] The disputes leading to the departure first of the Feuillants, then later the Girondists, were conducted in terms of the relative political virtue and patriotism of the disputants. In December 1793, all members of the Jacobin clubs were subject to a ""purifying scrutiny"", to determine whether they were ""men of virtue"".[179]
 The Revolution initiated a series of conflicts that began in 1792 and ended only with Napoleon's defeat at Waterloo in 1815. In its early stages, this seemed unlikely; the 1791 Constitution specifically disavowed ""war for the purpose of conquest"", and although traditional tensions between France and Austria re-emerged in the 1780s, Emperor Joseph II cautiously welcomed the reforms. Austria was at war with the Ottomans, as were the Russians, while both were negotiating with Prussia over partitioning Poland. Most importantly, Britain preferred peace, and as Emperor Leopold II stated after the Declaration of Pillnitz, ""without England, there is no case"".[180]
 In late 1791, factions within the Assembly came to see war as a way to unite the country and secure the Revolution by eliminating hostile forces on its borders and establishing its ""natural frontiers"".[181] France declared war on Austria in April 1792 and issued the first conscription orders, with recruits serving for twelve months. By the time peace finally came in 1815, the conflict had involved every major European power as well as the United States, redrawn the map of Europe and expanded into the Americas, the Middle East, and the Indian Ocean.[182]
 From 1701 to 1801, the population of Europe grew from 118 to 187 million; combined with new mass production techniques, this allowed belligerents to support large armies, requiring the mobilisation of national resources. It was a different kind of war, fought by nations rather than kings, intended to destroy their opponents' ability to resist, but also to implement deep-ranging social change. While all wars are political to some degree, this period was remarkable for the emphasis placed on reshaping boundaries and the creation of entirely new European states.[183]
 In April 1792, French armies invaded the Austrian Netherlands but suffered a series of setbacks before victory over an Austrian-Prussian army at Valmy in September. After defeating a second Austrian army at Jemappes on 6 November, they occupied the Netherlands, areas of the Rhineland, Nice and Savoy. Emboldened by this success, in February 1793 France declared war on the Dutch Republic, Spain and Britain, beginning the War of the First Coalition.[184] However, the expiration of the 12-month term for the 1792 recruits forced the French to relinquish their conquests. In August, new conscription measures were passed and by May 1794 the French army had between 750,000 and 800,000 men.[185] Despite high rates of desertion, this was large enough to manage multiple internal and external threats; for comparison, the combined Prussian-Austrian army was less than 90,000.[186]
 By February 1795, France had annexed the Austrian Netherlands, established their frontier on the left bank of the Rhine and replaced the Dutch Republic with the Batavian Republic, a satellite state. These victories led to the collapse of the anti-French coalition; Prussia made peace in April 1795, followed soon after by Spain, leaving Britain and Austria as the only major powers still in the war.[187] In October 1797, a series of defeats by Bonaparte in Italy led Austria to agree to the Treaty of Campo Formio, in which they formally ceded the Netherlands and recognised the Cisalpine Republic.[188]
 Fighting continued for two reasons; first, French state finances had come to rely on indemnities levied on their defeated opponents. Second, armies were primarily loyal to their generals, for whom the wealth achieved by victory and the status it conferred became objectives in themselves. Leading soldiers like Hoche, Pichegru and Carnot wielded significant political influence and often set policy; Campo Formio was approved by Bonaparte, not the Directory, which strongly objected to terms it considered too lenient.[188]
 Despite these concerns, the Directory never developed a realistic peace programme, fearing the destabilising effects of peace and the consequent demobilisation of hundreds of thousands of young men. As long as the generals and their armies stayed away from Paris, they were happy to allow them to continue fighting, a key factor behind sanctioning Bonaparte's invasion of Egypt. This resulted in aggressive and opportunistic policies, leading to the War of the Second Coalition in November 1798.[189]
 In 1789, the most populous French colonies were Saint-Domingue (today Haiti), Martinique, Guadeloupe, the Île Bourbon (Réunion) and the Île de la France. These colonies produced commodities such as sugar, coffee and cotton for exclusive export to France. There were about 700,000 slaves in the colonies, of which about 500,000 were in Saint-Domingue. Colonial products accounted for about a third of France's exports.[190]
 In February 1788, the Société des Amis des Noirs (Society of the Friends of Blacks) was formed in France with the aim of abolishing slavery in the empire. In August 1789, colonial slave owners and merchants formed the rival Club de Massiac to represent their interests. When the Constituent Assembly adopted the Declaration of the Rights of Man and of the Citizen in August 1789, delegates representing the colonial landowners successfully argued that the principles should not apply in the colonies as they would bring economic ruin and disrupt trade. Colonial landowners also gained control of the Colonial Committee of the Assembly from where they exerted a powerful influence against abolition.[191][192]
 People of colour also faced social and legal discrimination in mainland France and its colonies, including a bar on their access to professions such as law, medicine and pharmacy.[193] In 1789–90, a delegation of free coloureds, led by Vincent Ogé and Julien Raimond, unsuccessfully lobbied the Assembly to end discrimination against free coloureds. Ogé left for Saint-Domingue where an uprising against white landowners broke out in October 1790. The revolt failed and Ogé was killed.[194][192]
 In May 1791, the National Assembly granted full political rights to coloureds born of two free parents, but left the rights of freed slaves to be determined by the colonial assemblies. The assemblies refused to implement the decree and fighting broke out between the coloured population of Saint-Domingue and white colonists, each side recruiting slaves to their forces. A major slave revolt followed in August.[195]
 In March 1792, the Legislative Assembly responded to the revolt by granting citizenship to all free coloureds and sending two commissioners, Sonthonax and Polvérel, and 6,000 troops to Saint-Domingue to enforce the decree. On arrival in September, the commissioners announced that slavery would remain in force. Over 72,000 slaves were still in revolt, mostly in the north.[196]
 Brissot and his supporters envisaged an eventual abolition of slavery but their immediate concern was securing trade and the support of merchants for the revolutionary wars. After Brissot's fall, the new constitution of June 1793 included a new Declaration of the Rights of Man and the Citizen but excluded the colonies from its provisions. In any event, the new constitution was suspended until France was at peace.[197]
 In early 1793, royalist planters from Guadeloupe and Saint-Domingue formed an alliance with Britain. The Spanish supported insurgent slaves, led by Jean-François Papillon and Georges Biassou, in the north of Saint-Domingue. White planters loyal to the republic sent representatives to Paris to convince the Jacobin controlled Convention that those calling for the abolition of slavery were British agents and supporters of Brissot, hoping to disrupt trade.[198]
 In June, the commissioners in Saint-Domingue freed 10,000 slaves fighting for the republic. As the royalists and their British and Spanish supporters were also offering freedom for slaves willing to fight for their cause, the commissioners outbid them by abolishing slavery in the north in August, and throughout the colony in October. Representatives were sent to Paris to gain the approval of the convention for the decision.[198][199]
 The Convention voted for the abolition of slavery in the colonies on 4 February 1794 and decreed that all residents of the colonies had the full rights of French citizens irrespective of colour.[200] An army of 1,000 sans-culottes led by Victor Hugues was sent to Guadeloupe to expel the British and enforce the decree. The army recruited former slaves and eventually numbered 11,000, capturing Guadeloupe and other smaller islands. Abolition was also proclaimed on Guyane. Martinique remained under British occupation, while colonial landowners in Réunion and the Îles Mascareignes repulsed the republicans.[201] Black armies drove the Spanish out of Saint-Domingue in 1795, and the last of the British withdrew in 1798.[202]
 In republican controlled areas from 1793 to 1799, freed slaves were required to work on their former plantations or for their former masters if they were in domestic service. They were paid a wage and gained property rights. Black and coloured generals were effectively in control of large areas of Guadeloupe and Saint-Domingue, including Toussaint Louverture in the north of Saint-Domingue, and André Rigaud in the south. Historian Fréderic Régent states that the restrictions on the freedom of employment and movement of former slaves meant that, ""only whites, persons of color already freed before the decree, and former slaves in the army or on warships really benefited from general emancipation.""[201]
 Newspapers and pamphlets played a central role in stimulating and defining the Revolution. Prior to 1789, there have been a small number of heavily censored newspapers that needed a royal licence to operate, but the Estates-General created an enormous demand for news, and over 130 newspapers appeared by the end of the year. Among the most significant were Marat's L'Ami du peuple and Elysée Loustallot's Revolutions de Paris [fr].[203] Over the next decade, more than 2,000 newspapers were founded, 500 in Paris alone. Most lasted only a matter of weeks but they became the main communication medium, combined with the very large pamphlet literature.[204]
 Newspapers were read aloud in taverns and clubs, and circulated hand to hand. There was a widespread assumption that writing was a vocation, not a business, and the role of the press was the advancement of civic republicanism.[205] By 1793 the radicals were most active but initially the royalists flooded the country with their publication the ""L'Ami du Roi [fr]"" (Friends of the King) until they were suppressed.[206]
 To illustrate the differences between the new Republic and the old regime, the leaders needed to implement a new set of symbols to be celebrated instead of the old religious and monarchical symbols. To this end, symbols were borrowed from historic cultures and redefined, while those of the old regime were either destroyed or reattributed acceptable characteristics. These revised symbols were used to instil in the public a new sense of tradition and reverence for the Enlightenment and the Republic.[207]
 ""La Marseillaise"" (.mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}French pronunciation: [la maʁsɛjɛːz]) became the national anthem of France. The song was written and composed in 1792 by Claude Joseph Rouget de Lisle, and was originally titled ""Chant de guerre pour l'Armée du Rhin"". The French National Convention adopted it as the First Republic's anthem in 1795. It acquired its nickname after being sung in Paris by volunteers from Marseille marching on the capital.
 The song is the first example of the ""European march"" anthemic style, while the evocative melody and lyrics led to its widespread use as a song of revolution and incorporation into many pieces of classical and popular music. De Lisle was instructed to 'produce a hymn which conveys to the soul of the people the enthusiasm which it (the music) suggests.'[209]
 The guillotine remains ""the principal symbol of the Terror in the French Revolution.""[210] Invented by a physician during the Revolution as a quicker, more efficient and more distinctive form of execution, the guillotine became a part of popular culture and historic memory. It was celebrated on the left as the people's avenger, for example in the revolutionary song La guillotine permanente,[211] and cursed as the symbol of the Terror by the right.[212]
 Its operation became a popular entertainment that attracted great crowds of spectators. Vendors sold programmes listing the names of those scheduled to die. Many people came day after day and vied for the best locations from which to observe the proceedings; knitting women (tricoteuses) formed a cadre of hardcore regulars, inciting the crowd. Parents often brought their children. By the end of the Terror, the crowds had thinned drastically. Repetition had staled even this most grisly of entertainments, and audiences grew bored.[213]
 Cockades were widely worn by revolutionaries beginning in 1789. They now pinned the blue-and-red cockade of Paris onto the white cockade of the Ancien Régime. Camille Desmoulins asked his followers to wear green cockades on 12 July 1789. The Paris militia, formed on 13 July, adopted a blue and red cockade. Blue and red are the traditional colours of Paris, and they are used on the city's coat of arms. Cockades with various colour schemes were used during the storming of the Bastille on 14 July.[214]
 The Liberty cap, also known as the Phrygian cap, or pileus, is a brimless, felt cap that is conical in shape with the tip pulled forward. It reflects Roman republicanism and liberty, alluding to the Roman ritual of manumission, in which a freed slave receives the bonnet as a symbol of his newfound liberty.[215]
 Deprived of political rights by the Ancien Régime, the Revolution initially allowed women to participate, although only to a limited degree. Activists included Girondists like Olympe de Gouges, author of the Declaration of the Rights of Woman and of the Female Citizen, and Charlotte Corday, killer of Marat. Others like Théroigne de Méricourt, Pauline Léon and the Society of Revolutionary Republican Women supported the Jacobins, staged demonstrations in the National Assembly and took part in the October 1789 March to Versailles. Despite this, the 1791 and 1793 constitutions denied them political rights and democratic citizenship.[216]
 In 1793, the Society of Revolutionary Republican Women campaigned for strict price controls on bread, and a law that would compel all women to wear the tricolour cockade. Although both demands were successful, in October the male-dominated Jacobins who then controlled the government denounced the Society as dangerous rabble-rousers and made all women's clubs and associations illegal. Organised women were permanently shut out of the French Revolution after 30 October 1793.[217]
 At the same time, especially in the provinces, women played a prominent role in resisting social changes introduced by the Revolution. This was particularly so in terms of the reduced role of the Catholic Church; for those living in rural areas, closing of the churches meant a loss of normality.[218] This sparked a counter-revolutionary movement led by women; while supporting other political and social changes, they opposed the dissolution of the Catholic Church and revolutionary cults like the Cult of the Supreme Being.[219] Olwen Hufton argues some wanted to protect the Church from heretical changes enforced by revolutionaries, viewing themselves as ""defenders of faith"".[220]
 Olympe de Gouges was an author whose publications emphasised that while women and men were different, this should not prevent equality under the law. In her Declaration of the Rights of Woman and of the Female Citizen she insisted women deserved rights, especially in areas concerning them directly, such as divorce and recognition of illegitimate children.[221][full citation needed] Along with other Girondists, she was executed in November 1793 during the Terror.
 Madame Roland, also known as Manon or Marie Roland, was another important female activist whose political focus was not specifically women but other aspects of the government. A Girondist, her personal letters to leaders of the Revolution influenced policy; in addition, she often hosted political gatherings of the Brissotins, a political group which allowed women to join. She too was executed in November 1793.[222]
 The Revolution abolished many economic constraints imposed by the Ancien Régime, including church tithes and feudal dues although tenants often paid higher rents and taxes.[223] All church lands were nationalised, along with those owned by Royalist exiles, which were used to back paper currency known as assignats, and the feudal guild system eliminated.[224] It also abolished the highly inefficient system of tax farming, whereby private individuals would collect taxes for a hefty fee. The government seized the foundations that had been set up (starting in the 13th century) to provide an annual stream of revenue for hospitals, poor relief, and education. The state sold the lands but typically local authorities did not replace the funding and so most of the nation's charitable and school systems were massively disrupted.[225]
 Between 1790 and 1796, industrial and agricultural output dropped, foreign trade plunged, and prices soared, forcing the government to finance expenditure by issuing ever increasing quantities assignats. When this resulted in escalating inflation, the response was to impose price controls and persecute private speculators and traders, creating a black market. Between 1789 and 1793, the annual deficit increased from 10% to 64% of gross national product, while annual inflation reached 3,500% after a poor harvest in 1794 and the removal of price controls. The assignats were withdrawn in 1796 but inflation continued until the introduction of the gold-based Franc germinal in 1803.[226]
 The French Revolution had a major impact on western history, by ending feudalism in France and creating a path for advances in individual freedoms throughout Europe.[227][2] The revolution represented the most significant challenge to political absolutism up to that point in history and spread democratic ideals throughout Europe and ultimately the world.[228] Its impact on French nationalism was profound, while also stimulating nationalist movements throughout Europe.[229] Some modern historians argue the concept of the nation state was a direct consequence of the revolution.[230] As such, the revolution is often seen as marking the start of modernity and the modern period.[231]
 
The long-term impact on France was profound, shaping politics, society, religion and ideas, and polarising politics for more than a century. Historian François Aulard wrote: The revolution permanently crippled the power of the aristocracy and drained the wealth of the Church, although the two institutions survived. Hanson suggests the French underwent a fundamental transformation in self-identity, evidenced by the elimination of privileges and their replacement by intrinsic human rights.[233] After the collapse of the First French Empire in 1815, the French public lost many of the rights and privileges earned since the revolution, but remembered the participatory politics that characterised the period. According to Paul Hanson, ""Revolution became a tradition, and republicanism an enduring option.""[234]
 The Revolution meant an end to arbitrary royal rule and held out the promise of rule by law under a constitutional order. Napoleon as emperor set up a constitutional system and the restored Bourbons were forced to retain one. After the abdication of Napoleon III in 1871, the French Third Republic was launched with a deep commitment to upholding the ideals of the Revolution.[235][236] The Vichy regime (1940–1944), tried to undo the revolutionary heritage, but retained the republic. However, there were no efforts by the Bourbons, Vichy or any other government to restore the privileges that had been stripped away from the nobility in 1789. France permanently became a society of equals under the law.[234]
 Agriculture was transformed by the Revolution. With the breakup of large estates controlled by the Church and the nobility and worked by hired hands, rural France became more a land of small independent farms. Harvest taxes were ended, such as the tithe and seigneurial dues. Primogeniture was ended both for nobles and peasants, thereby weakening the family patriarch, and led to a fall in the birth rate since all children had a share in the family property.[237] Cobban argues the Revolution bequeathed to the nation ""a ruling class of landowners.""[238]
 Economic historians are divided on the economic impact of the Revolution. One suggestion is the resulting fragmentation of agricultural holdings had a significant negative impact in the early years of 19th century, then became positive in the second half of the century because it facilitated the rise in human capital investments.[239] Others argue the redistribution of land had an immediate positive impact on agricultural productivity, before the scale of these gains gradually declined over the course of the 19th century.[240]
 In the cities, entrepreneurship on a small scale flourished, as restrictive monopolies, privileges, barriers, rules, taxes and guilds gave way. However, the British blockade virtually ended overseas and colonial trade, hurting the cities and their supply chains. Overall, the Revolution did not greatly change the French business system, and probably helped freeze in place the horizons of the small business owner. The typical businessman owned a small store, mill or shop, with family help and a few paid employees; large-scale industry was less common than in other industrialising nations.[241]
 Historians often see the impact of the Revolution as through the institutions and ideas exported by Napoleon. Economic historians Dan Bogart, Mauricio Drelichman, Oscar Gelderblom, and Jean-Laurent Rosenthal describe Napoleon's codified law as the French Revolution's ""most significant export.""[242] According to Daron Acemoglu, Davide Cantoni, Simon Johnson, and James A. Robinson the French Revolution had long-term effects in Europe. They suggest that ""areas that were occupied by the French and that underwent radical institutional reform experienced more rapid urbanization and economic growth, especially after 1850. There is no evidence of a negative effect of French invasion.""[243]
 The Revolution sparked intense debate in Britain. The Revolution Controversy was a ""pamphlet war"" set off by the publication of A Discourse on the Love of Our Country, a speech given by Richard Price to the Revolution Society on 4 November 1789, supporting the French Revolution. Edmund Burke responded in November 1790 with his own pamphlet, Reflections on the Revolution in France, attacking the French Revolution as a threat to the aristocracy of all countries.[244][245] William Coxe opposed Price's premise that one's country is principles and people, not the State itself.[246]
 Conversely, two seminal political pieces of political history were written in Price's favour, supporting the general right of the French people to replace their State. One of the first of these ""pamphlets"" into print was A Vindication of the Rights of Men by Mary Wollstonecraft . Wollstonecraft's title was echoed by Thomas Paine's Rights of Man, published a few months later. In 1792 Christopher Wyvill published Defence of Dr. Price and the Reformers of England, a plea for reform and moderation.[247] This exchange of ideas has been described as ""one of the great political debates in British history"".[248]
 In Ireland, the effect was to transform what had been an attempt by Protestant settlers to gain some autonomy into a mass movement led by the Society of United Irishmen involving Catholics and Protestants. It stimulated the demand for further reform throughout Ireland, especially in Ulster. The upshot was a revolt in 1798, led by Wolfe Tone, that was crushed by Britain.[249]
 German reaction to the Revolution swung from favourable to antagonistic. At first it brought liberal and democratic ideas, the end of guilds, serfdom and the Jewish ghetto. It brought economic freedoms and agrarian and legal reform. Above all the antagonism helped stimulate and shape German nationalism.[250]
 France invaded Switzerland and turned it into the ""Helvetic Republic"" (1798–1803), a French puppet state. French interference with localism and traditions was deeply resented in Switzerland, although some reforms took hold and survived in the later period of restoration.[251][252]
 During the Revolutionary Wars, the French invaded and occupied the region now known as Belgium between 1794 and 1814. The new government enforced reforms, incorporating the region into France. Resistance was strong in every sector, as Belgian nationalism emerged to oppose French rule. The French legal system, however, was adopted, with its equal legal rights, and abolition of class distinctions.[253]
 The Kingdom of Denmark adopted liberalising reforms in line with those of the French Revolution. Reform was gradual and the regime itself carried out agrarian reforms that had the effect of weakening absolutism by creating a class of independent peasant freeholders. Much of the initiative came from well-organised liberals who directed political change in the first half of the 19th century.[254]
 The Constitution of Norway of 1814 was inspired by the French Revolution,[255] and was considered to be one of the most liberal and democratic constitutions at the time.[256]
 Initially, most people in the Province of Quebec were favourable toward the revolutionaries' aims. The Revolution took place against the background of an ongoing campaign for constitutional reform in the colony by Loyalist emigrants from the United States.[257] Public opinion began to shift against the Revolution after the Flight to Varennes and further soured after the September Massacres and the subsequent execution of Louis XVI.[258] French migration to the Canadas experienced a substantial decline during and after the Revolution. Only a limited number of artisans, professionals, and religious emigres were allowed to settle in the region during this period.[259] Most emigres settled in Montreal or Quebec City.[259] The influx of religious emigres also revitalised the local Catholic Church, with exiled priests establishing a number of parishes across the Canadas.[259]
 In the United States, the French Revolution deeply polarised American politics, and this polarisation led to the creation of the First Party System. In 1793, as war broke out in Europe, the Democratic-Republican Party led by former American minister to France Thomas Jefferson favored revolutionary France and pointed to the 1778 treaty that was still in effect. George Washington and his unanimous cabinet, including Jefferson, decided that the treaty did not bind the United States to enter the war. Washington proclaimed neutrality instead.[260]
 The first writings on the French revolution were near contemporaneous with events and mainly divided along ideological lines. These included Edmund Burke's conservative critique Reflections on the Revolution in France (1790) and Thomas Paine's response Rights of Man (1791).[261] From 1815, narrative histories dominated, often based on first-hand experience of the revolutionary years. By the mid-nineteenth century, more scholarly histories appeared, written by specialists and based on original documents and a more critical assessment of contemporary accounts.[262]
 Dupuy identifies three main strands in nineteenth century historiography of the Revolution. The first is represented by reactionary writers who rejected the revolutionary ideals of popular sovereignty, civil equality, and the promotion of rationality, progress and personal happiness over religious faith. The second stream is those writers who celebrated its democratic, and republican values. The third were liberals like Germaine de Staël and Guizot, who accepted the necessity of reforms establishing a constitution and the rights of man, but rejected state interference with private property and individual rights, even when supported by a democratic majority.[263]
 Jules Michelet was a leading 19th-century historian of the democratic republican strand, and Thiers, Mignet and Tocqueville were prominent in the liberal strand.[264] Hippolyte Taine's Origins of Contemporary France (1875–1894) was modern in its use of departmental archives, but Dupuy sees him as reactionary, given his contempt for the crowd, and Revolutionary values.[265]
 The broad distinction between conservative, democratic-republican and liberal interpretations of the Revolution persisted in the 20th-century, although historiography became more nuanced, with greater attention to critical analysis of documentary evidence.[265][266] Alphonse Aulard (1849–1928) was the first professional historian of the Revolution; he promoted graduate studies, scholarly editions, and learned journals.[267][268] His major work, The French Revolution, a Political History, 1789–1804 (1905), was a democratic and republican interpretation of the Revolution.[269]
 Socio-economic analysis and a focus on the experiences of ordinary people dominated French studies of the Revolution from the 1930s.[270] Georges Lefebvre elaborated a Marxist socio-economic analysis of the revolution with detailed studies of peasants, the rural panic of 1789, and the behaviour of revolutionary crowds.[271][272] Albert Soboul, also writing in the Marxist-Republican tradition, published a major study of the sans-culottes in 1958.[273]
 Alfred Cobban challenged Jacobin-Marxist social and economic explanations of the revolution in two important works, The Myth of the French Revolution (1955) and Social Interpretation of the French Revolution (1964). He argued the Revolution was primarily a political conflict, which ended in a victory for conservative property owners, a result which retarded economic development.[274][275]
 In their 1965 work, La Revolution française, François Furet and Denis Richet also argued for the primacy of political decisions, contrasting the reformist period of 1789 to 1790 with the following interventions of the urban masses which led to radicalisation and an ungovernable situation.[276]
 From the 1990s, Western scholars largely abandoned Marxist interpretations of the revolution in terms of bourgeoisie-proletarian class struggle as anachronistic. However, no new explanatory model has gained widespread support.[231][277] The historiography of the Revolution has expanded into areas such as cultural and regional histories, visual representations, transnational interpretations, and decolonisation.[276]
"
Proclamation of Neutrality,https://en.wikipedia.org/wiki/Proclamation_of_Neutrality,"The Proclamation of Neutrality was a formal announcement issued by U.S. President George Washington on April 22, 1793, that declared the nation neutral in the conflict between revolutionary France and Great Britain. It threatened legal proceedings against any American providing assistance to any country at war.
 News that Revolutionary France had declared war on Great Britain in February 1793, and with this declaration that France, by the country's own volition, was now at war with all of Europe, did not reach America until the first half of April of that year.[1] President Washington was at Mount Vernon attending the funeral of a nephew when he was given the news.[2] He hurried back to Pennsylvania and summoned a cabinet meeting on April 19. It was unanimously agreed to issue a proclamation ""forbidding citizens to take part in any hostilities in the seas, on behalf of or against any of the belligerent powers.""[3]
 Washington's members agreed that neutrality was essential; the nation was too young and its military was too small to risk any sort of engagement with either France or Britain. Secretary of State Thomas Jefferson, in particular, saw in this question, the influence of the Federalists — his political rivals; yet he too agreed a proclamation was in order, though perhaps not an official one.
 In a cabinet meeting of January 14, Thomas Jefferson argued that while neutrality was a sine qua non, there was no real need to make a Proclamation of Neutrality either immediately or even officially; perhaps there might be no need for an official declaration at all. The United States could declare its neutrality for a price, Jefferson intimated, ""Why not stall and make countries bid for [American] neutrality?""[4] In response, Secretary of the Treasury Alexander Hamilton declared that American neutrality was not negotiable. Jefferson eventually resigned from his duty as Secretary of State in disagreement with the Proclamation of Neutrality.
 The proclamation started a war of pamphlets between Alexander Hamilton (writing for the Federalists) and James Madison (writing for the Democratic-Republicans), commonly known as the Pacificus–Helvidius Debates. In his seven essays, written under the pen name ""Pacificus"", Hamilton dealt with objections to the proclamation. Among his arguments were:
 Thomas Jefferson (having read several of the ""Pacificus"" essays) encouraged James Madison to reply. Madison was initially hesitant. From his Virginia plantation, he offered Jefferson excuses as to why he could not write a reply, including that he didn't have the necessary books and papers to refute ""Pacificus"", that the summer heat was ""oppressive"", and that he had many house guests who were wearing out their welcome.[6] Ultimately, Madison agreed to Jefferson's request, though afterward, he wrote to him, ""I have forced myself in to the task of a reply. I can truly say I find it the most grating one I have ever experienced.""[7]
 Writing under the name ""Helvidius"", Madison's five essays showed the animosity that had evolved with the two political factions. He attacked Federalists, and Hamilton in particular, and anyone who supported the Neutrality Proclamation as secret monarchists, declaring: ""Several features with the signature of Pacificus were [as of] late published, which have been read with singular pleasure and applause by the foreigners and degenerate citizens among us, who hate our republican government and the French Revolution.""[7] Madison brought to light the strict constructionist's view of both the Constitution and the Proclamation, demanding that Congress, not the president, had full authority over all foreign affairs except those areas specified in the Constitution.
 The debate among Jefferson/Madison and Hamilton regarding the Proclamation is portrayed in the song ""Cabinet Battle #2"" in the musical Hamilton.
"
Jay Treaty,https://en.wikipedia.org/wiki/Jay_Treaty,"

 The Treaty of Amity, Commerce, and Navigation, Between His Britannic Majesty and the United States of America, commonly known as the Jay Treaty, and also as Jay's Treaty, was a 1794 treaty between the United States and Great Britain that averted war, resolved issues remaining since the 1783 Treaty of Paris (which ended the American Revolutionary War),[1] and facilitated ten years of peaceful trade between Americans and the British in the midst of the French Revolutionary Wars, which had begun in 1792.[2] For the Americans, the treaty's policy was designed by Treasury secretary Alexander Hamilton, supported by President George Washington. It angered France and bitterly divided American public opinion, encouraging the growth of two opposing American political parties, the pro-Treaty Federalists and the anti-Treaty Democratic-Republicans.
 The treaty was negotiated by John Jay (also a negotiator of the earlier Paris treaty) and gained several of the primary American goals. This included a British withdrawal from forts in the Northwest Territory that Britain had refused to relinquish under the terms of the Treaty of Paris. The British had refused to do so as the United States had reneged on Articles 4 and 6 of the Treaty of Paris; American state courts impeded the collection of debts owed to British creditors and upheld the continued confiscation of Loyalist-owned property in spite of an explicit understanding that such prosecutions would be immediately discontinued.[3] Both parties agreed that disputes over wartime debts and the boundaries of the Canada–United States border were to be sent to arbitration (one of the first major uses of arbitration in modern diplomatic history), which set a precedent used by other nations. American merchants were granted limited rights to trade with the British West Indies in exchange for limits on export of cotton from the U.S.
 Signed on November 19, 1794[4] during the Thermidorian Reaction in France, the treaty was submitted to the United States Senate for its advice and consent the following June. It was ratified by the Senate on June 24, 1795, by a two-thirds majority vote of 20–10 (exactly the minimum number necessary for concurrence). It was also ratified by the First Pitt ministry, and took effect February 29, 1796, the day when ratifications were officially exchanged.
 The treaty was hotly contested by Democratic-Republicans in each state. An effort was made to block it in the House of Representatives, which ultimately failed. Democratic-Republican politicians feared that closer American economic and political ties with Britain would strengthen Hamilton's Federalist Party, promote aristocracy and undercut republicanism. This debate crystallized the emerging partisan divisions and shaped the new ""First Party System"", with Federalists favoring the British and Democratic-Republicans favoring France. The treaty was to last for ten years, and efforts failed to agree on a replacement treaty in 1806 when Jefferson rejected the Monroe–Pinkney Treaty in the lead-up to the War of 1812.[5]
 The outbreak of the French Revolutionary Wars in Europe in 1792 ended the long peace that had enabled the new United States to flourish in terms of trade and finance. The Americans emerged as an important neutral country with a large shipping trade.[6] From the British perspective, improving their relations with the United States was a high priority lest it move into the French sphere of influence. British negotiators ignored elements in Britain that wanted harsher terms in order to get a suitable treaty.[7] From the American viewpoint, the most pressing foreign policy issues were normalizing the trade relations with Britain, the United States' leading trading partner, and resolving issues left over from the Treaty of Paris. As one observer explained, the British government was ""well disposed to America... They have made their arrangements upon a plan that comprehends the neutrality of the United States, and are anxious that it should be preserved.""[8]
 After Britain became involved in the conflict against France in 1793, the Royal Navy seized nearly 300 American merchant ships trading with the French West Indies.[9] The American public was outraged and Republicans in Jefferson's coalition demanded a declaration of war on Britain, but James Madison instead called for an embargo on British trade instead.[10] 
 At the same time, the British continued to supply weapons to Native Americans resisting U.S. expansion in the Ohio Country, further worsening Anglo-American relations. Congress voted on a trade embargo against Britain in March 1794.[11] It was approved in the House of Representatives but defeated in the Senate when Vice-President John Adams cast a tie-breaking vote against it.[12] At the national level American politics was divided between the factions of Jefferson and Madison, which favored the French, and the Federalists led by Hamilton, who saw Britain as a natural ally and thus sought to normalize relations with the British, especially in the area of trade. President George Washington sided with Hamilton. Hamilton devised a framework for negotiations, and Washington sent Chief Justice of the United States John Jay to London to negotiate a comprehensive treaty.
 The U.S. government had several outstanding issues to resolve with Britain:[13] 
 Both sides achieved many objectives. Several issues were sent to arbitration, which (after years of discussion) were resolved amicably mostly in favor of the U.S. Britain paid $11,650,000 for damages to American shipping and received £600,000 for unpaid pre-1775 debts. While international arbitration was not entirely unknown, the Jay Treaty gave it a strong impetus and is generally taken as the start of modern international arbitration.[20]
 The British agreed to vacate its forts in United States territory—six in the Great Lakes region and two at the north end of Lake Champlain—by June 1796; which was done. They were:
 The treaty was ""surprisingly generous"" in allowing Americans to trade with Great Britain on a most-favored-nation basis.[21] In return, the United States gave most favored nation trading status to Britain, and acquiesced in British anti-French maritime policies. American merchants obtained limited rights to trade in the British West Indies.[22] Two joint boundary commissions were set up to establish the boundary line in the Northeast (it agreed on one) and in the Northwest (this commission never met and the boundary was settled after the War of 1812).[23]
 Jay, a strong opponent of slavery despite being a slaveholder, dropped the issue of compensation to U.S. enslavers, which angered Southern slaveholders and was used as a target for attacks by Jeffersonians.[24] Jay was unsuccessful in negotiating a temporary end to the impressment of American citizens into the Royal Navy, which later became a key issue leading to the War of 1812.
 Article III states, ""It is agreed, that it shall at all times be free to His Majesty's subjects, and to the citizens of the United States, and also to the Indians dwelling on either side of the said boundary line, freely to pass and repass, by land or inland navigation into the respective territories and countries of the two parties on the continent of America, (the country within the limits of the Hudson's Bay Company only excepted) ... and freely carry on trade and commerce with each other."" Article III of the Jay Treaty declared the right of Native Americans, American citizens, and Canadian subjects to trade and travel between the United States and Canada, which was then a territory of Great Britain.[25] Some legal experts dispute whether the treaty rights were abrogated by the War of 1812.[26] Nevertheless, the United States has codified this right in the provisions of Section 289 of the Immigration and Nationality Act of 1952 and as amended in 1965. As a result of the Jay Treaty, ""Native Indians born in Canada are therefore entitled to enter the United States for the purpose of employment, study, retirement, investing, and/or immigration"" if they can prove that they have at least 50% blood quantum, and cannot be deported for any reason.[27][28] Article III of the Jay Treaty is the basis of most Native American claims.[29] Unlike other legal immigrants, Canadian-born Native Americans residing in the US are entitled to public benefits and domestic tuition fees on the same basis as citizens.[27]
 Washington submitted the treaty to the United States Senate for its consent in June 1795; a two-thirds vote was needed. The treaty was unpopular at first and gave the Jeffersonians a platform to rally new supporters. As historian Paul Varg explains,
 The Jeffersonians were opposed to Britain, preferring support for France in the wars raging in Europe, and they argued that the treaty with France from 1778 was still in effect. They considered Britain as the center of aristocracy and the chief threat to the United States' republican values. They denounced Hamilton and Jay (and even Washington) as monarchists who betrayed American values. They organized public protests against Jay and his treaty; one of their rallying cries said: Damn John Jay! Damn everyone that won't damn John Jay! Damn every one that won't put lights in his window and sit up all night damning John Jay![31] Town hall meetings in Philadelphia turned from debate to disorder in the summer of 1795 as rocks were thrown, British officials harassed, and a copy of the treaty burnt at the door of one of America's wealthiest merchants and U.S. Senator, William Bingham.[32]
 The treaty was one of the major catalysts for the advent of the First Party System in the United States by further dividing the two major political factions within the country. The Federalist Party, led by Hamilton, supported the treaty. On the contrary, the Democratic-Republican Party, led by Jefferson and Madison, opposed it. Jefferson and his supporters had a counter-proposal to establish ""a direct system of commercial hostility with Great Britain"", even at the risk of war. The Jeffersonians raised public opinion to fever pitch by accusing the British of supporting Native American attacks on U.S. colonizers on the frontier.[33] The fierce debates over the treaty in 1794–95, according to one historian, ""transformed the Republican movement into a Republican party"". To fight the treaty, the Jeffersonians ""established coordination in activity between leaders at the capital, and leaders, actives and popular followings in the states, counties and towns"".[34] Jay's failure to obtain compensation for ""lost"" slaves galvanized the South into opposition.[35]
 Washington supported the Jay Treaty because he did not want American merchant ships to be at risk of being seized by the powerful Royal Navy, and he decided to take his chances with a hostile French Navy that would mostly be bottled up in Europe by the British blockade. By backing the treaty, he sacrificed the unanimous respect and goodwill that the whole country had given across his service as commander-in-chief of the Continental Army, president of the Constitutional Convention, and President of the United States to that point. He was heavily criticized in Democratic-Republican areas of the country like his home state of Virginia. Numerous protestors would picket Mount Vernon and show their anger towards him. Newspapers and cartoons showed Washington being sent to the guillotine. A common protest rally cry was, ""A speedy death to General Washington."" Some protestors even wanted Washington to be impeached. It was only after Washington's death in 1799 when the whole country reunited and wholeheartedly respected him again.[36]
 The Federalists fought back and Congress rejected the Jefferson–Madison counter-proposals. Washington threw his great prestige behind the treaty, and Federalists rallied public opinion more effectively than did their opponents.[37] Hamilton convinced President Washington that it was the best treaty that could be expected. Washington insisted that the U.S. must remain neutral in the European wars; he signed it, and his prestige carried the day in Congress. The Federalists made a strong, systematic appeal to public opinion, which rallied their own supporters and shifted the debate. Washington and Hamilton outmaneuvered Madison, who was opposition leader.[38] Hamilton by then was out of the government, and he was the dominant figure who helped secure the treaty's approval by the needed .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}2⁄3 vote in the Senate. The Senate passed a resolution in June, advising the President to amend the treaty by suspending the 12th article, which concerned trade between the U.S. and the West Indies. In mid-August, the Senate ratified the treaty 20–10, with the condition that the treaty contain specific language regarding the June 24 resolution. President Washington signed it in late August. The treaty was proclaimed in effect on February 29, 1796, but there remained one final, bitter legislative battle. The House of Representatives, which had a Democratic-Republican majority, had to agree to appropriate the funds needed to fulfill the Jay Treaty's terms. In April 1796, after two months of bitter fighting that could have doomed the treaty if the House refused to pass the funding related to the Jay Treaty, Federalist Representative Fisher Ames limped to the podium despite being gravely sick and gave an impassioned speech that was later described as one of the greatest speeches in American history in defense of the Jay Treaty. After the 90-minute speech he fell exhausted in his chair and there was an emotional silence in a sign of bipartisan respect for his speech. In the final vote on April 29, 1796, the impasse was stuck in a 49 to 49 tie. The first Speaker of the House (no longer in that office by 1796), Democratic-Republican Representative Frederick Muhlenberg, was chairman of the Committee of the Whole that was responsible for this funding bill. The year before he had led protests that included burning copies of the Jay Treaty in front of the home of the British ambassador to the United States, George Hammond. Everyone in the House chamber believed Muhlenberg was going to kill the Jay Treaty. He shockingly voted yes to fund the treaty. The final vote after one representative flipped his vote to support Muhlenberg after Muhlenberg's tiebreaking decision was 51 to 48. As a symbol of how chaotic and violent the anti-Jay Treaty protests were from 1794 to 1796 Muhlenberg not only killed his political career with his decision but he was stabbed by his brother-in-law who believed he had committed treason when he voted in support of the funding of the Jay Treaty. Muhlenberg survived the attack but faded into obscurity for the rest of his life, never winning another election.[39][36][40][41]
 James Madison, then a member of the House of Representatives, argued that the treaty could not, under Constitutional law, take effect without approval of the House, since it regulated commerce and exercised legislative powers granted to Congress. The debate which followed was an early example of originalism, in which Madison, the ""Father of the Constitution"", lost.[42]  One feature of this nationwide constitutional debate was an advisory opinion on the subject written by Chief Justice Oliver Ellsworth, in which he rejected any alleged right of the House of Representatives to decide upon the merits of the treaty.[43] After defeat on the treaty in Congress, the Jeffersonian Republicans lost the 1796 presidential election on the issue.
 When Thomas Jefferson became president in 1801, he did not repudiate the treaty. He kept the Federalist minister, Rufus King, in London to negotiate a successful resolution to outstanding issues regarding cash payments and boundaries. The amity broke down when the treaty expired in 1805. Jefferson rejected a renewal of the Jay Treaty in the Monroe–Pinkney Treaty of 1806 as negotiated by his diplomats and agreed to by London. Relations turned increasingly hostile as a prelude to the War of 1812. In 1815, the Treaty of Ghent superseded the Jay Treaty.[citation needed]
 The treaty led to the permanent rupture of decades of close friendship and camaraderie between President Washington and the anti-Jay Treaty Thomas Jefferson and James Madison. Jefferson wrote a scathing private letter that secretly called Washington senile and an “apostate” who subverted American liberty to “the harlot England”. He also secretly financed and ordered newspapers to personally attack Washington with accusations of mental illness and treason. Madison, even though he wrote the Constitution, claimed as a partisan Democratic-Republican member of the House of Representatives that the House also has an equal role in the treaty-making process. Washington had to personally find the secret minutes of the 1787 Constitutional Convention where Madison himself said treaties are conducted by only the Senate and President, and he had to argue against Madison with Madison's own words from the Constitutional Convention. Madison forced Washington to invoke executive privilege over this issue. Washington never saw or spoke to Jefferson and Madison ever again after the ratification of the Jay Treaty. Martha Washington said that the election of Jefferson as president in 1800 was the second worst day of her life after the death of her husband, and she believed the shocking words and actions by them towards Washington hastened his death only 2 years after leaving office.[36][44][45][46]
 Historians Stanley Elkins and Eric McKitrick note that, in conventional diplomatic terms and as a ""piece of adversary bargaining"", Jay ""got much the worst of the 'bargain'. Such a view has to a great degree persisted ever since.""[47] They conclude that Jay did not succeed in asserting neutral rights, but he did obtain ""his other sine qua nons""; he got none of things that were ""desirable, but not indispensable"".[48] They add that Jay's record on the symbolic side was open to many objections. However, on the ""hard"" (or realistic) side, ""it was a substantial success, which included the prevention of war with Great Britain"".[49]
 Historian Marshall Smelser argues that the treaty effectively postponed war with Britain, or at least postponed it until the United States was strong enough to handle it.[50] Bradford Perkins argued in 1955 that the treaty was the first to establish a special relationship between Britain and the United States, with a second installment under Lord Salisbury. In his view, the treaty worked for ten years to secure peace between Britain and America: ""The decade may be characterized as the period of 'The First Rapprochement'."" As Perkins concludes,
 Starting at swords' point in 1794, the Jay Treaty reversed the tensions; Perkins concludes: ""Through a decade of world war and peace, successive governments on both sides of the Atlantic were able to bring about and preserve a cordiality which often approached genuine friendship.""[52] Perkins suggests that, except perhaps the opening of trade with British India, ""Jay did fail to win anything the Americans were not obviously entitled to, liberation of territory recognized as theirs since 1782, and compensation for seizures that even Britain admitted were illegal"". He also speculates that a ""more astute negotiator than the Chief Justice"" would have gotten better terms than he did.[53] He quoted the opinion of ""great historian"" Henry Adams that the treaty was a ""bad one"":
 Perkins gave more weight than other historians to valuable concessions regarding trade with British India and the concession on the West Indies trade. In addition, Perkins noted that the Royal Navy treated American commerce with ""relative leniency"" during the late 18th and early 19th centuries, and many impressed seamen were returned to America. As Spain assessed the informal Anglo-American alliance, it softened its previous opposition to the United States' use of the Mississippi River and signed Pinckney's Treaty, which the Americans wanted. When Jefferson took office, he gained renewal of the commercial articles that had greatly benefited American shipping.[54]
 Elkins and McKitrick find this more positive view open to ""one big difficulty"": it requires that the British negotiated in the same spirit. Unlike Perkins, they find ""little indication of this"".[55] George Herring's 2008 history of US foreign policy says that, in 1794, ""the United States and Britain edged toward war"" and concludes, ""The Jay Treaty brought the United States important concessions and served its interests well.""[56] Joseph Ellis finds the terms of the treaty ""one-sided in Britain's favor"", but asserts with a consensus of historians that it was
 In the HBO miniseries John Adams, Vice President John Adams is shown casting the tiebreaker vote in favor of ratifying the Jay Treaty. In reality, his vote was never required as the Senate passed the resolution by 20–10. Furthermore, the Vice President would never be required to cast a vote in a treaty ratification, because the Vice President votes only in case of a tie, and Article II of the Constitution requires that treaties receive a two-thirds vote for approval. Vice President Adams had however earlier cast a tie-breaking vote in opposition to a trade embargo on the British in 1794.[12]
  Works related to Jay's Treaty at Wikisource
"
Executive Office of the President of the United States,https://en.wikipedia.org/wiki/Executive_Office_of_the_President_of_the_United_States,"
 The Executive Office of the President of the United States (EOP) comprises the offices and agencies[2] that support the work of the president at the center of the executive branch of the United States federal government.[3] The office consists of several offices and agencies, such as the White House Office (the staff working closest with the president, including West Wing staff), the National Security Council, Homeland Security Council, Office of Management and Budget, Council of Economic Advisers, and others.[4]  The Eisenhower Executive Office Building houses most staff.
 The office is also referred to as a ""permanent government"", since many policy programs, and the people who are charged with implementing them, continue between presidential administrations.[5]
 The civil servants who work in the Executive Office of the President are regarded as nonpartisan and politically neutral, so they are capable of providing objective and impartial advice.[5]
 With the increase in technological and global advancement, the size of the White House staff has increased to include an array of policy experts responsible with managing various federal governmental functions and policy areas. As of 2015, it included approximately 1,800 positions,[6] most of which did not require confirmation from the U.S. Senate.
 The office is overseen by the White House chief of staff. Since January 20 2025, that position has been held by Susie Wiles, who was appointed by President Donald Trump.[7][8][9][10]
 In 1937, the Brownlow Committee, which was a presidentially commissioned panel of political science and public administration experts, recommended sweeping changes to the executive branch of the U.S. federal government, including the creation of the Executive Office of the President. Based on these recommendations, President Franklin D. Roosevelt in 1939 lobbied Congress to approve the Reorganization Act of 1939. The Act led to Reorganization Plan No. 1,[11] which created the office,[12] which reported directly to the president.
 The office encompassed two subunits at its outset, the White House Office (WHO) and the Bureau of the Budget, the predecessor to today's Office of Management and Budget, which was created in 1921 and originally located in the Treasury Department. It absorbed most of the functions of the National Emergency Council.[13] Initially, the new staff system appeared more ambitious on paper than in practice; the increase in the size of the staff was quite modest at the start. However, it laid the groundwork for the large and organizationally complex White House staff that emerged during the presidencies of Roosevelt's successors.[14]
 Roosevelt's efforts are also notable in contrast to those of his predecessors in office. During the 19th century, presidents had few staff resources. Thomas Jefferson had one messenger and one secretary at his disposal, both of whose salaries were paid by the president personally. It was not until 1857 that Congress appropriated money ($2,500) for the hiring of one clerk.[15]
 By Ulysses S. Grant's presidency (1869–1877), the staff had grown to three.[16] By 1900, the White House staff included one ""secretary to the president"" (then the title of the president's chief aide), two assistant secretaries, two executive clerks, a stenographer, and seven other office personnel. Under Warren G. Harding, there were thirty-one staff, although most were in clerical positions.
 During Herbert Hoover's presidency, two additional secretaries to the president were added by Congress, one of whom Hoover designated as his press secretary.[17] From 1933 to 1939, as he greatly expanded the scope of the federal government's policies and powers in response to the Great Depression, Roosevelt relied on his ""brain trust"" of top advisers, who were often appointed to vacant positions in agencies and departments, from which they drew their salaries since the White House lacked statutory or budgetary authority to create new staff positions.
 After World War II, in particular, during the Eisenhower presidency, the staff was expanded and reorganized. Eisenhower, a former U.S. Army general, had been Supreme Allied Commander during the war and reorganized the Executive Office to suit his leadership style.[18]
 As of 2009, the staff is much bigger. Estimates indicate some 3,000 to 4,000 persons serve in office staff positions with policy-making responsibilities, with a budget of $300 to $400 million (George W. Bush's budget request for Fiscal Year 2005 was for $341 million in support of 1,850 personnel).[19]
 Some observers have noted a problem of control for the president due to the increase in staff and departments, making coordination and cooperation between the various departments of the Executive Office more difficult.[20]
 The president had the power to reorganize the Executive Office due to the 1949 Reorganization Act which gave the president considerable discretion, until 1983 when it was renewed due to President Reagan's administration allegedly encountering ""disloyalty and obstruction"".[20]
 The chief of staff is the head of the Executive Office and can therefore ultimately decide what the president needs to deal with personally and what can be dealt with by other staff.
 Senior staff within the Executive Office of the President have the title Assistant to the President, second-level staff have the title Deputy Assistant to the President, and third-level staff have the title Special Assistant to the President.[21]
 The core White House staff appointments, and most Executive Office officials generally, are not required to be confirmed by the U.S. Senate, although there are a handful of exceptions (e.g., the director of the Office of Management and Budget, the chair of the Council of Economic Advisers, and the United States Trade Representative).[22]
 The information in the following table is current as of January 20, 2025. Only principal executives are listed; for subordinate officers, see individual office pages.

 The White House Office (including its various offices listed below[25]) is a sub-unit of the Executive Office of the President (office). The various agencies of the office are listed above.
 
 Congress as well as the president has some control over the Executive Office of the President. Some of this authority stems from its appropriation powers given by the Constitution, such as the ""power of the purse"", which affects the Office of Management and Budget and the funding of the rest of federal departments and agencies. Congress also has the right to investigate the operation of the Executive Office, normally holding hearings bringing forward individual personnel to testify before a congressional committee.[5]
 The Executive Office often helps with legislation by filling in specific points understood and written by experts, as Congressional legislation sometimes starts in broad terms.[5]
 This table specifies the budget of the Executive Office for the years 2008–2017, and the actual outlays for the years 1993–2007.
"
Republicanism in the United States,https://en.wikipedia.org/wiki/Republicanism_in_the_United_States,"

 The values and ideals of republicanism are foundational in the constitution and history of the United States.[2][3] As the United States constitution prohibits granting titles of nobility, republicanism in this context does not refer to a political movement to abolish such a social class, as it does in countries such as the United Kingdom, Australia, and the Netherlands. Instead, it refers to the core values that citizenry in a republic have,[4][5] or ought to have.
 Political scientists and historians have described these central values as liberty and inalienable individual rights; recognizing the sovereignty of the people as the source of all authority in law;[6] rejecting monarchy, aristocracy, and hereditary political power; virtue and faithfulness in the performance of civic duties; and vilification of corruption.[7] These values are based on those of Ancient Greco-Roman, Renaissance, and English models and ideas.[8] Articulated in the writings of the Founding Fathers (particularly Thomas Jefferson, James Madison, and John Adams),[9] they formed the intellectual basis for the American Revolution – the Declaration of Independence (1776), the Constitution (1787), and the Bill of Rights (1791), as well as the Gettysburg Address (1863).[10]
 Politicians and scholars have debated the connection of these values with issues like honest government, democracy, individualism, property rights, military service; or their compatibility with slavery, inequitable distribution of wealth, economic self-interest, limits on the rights of minorities, and national debt.
 In the United States Constitution, republic is mentioned once, in section four of Article Four, where it is stated: ""The United States shall guarantee to every State in this Union a Republican Form of Government ..."".
Two major political parties in American history have used the term in their name[11] – the Democratic-Republican Party of Thomas Jefferson (1793–1824; also known as the Jeffersonian Republican Party) and the Republican Party (founded in 1854 and named after the Jeffersonian party).[12]
 The colonial intellectual and political leaders in the 1760s and 1770s closely read history to compare governments and their effectiveness of rule.[13] The Revolutionists were especially concerned with the history of liberty in England and were primarily influenced by the ""country party"" (which opposed the ""court party"" that held power). Country party philosophy relied heavily on the classical republicanism of Roman heritage; it celebrated the ideals of duty and virtuous citizenship in a republic. It drew heavily on ancient Greek city-state and Roman republican examples.[14] The country party shared some of the political philosophy of Whiggism as well as Tory critics in England which roundly denounced the corruption surrounding the ""court party"" in London centering on the royal court. This approach produced a political ideology Americans called ""republicanism"", which was widespread in colonial America by 1775.[15] ""Republicanism was the distinctive political consciousness of the entire Revolutionary generation.""[16] J.G.A. Pocock explained the intellectual sources in America:
 American republicanism was centered on limiting corruption and greed. Virtue was of the utmost importance for citizens and representatives. Revolutionaries took a lesson from ancient Rome; they knew it was necessary to avoid the luxury that had destroyed the empire.[18] A virtuous citizen was one who ignored monetary compensation and made a commitment to resist and eradicate corruption. The republic was sacred; therefore, it was necessary to serve the state in a truly representative way, ignoring self-interest and individual will. Republicanism required the service of those who were willing to give up their own interests for a common good. According to Bernard Bailyn, ""The preservation of liberty rested on the ability of the people to maintain effective checks on wielders of power and hence in the last analysis rested on the vigilance and moral stamina of the people. ... "" Virtuous citizens needed to be strong defenders of liberty and challenge the corruption and greed in government. The duty of the virtuous citizen became a foundation for the American Revolution.[19][20]
 The commitment of Patriots to republican values of the era was a key intellectual foundation of the American Revolution. In particular, the key was Patriots' intense fear of political corruption and the threat it posed to liberty. Bernard Bailyn states, ""The fact that the ministerial conspiracy against liberty had risen from corruption was of the utmost importance to the colonists.""[21] In 1768 to 1773 newspaper exposés such as John Dickinson's series of ""Letters from a Farmer in Pennsylvania"" (1767–68) were widely reprinted and spread American disgust with British corruption. The patriot press provided emphasized British corruption, mismanagement, and tyranny.[22] Britain was increasingly portrayed as corrupt and hostile and that of a threat to the very idea of democracy; a threat to the established liberties that colonists enjoyed and to colonial property rights. The greatest threat to liberty was thought by many to be corruption – not just in London but at home as well. The colonists associated it with luxury and, especially, inherited aristocracy, which they condemned. Historian J.G.A. Pocock argues that Republicanism explains the American Revolution in terms of virtuous Republican resistance to British imperial corruption.[23]
 Historian Sarah Purcell studied the sermons preached by the New England patriot clergy in 1774–1776. They stirred up a martial spirit justified war against England. The preachers cited New England's Puritan history in defense of freedom, blamed Britain's depravity and corruption for the necessity of armed conflict. The sermons called on soldiers to behave morally and in a ""manly"" disciplined fashion. The rhetoric not only encouraged heavy enlistment but helped create the intellectual climate the Patriots needed to fight a civil war.[24] Historian Thomas Kidd argues that during the Revolution active Christians linked their religion to republicanism. He states, ""With the onset of the revolutionary crisis, a major conceptual shift convinced Americans across the theological spectrum that God was raising up America for some special purpose.""[25] Kidd further argues that ""new blend of Christian and republican ideology led religious traditionalists to embrace wholesale the concept of republican virtue.""[26]
 Historian Gordon Wood has tied the founding ideas to American exceptionalism: ""Our beliefs in liberty, equality, constitutionalism, and the well-being of ordinary people came out of the Revolutionary era. So too did our idea that we Americans are a special people with a special destiny to lead the world toward liberty and democracy.""[27] Americans were the protectors of liberty, they had a greater obligation and destiny to assert republican virtue. In Discourse of 1759 Jonathan Mayhew states ""An absolute submission to our prince, or whether disobedience and resistance may not be justified able in some cases ... to all those who bear the title of rulers in common but only to those who actually perform the duty of rulers by exercising a reasonable and just authority for the good of human society."" The notion that British rulers were not virtuous, nor exercising their authority for the ""good of human society"" prompted the colonial desire to protect and reestablish republican values in government. This need to protect virtue was a philosophical underpinning of the American Revolution.[28]
 The ""Founding Fathers"" were strong advocates of republican values, especially Samuel Adams, Patrick Henry, George Washington, Thomas Paine, Benjamin Franklin, John Adams, Thomas Jefferson, James Madison and Alexander Hamilton.[29]
 Thomas Jefferson defined a republic as:
 The Founding Fathers discoursed endlessly on the meaning of ""republicanism."" John Adams in 1787 defined it as ""a government, in which all men, rich and poor, magistrates and subjects, officers and people, masters and servants, the first citizen and the last, are equally subject to the laws.""[31]
 The open question, as Pocock suggested,[32] of the conflict between personal economic interest (grounded in Lockean liberalism) and classical republicanism, troubled Americans. Jefferson and Madison roundly denounced the Federalists for creating a national bank as tending to corruption and monarchism; Alexander Hamilton staunchly defended his program, arguing that national economic strength was necessary for the protection of liberty. Jefferson never relented but by 1815 Madison switched and announced in favor of a national bank, which he set up in 1816.
 John Adams often pondered the issue of civic virtue. Writing Mercy Otis Warren in 1776, he agreed with the Greeks and the Romans, that, ""Public Virtue cannot exist without private, and public Virtue is the only Foundation of Republics."" Adams insisted, ""There must be a positive Passion for the public good, the public Interest, Honor, Power, and Glory, established in the Minds of the People, or there can be no Republican Government, nor any real Liberty. And this public Passion must be Superior to all private Passions. Men must be ready, they must pride themselves, and be happy to sacrifice their private Pleasures, Passions, and Interests, nay their private Friendships and dearest connections, when they Stand in Competition with the Rights of society.""[33]
 Adams worried that a businessman might have financial interests that conflicted with republican duty; indeed, he was especially suspicious of banks. He decided that history taught that ""the Spirit of Commerce ... is incompatible with that purity of Heart, and Greatness of soul which is necessary for a happy Republic."" But so much of that spirit of commerce had infected America. In New England, Adams noted, ""even the Farmers and Tradesmen are addicted to Commerce."" As a result, there was ""a great Danger that a Republican Government would be very factious and turbulent there.""[34]
 A second stream of thought growing in significance was the classical liberalism of John Locke, including his theory of the ""social contract"". This had a great influence on the revolution as it implied the inborn right of the people to overthrow their leaders should those leaders betray the agreements implicit in the sovereign-follower relationship. Historians find little trace of Jean-Jacques Rousseau's influence in America.[35] In terms of writing state and national constitutions, the Americans used Montesquieu's analysis of the ideally ""balanced"" British Constitution. But first and last came a commitment to republicanism, as shown by many historians such as Bernard Bailyn and Gordon S. Wood.[36]
 For a century, historians have debated how important republicanism was to the Founding Fathers. The interpretation before 1960, following Progressive School historians such as Charles A. Beard, Vernon L. Parrington and Arthur M. Schlesinger, Sr., downplayed rhetoric as superficial and looked for economic motivations. Louis Hartz refined the position in the 1950s, arguing John Locke was the most important source because his property-oriented liberalism supported the materialistic goals of Americans.[37]
 In the 1960s and 1970s, two new schools emerged that emphasized the primacy of ideas as motivating forces in history (rather than material self-interest). Bernard Bailyn, Gordon Wood from Harvard formed the ""Cambridge School""; at Washington University the ""St. Louis School"" was led by J.G.A. Pocock. They emphasized slightly different approaches to republicanism.[38] However, some scholars, especially Isaac Kramnick and the late Joyce Appleby, continue to emphasize Locke, arguing that Americans are fundamentally individualistic and not devoted to civic virtue. The relative importance of republicanism and liberalism remains a topic of strong debate among historians, as well as the politically active of the present day.
 In 1792–93, Jefferson and Madison created a new ""Democratic-Republican party"" in order to promote their version of the doctrine. They wanted to suggest that Hamilton's version was illegitimate.[39] According to Federalist Noah Webster, a political activist bitter at the defeat of the Federalist party in the White House and Congress, the choice of the name ""Democratic-Republican"" was ""a powerful instrument in the process of making proselytes to the party. ... The influence of names on the mass of mankind, was never more distinctly exhibited, than in the increase of the democratic party in the United States. The popularity of the denomination of the Republican Party, was more than a match for the popularity of Washington's character and services, and contributed to overthrow his administration.""[40] The party, which historians later called the Democratic-Republican Party, split into separate factions in the 1820s, one of which became the Democratic Party. After 1832, the Democrats were opposed by another faction that named themselves ""Whigs"" after the Patriots of the 1770s who started the American Revolution. Both of these parties proclaimed their devotion to republicanism in the era of the Second Party System.
 Under the new government after the revolution, ""republican motherhood"" became an ideal, as exemplified by Abigail Adams and Mercy Otis Warren. The first duty of the republican woman was to instill republican values in her children, and to avoid luxury and ostentation.[41]
 Two generations later, the daughters and granddaughters of these ""Republican mothers"" appropriated republican values into their lives as they sought independence and equality in the workforce. During the 1830s, thousands of female mill workers went on strike to battle for their right to fair wages and independence, as there had been major pay cuts. Many of these women were daughters of independent land owners and descendants of men who had fought in the Revolutionary War; they identified as ""daughters of freemen"". In their fight for independence at the mills, women would incorporate rhetoric from the revolution to convey the importance and strength of their purpose to their corporate employers, as well as to other women. If the Revolutionary War was fought to secure independence from Great Britain, then these ""daughters of freemen"" could fight for the same republican values that (through striking) would give them fair pay and independence, just as the men had.[42]
 Supreme Court Justice Joseph Story (1779–1845), made the protection of property rights by the courts a major component of American republicanism. A precocious legal scholar, Story was appointed to the Court by James Madison in 1811. He and Chief Justice John Marshall made the Court a bastion of nationalism (along the lines of Marshall's Federalist Party) and a protector of the rights of property against runaway democracy. Story opposed Jacksonian democracy because it was inclined to repudiate lawful debts and was too often guilty of what he called ""oppression"" of property rights by republican governments.[43] Story held that, ""the right of the citizens to the free enjoyment of their property legally acquired"" was ""a great and fundamental principle of a republican government.""[44] Newmyer (1985) presents Story as a ""Statesman of the Old Republic"" who tried to rise above democratic politics and to shape the law in accordance with the republicanism of Story's heroes, Alexander Hamilton and John Marshall, as well as the New England Whigs of the 1820s and 1830s, such as Daniel Webster.[45] Historians agree that Justice Story – as much or more than Marshall or anyone else – did indeed reshape American law in a conservative direction that protected property rights.[46]
 According to journalist Jamelle Bouie, ""among the oldest and most potent strains of American thinking"" about self-government is the belief that it cannot coexist ""with mass immiseration and gross disparities of wealth and status"".[47]
 He quotes John Adams in a 1776 letter:
 Political scientists Jacob S. Hacker and Paul Pierson quote a warning by Greek-Roman historian Plutarch: ""An imbalance between rich and poor is the oldest and most fatal ailment of all republics.""[50] Some academic researchers allege that the US political system risks drifting towards oligarchy, through the influence of corporations, the wealthy and other special interest groups.[51][52]
 A study by political scientists Martin Gilens (Princeton University) and Benjamin Page (Northwestern University) released in April 2014, concluded that the U.S. government doesn't represent the interests of the majority of its citizens but instead is ""ruled by those of the rich and powerful"". The researchers, after analyzing nearly 1,800 U.S. policies between 1981 and 2002, stated that government policies tend to favor special interests and lobbying organizations, and that whenever a majority of citizens disagrees with the economic elites, the elites tend to prevail in getting their way.[53] While not characterizing the United States as an ""oligarchy"" or ""plutocracy"" outright, Gilens and Page give weight to the idea of a ""civil oligarchy"" as used by Jeffrey A. Winters, saying, ""Winters has posited a comparative theory of 'Oligarchy,' in which the wealthiest citizens – even in a 'civil oligarchy' like the United States – dominate policy concerning crucial issues of wealth- and income-protection."" In their study, Gilens and Page reached these conclusions:
 Jefferson and Albert Gallatin focused on the danger that the public debt, unless it was paid off, would be a threat to republican values. They were appalled that Hamilton was increasing the national debt and using it to solidify his Federalist base. Gallatin was the Republican Party's chief expert on fiscal issues and as Treasury Secretary under Jefferson and Madison worked hard to lower taxes and lower the debt, while at the same time paying cash for the Louisiana Purchase and funding the War of 1812. Burrows says of Gallatin:
 Andrew Jackson believed the national debt was a ""national curse"" and he took special pride in paying off the entire national debt in 1835.[56] Politicians ever since have used the issue of a high national debt to denounce the other party for profligacy and a threat to fiscal soundness and the nation's future.[57]
 Civic virtue required men to put civic goals ahead of their personal desires, and to volunteer to fight for their country. Military service thus was an integral duty of the citizen. As John Randolph of Roanoke put it, ""When citizen and soldier shall be synonymous terms, then you will be safe.""[58] Scott (1984) notes that in both the American and French revolutions, distrust of foreign mercenaries led to the concept of a national, citizen army, and the definition of military service was changed from a choice of careers to a civic duty.[59] Herrera (2001) explains that an appreciation of self-governance is essential to any understanding of the American military character before the Civil War. Military service was considered an important demonstration of patriotism and an essential component of citizenship. To soldiers, military service was a voluntary, negotiated, and temporary abeyance of self-governance by which they signaled their responsibility as citizens. In practice self-governance in military affairs came to include personal independence, enlistment negotiations, petitions to superior officials, militia constitutions, and negotiations regarding discipline. Together these affected all aspects of military order, discipline, and life.[60][61]
 The Founding Fathers wanted republicanism because its principles guaranteed liberty, with opposing, limited powers offsetting one another. They thought change should occur slowly, as many were afraid that a ""democracy"" – by which they meant a direct democracy – would allow a majority of voters at any time to trample rights and liberties. They believed the most formidable of these potential majorities was that of the poor against the rich.[62] They thought democracy could take the form of mob rule that could be shaped on the spot by a demagogue.[63] Therefore, they devised a written Constitution that could be amended only by a super majority, preserved competing sovereignties in the constituent states,[64] gave the control of the upper house (Senate) to the states, and created an Electoral College, comprising a small number of elites, to select the president. They set up a House of Representatives to represent the people. In practice the electoral college soon gave way to control by political parties. In 1776, most states required property ownership to vote, but most white male citizens owned farms in the 90% rural nation, so it was limiting to women, Native Americans and slaves. As the country urbanized and people took on different work, the property ownership requirement was gradually dropped by many states. Property requirements were gradually dismantled in state after state, so that all had been eliminated by 1850, so that few if any economic barriers remained to prevent white, adult males from voting.[65]
 The term republic does not appear in the Declaration of Independence, but does appear (once) in the constitution in Article IV which ""guarantee[s] to every State in this Union a Republican form of Government."" What exactly the writers of the constitution felt this should mean is uncertain. The Supreme Court, in Luther v. Borden (1849), declared that the definition of republic was a ""political question"" in which it would not intervene. During Reconstruction the Constitutional clause was the legal foundation for the extensive Congressional control over the eleven former Confederate states; there was no such oversight over the border slave states that had remained in the Union.[66]
 In two later cases, the Supreme Court did establish a basic definition. In United States v. Cruikshank (1875), the court ruled that the ""equal rights of citizens"" were inherent to the idea of republic. The opinion of the court from In re Duncan (1891)[67] held that the ""right of the people to choose their government"" is also part of the definition. It is also generally assumed that the clause prevents any state from being a monarchy – or a dictatorship. Due to the 1875 and 1891 court decisions establishing basic definition, in the first version (1892) of the Pledge of Allegiance, which included the word republic, and like Article IV which refers to a Republican form of government, the basic definition of republic is implied and continues to be so in all subsequent versions.
 Both democratic Ancient Greece and the ancient Roman Republic permitted slavery, but many early Americans questioned slavery's compatibility with Republican values. In 1850, Sen. William H. Seward declared on the Senate floor that slavery was incompatible with the ""security, welfare and greatness of nations"", and that when slavery ""prevails and controls in any republican state, just to that extent it subverts the principle of democracy and converts the state into an aristocracy or a despotism.""[68]
 The Republican Party was formed by antislavery forces across the North in reaction to the Kansas–Nebraska Act of 1854 that promoted democracy (or ""popular sovereignty"") by saying new settlers could decide themselves whether or not to have slavery. The party officially designated itself ""Republican"" because the name resonated with the struggle of 1776. ""In view of the necessity of battling for the first principles of republican government,"" resolved the Michigan state convention, ""and against the schemes of aristocracy the most revolting and oppressive with which the earth was ever cursed, or man debased, we will co-operate and be known as Republicans.""[69][70] The antebellum South took the reverse view, interpreting Northern policies against slavery as a threat to their republican values (in particular the system of checks and balances), according to J. Mills Thornton.[71]
 After the war, the Republicans believed that the Constitutional guarantee of republicanism enabled Congress to Reconstruct the political system of the former Confederate states. The main legislation was explicitly designed to promote republicanism.[72] Radical Republicans pushed forward to secure not only citizenship for freedmen through the 14th Amendment, but also to give them the vote through the 15th Amendment. They held that the concept of republicanism meant that true political knowledge was to be gained in exercising the right to vote and organizing for elections. Susan B. Anthony and other advocates of woman suffrage said republicanism covered them too, as they demanded the vote.[73][74]
 A central theme of the progressive era was fear of corruption, one of the core ideas of republicanism since the 1770s. The Progressives restructured the political system to combat entrenched interests (for example, through the direct election of Senators), to ban influences such as alcohol that were viewed as corrupting, and to extend the vote to women, who were seen as being morally pure and less corruptible.[75]
 Questions of performing civic duty were brought up in presidential campaigns and World War I. In the presidential election of 1888, Republicans emphasized that the Democratic candidate Grover Cleveland had purchased a substitute to fight for him in the Civil War, while his opponent General Benjamin Harrison had fought in numerous battles.[76] In 1917, a great debate took place over Woodrow Wilson's proposal to draft men into the U.S. Army after war broke out in Europe. Many said it violated the republican notion of freely given civic duty to force people to serve.[77] In the end, Wilson was successful and the Selective Service Act of 1917 was passed.
 Historians such as Richard Ellis and Michael Nelson argue that much constitutional thought, from Madison to Lincoln and beyond, has focused on ""the problem of majority tyranny.""
They conclude, ""The principles of republican government embedded in the Constitution represent an effort by the framers to ensure that the inalienable rights of life, liberty, and the pursuit of happiness would not be trampled by majorities.""[78] Madison, in particular, worried that a small localized majority might threaten inalienable rights, and in Federalist No. 10 he argued that the larger the population of the republic, the more diverse it would be and the less liable to this threat.[79] More broadly, in Federalist No. 10, Madison distinguished a democracy from a republic. Jefferson warned that ""an elective despotism is not the government we fought for.""[80]
 Madison wrote:
 As late as 1800, the word ""democrat"" was mostly used to attack an opponent of the Federalist party. Thus, George Washington in 1798 complained, ""that you could as soon scrub the blackamoor white, as to change the principles of a profest Democrat; and that he will leave nothing unattempted to overturn the Government of this Country.""[82] The Federalist Papers are pervaded by the idea that pure democracy is actually quite dangerous, because it allows a majority to infringe upon the rights of a minority.[83] Thus, in encouraging the states to participate in a strong centralized government under a new constitution and replace the relatively weak Articles of Confederation, Madison argued in Federalist No. 10 that a special interest may take control of a small area, e.g., a state, but it could not easily take over a large nation. Therefore, the larger the nation, the safer is republicanism.[84]
 By 1805, the ""Old Republicans"" or ""Quids"", a minority faction among Southern Republicans, led by Johan Randolph, John Taylor of Caroline and Nathaniel Macon, opposed Jefferson and Madison on the grounds that they had abandoned the true republican commitment to a weak central government.[85]
 In the 1830s, Alexis de Tocqueville warned about the ""tyranny of the majority"" in a democracy and suggested the courts should try to reverse the efforts of the majority of terminating the rights of an unpopular minority.[86] John Phillip Reid writes that Republicanism includes guarantees of rights that cannot be repealed by a majority vote.[87]
 Others argue that ""the historical evidence suggests that the founders believed that majority will – defined as the prevailing view of enfranchised citizens – should generally dictate national policy"".[88] James Madison equated ""a coalition of a majority of the whole society"" with ""justice and the general good,"" in the Federalist Papers; and Alexander Hamilton described ""representative democracy"" as ""happy, regular and durable.""[89][88] Alexander Hamilton wrote that:
 Over time, the pejorative connotations of ""democracy"" faded. By the 1830s, democracy was seen as an unmitigated positive and the term ""Democratic"" was assumed by the Democratic Party and the term ""Democrat"" was adopted by its members.[91] A common term for the party in the 19th century was ""The Democracy.""[92] In debates on Reconstruction, Radical Republicans, such as Senator Charles Sumner, argued that the republican ""guarantee clause"" in Article IV supported the introduction by force of law of democratic suffrage in the defeated South.[93]
 After 1800 the limitations on democracy were systematically removed; property qualifications for state voters were largely eliminated in the 1820s.[94] The initiative, referendum, recall, and other devices of direct democracy became widely accepted at the state and local level in the 1910s; and senators were made directly electable by the people in 1913. The last restrictions on black voting were made illegal in 1965. President Abraham Lincoln, used constitutional republic and democracy synonymously, casting the American experiment as government of the people, by the people, and for the people.[95]
 The idea that America is ""a republic, not a democracy"" has been a recurring theme in American Republicanism since the early 20th century. It declared that not only is majoritarian ""pure"" democracy a form of tyranny (unjust and unstable) but that democracy, in general, is a distinct form of government from republicanism and that the United States has been and should remain the second and not the first. 
 Critics of the socially-oriented New Deal programs proposed by Franklin Delano Roosevelt to fix the Great Depression threatened Republican ideals of property rights, free enterprise, and individual freedoms. Historian Matthew Dallek argues at this time the phrase ""a republic, not a democracy"" was used by the right to argue that wealthy white men should continue to rule.[96]
 Communications professor Heather Hendershot argues that the gains made by the Civil Rights Movement to reduce oppression of racial minorities provoked fears about ""democracy"" as a threat to white control of institutions, including government and education. The 1954 decision Brown v. Board of Education, the Civil Rights Act of 1964, and the Voting Rights Act of 1965 increased these fears over time.[96]
 Oil tycoon H.L. Hunt proclaimed in 1952 that democracy had been created by the devil.[96]  In 1955, segregationalist Herman E. Talmadge used the phrase and criticized those promoting ""democracy"" as Communist, anti-God, and in favor of racial mixing.[97]
 The claim was chanted at the 1964 Republican convention, where arch-conservative Barry Goldwater became the Republican Party nominee.
 Conservative radio broadcaster Dan Smoot, funded by Hunt, supported racist poll taxes and literacy tests. He argued that forms of democracy like a popular vote to elect the President was a threat because ""welfare people"" (meaning non-white people living in cities) would out-vote rural (white) people in places like Montana.[96] Underscoring the threat to white people, he wrote during the peak of his popularity in 1966: ""If a majority should develop hatred for all blue-eyed babies and order them eliminated, the babies could be legally executed because whatever a majority wants at any given moment.""[98]
 In the 21st century, the same slogan was tweeted by Senator Mike Lee of Utah in 2020.[99][100][101] Robert Draper, writing in The New York Times, identifies the idea as ""embodied by the Electoral College’s primacy over the popular vote in presidential elections"", and states that its proponents maintain that the founders of the constitution ""specifically rejected direct popular sovereignty in favor of a representative system in which the governing authorities are states and districts, not individual voters"".[99]
 Draper notes conservative Republican presidents in the 21st century (George W. Bush, Donald Trump) have not been reluctant to invoke the word ""democracy"" in addresses and policies; but as of 2022, in at least one state (Arizona) Draper traveled throughout, he found ""anti-democracy and anti-'democracy' sentiment, repeatedly voiced"", and distinct from anything he had ""encountered in over two decades of covering conservative politics.""[99]
 In a 2020 paper, ""America Is a Republic, Not a Democracy"", Bernard Dobski of The Heritage Foundation characterized attacks by liberals on the Electoral College for its ""undemocratic"" features (giving much more electoral weight to small states, and usually giving all the electoral votes to whichever candidate won the most popular votes), ""the most visible sign of this democratic antipathy to our republican institutions"", and compares the campaign to elect the president using the popular vote to calls for direct democracy through national referendums.[102] However, George Thomas notes that interest in the ""Republic not a Democracy"" concept among Republicans comes at a time when ""the Republican presidential candidate has prevailed in the Electoral College in three out of seven elections"" since 1988, ""but won the popular vote only once"" (in 2004), and that ""given current trends, minority rule"" in the United States ""could become routine"".[95] Thomas also notes that interest in the idea comes at a time when Donald Trump's 2020 presidential campaign was ""the first"" in American history to make no ""effort to win the popular vote, appealing only to the people who will deliver him an Electoral College win"";[95] and Astra Taylor notes that in general, Republican electoral strategies are showing less interest and ability in ""winning majorities"".[103]
In April 2024, the Washington State Republican Party passed a resolution asking people to use the word ""republic"" instead of ""democracy"", and endorsed repeal of the Seventeenth Amendment to the United States Constitution, which would mean ending direct election of U.S. senators.[104]
"
Peaceful transfer of power,https://en.wikipedia.org/wiki/Peaceful_transfer_of_power,"
A peaceful transition or transfer of power is a concept important to democratic governments in which the leadership of a government peacefully hands over control of government to a newly-elected leadership. This may be after elections or during the transition from a different kind of political regime, such as the post-communist period after the fall of the Soviet Union.[1]
 In scholarship examining democratization and emerging democracies, study of the successful transitions of power is used to understand the transition to constitutional democracy and the relative stability of that government.[2][3][4][5] A 2014 study concluded that 68 countries had never had a peaceful transition of power due to an election since 1788.[6][1]
 In scholarship examining democratization and emerging democracies, study of the successful transitions of power is used to understand the transition to constitutional democracy and the relative stability of that government (democratic consolidation).[2][3][4][5]
 A 2014 study by Adam Przeworski of 3,000 elections from 1788 to 2008, published in the journal Comparative Political Studies concluded that 68 countries (including Russia and China) had never had a peaceful transition of power between parties following an election, making it a ""rare and a recent practice.""[6][1] The same study found that once a country has an initial peaceful transfer of power (an ""alternation""), it is very likely to keep doing so, making the peaceful transition of power a habit-forming activity.[6][1] In a stable institutionalized democracy, a peaceful transition is the expected outcome of an election.[6][1]
 Peaceful transitions require a number of strong democratic institutions and norms to exist, such as the willingness of opposition parties to serve as a loyal opposition. Transitions by election put power holders in vulnerable positions, as not only do they risk potential changes in policy and practice and thus their means of power, but they also risk political retribution or retaliation.[7]
 The first peaceful transition of power in a country is often treated as an important stage in a government transition towards democracy such as seen in elections in the Democratic Republic of the Congo.[8] Successful transitions during tense political moments such as the Velvet Revolution in Armenia in 2018 are interpreted as signs of improved governance within the country, an important milestone in democratization and functioning civil society.[9] Alternately, the lack of peaceful transfers of power, such as in elections in Georgia from 1995 to 2008 in which the only transition between presidents was via the 2003 Rose Revolution, may harm the international reputation of the country as a ""democracy"".[10]
 Since achieving independence from European colonial powers, Africa has had a mixed record in achieving peaceful transitions of power, with variations among nations.[11]
 The first peaceful transition of power between civilians in Nigeria took place in 2007, although the outgoing and incoming presidents were of the same party and the preceding election was characterized by widespread irregularities.[12] In 2018, Liberia had its first electoral transfer of power since 1944.[13] The first peaceful transition of power in the Democratic Republic of the Congo took place in 2019, with outgoing president Joseph Kabila yielding power to opposition leader Felix Tshisekedi.[14] The first transition of power from one democratically elected leader to another in Niger took place in 2021, briefly overcoming the nation's history of coups d'etat[15] before another military coup occurred in 2023.[16]
 The symbol of peaceful transition of power is when the outgoing president and/or vice president, after their respective successors recite the oath of office, switch chairs, so that the incumbent president is on the furthest left side of the altar at the People's Consultative Assembly main session's room, and the sitting vice president is immediately on the right side of the speaker and deputies speaker's desk. As of 2022[update], starting in 1978, the vice presidents always did this symbolic transfer when there was no vacancy in the office except in 2004, when Hamzah Haz did not attend the ceremony, and in 2009, when Jusuf Kalla was already seated on the furthest right side so no switch was needed. For presidents, as of 2025[update], this symbol of peaceful transition happened only three times, in 1999 during the inauguration of Abdurrahman Wahid, in 2014 for the first inauguration of Joko Widodo, and 2024 for the inauguration of Prabowo Subianto.
 The transfer of power resulting from the 2012 Georgian parliamentary election was considered an important case of peaceful transfer of power in the post-Soviet political development of Georgia, which, since the Soviet period, had earlier gone through changes such as the Rose Revolution in 2003.[10]
 A peaceful transition of power has historically been the norm in United States presidential transitions. The transition from John Adams to Thomas Jefferson in 1801 was considered an important milestone for the country's fledgling democracy. It was the first time the presidency was handed over to a political opponent. From then until 2020, the losing party in every presidential election ""willingly and peacefully"" relinquished power to the opposition.[17][18] The transition is institutionalized through symbolic acts like the presidential inaugurations.[19][20] Outgoing U.S. presidents traditionally attend the inaugurations of their successors, a symbol of the peaceful passage of power from one administration to the next.[21] Historically contentious elections, such as the 2000 election between George W. Bush and Al Gore, did not derail this peaceful transition process. Despite the contested results and the subsequent Supreme Court ruling that ultimately determined the outcome, the tradition of the losing party accepting the result and facilitating a smooth handover of power endured. With Gore making  especially sure that the results of the elections were respected and ensured Bush would be accepted as President by his supporters despite the various continuous legal battles. This concept of ""loser’s consent,""  plays a vital role in maintaining democratic stability.[22] As political parties in the U.S. have come to accept electoral defeat as part of the democratic process, reinforcing the idea that the legitimacy of the electoral system depends on the willingness of political actors to adhere to its outcomes.
 
During the 2020 presidential election, experts described a risk of democratic backsliding in the U.S.,[23][24] as incumbent Republican President Donald Trump publicly refused to commit to a peaceful transfer of power if he lost his reelection bid.[25] In September 2020, after Trump's statements, the U.S. Senate unanimously passed a resolution committing to a peaceful transition of power and opposing any attempt ""by the President or any person in power to overturn the will of the people of the United States""; many senators cited the peaceful transition of power's centrality to U.S. democracy.[26] Business leaders also made statements calling for a peaceful transfer.[27] Trump stated on 15 October 2020 that he would accept a peaceful transfer (after a long period of ambiguous answers to the question) while still falsely alleging fraud and waging a legal battle to attempt to overturn the election results.[clarify][28]
 Trump was defeated in the 2020 election by Joe Biden in both the popular vote and the electoral vote, but refused to accept defeat. Trump falsely claimed election fraud, initiated a seven-part plan to overturn the election, and engaged in an aggressive and unprecedented[29] campaign to remain in power.[30][31] Trump's fellow Republicans had varied reactions to Trump's false election-fraud claims.[32][33][34][35] Trump's strong grip on the Republican led to a majority of the party supporting  or refusing to actively oppose him.[36] Among those who stood firm against Trump's attempts to subvert the 2020 election results were Vice President Mike Pence, Georgia Secretary of State Brad Raffensperger, and the courts. Pence, despite intense pressure from Trump to reject the Electoral College results, upheld his constitutional duty by certifying Biden’s victory. Pence’s stance was crucial in safeguarding the integrity of the democratic process. Similarly, Raffensperger, a Republican, resisted Trump’s phone call pressuring him to ""find"" enough votes to alter Georgia’s outcome. Raffensperger and his team, despite immense political pressure, upheld the accurate count, demonstrating their commitment to the rule of law. The courts also played a vital role in protecting democracy. Across the country, judges dismissed numerous baseless lawsuits aimed at overturning the results, reinforcing that the election was fair and legitimate. 
 
On 6 January 2021, a pro-Trump mob, inflamed by the unsuccessful Republican nominee and outgoing president's false claims, attacked the Capitol in Washington, D.C., in a failed attempt to keep Trump in power. The mob disrupted the counting of the electoral votes by a joint session of Congress for several hours.[37][38][39] Five people died either shortly before, during, or following the attack.[40] Republican Senate Majority Leader Mitch McConnell noted that ""if this election were overturned by mere allegations from the losing side, our democracy would enter a death spiral.""[41] On 7 January 2021, Trump condemned the riots and committed to the peaceful transition of power, but refused to mention Biden’s name in his farewell address and did not attend Biden’s inauguration.[42] After he transferred power, he routinely repeated election lies and defended the riots.[43]
Debate continues as to whether the events of 2020 represent a temporary aberration or a deeper, more sustained threat to the democratic fabric of the nation. Some argue that the rise of political polarization, the spread of disinformation, and the increasing willingness of political actors to reject established norms could point to a more profound crisis of legitimacy[44] Others believe that the resilience of U.S. institutions in the face of these challenges demonstrates that the foundations of democracy, while tested, remain robust.  [45]
 2025 saw a return to peaceful transfer of power between Biden and a returning Trump.
 In Venezuela in 1958, the Puntofijo Pact allowed a political agreement to respect the election results,[46] allowing for a peaceful transition of power after the ouster of dictator Marcos Pérez Jiménez and during the country's democratic period.[47]
"
President (government title),https://en.wikipedia.org/wiki/President_(government_title)#United_States,"


 President is a common title for the head of state in most republics. Depending on the country, a president could be head of government, a ceremonial figurehead, or something between these two extremes.
 The functions exercised by a president vary according to the form of government. In parliamentary republics, they are usually, but not always, limited to those of the head of state and are thus largely ceremonial. In presidential and selected parliamentary (e.g. Botswana and South Africa) republics[1][2] the role of the president is more prominent, encompassing the functions of the head of government.[3] In semi-presidential republics, the president has some discretionary powers like over foreign affairs, appointment of the head of government and defence, but they are not themselves head of government.[3] A leader of a one-party state may also hold the position of president for ceremonial purposes or to maintain an official state position.[4]
 The title ""Mr. President""[5][6] may apply to a person holding the title of president or presiding over certain other governmental bodies.[7] ""Mr. President"" has subsequently been used by governments to refer to their heads of state. It is the conventional translation of non-English titles such as Monsieur le Président for the president of the French Republic. It also has a long history of usage as the title of the presiding officers of legislative and judicial bodies. The speaker of the House of Commons of Canada is addressed as président de la Chambre des communes in French and as Mr. Speaker in English.
 The title president is derived from the Latin prae- ""before"" + sedere ""to sit"". The word ""presidents"" is also used in the King James Bible at Daniel 6:2 to translate the Aramaic term סָרְכִ֣ין (sā·rə·ḵîn), a word of likely Persian origin, meaning ""officials"", ""commissioners"", ""overseers"" or ""chiefs"". As such, it originally designated the officer who presides over or ""sits before"" a gathering and ensures that debate is conducted according to the rules of order (see also chairman and speaker), but today it most commonly refers to an executive official in any social organization. Early examples are from the universities of Oxford and Cambridge (from 1464) and the founding president of the Royal Society William Brouncker in 1660. This usage survives today in the title of such offices as ""President of the Board of Trade"" and ""Lord President of the Council"" in the United Kingdom, as well as ""President of the Senate"" in the United States (one of the roles constitutionally assigned to the vice president). The officiating priest at certain Anglican religious services, too, is sometimes called the ""president"" in this sense.
 The most common modern usage is as the title of a head of state in a republic. The first usage of the word president to denote the highest official in a government was during the Commonwealth of England.
 Thomas Hungerford, who became the first speaker of the English House of Commons in 1376, used the title, ""Mr. Speaker"", a precedent followed by subsequent speakers of the House of Commons.
 After the abolition of the monarchy, the English Council of State, whose members were elected by the House of Commons, became the executive government of the Commonwealth. The Council of State was the successor of the Privy Council, which had previously been headed by the lord president; its successor the Council of State was also headed by a lord president, the first of which was John Bradshaw. However, the lord president alone was not head of state, because that office was vested in the council as a whole.
 The speaker of the House of Commons of Canada, established in 1867, is also addressed as ""Monsieur le Président"" or ""Madame la Présidente"" in French.
 In pre-revolutionary France, the president of a Parlement evolved into a powerful magistrate, a member of the so-called noblesse de robe (""nobility of the gown""), with considerable judicial as well as administrative authority. The name referred to his primary role of presiding over trials and other hearings. In the 17th and 18th centuries, seats in the Parlements, including presidencies, became effectively hereditary, since the holder of the office could ensure that it would pass to an heir by paying the crown a special tax known as the paulette. The post of ""first president"" (premier président), however, could be held by only the King's nominees. The Parlements were abolished by the French Revolution. In modern France the chief judge of a court is known as its president (président de la cour).
 By the 18th century, the president of a French parlement was addressed as ""Monsieur le Président"". In Pierre Choderlos de Laclos's 1782 novel Les Liaisons dangereuses (""Dangerous Liaisons""), the wife of a magistrate in a parlement is referred to as Madame la Présidente de Tourvel (""Madam President of Tourvel""). The fictional name Tourvel refers not to the parlement in which the magistrate sits, but rather, in imitation of an aristocratic title, to his private estate. This influenced parliamentary usage in France. When the Second French Republic was established in 1848, ""Monsieur le Président"" became the title of the president of the French Republic.
 The modern usage of the term president to designate a single person who is the head of state of a republic can be traced directly to the United States Constitution of 1787, which created the office of President of the United States. Previous American governments had included ""presidents"" (such as the president of the Continental Congress or the president of the Massachusetts Provincial Congress), but these were presiding officers in the older sense, with no executive authority. It has been suggested that the executive use of the term was borrowed from early American colleges and universities, which were usually headed by a president. British universities were headed by an official called the ""Chancellor"" (typically a ceremonial position) while the chief administrator held the title of ""Vice-Chancellor"". But America's first institutions of higher learning (such as Harvard University and Yale University) did not resemble a full-sized university so much as one of its constituent colleges. A number of colleges at Cambridge University featured an official called the ""president"". The head, for instance, of Magdalene College, Cambridge was called the master and his second the president. The first president of Harvard, Henry Dunster, had been educated at Magdalene. Some have speculated that he borrowed the term out of a sense of humility, considering himself only a temporary placeholder. The presiding official of Yale College, originally a ""rector"" (after the usage of continental European universities), became ""president"" in 1745.
 A common style of address for presidents, ""Mr/Mrs. President"", is borrowed from British Parliamentary tradition, in which the presiding Speaker of the House of Commons is referred to as ""Mr/Mrs. Speaker"". Coincidentally, this usage resembles the older French custom of referring to the president of a parlement as ""Monsieur/Madame le Président"", a form of address that in modern France applies to both the president of the Republic and to chief judges. In the United States, the title ""Mr. President"" is used in a number of formal instances as well: for example anyone presiding over the United States Senate is addressed as ""Mr./Madam President"", especially the vice president, who is the president of the Senate. Other uses of the title include presidents of state and local legislatures; however, only the president of the United States uses the title outside of formal sessions.
 The 1787 Constitution of the United States did not specify the manner of address for the president. When George Washington was sworn in as the first president of the United States on April 30, 1789, however, the administering of the oath of office ended with the proclamation: ""Long live George Washington, President of the United States.""[8] No title other than the name of the office of the executive was officially used at the inauguration. The question of a presidential title was being debated in Congress at the time, however, having become official legislative business with Richard Henry Lee's motion of April 23, 1789. Lee's motion asked Congress to consider ""what titles it will be proper to annex to the offices of President and Vice President of the United States – if any other than those given in the Constitution"".[9] Vice President John Adams, in his role as President of the United States Senate, organized a congressional committee. There Adams agitated for the adoption of the style of Highness (as well as the title of Protector of Their [the United States'] Liberties) for the president.[10] Adams and Lee were among the most outspoken proponents of an exalted presidential title.[9]
 Others favored the variant of Electoral Highness or the lesser Excellency, the latter of which was vociferously opposed by Adams, who contended that it was far beneath the presidential dignity, as the executives of the states, some of which were also titled ""President"" (e.g. the president of Pennsylvania), at that time often enjoyed the style of Excellency; Adams said the president ""would be leveled with colonial governors or with functionaries from German princedoms"" if he were to use the style of Excellency. Adams and Richard Henry Lee both feared that cabals of powerful senators would unduly influence a weak executive, and saw an exalted title as a way of strengthening the presidency.[11] On further consideration, Adams deemed even Highness insufficient and instead proposed that the executive, both the president and the vice president (i.e., himself), be styled Majesty to prevent the ""great danger"" of an executive with insufficient dignity.[10] Adams' efforts were met with widespread derision; Thomas Jefferson called them ""the most superlatively ridiculous thing I ever heard of"", while Benjamin Franklin considered it ""absolutely mad"".[10]
 Washington consented to the demands of James Madison and the United States House of Representatives that the title be altered to ""Mr. President"".[12][13][14][15] Nonetheless, later ""The Honorable"" became the standard title of the President in formal address, and ""His/Her Excellency"" became the title of the President when addressed formally internationally.
 Historically, the title was reserved for the incumbent president only, and was not to be used for former presidents, holding that it was not proper to use the title as a courtesy title when addressing a former president.[16][17][18][19][20] According to the official website of the United States of America, the correct way to address a letter is to use ""The Honorable John Doe"" and the correct salutation is ""Mr. Doe"".[21]
 Once the United States adopted the title of ""president"" for its republican head of state, many other nations followed suit.
 Haiti became the first presidential republic in the Caribbean when Henri Christophe assumed the title in 1807. Almost all the Pan-American nations that became independent from Spain in the early 1810s and 1820s chose a US-style president as their chief executive. The first European president was the president of the Italian Republic of 1802, a client state of revolutionary France, in the person of Napoleon Bonaparte. The first African president was the president of Liberia (1848),[22] while the first Asian president was the president of the Republic of China (1912).[23]
 In the twentieth and twenty-first centuries, the powers of presidencies have varied from country to country. The spectrum of power has included presidents-for-life and hereditary presidencies to ceremonial heads of state.
 Presidents in the countries with a democratic or representative form of government are usually elected for a specified period of time and in some cases may be re-elected by the same process by which they are appointed, i.e. in many nations, periodic popular elections. The powers vested in such presidents vary considerably. Some presidencies, such as that of Ireland, are largely ceremonial, whereas other systems vest the president with substantive powers such as the appointment and dismissal of prime ministers or cabinets, the power to declare war, and powers of veto on legislation. In many nations the president is also the commander-in-chief of the nation's armed forces, though this varies significantly around the world.
 In almost all states with a presidential system of government, the president exercises the functions of head of state and head of government, i.e. the president directs the executive branch of government. When a president is not only head of state, but also head of government, this is known in Europe as a President of the Council (from the French Président du Conseil), used 1871–1940 and 1944–1958 in the Third and Fourth French Republics. In the United States the president has always been both Head of State and Head of Government and has always had the title of President.
 Presidents in this system are either directly elected by popular vote or indirectly elected by an electoral college or some other democratically elected body.
 In the United States, the president is indirectly elected by the Electoral College made up of electors chosen by voters in the presidential election. In most states of the United States, each elector is committed to voting for a specified candidate determined by the popular vote in each state, so that the people, in voting for each elector, are in effect voting for the candidate. However, for various reasons the numbers of electors in favour of each candidate are unlikely to be proportional to the popular vote. Thus, in five close United States elections (1824, 1876, 1888, 2000, and 2016), the candidate with the most popular votes still lost the election.
 In Mexico, the president is directly elected for a six-year term by popular vote. The candidate who wins the most votes is elected president even without an absolute majority. The president is allowed to serve only one term.
 In Brazil, the president is directly elected for a four-year term by popular vote. A candidate has to have more than 50% of the valid votes. If no candidates achieve a majority of the votes, there is a runoff election between the two candidates with most votes. Again, a candidate needs a majority of the vote to be elected. In Brazil, a president cannot be elected to more than two consecutive terms, but there is no limit on the number of terms a president can serve.
 Many South American, Central American, African and some Asian nations follow the presidential model.
 A second system is the semi-presidential system, also known as the French model. In this system, as in the parliamentary system, there are both a president and a prime minister; but unlike the parliamentary system, the president may have significant day-to-day power. For example, in France, when their party controls the majority of seats in the National Assembly, the president can operate closely with the parliament and prime minister, and work towards a common agenda. When the National Assembly is controlled by their opponents, however, the president can find themselves marginalized with the opposition party prime minister exercising most of the power. Though the prime minister remains an appointee of the president, the president must obey the rules of parliament, and select a leader from the house's majority holding party. Thus, sometimes the president and prime minister can be allies, sometimes rivals; the latter situation is known in France as cohabitation. Variants of the French semi-presidential system, developed at the beginning of the Fifth Republic by Charles de Gaulle, are used in France, Portugal, Romania, Sri Lanka and several post-colonial countries which have emulated the French model. In Finland, although the 2000 constitution moved towards a ceremonial presidency, the system is still formally semi-presidential, with the president of Finland retaining e.g. foreign policy and appointment powers.
 The parliamentary republic, is a parliamentary system in which the presidency is largely ceremonial with either de facto or no significant executive authority (such as the president of Austria) or de jure no significant executive power (such as the president of Ireland), and the executive powers rests with the prime minister who automatically assumes the post as head of a majority party or coalition, but takes oath of office administered by the president. However, the president is head of the civil service, commander in chief of the armed forces and in some cases can dissolve parliament. Countries using this system include Austria, Armenia, Albania, Bangladesh, Czech Republic, Germany, Greece, Hungary, Iceland, India, Ireland, Israel, Italy,[24] Malta, Pakistan, and Singapore.
 A variation of the parliamentary republic is a system with an executive president in which the president is the head of state and the government but unlike a presidential system, is elected by and accountable to a parliament, and referred to as president. Countries using this system include Botswana, Nauru and South Africa.
 In dictatorships, the title of president is frequently taken by self-appointed or military-backed leaders. Such is the case in many states: Idi Amin in Uganda, Mobutu Sese Seko in Zaire, Ferdinand Marcos in the Philippines, Suharto in Indonesia, and Saddam Hussein in Iraq are some examples. Other presidents in authoritarian states have wielded only symbolic or no power such as Craveiro Lopes in Portugal and Joaquín Balaguer under the ""Trujillo Era"" of the Dominican Republic.
 President for Life is a title assumed by some dictators to try to ensure their authority or legitimacy is never questioned. Presidents like Alexandre Pétion, Rafael Carrera, Josip Broz Tito and François Duvalier died in office. Kim Il Sung was named Eternal President of the Republic after his death.
 Only a tiny minority of modern republics do not have a single head of state. Some examples of this are:
 The president of China is the head of state of the People's Republic of China. Under the country's constitution, the presidency is a largely ceremonial office with limited power. However, since 1993, as a matter of convention, the presidency has been held simultaneously by the General Secretary of the Chinese Communist Party, the top leader in the single-party system.
 Between 1982 and 2019, the  Chinese constitution stipulated that the president could not serve more than two consecutive terms. During the Mao era, as well as since March 2018, there were no term limits attached to this office. Under President Xi Jinping, the term limits of the presidency were abolished, but its powers and ceremonial role remained unchanged.[25]
 In Laos, which is a one-party state similar to that of China, the president is elected by the Laotian National Assembly, and is not considered to be head of state. Since 1998, however, presidential officeholders in Laos often occupy their position as General Secretary of the Lao People's Revolutionary Party concurrently, and thus exercise political power via that role. In accordance with the current Laotian Constitution (rev. in 2015), the president may not serve more than 2 consecutive terms, and elections are to take place every 5 years.[26][27]
 The President of Laos is the commander-in-chief of the Lao People's Armed Forces. They also possess the authority (with the consent of the national assembly) to appoint the prime minister and vice president, as well as other roles. The current President of Laos, elected in March 2021, is Thongloun Sisoulith.[26]
 In Cuba, officially the Republic of Cuba, the president, recognized officially as the President of the Republic, is considered to be the head of state. It is the highest state office in Cuba, but still ranks below the First Secretary of the Communist Party of Cuba. The current president, Miguel Díaz-Canel, however, has held both titles concurrently since April 2021, having been granted the rank of First Secretary by the National Assembly of People's Power following the resignation and retirement of Raúl Castro.[28]
 The president is elected by representatives within Cuba's parliament, the National Assembly of People's Power, and in accordance with the Cuban Constitution, which was rewritten in February 2019 following a national referendum, is limited to two consecutive five-year terms. The president is also considered to be responsible for the National Assembly. The current president was elected by the National Assembly in December 2019 with an almost unanimous vote, and was re-elected in a similar fashion in April 2023.[29][30][31]
 As the country's head of state, in most countries the president is entitled to certain perquisites, and may have a prestigious residence, often a lavish mansion or palace, sometimes more than one (e.g. summer and winter residences, or a country retreat) Customary symbols of office may include an official uniform, decorations, a presidential seal, coat of arms, flag and other visible accessories, as well as military honours such as gun salutes, ruffles and flourishes, and a presidential guard. A common presidential symbol is the presidential sash worn most often by presidents in Latin America and Africa as a symbol of the continuity of the office.[32]
 United Nations member countries in columns, other entities at the beginning:
 Some countries with parliamentary systems use a term meaning/translating as ""president"" (in some languages indistinguishable from chairman) for the head of parliamentary government, often as President of the Government, President of the Council of Ministers or President of the Executive Council.
 However, such an official is explicitly not the president of the country. These officials are called ""president"" using an older sense of the word, to denote the fact that the official heads the cabinet. A separate head of state generally exists in their country who instead serves as the president or monarch of the country.
 Thus, such officials are really premiers, and to avoid confusion are often described simply as 'prime minister' when being mentioned internationally.
 There are several examples for this kind of presidency:
 President can also be the title of the chief executive at a lower administrative level, such as the parish presidents of the parishes of the U.S. state of Louisiana, the presiding member of city council for villages in the U.S. state of Illinois, or the municipal presidents of Mexico's municipalities. Perhaps the best known sub-national presidents are the borough presidents of the five boroughs of New York City.
 In Poland, the president of the city (Polish: Prezydent miasta) is the executive authority of the municipality elected in direct elections, the equivalent of the mayor. The Office of the President (Mayor) is also found in Germany and Switzerland.
 Governors of ethnic republics in the Russian Federation used to have the title of President, occasionally alongside other, secondary titles such as Chairman of the Government (also used by Prime Minister of Russia). This likely reflects the origin of Russian republics as homelands for various ethnic groups: while all federal subjects of Russia are currently de jure equal, their predecessors, the ASSRs, used to enjoy more privileges than the ordinary krais and oblasts of the RSFSR (such as greater representation in the Soviet of Nationalities). Thus, the ASSRs and their eventual successors would have more in common with nation-states than with ordinary administrative divisions, at least in spirit, and would choose titles accordingly.
 Over the course of the 2010s the presidents of Russian republics would progressively change their title to that of Head (Russian: глава), a proposition suggested by the president of Chechnya Ramzan Kadyrov and later made law by the Parliament of Russia and President Dmitriy Medvedev in 2010. Despite this, however, presidents of Tatarstan resisted the move, before 2023, where it was switched to the Head or Rais, the latter of which could be translated from Tatar as ""President"".
 The lord president of the Council is one of the Great Officers of State in the United Kingdom who presides over meetings of British Privy Council; the Cabinet headed by the prime minister is technically a committee of the council, and all decisions of the Cabinet are formally approved through Orders in Council. Although the lord president is a member of the Cabinet, the position is largely a ceremonial one and is traditionally given to either the leader of the House of Commons or the leader of the House of Lords.
 Historically the president of the Board of Trade was a cabinet member.
 In Alderney, the elected head of government is called the president of the States of Alderney.
 In the Isle of Man, there is a president of Tynwald.
 In Spain, the executive leaders of the autonomous communities (regions) are called presidents. In each community, they can be called Presidente de la Comunidad or Presidente del Consejo among others. They are elected by their respective regional assemblies and have similar powers to a state president or governor.
 Below a president, there can be a number of or ""vice presidents"" (or occasionally ""deputy presidents"") and sometimes several ""assistant presidents"" or ""assistant vice presidents"", depending on the organisation and its size. These posts do not hold the same power but more of a subordinate position to the president. However, power can be transferred in special circumstances to the deputy or vice president. Normally vice presidents hold some power and special responsibilities below that of the president. The difference between vice/deputy presidents and assistant/associate vice presidents is the former are legally allowed to run an organisation, exercising the same powers (as well as being second in command) whereas the latter are not.
 In some countries the speaker of their unicameral legislatures, or of one or both houses of bicameral legislatures, the speakers have the title of president of ""the body"", as in the case of Spain, where the Speaker of the Congress is the president of the Congress of Deputies and the Speaker of the Senate is the president of the Senate.
 The term 'President' is usually used in judiciary as chief justice of constitutional courts.
 In French legal terminology, the president of a court consisting of multiple judges is the foremost judge; he chairs the meeting of the court and directs the debates (and is thus addressed as ""Mrs President"", ""Madame la Présidente"", ""Mr President"", or ""Monsieur le Président""). In general, a court comprises several chambers, each with its own president; thus the most senior of these is called the ""first president"" (as in: ""the First President of the Court of Cassation is the most senior judge in France""). Similarly in English legal practice the most senior judge in each division uses this title (e.g. President of the Family Division, President of the Court of Appeal).
 In the Spanish Judiciary, the leader of a court of multiples judges is called President of the Court. The same happens with the different bodies of the Spanish judicial system, where we can find a president of the Supreme Court, a president of the National Court and presidents in the Regional High Courts of Justice and in the Provincial Courts. The body that rules over the Judiciary in Spain is the General Council of the Judiciary, and its president is the president of the Supreme Court, which is normally called President of the Supreme Court and of the GCJ.
 The Constitutional Court is not part of the Judiciary, but the leader of it is called President of the Constitutional Court.
 In the recently established Supreme Court of the United Kingdom, the most senior judge is called the president of the Supreme Court. The lady/lord president of the Court of Session is head of the judiciary in Scotland, and presiding judge (and Senator) of the College of Justice and Court of Session, as well as being Lady/Lord Justice General of Scotland and head of the High Court of Justiciary, the offices having been combined in 1784.
 Titles for a president's spouse, if female, have ranged from ""Marquise"" to ""Lady"" to simply ""Mrs."" (or ""Ms."").[14] If male the title of the president's spouse may be ""Marquis"", ""Lord"", or merely ""Mr."".
 President George Washington's wife, Martha Washington, was often called ""Lady Washington"". By the 1850s in the United States, the term ""lady"" had changed from a title of nobility to a term of address for a respected and well-mannered woman. The use of ""First Lady"" to refer to the wife of the president of the United States was popularized about the time of the US Civil War. Dolley Madison, the wife of President James Madison, was remembered after her death in 1849 by President Zachary Taylor as ""truly our First Lady for a half a century"".[33] First ladies are usually referred to simply as ""Mrs. [last name]"".[34]
 On 8 November 2016, the night of the 2016 presidential election in the United States, images of leaked pre-printed copies of Newsweek magazine showed the magazine celebrating the expected win of the Democratic candidate Hillary Clinton, with the cover titled ""Madam President"". It was unusual that it was both published and distributed before the results were known, leading to accusations of bias; the cover was pulled from newsstands after it became clear that Donald Trump had secured a majority of electoral votes, winning the election.[35]
"
Twenty-second Amendment to the United States Constitution,https://en.wikipedia.org/wiki/Twenty-second_Amendment_to_the_United_States_Constitution#Background,"

 The Twenty-second Amendment (Amendment XXII) to the United States Constitution limits the number of times a person can be elected to the office of President of the United States to two terms, and sets additional eligibility conditions for presidents who succeed to the unexpired terms of their predecessors.[1] Congress approved the Twenty-second Amendment on March 21, 1947, and submitted it to the state legislatures for ratification. That process was completed on February 27, 1951, when the requisite 36 of the 48 states had ratified the amendment (neither Alaska nor Hawaii had yet been admitted as a state), and its provisions came into force on that date.
 The amendment prohibits anyone who has been elected president twice from being elected again. Under the amendment, someone who fills an unexpired presidential term lasting more than two years is also prohibited from being elected president more than once. Scholars debate whether the amendment prohibits affected individuals from succeeding to the presidency under any circumstances or whether it applies only to presidential elections. Until the amendment's ratification, the president had not been subject to term limits, but both George Washington and Thomas Jefferson (the first and third presidents) decided not to run for a third term, establishing a two-term tradition. In the 1940 and 1944 presidential elections, Franklin D. Roosevelt became the only president to be elected for a third and fourth term, giving rise to concerns about a president serving unlimited terms.[2]
 The Twenty-second Amendment was a reaction to Franklin D. Roosevelt's election to an unprecedented four terms as president, but presidential term limits had long been debated in American politics. Delegates to the Constitutional Convention of 1787 considered the issue extensively (alongside broader questions, such as who would elect the president, and the president's role). Many, including Alexander Hamilton and James Madison, supported lifetime tenure for presidents, while others favored fixed terms. Virginia's George Mason denounced the life-tenure proposal as tantamount to elective monarchy.[4] An early draft of the U.S. Constitution provided that the president was restricted to one seven-year term.[5] Ultimately, the Framers approved four-year terms with no restriction on how many times a person could be elected president.
 Though dismissed by the Constitutional Convention, term limits for U.S. presidents were contemplated during the presidencies of George Washington and Thomas Jefferson. As his second term entered its final year in 1796, Washington was exhausted from years of public service, and his health had begun to decline. He was also bothered by his political opponents' unrelenting attacks, which had escalated after the signing of the Jay Treaty, and believed he had accomplished his major goals as president. For these reasons, he decided not to run for a third term, a decision he announced to the nation in his September 1796 Farewell Address.[6] Eleven years later, as Thomas Jefferson neared the halfway point of his second term, he wrote,
 Since Washington made his historic announcement, numerous academics and public figures have looked at his decision to retire after two terms, and have, according to political scientist Bruce Peabody, ""argued he had established a two-term tradition that served as a vital check against any one person, or the presidency as a whole, accumulating too much power"".[8] Various amendments aimed at changing informal precedent to constitutional law were proposed in Congress in the early to mid-19th century, but none passed.[4][9] Three of the next four presidents after Jefferson—Madison, James Monroe, and Andrew Jackson—served two terms, and each adhered to the two-term principle;[1] Martin Van Buren was the only president between Jackson and Abraham Lincoln to be nominated for a second term, though he lost the 1840 election and so served only one term.[9] At the outset of the Civil War, the seceding states drafted the Constitution of the Confederate States of America, which in most respects resembled the United States Constitution, but limited the president to a single six-year term.
 In spite of the strong two-term tradition, a few presidents before Roosevelt attempted to secure a third term. Following Ulysses S. Grant's reelection in 1872, there were serious discussions within Republican political circles about the possibility of his running again in 1876. But interest in a third term for Grant evaporated in the light of negative public opinion and opposition from members of Congress, and Grant left the presidency in 1877 after two terms. Even so, as the 1880 election approached, he sought nomination for a (non-consecutive) third term at the 1880 Republican National Convention, but narrowly lost to James A. Garfield, who won the 1880 election.[9]
 Theodore Roosevelt succeeded to the presidency on September 14, 1901, following William McKinley's assassination (194 days into his second term), and was handily elected to a full term in 1904. He declined to seek a third (second full) term in 1908, but did run again in the 1912 election, losing to Woodrow Wilson. Wilson himself, despite his ill health following a serious stroke, aspired to a third term. Many of his advisers tried to convince him that his health precluded another campaign, but Wilson nonetheless asked that his name be placed in nomination for the presidency at the 1920 Democratic National Convention.[10] Democratic Party leaders were unwilling to support Wilson, and the nomination went to James M. Cox, who lost to Warren G. Harding. Wilson again contemplated running for a (non-consecutive) third term in 1924, devising a strategy for his comeback, but again lacked any support. He died in February of that year.[11]
 Franklin Roosevelt spent the months leading up to the 1940 Democratic National Convention refusing to say whether he would seek a third term. His vice president, John Nance Garner, along with Postmaster General James Farley, announced their candidacies for the Democratic nomination. When the convention came, Roosevelt sent a message to the convention saying he would run only if drafted, saying delegates were free to vote for whomever they pleased. This message was interpreted to mean he was willing to be drafted, and he was renominated on the convention's first ballot.[9][12] Roosevelt won a decisive victory over Republican Wendell Willkie, becoming the only president to exceed eight years in office. His decision to seek a third term dominated the election campaign.[13] Willkie ran against the open-ended presidential tenure, while Democrats cited the war in Europe as a reason for breaking with precedent.[9]
 Four years later, Roosevelt faced Republican Thomas E. Dewey in the 1944 election. Near the end of the campaign, Dewey announced his support of a constitutional amendment to limit presidents to two terms. According to Dewey, ""four terms, or sixteen years [a direct reference to the president's tenure in office four years hence] is the most dangerous threat to our freedom ever proposed.""[14] He also discreetly raised the issue of the president's age. Roosevelt exuded enough energy and charisma to retain voters' confidence and was elected to a fourth term.[15]
 While he quelled rumors of poor health during the campaign, Roosevelt's health was deteriorating. On April 12, 1945, only 82 days after his fourth inauguration, he suffered a cerebral hemorrhage and died, to be succeeded by Vice President Harry Truman.[16] In the midterm elections 18 months later, Republicans took control of the House and the Senate. As many of them had campaigned on the issue of presidential tenure, declaring their support for a constitutional amendment that would limit how long a person could serve as president, the issue was given priority in the 80th Congress when it convened in January 1947.[8]
 The House of Representatives took quick action, approving a proposed constitutional amendment (House Joint Resolution 27) setting a limit of two four-year terms for future presidents. Introduced by Earl C. Michener, the measure passed 285–121, with support from 47 Democrats, on February 6, 1947.[17] Meanwhile, the Senate developed its own proposed amendment, which initially differed from the House proposal by requiring that the amendment be submitted to state ratifying conventions for ratification, rather than to the state legislatures, and by prohibiting any person who had served more than 365 days in each of two terms from further presidential service. Both of these provisions were removed when the full Senate took up the bill; however, a new provision was added. Put forward by Robert A. Taft, it clarified procedures governing the number of times a vice president who succeeded to the presidency might be elected to office. The amended proposal was passed 59–23, with 16 Democrats in favor, on March 12.[1][18]
 On March 21, the House agreed to the Senate's revisions and approved the resolution to amend the Constitution. Afterward, the amendment imposing term limitations on future presidents was submitted to the states for ratification. The ratification process was completed on February 27, 1951, 3 years, 343 days after it was sent to the states.[19][20]
 Once submitted to the states, the 22nd Amendment was ratified by:[3]
 Conversely, two states—Massachusetts and Oklahoma—rejected the amendment, while five (Arizona, Kentucky, Rhode Island, Washington, and West Virginia) took no action.[18]
 Because of the grandfather clause in Section 1, the amendment did not apply to Harry S. Truman, who was the incumbent president at the time it was submitted to the states by the Congress. This full exemption allowed Truman to run again in 1952. He had served nearly all of Franklin Roosevelt's unexpired 1945–1949 term and had been elected to a full four-year term beginning in 1949.[13] But with his job approval rating at around 27%,[21][22] and after a poor performance in the 1952 New Hampshire primary, Truman chose not to seek his party's nomination. Since becoming operative in 1951, the amendment has barred six twice-elected presidents from election to a third term: Dwight D. Eisenhower, Richard Nixon, Ronald Reagan, Bill Clinton, George W. Bush, and Barack Obama.[23] Donald Trump, who was elected to two non-consecutive terms, will also be barred from seeking a third term as president.[24]
 Additionally, Lyndon B. Johnson was eligible for two terms as president, and Gerald Ford for one term, under the 22nd Amendment.  In Johnson's case, he had finished what was fourteen months, a little over a year, left of John F. Kennedy's presidency.  Thus, he was eligible for two terms and would have then been term limited to January 20, 1973.  With Ford, he served over 28 months, or just over two years, of the remainder of Richard Nixon's presidency.  Had Ford won the 1976 election, he would have been term limited to January 20, 1981.
 As worded, the focus of the 22nd Amendment is on limiting individuals from being elected to the presidency more than twice. Questions have been raised about the amendment's meaning and application, especially in relation to the 12th Amendment, ratified in 1804, which states, ""no person constitutionally ineligible to the office of President shall be eligible to that of Vice-President of the United States.""[25] While the 12th Amendment stipulates that the constitutional qualifications of age, citizenship, and residency apply to the president and vice president, it is unclear whether someone who is ineligible to be elected president due to term limits could be elected vice president. Because of the ambiguity, a two-term former president could possibly be elected vice president and then succeed to the presidency as a result of the incumbent's death, resignation, or removal from office, or succeed to become Acting President from another stated office in the presidential line of succession.[9][26]
 It has been argued that the 22nd Amendment and 12th Amendment bar any two-term president from later serving as vice president as well as from succeeding to the presidency from any point in the presidential line of succession.[27][28] Others contend that the original intent of the 12th Amendment concerns qualification for service (age, residence, and citizenship), while the 22nd Amendment concerns qualifications for election, and thus a former two-term president is still eligible to serve as vice president. Neither amendment restricts the number of times someone can be elected to the vice presidency and then succeed to the presidency to serve out the balance of the term, although the person could be prohibited from running for election to an additional term.[29][30] The practical applicability of this distinction has not been tested, as no person has been elected president and vice president in that order, regardless of terms served.
 Over the years, several presidents have voiced their antipathy toward the amendment. After leaving office, Harry Truman described the amendment as stupid and one of the worst amendments of the Constitution with the exception of the Prohibition Amendment.[31] A few days before leaving office in January 1989, President Ronald Reagan said he would push for a repeal of the 22nd Amendment because he thought it infringed on people's democratic rights.[32] In a November 2000 interview with Rolling Stone, President Bill Clinton suggested that the 22nd Amendment should be altered to limit presidents to two consecutive terms but then allow non-consecutive terms, because of longer life expectancies.[33]
 The first efforts in Congress to repeal the 22nd Amendment were undertaken in 1956, five years after the amendment's ratification. Over the next 50 years, 54 joint resolutions seeking to repeal the two-term presidential election limit were introduced.[1] Between 1997 and 2013, Representative José E. Serrano introduced nine resolutions (one per Congress, all unsuccessful) to repeal the amendment.[34] Repeal has also been supported by Representatives Barney Frank and David Dreier, and Senators Mitch McConnell[35] and Harry Reid.[36] 
 In January 2025, Representative Andy Ogles introduced a joint resolution proposing that the 22nd Amendment be altered to allow a president to serve a third term, provided that their first two are non-consecutive. The language of the bill was intended specifically to allow for the incumbent President Donald Trump to serve a third term, as he is the only living president to serve non consecutive terms.[37][38][39]
"
George Washington's Farewell Address,https://en.wikipedia.org/wiki/George_Washington%27s_Farewell_Address,"

 Washington's Farewell Address[1] is a letter written by President George Washington as a valedictory to ""friends and fellow-citizens"" after 20 years of public service to the United States.[2] He wrote it near the end of the second term of his presidency before retiring to his home at Mount Vernon in Virginia.
 The letter was first published as The Address of Gen. Washington to the People of America on His Declining the Presidency of the United States in Claypoole's American Daily Advertiser on September 19, 1796, about ten weeks before the presidential electors cast their votes in the 1796 election. In it, he writes about the importance of national unity while warning Americans of the political dangers of regionalism, partisanship and foreign influence, which they must avoid to remain true to their values.[3] It was almost immediately reprinted in newspapers around the country, and later in pamphlet form.[4]
 The first draft was originally prepared by James Madison in June 1792, as Washington contemplated retiring at the end of his first term in office.[5] However, he set it aside and ran for a second term because of heated disputes between Secretary of the Treasury Alexander Hamilton and Secretary of State Thomas Jefferson which convinced Washington that the growing tensions would rip apart the country without his leadership. This included the state of foreign affairs, and divisions between the newly formed Federalist and Democratic-Republican parties.[6]
 As his second term came to a close four years later, Washington prepared a revision of the original letter with the help of Hamilton to write a new farewell address to announce his intention to decline a third term in office. He reflects on the emerging issues of the American political landscape in 1796, expresses his support for the government eight years after the adoption of the Constitution, defends his administration's record, and gives valedictory advice to the American people. The letter also attempted to reunite the country, which had partly turned against Washington following the controversial 1794 Jay Treaty.[7][8][9]
 The thought of the United States without George Washington as its president caused concern among many Americans. Thomas Jefferson disagreed with many of Washington's policies and later led the Democratic-Republicans in opposition to many Federalist policies, but he joined his political rival Alexander Hamilton, leader of the Federalists, in convincing Washington to delay his retirement and serve a second term. The two men feared that the nation would be torn apart without his leadership. Washington most likely referred to this when he told the American people that he had wanted to retire before the last election, but he was convinced by people who were, in his words, ""entitled to my confidence"" that it was his duty to serve a second term.[6] 
 All of the ideas presented in Washington's Farewell Address came from Washington; however, Alexander Hamilton wrote most of it.[10]
 Washington sought to convince the American people that his service was no longer necessary by telling them, as he had in his first inaugural address, that he truly believed that he was never qualified to be president. If he accomplished anything during his presidency, he said, it was as a result of their support and efforts to help the country survive and prosper. Despite his confidence that the country would survive without his leadership, Washington used the majority of the letter to offer advice as a ""parting friend"" on what he believed were the greatest threats to the nation.[6]
 The Address expresses Washington's understanding of republicanism by affirming popular government and warning about threats to ""Republican liberty"".[11] He begins his warnings to the American people by stressing that their independence, peace at home and abroad, safety, prosperity, and liberty are all dependent upon unity among the states. He warns them that the union of states created by the Constitution will come under the most frequent and focused attacks by foreign and domestic enemies of the country. He warns the American people to be suspicious of anyone who seeks to abandon the Union, secede a portion of the country from the rest, or weaken the bonds that hold together the constitutional union. To promote the strength of the Union, he urges the people to place their identity as Americans above their identities as members of a state, city, or region, and to focus their efforts and affection on the country above all other local interests. He reminds the people that they do not have more than slight differences in religion, manners, habits, and political principles and that their triumph and possession of independence and liberty are the results of working together.[6]
 Washington continues to express his support of the Union by giving some examples of how he believes that the country, its regions, and its people are already benefiting from the unity which they currently share. He then looks to the future in his belief that the combined effort and resources of its people will protect the country from foreign attack and allow them to avoid wars between neighboring nations that often happen due to rivalries and competing relations with foreign nations. He argues that the security provided by the Union will also allow the United States to avoid the creation of an overgrown military which he sees as a great threat to liberty, especially the republican liberty that the United States has created.
 Washington goes on to warn the American people to question the ulterior motives of any person or group who argues that the land within the borders of the United States is too large to be ruled as a republic, an argument made by many during the debate on the proposed purchase of the Louisiana Territory, calling on the people to give the experiment of a large republic a chance to work before deciding that it cannot be done. He then offers strong warnings on the dangers of sectionalism, arguing that the true motives of a sectionalist are to create distrust or rivalries between regions and people to gain power and take control of the government. Washington points to the Jay Treaty and Pinckney's Treaty which established the borders of the United States western territories between Spanish Mexico and British Canada and secured the rights of western farmers to ship goods along the Mississippi River to New Orleans. He holds up these treaties as proof that the eastern states along the Atlantic Coast and the federal government are looking out for the welfare of all the American people and can win fair treatment from foreign countries as a united nation.[6]
 Washington goes on to state his support for the new constitutional government, calling it an improvement upon the nation's original attempt in the Articles of Confederation. He reminds the people that it is the right of the people to alter the government to meet their needs, but it should only be done through constitutional amendments. He reinforces this belief by arguing that violent takeovers of the government should be avoided at all costs and that it is the duty of every member of the republic to follow the constitution and to submit to the laws of the government until it is constitutionally amended by the majority of the American people.[2]
 Washington warns the people that political factions may seek to obstruct the execution of the laws created by the government or to prevent the branches of government from exercising the powers provided to them by the constitution. Such factions may claim to be trying to answer popular demands or solve pressing problems, but their true intentions are to take the power from the people and place it in the hands of unjust men.[peacock prose][2]
 Washington calls the American people to only change the Constitution through amendments, but he then warns them that groups seeking to overthrow the government may strive to pass constitutional amendments to weaken the government to a point where it is unable to defend itself from political factions, enforce its laws, and protect the people's rights and property. As a result, he urges them to give the government time to realize its full potential, and only amend the constitution after thorough time and thought have proven that it is truly necessary instead of simply making changes based upon opinions and hypotheses of the moment.[2]
 Washington continues to advance his idea of the dangers of sectionalism and expands his warning to include the dangers of political parties to the country as a whole. These warnings are given in the context of the recent rise of two opposing parties within the government—the Democratic-Republican Party led by Jefferson, and Hamilton's Federalist Party. Washington had striven to remain neutral during a conflict between Britain and France brought about by the French Revolution, while the Democratic-Republicans had made efforts to align with France, and the Federalists had made efforts to ally with Great Britain.
 Washington recognizes that it is natural for people to organize and operate within groups such as political parties, but he also argues that every government has recognized political parties as an enemy and has sought to repress them because of their tendency to seek more power than other groups and to take revenge on political opponents.[6] He feels that disagreements between political parties weakened the government.
 Moreover, he makes the case that ""the alternate domination"" of one party over another and coinciding efforts to exact revenge upon their opponents have led to horrible atrocities, and ""is itself a frightful despotism. But this leads at length to a more formal and permanent despotism.""  From Washington's perspective and judgment, political parties eventually and ""gradually incline the minds of men to seek security… in the absolute power of an individual"",[2] leading to despotism. He acknowledges the fact that parties are sometimes beneficial in promoting liberty in monarchies, but he argues that political parties must be restrained in a popularly elected government because of their tendency to distract the government from their duties, create unfounded jealousies among groups and regions, raise false alarms among the people, promote riots and insurrection, and provide foreign nations and interests access to the government where they can impose their will upon the country.
 Washington continues his defense of the Constitution by stating that the system of checks and balances and separation of powers within it are important means of preventing a single person or group from seizing control of the country. He advises the American people that, if they believe that it is necessary to modify the powers granted to the government through the Constitution, it should be done through constitutional amendments instead of through force.
 One of the most referenced parts of Washington's letter is his strong support of the importance of religion and morality in promoting private and public happiness and in promoting the political prosperity of the nation. He argues that religious principles promote the protection of property, reputation, life, and honor that are the foundations of justice. He cautions against the belief that the nation's morality can be maintained without religion:
 Washington refers to religious principle as the foundation of a civilized society. He also argues that the American government needs to ensure ""the general diffusion of knowledge""[7] throughout the United States; the government has been created to enforce the opinion of the people, so the opinion of the people should be informed and knowledgeable.
 Washington provides strong support for the balanced federal budget, arguing that the nation's credit is an important source of strength and security. He urges the American people to preserve the national credit by avoiding war, avoiding unnecessary borrowing, and paying off any national debt accumulated in times of war as quickly as possible in times of peace so that future generations do not have to take on the financial burdens. Despite his warnings to avoid taking on debt, Washington does state his belief that sometimes it is necessary to spend money to prevent dangers or wars that will cost more if not properly prepared for. At these times, he argues, it is necessary for the people to cooperate by paying taxes to cover these precautionary expenses. He emphasizes how important it is for the government to be careful in choosing the items that will be taxed, but also tells the American people that, no matter how hard the government tries, there will never be a tax which is not inconvenient and unpleasant to those who must pay it.
 Washington dedicates a large part of his farewell address to discussing foreign relations and the dangers of permanent alliances between the United States and foreign nations, which he views as foreign entanglements.[12] He advocates a policy of good faith and justice towards all nations, again making reference to proper behavior based upon religious doctrine and morality. He urges the American people to avoid long-term friendly relations or rivalries with any nation, arguing that attachments with or animosity toward other nations will only cloud the government's judgment in its foreign policy. He argues that longstanding poor relations will only lead to unnecessary wars due to a tendency to blow minor offenses out of proportion when committed by nations viewed as enemies of the United States. He continues this argument by claiming that alliances are likely to draw the United States into wars that have no justification and no benefit to the country beyond simply defending the favored nation. Alliances, he warns, often lead to poor relations with nations who feel that they are not being treated as well as America's allies, and threaten to influence the American government into making decisions based upon the will of their allies instead of the will of the American people.
 Washington makes an extended reference to the dangers of foreign nations who will seek to influence the American people and government; nations who may be considered friendly as well as nations considered enemies will equally try to influence the government to do their will. ""Real patriots"", he warns, who ""resist the intrigues"" of foreign nations may find themselves ""suspected and odious"" in the eyes of others, yet he urges the people to stand firm against such influences all the same. He portrays those who attempt to further such foreign interests as becoming the ""tools and dupes"" of those nations, stealing the applause and praise of their country away from the ""real patriots"" while actually working to ""surrender"" American interests to foreign nations.
 Washington goes on to urge the American people to take advantage of their isolated position in the world, and to avoid attachments and entanglements in foreign affairs, especially those of Europe, which he argues have little or nothing to do with the interests of America. He argues that it makes no sense for the American people to become embroiled in European affairs when their isolated position and unity allow them to remain neutral and focus on their own affairs. He argues that the country should avoid permanent alliances with all foreign nations, although temporary alliances during times of extreme danger may be necessary. He states that current treaties should be honored but not extended.
 Washington wraps up his foreign policy stance by advocating free trade with all nations, arguing that trade links should be established naturally and the role of the government should be limited to ensuring stable trade, defending the rights of American merchants and any provisions necessary to ensure the conventional rules of trade.
 Washington uses this portion of the address to explain that he does not expect his advice to make any great impression upon the people or to change the course of American politics, but he does hope that the people will remember his devoted service to his country.
 Washington then explains his reasoning behind the Proclamation of Neutrality which he made during the French Revolutionary Wars, despite the standing Treaty of Alliance with France. He explains that the United States had a right to remain neutral in the conflict and that the correctness of that decision ""has been virtually admitted by all"" nations since. Justice and humanity required him to remain neutral during the conflict, he argues, and the neutrality was also necessary to allow the new government a chance to mature and gain enough strength to control its own affairs.
 Washington closes his letter to the American people by asking them to forgive any failures that may have occurred during his service to the country, assuring them that they were due to his weaknesses and by no means intentional. The sentences express his excitement about joining his fellow Americans as a private citizen in the free government they created together during his 45 years of public service.
 Washington's Farewell Address is considered to be one of the most important documents in American history[4] and the foundation of the Federalist Party's political doctrine.
 Washington later accepted a commission from President John Adams, despite his stated desire to retire from public service, as the Senior Officer of a Provisional Army formed to defend the nation against a possible invasion by French forces during the Quasi-War.[13] Washington held true to his statements in his farewell address, despite spending months organizing the Officer Corps of the Provisional Army, and declined suggestions that he return to public office in the presidential election of 1800.[13]
 Washington's statements on the importance of religion and morality in American politics and his warnings on the dangers of foreign alliances influenced political debates into the twentieth century,[4] and have received special consideration as advice from an American hero.
 Washington's hope that the United States would end permanent alliances with foreign nations was realized in 1800 with the Convention of 1800, the Treaty of Mortefontaine which officially ended the 1778 Treaty of Alliance, in exchange for ending the Quasi-War and establishing most favored nation trade relations with Napoleonic France.[14] In 1823, Washington's foreign policy goals were further realized in the Monroe Doctrine, which promised non-interference in European affairs so long as the nations of Europe did not seek to colonize or interfere with the newly independent Latin American nations of Central and South America. The United States did not enter into any permanent military alliances with foreign nations until the 1949 North Atlantic Treaty[15] which formed NATO.
 Philadelphia residents signed a petition in January 1862 during the American Civil War requesting that Congress commemorate the 130th anniversary of Washington's birth by reading his Farewell Address ""in one or the other of the Houses of Congress.""[7] It was first read in the House of Representatives in February 1862, and reading Washington's address became a tradition in both houses by 1899. The House of Representatives abandoned the practice in 1984,[7] but the Senate continues the tradition. Washington's Birthday is observed by selecting a member of the Senate to read the address aloud on the Senate floor, alternating between political parties each year since 1896.[7] Additionally, some senators may choose to make brief remarks and commentary before reading the speech. After finishing, readers make an entry into a black, leather-bound journal maintained by the Secretary of the Senate upon finishing.[16]
 During Strom Thurmond's filibuster of the Civil Rights Act of 1957, George Washington's Farewell Address was read by Strom Thurmond.[17]
 According to political journalist John Avlon, the Farewell Address was ""once celebrated as a civic Scripture, more widely reprinted than the Declaration of Independence"" but adds that it ""is now almost forgotten.""[18] He suggested that it had long been ""eclipsed in the national memory"" until the Broadway musical Hamilton brought it back to popular awareness in the song ""One Last Time"", where lines are sung by Washington and Hamilton from the end of the Address.[19]
"
Mount Vernon,https://en.wikipedia.org/wiki/Mount_Vernon,"


 Mount Vernon is the former residence and plantation of George Washington, a Founding Father, commander of the Continental Army in the Revolutionary War, and the first president of the United States, and his wife, Martha. An American landmark, the estate lies on the banks of the Potomac River in Fairfax County, Virginia, approximately 15 miles south of Washington, D.C..
 The Washington family acquired land in the area in 1674. Around 1734, the family embarked on an expansion of its estate that continued under George Washington, who began leasing the estate in 1754 before becoming its sole owner in 1761.[4]
 The mansion was built of wood in a loose Palladian style; the original house was built in about 1734 by George Washington's father Augustine Washington.[4] George Washington expanded the house twice, once in the late 1750s and again in the 1770s.[4] It remained Washington's home for the rest of his life. Following his death in 1799, the estate progressively declined under the ownership of several successive generations of the family as revenues were insufficient to maintain it adequately.
 In 1858, the house's historical importance was recognized and was taken over by the Mount Vernon Ladies' Association, along with part of the Washington property estate. The mansion and its surrounding buildings escaped damage from the American Civil War, which damaged many properties in the Confederate States of America during the Civil War.
 Mount Vernon was designated a National Historic Landmark in 1960 and is listed on the National Register of Historic Places. It is still owned and maintained in trust by the Mount Vernon Ladies' Association, being open to the public daily[5] in recognition of George Washington's 1794 acknowledgement of public interest in his estate: ""I have no objection to any sober or orderly person's gratifying their curiosity in viewing the buildings, Gardens, &ca. about Mount Vernon.""[6]
 When George Washington's ancestors acquired the estate, it was known as Little Hunting Creek Plantation, named after the nearby Little Hunting Creek.[7] When Washington's older half-brother, Lawrence Washington, inherited it, he renamed it after Edward Vernon,[8] a vice admiral and his commanding officer during the War of Jenkins' Ear who captured Portobelo from the Spanish.[9] When George Washington inherited the property, he retained the name.[7]
 The estate contained 8,000 acres (3,200 ha) when George Washington lived there.[10] As of 2011, the property consists of 500 acres (200 ha),[11] including the mansion and over 30 other buildings near the riverfront.[12]
 Construction on the present mansion at Mount Vernon began in approximately 1734 and was built in incremental stages by an unknown architect under the supervision of Augustine Washington.[4] This staggered and unplanned evolution is indicated by the off-center main door. As completed and seen today, the house is in a loose Palladian style. The principal block, dating from about 1734, was a one-story house with a garret.[4] In the 1750s, the roof was raised to a full second story and a third floor garret. There were also one-story extensions added to the north and south ends of the house; these were torn down during the next building phase.[13] The present day mansion is 11,028 sq ft (1,025 m2).[14]
 In 1774, the second expansion began. A two-story wing was added to the south side. Two years later a large two-story room was added to the north side.[13] Two single-story secondary wings were built in 1775. These secondary wings, which house the servants hall on the northern side and the kitchen on the southern side, are connected to the corps de logis by symmetrical, quadrant colonnades, built in 1778. The completion of the colonnades cemented the classical Palladian arrangement of the complex and formed a distinct cour d'honneur, known at Mount Vernon as Mansion Circle, giving the house its imposing perspective.
 The corps de logis has a hipped roof with dormers and the secondary wings have gable roofs with dormers. In addition to its second story, the importance of the corps de logis is further emphasized by two large chimneys piercing the roof and by a cupola surmounting the center of the house; this octagonal focal point has a short spire topped by a gilded dove of peace.[15] This placement of the cupola is more in the earlier Carolean style than Palladian and was probably incorporated to improve ventilation of the enlarged attic and enhance the overall symmetry of the structure and the two wings; a similar cupola crowns the Governor's House at Williamsburg, of which Washington would have been aware.
 Though no architect is known to have designed Mount Vernon, some attribute the design to John Ariss, a prominent Virginia architect who designed Paynes Church in Fairfax County (now destroyed) and likely Mount Airy in Richmond County.[16] Other sources credit Colonel Richard Blackburn, who also designed Rippon Lodge in Prince William County and the first Falls Church.[17][18] Blackburn's granddaughter Anne married Bushrod Washington, George's nephew, and is interred at the Washingtons' tomb on the grounds. Most architectural historians believe that the design of Mount Vernon is solely attributable to Washington alone and that the involvement of any other architects is based on conjecture.[19]
 The rooms at Mount Vernon have mostly been restored to their appearance at the time of George and Martha Washington's occupancy. Rooms include Washington's study, two dining rooms, the larger of which is known as the New Room, the West Parlour, the Front Parlour, the kitchen and some bedrooms.[20]
 The interior design follows the classical concept of the exterior, but owing to the mansion's piecemeal evolution, the internal architectural features – the doorcases, mouldings and plasterwork – are not consistently faithful to one specific period of the 18th-century revival of classical architecture. Instead they range from Palladianism to a finer and later neoclassicism in the style of Robert Adam.[20] This varying of the classical style is best exemplified in the doorcases and surrounds of the principal rooms. In the West Parlour and Small Dining rooms there are doorcases complete with ionic columns and full pediments, whereas in the hall and passageways the doors are given broken pediments supported by an architrave.[20] Many of the rooms are lined with painted panelling and have ceilings ornamented by plasterwork in a Neoclassical style; much of this plasterwork can be attributed to an English craftsman, John Rawlins, who arrived from London in 1771 bringing with him the interior design motifs then fashionable in the British capital.[20]
 Visitors to Mount Vernon now see Washington's study, a room to which in the 18th century only a privileged few were granted entry. This simply furnished room has a combined bathroom, dressing room and office; the room was so private that few contemporary descriptions exist. Its walls are lined with naturally grained paneling and matching bookcases.[21] In contrast to the privacy of the study, since Washington's time, the grandest, most public and principal reception room has been the so-called New Room or Large Dining Room – a two-storied salon notable for its large Palladian window, occupying the whole of the mansion's northern elevation, and its fine Neoclassical marble chimneypiece.[22] The history of this chimneypiece to some degree explains the overall restrained style of the house. When it was donated to Washington by English merchant Samuel Vaughan, Washington was initially reluctant to accept the gift, stating that it was ""too elegant & costly I fear for my own room, & republican stile of living.""[23]
 Efforts have been made to restore the rooms and maintain the atmosphere of the 18th century; this has been achieved by using original color schemes and by displaying furniture, carpets and decorative objects which are contemporary to the house. The rooms contain portraits and former possessions of George Washington and his family.[20]
 The gardens and grounds contain English boxwoods, taken from cuttings sent by Major General Henry Lee III a Governor of Virginia and the father of Robert E. Lee, which were planted in 1786 by George Washington and now crowd the entry path. A carriage road skirts a grassy bowling green to approach the mansion entrance. To each side of the green is a garden contained by red brick walls. These Colonial Revival gardens[24] grew the household's vegetables, fruit and other perishable items for consumption. The upper garden, located to the north, is bordered by the greenhouse.[25] Ha-ha walls are used to separate the working farm from the pleasure grounds that Washington created for his family and guests.[26] The overseer's quarter, spinning room, salt house, and gardener's house are between the upper garden and the mansion.
 The lower garden, or southern garden, is bordered on the east by the storehouse and clerk's quarters, smokehouse, wash house, laundry yard, and coach house. A paddock and stable are on the southern border of the garden; east of them, a little down the hillside, is the icehouse. The original tomb is located along the river. The newer tomb in which the bodies of George and Martha Washington have rested since 1831 is south of the fruit garden; the slave burial ground is nearby, a little farther down the hillside. A ""Forest Trail"" runs through woods down to a recreated pioneer farm site on low ground near the river; the 4-acre (16,000 m2) working farm includes a re-creation of Washington's 16-sided treading barn.[27]
 A museum and education center are on the grounds and exhibit examples of Washington's survey equipment, weapons, and clothing, and the  dentures worn by Washington as the first U.S. president. In 2013, the Fred W. Smith National Library for the Study of George Washington opened on Mount Vernon;[28] the library, which is open for scholarship by appointment only, fosters new scholarship about George Washington and safeguards original Washington books and manuscripts.
 In 1674, John Washington, the great-grandfather of George Washington, and Nicholas Spencer came into possession of the land from which Mount Vernon plantation would be carved, originally known by its Piscataway name of Epsewasson.[29][a] The successful patent on the acreage was largely executed by Spencer, who acted as agent for his cousin Thomas Colepeper, 2nd Baron Colepeper,[29] the English landowner who controlled the Northern Neck of Virginia, in which the tract lay.[30]
 When John Washington died in 1677, his son Lawrence, George Washington's grandfather, inherited his father's stake in the property. In 1690, he agreed to formally divide the estimated 5,000 acre (20 km2) estate with the heirs of Nicholas Spencer, who had died the previous year. The Spencers took the larger southern half bordering Dogue Creek in the September 1674 land grant from Lord Culpeper, leaving the Washingtons the portion along Little Hunting Creek. The Spencer heirs paid Lawrence Washington 2,500 lb (1,100 kg) of tobacco as compensation for their choice.[29]
 Lawrence Washington died in 1698, bequeathing the property to his daughter Mildred. On 16 April 1726, she agreed to a one-year lease on the estate to her brother Augustine Washington, George Washington's father, for a peppercorn rent; a month later the lease was superseded by Augustine's purchase of the property for £180.[31] He built the original house on the site around 1734, when he and his family moved from Pope's Creek to Eppsewasson,[32] which he renamed Little Hunting Creek.[33] The original stone foundations of what appears to have been a two-roomed house with a further two rooms in a half-story above are still partially visible in the present house's cellar.[32]
 Augustine Washington recalled his eldest son, Lawrence, George's half-brother, home from school in England in 1738, and set him up on the family's Little Hunting Creek tobacco plantation, thereby allowing Augustine to move his family back to Fredericksburg at the end of 1739.[7] In 1739, Lawrence, having reached 21 years of age, began buying up parcels of land from the adjoining Spencer tract, starting with a plot around the grist mill on Dogue Creek. In mid-1740, Lawrence received a coveted officer's commission in the British Army and made preparations to go off to war in the Caribbean with the newly formed American Regiment to fight in the War of Jenkins' Ear.[34] He served under Admiral Edward Vernon; returning home, he named his estate after his commander.
 Lawrence died in 1752, and his will stipulated that his widow should own a life estate in Mount Vernon, the remainder interest falling to his half-brother George; George Washington was already living at Mount Vernon and probably managing the plantation. Lawrence's widow, Anne Fairfax, remarried into the Lee family and moved out.[35] Following the death of Anne and Lawrence's only surviving child in 1754, George, as executor of his brother's estate leased his sister-in-law's estate. Upon the death of Anne Fairfax in 1761, he succeeded to the remainder interest and became sole owner of the property.[36]
 In 1758, Washington began the first of two major additions and improvements by raising the house to two-and-a-half stories.[36] The second expansion was begun during the 1770s, shortly before the outbreak of the Revolutionary War. Washington had rooms added to the north and south ends, unifying the whole with the addition of the cupola and two-story piazza overlooking the Potomac River. The final expansion increased the mansion to 21 rooms and an area of 11,028 square feet.[26] The great majority of the work was performed by enslaved African Americans and artisans.[37]
 George Washington expanded the estate by purchasing surrounding parcels of land beginning in the late 1750s and was still adding to the estate into the 1780s, including the River Farm estate.[38] From 1759 until the Revolutionary War, Washington, who at the time aspired to become a prominent agriculturist, had five separate farms as part of his estate. He took a scientific approach to farming and kept extensive and meticulous records of both labor and results.
 In a letter dated 20 September 1765, Washington writes about receiving poor returns for his tobacco production:
 In the same letter he asks about the prices of flax and hemp, with a view to their production:
 The tobacco market declined, and many planters in Northern Virginia converted to mixed crops. By 1766, Washington ceased growing tobacco at Mount Vernon and replaced the crop with wheat, corn, and other grains. Besides hemp and flax, he experimented with 60 other crops including cotton and silk. He also derived income from a gristmill which produced cornmeal and flour for export and also ground neighbors' grain for fees. Washington similarly sold the services of the estate's looms and blacksmith.
 Washington built and operated a small fishing fleet, permitting Mount Vernon to export fish. Washington practiced the selective breeding of sheep in an effort to produce better quality wool. He was not as invested in animal husbandry as he was in cropping experiments, which were elaborate and included complex field rotations, nitrogen fixing crops and a range of soil amendments.[40] The Washington household consumed a wider range of protein sources than was typical for the Chesapeake population of his day, which consumed a great deal of beef.[41]
 The new crops were less labor-intensive than tobacco; hence, the estate had a surplus of slaves. But Washington refused to break up families for sale. Washington began to hire skilled indentured servants from Europe to train the redundant slaves for service on and off the estate.[42]
Following his service in the war, Washington returned to Mount Vernon and in 1785–1786 spent a great deal of effort improving the landscaping of the estate. It is estimated that during his two terms as President of the United States (1789–1797), Washington spent a total of 434 days in residence at Mount Vernon. After his presidency, Washington tended to repairs to the buildings, socializing, and further gardening. In 1797, farm manager James Anderson, a recent Scottish immigrant, suggested the establishment of a whisky distillery,[43] which proved to be the estate's most profitable business venture over the decade of its operation.[44]
 In his will, written several months before his death in December 1799, George Washington left directions for the emancipation of all the slaves who belonged to him. Of the 317 slaves at Mount Vernon in 1799, a little less than half, 123 individuals, belonged to George Washington. Under the terms of his will, these slaves were to be set free upon Martha Washington's death.[45]
 In accordance with state law, George Washington stipulated in his will that elderly slaves or those who were too sick to work were to be supported throughout their lives by his estate. Children without parents, or those whose families were too poor or indifferent to see to their education, were to be bound out (or apprenticed) to masters and mistresses who would teach them reading, writing, and a useful trade, until they were ultimately freed at the age of twenty-five.[45]
 When Martha Washington's first husband, Daniel Parke Custis, died without a will, she received a life interest in one-third of his estate, including the slaves. Neither George nor Martha Washington could free these slaves by law. Upon Martha's death, these slaves reverted to the Custis estate and were divided among her grandchildren. By 1799, 153 slaves at Mount Vernon were part of this dower property.[45]
 Martha signed a deed of manumission in December 1800.[46] Abstracts of court records in Fairfax County, Virginia record this transaction. The slaves received their freedom on January 1, 1801.[45]
 On December 12, 1799, Washington spent several hours riding over the plantation, in snow, sleet, and freezing rain. He ate his supper later that evening without changing from his wet clothes. The following day, he awoke with a severe sore throat (either quinsy or acute epiglottitis) and became increasingly hoarse as the day progressed. All the available medical treatments failed to improve his condition, and he died at Mount Vernon at around 10 pm on December 14, 1799, aged 67.
 On December 18, a funeral was held at Mount Vernon, where his body was interred.[47] Congress passed a joint resolution to construct a marble monument in the United States Capitol for his body, an initiative supported by Martha. In December 1800, the United States House passed an appropriations bill for $200,000 (~$4.55 million in 2023) to build the mausoleum, which was to be a pyramid with a base 100 feet (30 m) square. Southerners who wanted his body to remain at Mount Vernon defeated the measure.[48]
 In accordance with his will, Washington was entombed in a family crypt he had built upon first inheriting the estate. It was in disrepair by 1799, so Washington's will also requested that a new, larger tomb be built. This was not executed until 1831, nearly the centennial of his birth. The need for a new tomb was confirmed when an unsuccessful attempt was made to steal his skull.[49] A joint Congressional committee in early 1832 debated the removal of Washington's body from Mount Vernon to a crypt in the Capitol, built by Charles Bulfinch in the 1820s. Southern opposition was intense, exacerbated by an ever-growing rift between North and South. Congressman Wiley Thompson of Georgia expressed the Southerners' fears when he said:
 In 1831, the bodies of George and Martha Washington, along with other members of the family, were moved from the old crypt to the new family tomb.[50] On October 7, 1837, Washington's remains, encased in a lead inner casket, were transferred from the closed tomb to a sarcophagus presented by John Struthers of Philadelphia. It was placed on the right side of the gateway to the tomb. A similar structure was provided for Martha's remains, which was placed on the left.[51] Other members of the Washington family are interred in an inner vault, behind the vestibule containing the sarcophagi.
 Following Martha Washington's death in 1802, George Washington's will was carried out in accordance with the terms of his bequests. The largest part of his estate, which included both his papers and Mount Vernon, passed to his nephew, Bushrod Washington, an Associate Justice of the Supreme Court of the United States.[52] The younger Washington and his wife then moved to Mount Vernon.[53]
 Bushrod Washington did not inherit much cash and was unable to support the upkeep of the estate's mansion on the proceeds from the property and his Supreme Court salary. He sold some of his own slaves to gain working capital.[54] However, the farms' low revenues left him short, and he was unable to adequately maintain the mansion.
 Following Bushrod Washington's death in 1829, ownership of the plantation passed to George Washington's grandnephew, John Augustine Washington II. After he died in 1832, his wife, Jane Charlotte inherited the estate, and her son began managing it. Upon her death in 1855, John Augustine Washington III inherited the property. As his funds dwindled and the wear and tear of hundreds of visitors began to take its toll, Washington could do little to maintain the mansion and its surroundings.[55]
 Washington suggested to the United States Congress that the federal government purchase the mansion.[55] However, Congress paid little interest to Washington's offer, as the legislature was focusing on the conditions that shortly led to the American Civil War.[55] Washington then traveled to Richmond, where he was equally unsuccessful in appealing to the Virginia General Assembly for the state to purchase the mansion.[55] The mansion's decline continued.[55]
 In 1858, Washington sold the mansion and a portion of the estate's land to the Mount Vernon Ladies' Association, which was under the leadership of Ann Pamela Cunningham.[55] The association paid the final installment of the purchase price of $200,000 ($6.3 million in 2020 dollars) in December 1859, taking possession in February 1860.[55] The estate first opened to the public during that year.[10]
 The estate served as a neutral ground for both sides during the Civil War, although fighting raged across the nearby countryside. Troops from both the Union and the Confederacy toured the building. The two women caretakers asked that the soldiers leave their arms behind and either change to civilian clothes or at least cover their uniforms. They usually did as asked.[56]
 Harrison Howell Dodge became the third resident superintendent in 1885. During his 52 years' overseeing the estate, he doubled the facility's acreage, improved the grounds, and added many historic artifacts to the collections. Dodge reviewed George Washington's writings about the estate, visited other Colonial-era gardens, and traveled to England to see gardens dating from the Georgian period. Using that knowledge, Dodge oversaw the restoration of the site completed by Charles Wilson Killam,[57] and put in place a number of improvements that Washington had planned but had never implemented.[58]
 Charles Wall was assistant superintendent from 1929 to 1937, then resident superintendent for 39 years. He oversaw restoration of the house by Killam and planted greenery consistent with what was used in the 18th century. In 1974, a campaign he organized was successful in preserving as parkland areas in Maryland across the Potomac River from Mount Vernon, as part of an effort to retain the bucolic vista from the house.[59] His office was the same one used in the 18th century by Washington.[60]
 Steamboats began to carry tourists to the Mount Vernon estate in 1878.[61] In 1892, the Washington, Alexandria and Mount Vernon Electric Railway opened, providing electric trolley service between Alexandria and the estate.[62][63][64] The electric railway and its successors carried tourists and others between Washington, D.C., and Mount Vernon from 1896 to 1932, when the federal government acquired part of its route on which to construct the George Washington Memorial Parkway.[63]
 [65] The parkway, originally named the Mount Vernon Memorial Parkway, opened in 1932.[62]
 In 2007, the estate opened a reconstruction of George Washington's distillery on the site of Washington's original distillery, a short distance from his mansion on the Potomac River. Construction of the distillery cost $2.1 million. The fully functional replica received special legislation from the Virginia General Assembly to produce up to 5,000 US gal (19,000 L) of whiskey annually, for sale only at the Mount Vernon gift shop.
 Frank Coleman, spokesman for the Distilled Spirits Council that funded the reconstruction, said the distillery ""will become the equivalent of a national distillery museum"" and serve as a gateway to the American Whiskey Trail.[66] In 2019, Mount Vernon began an annual whiskey festival.[67]
 As of 2020, the estate had received more than 85 million visitors.[10] In addition to the mansion, visitors can see original and reconstructed outbuildings and barns (including slaves' quarters), an operational blacksmith shop, and the Pioneer Farm. Each year on Christmas Day, Aladdin the Christmas Camel recreates Washington's 1787 hiring of a camel for 18 shillings to entertain his guests with an example of the animal that brought the Three Wise Men to Bethlehem to visit the newborn Jesus.[68]
 Mount Vernon remains a privately owned property. The non-profit Mount Vernon Ladies' Association has not received any funds from the federal government to support the restoration and maintenance of the mansion and the estate's 500-acre (2.0 km2) grounds or its educational programs and activities.[69]
 The association derives its income from charitable donations and the sales of tickets, produce and goods to visitors. These enable the Association to continue its mission ""to preserve, restore, and manage the estate of George Washington to the highest standards and to educate visitors and people throughout the world about the life and legacies of George Washington, so that his example of character and leadership will continue to inform and inspire future generations.""[70] Admission to Mount Vernon is free on Presidents' Day (the third Monday of February) and on George Washington's birthday (February 22).[71]
 Mount Vernon was featured in a 1-cent United States postage stamp in 1936 within the Army and Navy Commemorative Series. The green stamp, which was the first in the series, also contained portraits of George Washington and Nathanael Greene, a Major General of the Continental Army during the Revolutionary War.[72] 
 In 1956, a 1.5-cent stamp within the Liberty Issue of U.S. postage stamps memorialized Mount Vernon as a national shrine. The Liberty Issue was originally planned to honor six presidents, six famous Americans, and six historic national shrines. The Mount Vernon stamp, which featured a view of Washington's home facing the Potomac River, was the issue's first that commemorated a shrine.[73]
 Mount Vernon was designated a National Historic Landmark on December 19, 1960, and listed on the National Register of Historic Places on October 15, 1966.[1][2] Development and improvement of the estate is an ongoing concern. Following a $110 million fundraising campaign, two new buildings that GWWO, Inc./Architects had designed opened in 2006 as venues for additional background on George Washington and the American Revolution. The Ford Orientation Center introduces visitors to George Washington and Mount Vernon with displays and a film. The Donald W. Reynolds Museum and Education Center houses many artefacts related to Washington along with multimedia displays and further films using modern entertainment technology.
 Mount Vernon was put on the tentative list for World Heritage Site status in the early 2000s. It was submitted but failed to get approved. In 2014, Mount Vernon awarded its first Cyrus A. Ansary Prize for Courage and Character to former President George H. W. Bush.[74][75]
 The airspace surrounding Mount Vernon is restricted to prevent damage from aircraft vibrations.[76][77] As a consequence, overhead/aerial photography has been limited and requires unique approaches.[78]
 In 1955, a 485-acre farm across from Mount Vernon went up for sale. There were rumors that an oil company was to buy it. Charles Wagner, a resident of the Moyaone Association, a community next to the proposed site, reached out to Charles Wall, the Resident Director of Mount Vernon.[79] The Mount Vernon Ladies' Association and its then leader, Ohio Member of Congress Frances P. Bolton, had expressed a desire to protect the view from Mount Vernon. At this point Bolton, Wagner, Wall, and Moyaone resident Robert W. Straus developed a decades-long plan to protect the Mount Vernon viewshed, which came to be known as Operation Overview.[80][81]
 The first step was taken in 1957 when Bolton founded the Accokeek Foundation, one of the nation's first land trusts.[81] The Foundation was used to purchase 200 acres (81 ha) of land across from Mount Vernon to help preserve the area,[82]
 In 1961 and at Bolton's instigation, a joint resolution to preserve the viewshed was introduced in the United States Senate by Senator Clinton Anderson with identical text in the United States House of Representatives by Representative John P. Saylor. The resolution was quickly passed and signed by President John F. Kennedy. Its purpose was to ""preserve lands which provide the principal overview from the Mount Vernon Estate and Fort Washington"" in order to designate 133 acres (54 ha) around Mockley Point, which was to be the site of water treatment plant, as a national landmark. The resolution also authorized the National Park Service to receive donations and scenic easements from adjacent communities.[83] At this point Bolton and the Accokeek Foundation transferred their land to the National Park Service to form Piscataway Park.[79] In addition, Moyaone Association residents transferred conservation easements to the Park Service to further protect the viewshed. In 2020, the Moyaone Reserve was given National Register of Historic Places status.[84]
 The Fairfax Connector Routes 101, 151 and 152 buses travel daily between the Mount Vernon estate and the Huntington station on Washington Metro's Yellow Line.[85] The Route 11C Metrobus travels between the estate and the Braddock Road station on Metro's Blue and Yellow Lines during weekday peak hours.[86]
 The 17-mile (27 km)-long Mount Vernon Trail travels along the George Washington Memorial Parkway and the Potomac River between the Mount Vernon estate and Rosslyn in Arlington County, Virginia, where it connects to the Custis Trail.[87][88] The shared-use path is a part of the Potomac Heritage Trail, the East Coast Greenway and U.S. Bicycle Route 1.
 The Mount Vernon Trail connects to shared-use paths that travel on the Francis Scott Key Bridge, the Theodore Roosevelt Bridge, the Arlington Memorial Bridge and the George Mason Memorial Bridge (one of the 14th Street bridges).[88][89] The bridges cross the river into Washington, D.C., where their shared-use paths connect to the Rock Creek and Potomac Parkway Trail, the Chesapeake and Ohio Canal towpath and the Capital Crescent Trail.[88][89]
  Media related to Mount Vernon at Wikimedia Commons
"
George Washington and slavery,https://en.wikipedia.org/wiki/George_Washington_and_slavery," 
 The history of George Washington and slavery reflects Washington's changing attitude toward the ownership of human beings. The preeminent Founding Father of the United States and a hereditary slaveowner, Washington became uneasy with it, though kept the opinion in private communications only.  Slavery was then a longstanding institution dating back over a century in Virginia where he lived; it was also longstanding in other American colonies and in world history.  Washington's will immediately freed one of his slaves, and required his remaining 123 slaves to serve his wife and be freed no later than her death; they ultimately became free one year after his own death.
 In the Colony of Virginia where Washington grew up, he became a third generation slave-owner at 11 years of age upon the death of his father in 1743, when he inherited his first ten slaves. In adulthood his personal slaveholding grew through inheritance, purchase, and the natural increase of children born into slavery. In 1759, he also gained control of dower slaves belonging to the Custis estate on his marriage to Martha Dandridge Custis. Washington's early attitudes about slavery reflected the prevailing Virginia planter views of the day, which included few moral qualms, if any. In 1774, Washington publicly denounced the slave trade on moral grounds in the Fairfax Resolves. After the Revolutionary War, he continued to own slaves, but supported the abolition of slavery by a gradual legislative process.
 Washington had a strong work ethic and demanded the same from both hired workers and slaves. He provided his enslaved population with basic food, clothing and accommodation comparable to general practice at the time, which was not always adequate, and with medical care. In return, he forced them to work from sunrise to sunset over the six-day working week that was standard at the time. Some three-quarters of his enslaved workers labored in the fields, while the remainder worked at the main residence as domestic servants and artisans. They supplemented their diet by hunting, trapping, and growing vegetables in their free time, and bought extra rations, clothing and housewares with income from selling game and produce. They built their own community around marriage and family, though Washington allocated the enslaved to his farms according to business needs, causing many husbands to live separately from their wives and children during the work week. Washington used both reward and punishment to manage his enslaved population, but was constantly disappointed when they failed to meet his exacting standards. A significant proportion of the enslaved people at the Mount Vernon estate resisted their enslavement by various means, such as theft to supplement food and clothing or to provide income, feigning illness, and escaping to freedom.
 As commander-in-chief of the Continental Army in 1775, he initially refused to accept African-Americans, free or enslaved, into the ranks, but bowed to the demands of war, and thereafter led a racially integrated army.  In 1778, Washington expressed moral aversion to selling some of his enslaved workers at a public venue or splitting their families. At war's end, Washington demanded without success that the British respect the preliminary peace treaty which he said required return of all escaped slaves. Politically, Washington felt that the divisive issue of American slavery threatened national cohesion; he never spoke publicly about it even in his speeches addressing the new nation’s challenges, and he signed laws that protected slavery as well as laws that curtailed slavery. In Pennsylvania, he worked around the technicalities of state laws with his personal population as to not lose them.
 Privately, Washington considered freeing his enslaved population in the mid 1790s. Those plans failed because of his inability to raise the finances he deemed necessary, the refusal of his family to approve emancipation of the dower slaves, and his aversion to splitting the many families that included both dower slaves and his own slaves. By the time of Washington's death in 1799 there were 317 enslaved people at Mount Vernon. 124 were owned outright by Washington, 40 were rented, and the remainder were dower slaves owned by the estate of Martha Washington's first husband, Daniel Parke Custis, on behalf of their grandchildren. Washington's will was widely published upon his death, and provided for the eventual emancipation of the enslaved population owned by him, one of the few slave-owning founders to do so.  He could not legally free the dower slaves, and so the will said that, except for his valet William Lee who was freed immediately, his enslaved workers were bequeathed to his widow Martha until her death. She felt unsafe amidst slaves whose freedom depended on her demise, and freed them in 1801.
 Slavery was introduced into the English colony of Virginia when the first Africans were transported to Point Comfort in 1619. Those who accepted Christianity became ""Christian servants"" with time-limited servitude, or even freed, but this mechanism for ending bondage was gradually shut down. In 1667, the Virginia Assembly passed a law that barred baptism as a means of conferring freedom. Africans who had been baptised before arriving in Virginia could be granted the status of indentured servant until 1682, when another law declared them to be slaves. In the lowest stratum of Virginian society, white people and people of African descent shared common disadvantages and a common lifestyle, which included intermarriage until the Assembly made such unions punishable by banishment in 1691.[1]
 In 1671, Virginia counted 6,000 white indentured servants among its 40,000 population but only 2,000 people of African descent, up to a third of whom in some counties were free. Towards the end of the 17th century, English policy shifted in favor of retaining cheap labor rather than shipping it to the colonies, and the supply of indentured servants in Virginia began to dry up; by 1715, annual immigration was in the hundreds, compared with 1,500–2,000 in the 1680s. As tobacco planters put more land under cultivation, they made up the shortfall in labor with increasing numbers of enslaved workers. The institution was rooted in race with the Virginia Slave Codes of 1705, and from around 1710 the growth in the enslaved population was fueled by natural increase. Between 1700 and 1750 the number of enslaved people in the colony increased from 13,000 to 105,000, nearly eighty percent of them born in Virginia.[2] In Washington's lifetime, slavery was deeply ingrained in the economic and social fabric of Virginia, where some forty percent of the population and virtually all African Americans were enslaved.[3]
 George Washington was born in 1732, the first child of his father Augustine's second marriage. Augustine was a tobacco planter with some 10,000 acres (4,000 ha) of land and 50 slaves. On his death in 1743, he left his 2,500-acre (1,000 ha) Little Hunting Creek to George's older half-brother Lawrence, who renamed it Mount Vernon. Washington inherited the 260-acre (110 ha) Ferry Farm and ten slaves.[4] He leased Mount Vernon from Lawrence's widow two years after his brother's death in 1752 and inherited it in 1761.[5] He was an aggressive land speculator, and by 1774 he had amassed some 32,000 acres (13,000 ha) of land in the Ohio Country on Virginia's western frontier. At his death he possessed over 80,000 acres (32,000 ha).[6][7][8] In 1757, he began a program of expansion at Mount Vernon that would ultimately result in an 8,000-acre (3,200 ha) estate with five separate farms, on which he initially grew tobacco.[9][a]
 Agricultural land required labor to be productive, and in the 18th-century American south institutional slave labor produced the greatest profits. Washington inherited slaves from Lawrence, acquired more as part of the terms of leasing Mount Vernon, and inherited slaves again on the death of Lawrence's widow in 1761.[12][13] On his marriage in 1759 to Martha Dandridge Custis, Washington gained control of eighty-four dower slaves. Martha had a life interest in those dower slaves,[14] whom she held in trust for the heirs of the Custis estate, and although Washington had no legal title to them, he managed them as his own property.[15][16][17] Between 1752 and 1773, he purchased at least seventy-one slaves – men, women and children.[18][19] He scaled back significantly his purchasing of enslaved workers after the American Revolution but continued to acquire them, mostly through natural increase and occasionally in settlement of debts.[20][18] In 1786, he listed 216 enslaved people – 122 men and women and 88 children[b] – making him one of the largest slaveholders in Fairfax County. Of that total, 103 belonged to Washington, the remainder being dower slaves. By the time of Washington's death in 1799, the population enslaved at Mount Vernon had increased to 317 people, including 143 children. Of that total, he owned 124, leased 40 and controlled 153 dower slaves.[22][23]
 Washington thought of his workers as part of an extended family with him the father figure at its head. He displayed elements of both patriarchy and paternalism in his attitudes to the slaves he controlled. The patriarch in him expected absolute obedience and manifested itself in a strict, rigorous control of the enslaved workers and the emotional distance he maintained from them.[24][25] There are examples of genuine affection between master and enslaved, such as was the case with his valet William Lee, but such cases were the exception.[26][27] The paternalist in him saw his relationship with his enslaved people as one of mutual obligations; he provided for them and they in return served him, a relationship in which the enslaved were able to approach Washington with their concerns and grievances.[24][28] Paternal masters regarded themselves as generous and deserving of gratitude.[29] When Martha's maid Oney Judge escaped in 1796, Washington complained about ""the ingratitude of the girl, who was brought up and treated more like a child than a Servant"".[30]
 Although Washington employed a farm manager to run the estate and an overseer at each of the farms, he was a hands-on manager who ran his business with a military discipline and involved himself in the minutiae of everyday work.[35][36] During extended absences while on official business, he maintained close control through weekly reports from the farm manager and overseers.[37] He demanded from all of his workers the same meticulous eye for detail that he exercised himself; a former enslaved worker would later recall that the ""slaves...did not quite like"" Washington, primarily because ""he was so exact and so strict...if a rail, a clapboard, or a stone was permitted to remain out of its place, he complained; sometimes in language of severity.""[38][39] In Washington's view, ""lost labour is never to be regained"", and he required ""every labourer (male or female) [do] as much in the 24 hours as their strength without endangering the health, or constitution will allow of"". He had a strong work ethic and expected the same from his workers, enslaved and hired.[40][41] He was constantly disappointed with enslaved workers who did not share his motivation and resisted his demands, leading him to regard them as indolent and insist that his overseers supervise them closely at all times.[42][43][44]
 In 1799, nearly three-quarters of the enslaved population, over half of them female, worked in the fields. They were kept busy year round, their tasks varying with the season.[45] The remainder worked as domestic servants in the main residence or as artisans, such as carpenters, joiners, coopers, spinners and seamstresses.[46] Between 1766 and 1799, seven dower slaves worked at one time or another as overseers.[47] Slaves were expected to work from sunrise to sunset over a six-day work week that was standard on Virginia plantations. With two hours off for meals, their workdays would range between seven and a half hours to thirteen hours, depending on season. They were given three or four days off at Christmas and a day each at Easter and Whitsunday.[48] Domestic slaves started early, worked into the evenings and did not necessarily have Sundays and holidays free.[49] On special occasions when enslaved workers were required to put in extra effort, such as working through a holiday or bringing in the harvest, they were paid or compensated with extra time off.[50]
 Washington instructed his overseers to treat enslaved people ""with humanity and tenderness"" when sick.[42] Enslaved people who were less able, through injury, disability or age, were given light duties, while those too sick to work were generally, though not always, excused work while they recovered.[51] Washington provided them with good, sometimes costly medical care – when an enslaved person named Cupid fell ill with pleurisy, Washington had him taken to the main house where he could be better cared for and personally checked on him throughout the day.[43][52] The paternal concern for the welfare of his enslaved workers was mixed with an economic consideration for the lost productivity arising from sickness and death among the labor force.[53][27]
 At Mansion House Farm, most of the enslaved people were housed in a two-story frame building known as the ""Quarters for Families"". This was replaced in 1792 by brick-built accommodation wings either side of the greenhouse comprising four rooms in total, each some 600 square feet (56 m2). The Mount Vernon Ladies' Association have concluded these rooms were communal areas furnished with bunks that allowed little privacy for the predominantly male occupants. Other enslaved people at Mansion House Farm lived over the outbuildings where they worked or in log cabins.[54] Such cabins were the standard slave accommodation at the outlying farms, comparable to the accommodation occupied by the lower strata of free white society across the Chesapeake area and by the enslaved on other Virginia plantations.[55] They provided a single room that ranged in size from 168 square feet (15.6 m2) to 246 square feet (22.9 m2) to house a family.[56] The cabins were often poorly constructed, daubed with mud for draft- and water-proofing, with dirt floors. Some cabins were built as duplexes; some single-unit cabins were small enough to be moved on carts.[57] There are few sources which shed light on living conditions in these cabins, but one visitor in 1798 wrote, ""husband and wife sleep on a mean pallet, the children on the ground; a very bad fireplace, some utensils for cooking, but in the middle of this poverty some cups and a teapot"". Other sources suggest the interiors were smoky, dirty and dark, with only a shuttered opening for a window and the fireplace for illumination at night.[58]
 Washington provided his enslaved people with a blanket each fall at most, which they used for their own bedding and which they were required to use to gather leaves for livestock bedding.[59] Enslaved people at the outlying farms were issued with a basic set of clothing each year, comparable to the clothing issued on other Virginia plantations. Slaves slept and worked in their clothes, leaving them to spend many months in garments that were worn, ripped and tattered.[60] Domestic slaves at the main residence who came into regular contact with visitors were better clothed; butlers, waiters and body servants were dressed in a livery based on the three-piece suit of an 18th-century gentleman, and maids were provided with finer quality clothing than their counterparts in the fields.[61]
 Washington desired his enslaved workers to be fed adequately but no more.[62] Each enslaved person was provided with a basic daily food ration of one US quart (0.95 L) or more of cornmeal, up to eight ounces (230 g) of herring and occasionally some meat, a fairly typical ration for the enslaved population in Virginia that was adequate in terms of the calorie requirement for a young man engaged in moderately heavy agricultural labor but nutritionally deficient.[63] The basic ration was supplemented by enslaved people's own efforts hunting (for which some were allowed guns) and trapping game. They grew their own vegetables in small garden plots they were permitted to maintain in their own time, on which they also reared poultry.[64]
 Washington often tipped enslaved people on his visits to other estates, and it is likely that his own enslaved workers were similarly rewarded by visitors to Mount Vernon. Enslaved people occasionally earned money through their normal work or for particular services rendered – for example, Washington rewarded three of his own enslaved with cash for good service in 1775, an enslaved person received a fee for the care of a mare that was being bred in 1798 and the chef Hercules profited well by selling slops from the presidential kitchen.[65] Enslaved people also earned money from their own endeavors, by selling to Washington or at the market in Alexandria food they had caught or grown and small items they had made.[66] They used the proceeds to purchase from Washington or the shops in Alexandria better clothing, housewares and extra provisions such as flour, pork, whiskey, tea, coffee and sugar.[67]
 Although the law did not recognize slave marriages, Washington did, and by 1799 some two-thirds of adult slaves at Mount Vernon were married.[68] To minimize time lost in getting to the workplace and thus increase productivity, enslaved people were accommodated at the farm on which they worked. Because of the unequal distribution of males and females across the five farms, enslaved people often found partners on different farms, and in their day-to-day lives husbands were routinely separated from their wives and children. Washington occasionally rescinded orders so as not to separate spouses, but the historian Henry Wiencek writes, ""as a general management practice [Washington] institutionalized an indifference to the stability of enslaved families.""[69] Only thirty-six of the ninety-six married slaves at Mount Vernon in 1799 lived together, while thirty-eight had spouses who lived on separate farms and twenty-two had spouses who lived on other plantations.[70] The evidence suggests couples that were separated did not regularly visit during the week, and doing so prompted complaints from Washington that enslaved people were too exhausted to work after such ""night walking"", leaving Saturday nights, Sundays and holidays as the main time such families could spend together.[71] Despite the stress and anxiety caused by this indifference to family stability – on one occasion an overseer wrote that the separation of families ""seems like death to them"" – marriage was the foundation on which the enslaved population established their own community, and longevity in these unions was not uncommon.[72][73]
 Large families that covered multiple generations, along with their attendant marriages, were part of an enslaved community-building process that transcended ownership. Washington's head carpenter Isaac, for example, lived with his wife Kitty, a dower-slave milkmaid, at Mansion House Farm. The couple had nine daughters ranging in age from six to twenty-seven in 1799, and the marriages of four of those daughters had extended the family to other farms within and outside the Mount Vernon estate and produced three grandchildren.[74][75] Children were born into slavery, their ownership determined by the ownership of their mothers.[76] The value attached to the birth of an enslaved child, if it was noted at all, is indicated in the weekly report of one overseer, which stated, ""Increase 9 Lambs & 1 male child of Lynnas."" New mothers received a new blanket and three to five weeks of light duties to recover. An infant remained with its mother at her place of work.[77] Older children, the majority of whom lived in single-parent households in which the mother worked from dawn to dusk, performed small family chores but were otherwise left to play largely unsupervised until they reached an age when they could begin to be put to work for Washington, usually somewhere between eleven and fourteen years old.[78] In 1799, nearly sixty percent of the slave population was under nineteen years old and nearly thirty-five percent under nine.[74]
 There is evidence that enslaved people passed on their African cultural values through telling stories, among them the tales of Br'er Rabbit which, with their origins in Africa and stories of a powerless individual triumphing through wit and intelligence over powerful authority, would have resonated with the slaves.[79] African-born slaves brought with them some of the religious rituals of their ancestral home, and there is an undocumented tradition of voodoo being practiced at one of the Mount Vernon farms.[80] Although the slave condition made it impossible to adhere to the Five Pillars of Islam, some slave names indicate a Muslim cultural origin.[81] Anglicans reached out to American-born slaves in Virginia, and some of the Mount Vernon enslaved population are known to have been christened before Washington acquired the estate. There is evidence in the historical record from 1797 that the enslaved population at Mount Vernon had contacts with Baptists, Methodists and Quakers.[82] Those three Protestant groups advocated abolition, raising hopes of freedom among the enslaved, and the congregation of the Alexandria Baptist Church, founded in 1803, included enslaved people formerly owned by Washington.[83]
 In 1799 there were some twenty mulatto (mixed race) enslaved people at Mount Vernon. However, there is no credible evidence that George Washington had sex with any slave.[84][85][c]
 The probability of paternal relationships between enslaved and hired white workers is indicated by some surnames: Betty and Tom Davis, probably the children of Thomas Davis, a white weaver at Mount Vernon in the 1760s; George Young, likely the son of a man of the same name who was a clerk at Mount Vernon in 1774; and Judge and her sister Delphy, the daughters of Andrew Judge, an indentured tailor at Mount Vernon in the 1770s and 1780s.[88] There is evidence to suggest that white overseers – working in close proximity to enslaved people under the same demanding master while physically and socially isolated from their own peer group, a situation that drove some to drink –  had sexual relations with the enslaved people they supervised (sex between black or white overseers and enslaved people whom they supervised was rape, assuming that consent was impossible).[89] Some white visitors to Mount Vernon seemed to have expected enslaved women to provide sexual favors.[90] The living arrangements left some enslaved females alone and vulnerable, and the Mount Vernon research historian Mary V. Thompson writes that relationships ""could have been the result of mutual attraction and affection, very real demonstrations of power and control, or even exercises in the manipulation of an authority figure"".[91]
 Although some of the enslaved population at Mount Vernon came to feel a (coerced) loyalty toward Washington, the resistance displayed by a significant percentage of them is indicated by the frequent assertions Washington made about ""rogueries"" and ""old tricks"".[92][93] The most common act of resistance was theft (e.g. as a form of self-help) which was so common that Washington made allowances for it as part of normal wastage. Food was stolen both to supplement rations and to sell, and Washington believed the selling of tools was another source of income for enslaved people. Because cloth and clothing were commonly taken without permission, Washington required seamstresses to show the results of their work and the leftover scraps before issuing them more material. Sheep were washed before shearing to prevent the theft of wool, and storage areas were kept locked and keys left with trusted individuals.[94] In 1792, Washington ordered the culling of enslaved people's dogs he believed were being used in a spate of livestock theft and ruled that enslaved people who kept dogs without authorization were to be ""severely punished"" and their dogs hanged.[95]
 Another means by which enslaved people resisted, one that was virtually impossible to prove, was feigning illness. Over the years Washington became increasingly skeptical about absenteeism due to sickness among his enslaved population and concerned about the diligence or ability of his overseers in recognizing genuine cases of physical illness. Between 1792 and 1794, while Washington was away from Mount Vernon as President, the number of days lost to sickness increased tenfold compared to 1786, when he was resident at Mount Vernon and able to exert control over the situation personally. In one case, Washington suspected an enslaved person of frequently avoiding work over a period of decades through acts of deliberate self harm.[96]
 Enslaved people asserted some independence and frustrated Washington by the pace and quality of their work.[97] In 1760, Washington noted that four of his carpenters quadrupled their output of timber under his personal supervision.[98] Thirty-five years later, he denigrated his carpenters as an ""idle...set of rascals"" who would take a month or more to complete at Mount Vernon work that was being done in two or three days in Philadelphia. The output of seamstresses dropped off when Martha was away, and spinners found they could slow their pace by playing the overseers off against her.[99] Tools were regularly lost or damaged, thus stopping work, and Washington despaired of employing innovations that might improve efficiency because he assumed enslaved workers were too clumsy to operate the new machinery involved.[100]
 The most emphatic act of resistance was to run away, and between 1760 and 1799 at least forty-seven enslaved people under Washington's control did so.[101] Seventeen of these, fourteen men and three women, escaped to a British warship that anchored in the Potomac River near Mount Vernon in 1781.[102] In general, the best chance for a successful escape lay with second- or third-generation African-American enslaved people who had good English, possessed skills that would allow them to support themselves as free people and were in close enough contact with their masters to receive special privileges. Oney Judge, who was an especially talented seamstress, and Hercules Posey escaped in 1796 and 1797 respectively and eluded recapture.[103] Washington took seriously the recapture of these brave fugitives, and in three cases enslaved people who had escaped were sold off in the West Indies after recapture, effectively a death sentence in the severe conditions the enslaved were subjected to there.[104][105][106]
 Washington used both reward and punishment to encourage discipline and productivity in his enslaved population.[108] In one case, he suggested ""admonition and advice"" would be more effective than ""further correction"", and he occasionally appealed to an enslaved person's sense of pride to encourage better performance. Rewards in the form of better blankets and clothing fabric were given to the ""most deserving"", and there are examples of cash payments being awarded for good behavior.[109] He opposed the use of the lash in principle, but saw the practice as a necessary evil and sanctioned its occasional use, generally as a last resort, on enslaved people, both male and female, if they did not, in his words, ""do their duty by fair means"".[108] There are accounts of carpenters being whipped in 1758 when the overseer ""could see a fault"", of an enslaved person called Jemmy being whipped for stealing corn and escaping in 1773 and of a seamstress called Charlotte being whipped in 1793 by an overseer ""determined to lower Spirit or skin her Back"" for impudence and refusing to work.[110][111]
 Washington regarded the 'passion' with which one of his overseers administered floggings to be counter-productive, and Charlotte's protest that she had not been whipped in fourteen years indicates the frequency with which physical punishment was used.[112][113] Whippings were administered by overseers after review, a system Washington required to ensure enslaved people were spared capricious and extreme punishment. Washington did not himself flog enslaved people, but he did at times use verbal abuse and physical violence when they failed to perform as he expected.[114][d] Contemporaries generally described Washington as having a calm demeanor, but there are several reports from those who knew him privately that mention his temper. One wrote that ""in private and particularly with his servants, its violence sometimes broke out"". Another reported that Washington's servants ""seemed to watch his eye and to anticipate his every wish; hence a look was equivalent to a command"".[116] Threats of demotion to fieldwork, corporal punishment and being shipped to the West Indies were part of the system by which he controlled his enslaved population.[104][117]
 Washington's early views on slavery were no different from any Virginia planter of the time.[53] He demonstrated no moral qualms about the institution, and referred to slaves as ""a Species of Property"" during those years as he would later in life when he favored abolition.[118] The economics of slavery prompted the first doubts in Washington about the institution, marking the beginning of a slow evolution in his attitude towards it. By 1766, he had transitioned his business from the labor-intensive planting of tobacco to the less demanding farming of grain crops. His slaves were employed on a greater variety of tasks that needed more skills than tobacco planting required of them; as well as the cultivation of grains and vegetables, they were employed in cattle herding, spinning, weaving and carpentry. Some scholars have asserted that the transition left Washington with a surplus of slaves and revealed to him the inefficiencies of the slave labor system,[119][120] but another view is that the transition did not result in a surplus of slaves because Washington found other productive work for them.[121]
 There is little evidence that Washington seriously questioned the ethics of slavery before the Revolution.[120] In the 1760s he often participated in tavern lotteries, events in which defaulters' debts were settled by raffling off their assets to a high-spirited crowd.[122] In 1769, Washington co-managed one such lottery in which fifty-five slaves were sold, among them six families and five females with children. The more valuable married males were raffled together with their wives and children; less valuable slaves were separated from their families into different lots. Robin and Bella, for example, were raffled together as husband and wife while their children, twelve-year-old Sukey and seven-year-old Betty, were listed in a separate lot. Only chance dictated whether the family would remain together, and with 1,840 tickets on sale the odds were not good.[123]
 The historian Henry Wiencek concludes that the repugnance Washington felt at this cruelty in which he had participated prompted his decision not to break up slave families by sale or purchase, and marks the beginning of a transformation in Washington's thinking about the morality of slavery.[124] Wiencek writes that in 1775 Washington took more slaves than he needed rather than break up the family of a slave he had agreed to accept in payment of a debt.[125] The historians Philip D. Morgan and Peter Henriques[e] are skeptical of Wiencek's conclusion and believe there is no evidence of any change in Washington's moral thinking at this stage. Morgan writes that in 1772, Washington was ""all business"" and ""might have been buying livestock"" in purchasing more slaves who were to be, in Washington's words, ""strait Limb'd, & in every respect strong & likely, with good Teeth & good Countenance"". Morgan gives a different account of the 1775 purchase, writing that Washington resold the slave because of the slave's resistance to being separated from family and that the decision to do so was ""no more than the conventional piety of large Virginia planters who usually said they did not want to break up slave families – and often did it anyway"".[127][128]
 From the late 1760s, Washington became increasingly radicalized against the North American colonies' subservient status within the British Empire.[129] In 1774 he was a key participant in the adoption of the Fairfax Resolves which, alongside the assertion of colonial rights, condemned the transatlantic slave trade on moral grounds.[130][120] Washington was a signatory to that entire document, and thus publicly endorsed clause 17 ""declaring our earnest wishes to see an entire stop forever put to such wicked, cruel, and unnatural trade.""[131]
 He began to express the growing rift with Great Britain in terms of slavery, stating in the summer of 1774 that the British authorities were ""endeavouring by every piece of Art & despotism to fix the Shackles of Slavry [sic]"" upon the colonies. Two years later, on taking command of the Continental Army at Cambridge at the start of the American Revolutionary War, he wrote in orders to his troops that ""it is a noble Cause we are engaged in, it is the Cause of virtue and mankind...freedom or Slavery must be the result of our conduct.""[132] The hypocrisy or paradox inherent in slave owners characterizing a war of independence as a struggle for their own freedom from slavery was not lost on the British writer Samuel Johnson, who asked, ""How is it that we hear the loudest yelps for liberty among the drivers of Negroes?""[133][134] As if answering Johnson, Washington wrote to a friend in August 1774, ""The crisis is arrived when we must assert our rights, or submit to every imposition that can be heaped upon us, till custom and use shall make us tame and abject slaves, as the blacks we rule over with such arbitrary sway.""[135]
 Washington shared the common Southern concern about arming African Americans, enslaved or free, and initially refused to accept either into the ranks of the Continental Army. He reversed his position on free African Americans when the royal governor of Virginia, Lord Dunmore, issued a proclamation in November 1775 offering freedom to rebel-owned slaves who enlisted in the British forces. Three years later and facing acute manpower shortages, Washington approved a Rhode Island initiative to raise a battalion of African-American soldiers.[136][137]
 Washington gave a cautious response to a 1779 proposal from his young aide John Laurens for the recruitment of 3,000 South Carolinian enslaved workers who would be rewarded with emancipation. He was concerned that such a move would prompt the British to do the same, leading to an arms race in which the Americans would be at a disadvantage, and that it would promote discontent among those who remained enslaved.[138][139][f] In 1780, he suggested to one of his commanders the integration of African-American recruits ""to abolish the name and appearance of a Black Corps.""[143]
 During the war, some 5,000 African Americans served in a Continental Army that was more integrated than any American force before the Vietnam War, and another 1,000 served in the Continental Navy. They represented less than three percent of all American forces mobilized, though in 1778 they provided as much as 13% of the Continental Army.[144][145] By the end of the war African-Americans were serving alongside whites in virtually all units other than those raised in the deep south.[143][146]
 The first indication of a shift in Washington's attitude on slavery appeared during the war, in correspondence of 1778 and 1779 with Lund Washington, who managed Mount Vernon in Washington's absence.[147] In the exchange of letters, a conflicted Washington expressed a desire ""to get quit of Negroes"", but made clear his reluctance to sell them at a public venue and his wish that ""husband and wife, and Parents and children are not separated from each other"".[148] His determination not to separate families became a major complication in his deliberations on the sale, purchase and, in due course, emancipation of his own slaves.[149] Washington's restrictions put Lund in a difficult position with two female slaves he had already all but sold in 1778, and Lund's irritation was evident in his request to Washington for clear instructions.[150] Despite Washington's reluctance to break up families, there is little evidence that moral considerations played any part in his thinking at this stage. He sought to liberate himself from an economically unviable system, not to liberate his slaves. They were still a property from which he expected to profit. During a period of severe wartime depreciation, the question was not whether to sell his enslaved people, but when, where, and how best to sell them. Lund sold nine enslaved including the two females, in January 1779.[151][152][153]
 Washington's actions at the war's end reveal little in the way of antislavery inclinations. He was anxious to recover his own slaves, and refused to consider British offers of compensation for the upwards of 80,000 formerly enslaved people evacuated by the British; Washington insisted without success that the British return them to their owners as per a clause in the Preliminary Articles of Peace which prohibited the British from ""carrying away any Negroes, or other Property of the American Inhabitants"".[154][155][156] Before resigning his commission in 1783, Washington took the opportunity to give his opinion on the challenges that threatened the existence of the new nation, in his Circular to the States. That circular letter inveighed against ""local prejudices"" but explicitly declined to name any of them, ""leaving the last to the good sense and serious consideration of those immediately concerned.""[155][157]
 Emancipation became a major issue in Virginia after liberalization in 1782 of the law regarding manumission, which is the act of an owner freeing his slaves. Before 1782, a manumission had required obtaining consent from the state legislature, which was arduous and rarely granted.[158] After 1782, inspired by the rhetoric that had driven the revolution, it became popular to free slaves. The free African-American population in Virginia rose from some 3,000 to more than 20,000 between 1780 and 1800; the 1800 United States census tallied about 350,000 slaves in Virginia, and the proslavery interest re-asserted itself around that time.[159][160][161] The historian Kenneth Morgan writes, ""...the revolutionary war was the crucial turning-point in [Washington's] thinking about slavery. After 1783...he began to express inner tensions about the problem of slavery more frequently, though always in private...""[162] Although Philip Morgan identifies several turning points and believes no single one was pivotal,[g] most historians agree the Revolution was central to the evolution of Washington's attitudes on slavery.[166][167] It is likely that revolutionary rhetoric about the rights of men, the close contact with young antislavery officers who served with Washington – such as Laurens, the Marquis de Lafayette and Alexander Hamilton – and the influence of northern colleagues were contributory factors in that process.[168][169][h]
 Washington was drawn into the postwar abolitionist discourse through his contacts with antislavery friends, their transatlantic network of leading abolitionists and the literature produced by the antislavery movement,[172] though he was reluctant to volunteer his own opinion on the matter and generally did so only when the subject was first raised with him.[162] At his death, Washington's extensive library included at least seventeen publications on slavery. Six of them had been collated into an expensively bound volume titled Tracts on Slavery, indicating that he attached some importance to that selection. Five of the six were published in or after 1788.[i] All six shared common themes that slaves first had to be educated about the obligations of liberty before they could be emancipated, a belief Washington is reported to have expressed himself in 1798, and that abolition should be realized by a gradual legislative process, an idea that began to appear in Washington's correspondence during the Confederation period.[174][175]
 Washington was not impressed by what Dorothy Twohig – a former editor-in-chief of The Washington Papers – described as the ""imperious demands"" and ""evangelical piety"" of Quaker efforts to advance abolition, and in 1786 he complained about their ""tamper[ing] with & seduc[ing]"" slaves who ""are happy & content to remain with their present masters"".[176][177] Only the most radical of abolitionists called for immediate emancipation. Immediate (instead of gradual) emancipation would have quickly cured a grave injustice, but cautious abolitionists feared that sudden emancipation would also disrupt the labor market, as well as disrupting those elderly and infirm people whom slaveholders had been required to care for.[178] Large numbers of unemployed poor, of whatever color, was a cause for concern in 18th-century America, to the extent that expulsion and foreign resettlement was often part of the discourse on emancipation.[178] A sudden end to slavery would also have caused a significant financial loss to slaveowners whose human property represented a valuable asset. Gradual emancipation was seen as a way of mitigating such a loss and reducing opposition from those with a financial self-interest in maintaining slavery.[179]
 In 1783, Lafayette proposed a joint venture to establish an experimental settlement for freed slaves which, with Washington's example, ""might render it a general practise"", but Washington demurred. As Lafayette forged ahead with his plan, Washington offered encouragement but expressed concern in 1786 about ""much inconvenience and mischief"" an abrupt emancipation might generate, and he gave no tangible support to the idea.[152][180][j]
 Washington expressed support for emancipation legislation to prominent Methodists Thomas Coke and Francis Asbury in 1785, but declined to sign their petition which (as Coke put it) asked ""the General Assembly of Virginia, to pass a law for the immediate or gradual emancipation of all the slaves"".[183][184][185] Washington privately conveyed his support for such legislation to most of the great men of Virginia,[186][183] and promised to comment publicly on the matter by letter to the Virginia Assembly if the Assembly would begin serious deliberation about the Methodists' petition.[187][185] The historian Lacy Ford writes that Washington may have dissembled: ""In all likelihood, Washington was honest about his general desire for gradual emancipation but dissembled about his willingness to speak publicly on its behalf; the Mount Vernon master almost certainly reasoned that the legislature would table the petition immediately and thus release him from any obligation to comment publicly on the matter."" The measure was rejected without any dissent in the Virginia House of Delegates, because abolitionist legislators quickly backed down rather than suffer inevitable defeat.[183][186][187] Washington wrote in despair to Lafayette: ""Some petitions were presented to the Assembly at its last session for the abolition of slavery, but they could scarce obtain a reading.""[185] James Thomas Flexner's interpretation is somewhat different from Lacy Ford's: ""Washington was willing to back publicly the Methodists' petition for gradual emancipation if the proposal showed the slightest possibility of being given consideration by the Virginia legislature.""[185] Flexner adds that, if Washington had been more audacious in pursuing emancipation in Virginia, then ""he undoubtedly would have failed to achieve the end of slavery, and he would certainly have made impossible the role he played in the Constitutional Convention and the Presidency.""[188]
 Henriques identifies Washington's concern for the judgement of posterity as a significant factor in Washington's thinking on slavery, writing, ""No man had a greater desire for secular immortality, and [Washington] understood that his place in history would be tarnished by his ownership of slaves.""[189] Philip Morgan similarly identifies the importance of Washington's driving ambition for fame and public respect as a man of honor;[169] in December 1785, the Quaker and fellow Virginian Robert Pleasants ""[hit] Washington where it hurt most"", Morgan writes, when he told Washington that to remain a slaveholder would forever tarnish his reputation.[190][k] In correspondence the next year with Maryland politician John Francis Mercer, Washington expressed ""great repugnance"" at buying slaves, stated that he would not buy any more ""unless some peculiar circumstances should compel me to it"" and made clear his desire to see the institution of slavery ended by a gradual legislative process.[195][196] He expressed his support for abolitionist legislation privately, but widely,[197] sharing those views with leading Virginians,[185] and with other leaders including Mercer and founding father Robert Morris of Pennsylvania to whom Washington wrote:[198]
 Washington still needed labor to work his farms, and there was little alternative to slavery. Hired labor south of Pennsylvania was scarce and expensive, and the Revolution had cut off the supply of indentured servants and convict labor from Great Britain.[197][38] Washington significantly reduced his slave purchases after the war, though it is not clear whether this was a moral or practical decision; he repeatedly stated that his inventory and its potential progeny were adequate for his current and foreseeable needs.[199][200] Nevertheless, he negotiated with John Mercer to accept six slaves in payment of a debt in 1786 and expressed to Henry Lee a desire to purchase a bricklayer the next year.[177][20][l] In 1788, Washington acquired thirty-three slaves from the estate of Bartholomew Dandridge in settlement of a debt and left them with Dandridge's widow on her estate at Pamocra, New Kent County, Virginia.[205][206] Later the same year, he declined a suggestion from the leading French abolitionist Jacques Brissot to form and become president of an abolitionist society in Virginia, stating that although he was in favor of such a society and would support it, the time was not yet right to confront the issue.[207] Historian James Flexner has written that, generally speaking, ""Washington limited himself to stating that, if an authentic movement toward emancipation could be started in Virginia, he would spring to its support. No such movement could be started.""[208]
 Washington presided over the Constitutional Convention in 1787, during which it became obvious how explosive the slavery issue was, and how willing the antislavery faction was to accept the preservation of this oppressive institution to ensure national unity and the establishment of a strong federal government. The Constitution allowed but did not require the preservation of slavery, and it deliberately avoided use of the word ""slave"" which could have been interpreted as authorizing the treatment of human beings as property throughout the country.[209] Each state was allowed to keep it, change it, or eliminate it as they wished, though Congress could make various policies that would affect this decision in each state. As of 1776, slavery was legal in all 13 colonies, but by Washington's death in December 1799 there were eight free states and nine slave states, and that split was considered entirely constitutional.[210]
 The support of the southern states for the new constitution was secured by granting them concessions that protected slavery, including the Fugitive Slave Clause, plus clauses that promised Congress would not prohibit the transatlantic slave trade for twenty years, and that empowered (but did not require) Congress to authorize suppression of insurrections such as slave rebellions.[211][212] The Constitution also included the Three-Fifths Compromise which cut both ways: for purposes of taxation and representation, three out of every five slaves would be counted, which meant that each slave state would have to pay less taxes but would also have less representation in Congress than if every slave was counted.[213] After the convention, Washington's support was critical for getting the states to ratify the document.[214]
 Washington's preeminent position ensured that any actions he took with regard to his own slaves would become a statement in a national debate about slavery that threatened to divide the country. Wiencek suggests Washington considered making precisely such a statement on taking up the presidency in 1789. A passage in the notebook of Washington's biographer David Humphreys[m] dated to late 1788 or early 1789 recorded a statement that resembled the emancipation clause in Washington's will a decade later. Wiencek argues the passage was a draft for a public announcement Washington was considering in which he would declare the emancipation of some of his slaves. It marks, Wiencek believes, a moral epiphany in Washington's thinking, the moment he decided not only to emancipate his slaves but also to use the occasion to set the example Lafayette had urged in 1783.[217] Other historians dispute Wiencek's conclusion; Henriques and Joseph Ellis concur with Philip Morgan's opinion that Washington experienced no epiphanies in a ""long and hard-headed struggle"" in which there was no single turning point. Morgan argues that Humphreys' passage is the ""private expression of remorse"" from a man unable to extricate himself from the ""tangled web"" of ""mutual dependency"" on slavery, and that Washington believed public comment on such a divisive subject was best avoided for the sake of national unity.[218][219][128][n]
 Washington took up the presidency at a time when revolutionary sentiment against slavery was giving way to a resurgence of proslavery interests. No state considered making slavery an issue during the ratification of the new constitution, southern states reinforced their slavery legislation and prominent antislavery figures were muted about the issue in public. Washington understood there was little widespread organized support for abolition.[223] He had a keen sense both of the fragility of the fledgling Republic and of his place as a unifying figure, and he was determined not to endanger either by confronting an issue as divisive and entrenched as slavery.[224][225]
 He was president of a government that provided materiel and financial support for French efforts to suppress the Saint Domingue slave revolt in 1791, and which also implemented the proslavery Fugitive Slave Act of 1793.[226][227][228]  The Fugitive Slave Act gave effect to the Constitution's Fugitive Slave Clause and Extradition Clause, the Act was passed overwhelmingly in Congress (e.g. the vote was 48 to 7 in the House), it was then signed by Washington, and the Act was decried by free blacks who correctly believed it would allow bounty hunting and kidnapping.[229]  Although the wording of the act required anyone seized as a fugitive slave to be taken before a judge or magistrate to certify the seizure before removing them from the state,[230] this requirement was often ignored in practice. Indeed, the Act was written amidst a controversy about a free black man named John Davis who was kidnapped from Pennsylvania and brought to Virginia, but the Act did not even resolve that controversy; the kidnappers from Virginia were never extradited to Pennsylvania, and John Davis remained a slave.[231]
 On the anti-slavery side of the ledger, in August 1789 Washington signed a reenactment of the 1787 Northwest Ordinance which had freed all new slaves brought after 1787 into a vast expanse of federal territory north of the Ohio River, except for slaves escaping from slave states.[232][233] That 1787 law had lapsed when the new U.S. Constitution was ratified in March 1789, and Congress reaffirmed it with the Northwest Ordinance of 1789, which included the slavery restriction.[234] Washington also signed into law the Slave Trade Act of 1794 that banned the involvement of American ships and American exporters in the international slave trade.[235] Moreover, according to Washington biographer James Thomas Flexner, Washington as President weakened slavery by favoring Hamilton's economic plans over Jefferson's agrarian economics.[208]
 Washington never spoke publicly on the issue of slavery during his eight years as president, nor did he respond to, much less act upon, any of the antislavery petitions he received. He described a February 1790 Quaker petition to Congress urging an immediate end to the slave trade as ""an illjudged piece of business"" that ""occasioned a great waste of time"", although historian Paul F. Boller has observed that Congress extensively debated that petition only to conclude it had no power to do anything about it, so ""The Quaker Memorial may have been a waste of time so far as immediate practical results were concerned.""[236][237] Washington endorsed James Madison's maneuvering in the House to table any discussion of ending the slave trade until 1808, as specified by the Constitution.[238]  His abolitionist aspirations for the nation centered around the hope that slavery would disappear naturally over time with the prohibition of slave imports in 1808, the earliest date such legislation could be passed as agreed at the Constitutional Convention.[178][239]
 Late in his presidency, Washington told his Secretary of State, Edmund Randolph, that in the event of a confrontation between North and South, he had ""made up his mind to remove and be of the Northern"" (i.e. leave Virginia and move up north).[240] In 1798, he imagined just such a conflict when he said, ""I can clearly foresee that nothing but the rooting out of slavery can perpetuate the existence of our union.""[241][175] But there is no indication Washington ever favored an immediate rather than gradual end to slavery. Indeed, the gradual dying out of slavery remained possible, until Eli Whitney invented the cotton gin in 1793 which led within five years to a vastly greater demand for slave labor.[242]
 As well as political caution, economic imperatives remained an important consideration with regard to Washington's personal position as a slaveholder and his efforts to free himself from his dependency on slavery.[243][165] He was one of the largest debtors in Virginia at the end of the war,[244] and by 1787 the business at Mount Vernon had failed to make a profit for more than a decade. Persistently poor crop yields due to pestilence and poor weather, the cost of renovations at his Mount Vernon residence, the expense of entertaining a constant stream of visitors, the failure of Lund to collect rent from Washington's tenant farmers and wartime depreciation all helped to make Washington cash poor.[245][246]
 The overheads of maintaining a surplus of slaves, including the care of the young and elderly, made a substantial contribution to his financial difficulties.[248][200] In 1786, the ratio of productive to non-productive slaves was approaching 1:1, and the c. 7,300-acre (3,000 ha) Mount Vernon estate was being operated with 122 working slaves. Although the ratio had improved by 1799 to around 2:1, the Mount Vernon estate had grown by only 10 percent to some 8,000 acres (3,200 ha) while the working slave population had grown by 65 percent to 201. It was a trend that threatened to bankrupt Washington.[249][250] The slaves Washington had bought early in the development of his business were beyond their prime and nearly impossible to sell, and from 1782 Virginia law made slaveowners liable for the financial support of slaves they freed who were too young, too old or otherwise incapable of working.[251][252]
 During his second term, Washington began planning for a retirement that would provide him ""tranquillity with a certain income"".[253] In December 1793, he sought the aid of the British agriculturalist Arthur Young in finding farmers to whom he would lease all but one of his farms, on which his slaves would then be employed as laborers.[254][255] The next year, he instructed his secretary Tobias Lear to sell his western lands, ostensibly to consolidate his operations and put his financial affairs in order. Washington concluded his instructions to Lear with a private passage in which he expressed repugnance at owning slaves and declared that the principal reason for selling the land was to raise the finances that would allow him to liberate them.[243][256] It is the first clear indication that Washington's thinking had shifted from selling his slaves to freeing them.[253] In November the same year (1794), Washington declared in a letter to his friend and neighbor Alexander Spotswood: ""Were it not then, that I am principled agt. [sic] selling Negroes, as you would Cattle in the market, I would not, in twelve months from this date, be possessed of one as a slave.""[257][21]
 In 1795 and 1796, Washington devised a complicated plan that involved renting out his western lands to tenant farmers to whom he would lease his own slaves, and a similar scheme to lease the dower slaves he controlled to Dr. David Stuart for work on Stuart's Eastern Shore plantation. This plan would have involved breaking up slave families, but it was designed with an end goal of raising enough finances to fund their eventual emancipation (a detail Washington kept secret) and prevent the Custis heirs from permanently splitting up families by sale.[258][259][o]
 None of these schemes could be realized because of his failure to sell or rent land at the right prices, the refusal of the Custis heirs to agree to them and his own reluctance to separate families.[261][262] Wiencek speculates that, because Washington gave such serious consideration to freeing his slaves knowing full well the political ramifications that would follow, one of his goals was to make a public statement that would sway opinion towards abolition.[263] Philip Morgan argues that Washington freeing his slaves while President in 1794 or 1796 would have had no profound effect, and would have been greeted with public silence and private derision by white southerners.[264]
 Wiencek writes that if Washington had found buyers for his land at what seemed like a fair price, this plan would have ultimately freed ""both his own and the slaves controlled by Martha's family"",[265] and to accomplish this goal Washington would ""yield up his most valuable remaining asset, his western lands, the wherewithal for his retirement.""[266] Ellis concludes that Washington prioritized his own financial security over the freedom of the enslaved population under his control, and writes, on Washington's failure to sell the land at prices he thought fair, ""He had spent a lifetime acquiring an impressive estate, and he was extremely reluctant to give it up except on his terms.""[267] In discussing another of Washington's plans, drawn up after he had written his will, to transfer enslaved workers to his estates in western Virginia, Philip Morgan writes, ""Indisputably, then, even on the eve of his death, Washington was far from giving up on slavery. To the last, he was committed to making profits, even at the expense of the disruptions such transfers would indisputably have wrought on his slaves.""[268]
 As Washington subordinated his desire for emancipation to his efforts to secure financial independence, he took care to retain his slaves.[269] From 1791, he arranged for those who served in his personal retinue in Philadelphia while he was President to be rotated out of the state before they became eligible for emancipation after six months residence per Pennsylvanian law. Not only would Washington have been deprived of their services if they were freed, most of the slaves he took with him to Philadelphia were dower slaves, which meant that he would have had to compensate the Custis estate for the loss. Because of his concerns for his public image and that the prospect of emancipation would generate discontent among the slaves before they became eligible for emancipation, he instructed that they be shuffled back to Mount Vernon ""under pretext that may deceive both them and the Public"".[270]
 Washington spared no expense in efforts to recover Hercules and Judge when they absconded. In Judge's case, Washington persisted for three years. He tried to persuade her to return when his agent eventually tracked her to New Hampshire, but refused to promise her freedom after his death; ""However well disposed I might be to a gradual emancipation"", he said, ""or even to an entire emancipation of that description of People (if the latter was in itself practicable at this moment) it would neither be politic or just to reward unfaithfulness with a premature preference"". Both Hercules and Judge eluded capture.[17] Washington's search for a new chef to replace Hercules in 1797 is the last known instance in which he considered buying a slave, despite his resolve ""never to become the Master of another Slave by purchase""; in the end he chose to hire a white chef.[271]
 Historian Joseph Ellis writes that Washington did not favor the continuation of legal slavery, and adds ""[n]or did he ever embrace the racial arguments for black inferiority that Jefferson advanced....He saw slavery as the culprit, preventing the development of diligence and responsibility that would emerge gradually and naturally after emancipation.""[272] Other historians, such as Stuart Leibinger, agree with Ellis that, ""Unlike Jefferson, Washington and Madison rejected innate black inferiority....""[273]
 The historian James Thomas Flexner says that ""The record in relation to George Washington is a conspicuous demonstration of how black history has been neglected. One example: the two‐volume index to the thirty‐nine volume set of Washington's 'Writings' specifies almost everything except the names of slaves.""[208]  In regard to Washington's racial views, Flexner says that the charge of racism has come from historical revisionism and lack of investigation, in view of the fact that slavery was ""not invented for blacks, the institution was as old as history and had not, when Washington was a child, been officially challenged anywhere.""[208]
 Kenneth Morgan writes that, ""Washington's engrained sense of racial superiority to African Americans did not lead to expressions of negrophobia...Yet Washington wanted his white workers to be housed away from the blacks at Mt. Vernon, believing that close racial intermixture was undesirable.""[274] According to historian Albert Tillson, one reason why enslaved Black people were lodged separately at Mount Vernon is because Washington felt that some white workers had habits that were ""not good"" (e.g., Tillson mentions instances of ""interracial drinking"" in the Chesapeake area), and another reason is that, Tillson reports, Washington ""expected such accommodations would eventually disgust the white family.""[275]
 Philip Morgan writes that ""The youthful Washington revealed prejudices toward blacks, quite natural for the day"" and that ""blackness, in his mind, was synonymous with uncivilized behaviour.""[276] Washington's prejudices were not hard and fast; his retention of African-Americans in the Virginia Regiment contrary to the rules, his employment of African-American overseers, his use of African-American doctors and his praise for the ""great poetical Talents"" of the African-American poet Phillis Wheatley, who had lauded him in a poem in 1775, show that he recognized the skills and talents of African-Americans.[277] Historian Henry Wiencek rendered this judgment:[278]
 Martha Washington's views about slavery and race were different from her husband's, and were less favorable to African Americans. For example, she said in 1795 that, ""The Blacks are so bad in their nature that they have not the least grat[i]tude for the kindness that may be shewed to them."" She refused to follow the example he set by emancipating his slaves, and instead she bequeathed the only slave she directly owned (named Elish) to her grandson.[279][280]
 In July 1799, five months before his death, Washington wrote his will, in which he stipulated that one of his slaves should be freed, and the remainder forced to work for his widow, to be freed on her death. In the months that followed, he considered a plan to repossess tenancies in Berkeley and Frederick Counties and transferring half of his Mount Vernon slaves to work them. It would, Washington hoped, ""yield more nett profit"" which might ""benefit myself and not render the [slaves'] condition worse"", despite the disruption such relocation would have had on the slave families. The plan died with Washington on December 14, 1799.[281][p]
 Washington's slaves were the subjects of the longest provisions in the 29-page will, taking three pages in which his instructions were more forceful than in the rest of the document. His valet, William Lee, was freed immediately and the use of his remaining 123 slaves was bequeathed to his widow until her death.[283][284] The deferral was intended to postpone the pain of separation that would occur when his slaves were freed but their spouses among the dower slaves remained in bondage, a situation which affected 20 couples and their children. It is possible Washington hoped Martha and her heirs who would inherit the dower slaves would solve this problem by following his example and emancipating them.[285][286][76] Those too old or infirm to work were to be supported by his estate, as mandated by state law.[287] In the late 1790s, about half the enslaved population at Mount Vernon was too old, too young, or too infirm to be productive.[288]
 Washington went beyond the legal requirement to support and maintain younger slaves until adulthood, stipulating that those children whose education could not be undertaken by parents were to be taught reading, writing, and a useful trade by their masters and then be freed at the age of 25.[287] He forbade the sale or transportation of any of his slaves out of Virginia before their emancipation.[284] Including the Dandridge slaves, who were to be emancipated under similar terms, more than 160 slaves would be freed.[205][206] Although Washington was not alone among Virginian slaveowners in providing for the delayed freedom of their slaves, he was unusual among those doing it for doing it so late, after the post-revolutionary support for emancipation in Virginia had faded. He was also unusual for being one of the few slaveowning founders to do so.[289] Other founders who freed their slaves include John Dickinson and Caesar Rodney, both of Delaware.[290]
 Washington's action was ignored by southern slaveholders, and slavery continued at Mount Vernon.[291][292] Already from 1795, dower slaves were being transferred to Martha's three granddaughters as the Custis heirs married.[293] Martha felt threatened by being surrounded with slaves whose freedom depended on her death and freed her late husband's slaves on January 1, 1801.[294][q]  That action by Martha Washington was not a violation of her husband's will, which did not require her to keep any slaves in bondage.[296]
 Able-bodied slaves were freed and left to support themselves and their families.[297] Within a few months, almost all of Washington's former slaves had left Mount Vernon, leaving 121 adult and working-age children still working the estate. Five freedwomen were listed as remaining: an unmarried mother of two children; two women, one of them with three children, married to Washington slaves too old to work; and two women who were married to dower slaves.[298] William Lee remained at Mount Vernon, where he worked as a shoemaker.[299] After Martha's death on May 22, 1802, most of the remaining dower slaves passed to her grandson, George Washington Parke Custis, to whom she bequeathed the only slave she held in her own name.[300]
 There are few records of how the newly freed slaves fared.[301] Custis later wrote that ""although many of them, with a view to their liberation, had been instructed in mechanic trades, yet they succeeded very badly as freemen; so true is the axiom, 'that the hour which makes man a slave, takes half his worth away'"". The son-in-law of Custis's sister wrote in 1853 that the descendants of those who remained slaves, many of them now in his possession, had been ""prosperous, contented and happy"", while those who had been freed had led a life of ""vice, dissipation and idleness"" and had, in their ""sickness, age and poverty"", become a burden to his in-laws.[302] Such reports were influenced by the innate racism of the well-educated, upper-class authors and ignored the social and legal impediments that prejudiced the chances of prosperity for former slaves, which included laws that made it illegal to teach freedpeople to read and write and, in 1806, required newly freed slaves to leave the state.[303][304]
 There is evidence that some of Washington's former slaves were able to buy land, support their families and prosper as free people. By 1812, Free Town in Truro Parish, the earliest known free African-American settlement in Fairfax County, contained seven households of former Washington slaves. By the mid 1800s, a son of Washington's carpenter Davy Jones and two grandsons of his postilion Joe Richardson had each bought land in Virginia. Francis Lee, younger brother of William, was well known and respected enough to have his obituary printed in the Alexandria Gazette on his death at Mount Vernon in 1821. Sambo Anderson – who hunted game, as he had while Washington's slave, and prospered for a while by selling it to the most respectable families in Alexandria – was similarly noted by the Gazette when he died near Mount Vernon in 1845.[305] Research published in 2019 has concluded that Hercules worked as a cook in New York, where he died on May 15, 1812.[306]
 A decade after Washington's death, the Pennsylvanian jurist Richard Peters wrote that Washington's servants ""were devoted to him; and especially those more immediately about his person. The survivors of them still venerate and adore his memory."" In his old age, Anderson said he was ""a much happier man when he was a slave than he had ever been since"", because he then ""had a good kind master to look after all my wants, but now I have no one to care for me"".[307] When Judge was interviewed in the 1840s, she expressed considerable bitterness, not at the way she had been treated as a slave, but at the fact that she had been enslaved. When asked, having experienced the hardships of being a freewoman and having outlived both husband and children, whether she regretted her escape, she replied, ""No, I am free, and have, I trust, been made a child of God by [that] means.""[308]
 Washington's will was both private testament and public statement on the institution.[284][221] It was published widely – in newspapers nationwide, as a pamphlet which, in 1800 alone, extended to thirteen separate editions, and included in other works – and became part of the nationalist narrative.[309] In the eulogies of the antislavery faction, the inconvenient fact of Washington's slaveholding was downplayed in favor of his final act of emancipation. Washington ""disdained to hold his fellow-creatures in abject domestic servitude,"" wrote the Massachusetts Federalist Timothy Bigelow before calling on ""fellow-citizens in the South"" to emulate Washington's example. In this narrative, Washington was a proto-abolitionist who, having added the freedom of his slaves to the freedom from British slavery he had won for the nation, would be mobilized to serve the antislavery cause.[310]
 An alternative narrative more in line with proslavery sentiments embraced rather than excised Washington's ownership of slaves. Washington was cast as a paternal figure, the benevolent father not only of his country but also of a family of slaves bound to him by affection rather than coercion.[311] In this narrative, slaves idolized Washington and wept at his deathbed, and in an 1807 biography, Aaron Bancroft wrote, ""In domestick [sic] and private life, he blended the authority of the master with the care and kindness of the guardian and friend.""[312] The competing narratives allowed both North and South to claim Washington as the father of their countries during the American Civil War that ended slavery more than half a century after his death.[313]
 There is tension between Washington's stance on slavery, and his broader historical role as a proponent of liberty. He was a slaveholder who led a war for liberty, and then led the establishment of a national government that secured liberty for many of its citizens, and historians have considered this a paradox.[134] The historian Edmund Sears Morgan explained that Washington was not alone in this regard: ""Virginia produced the most eloquent spokesmen for freedom and equality in the entire United States: George Washington, James Madison, and, above all, Thomas Jefferson. They were all slaveholders and remained so throughout their lives.""[314] Washington recognized this paradox, rejected the notion of black inferiority, and was somewhat more humane than other slaveowners, but failed to publicly become an active supporter of emancipation laws. Historians have found evidence of several reasons for that failure, including Washington's fears of disunion, the racism of many other Virginians, the problem of compensating owners, slaves' lack of education, and the unwillingness of Virginia's leaders to seriously consider such a step.[272][273]
 In 1929, a plaque was embedded in the ground at Mount Vernon less than 50 yards (45 m) from the crypt housing the remains of Washington and Martha, marking a plot neglected by both groundsmen and tourist guides where slaves had been buried in unmarked graves. The inscription read, ""In memory of the many faithful colored servants of the Washington family, buried at Mount Vernon from 1760 to 1860. Their unidentified graves surround this spot."" The site remained untended and ignored in the visitor literature until the Mount Vernon Ladies' Association erected a more prominent monument surrounded with plantings and inscribed, ""In memory of the Afro Americans who served as slaves at Mount Vernon this monument marking their burial ground dedicated September 21, 1983."" In 1985, a ground-penetrating radar survey identified sixty-six possible burials. As of late 2017, an archaeological project begun in 2014 has identified, without disturbing the contents, sixty-three burial plots in addition to seven plots known before the project began.[315][316][317]
 
"
Manumission,https://en.wikipedia.org/wiki/Manumission,"
 Manumission, or enfranchisement, is the act of freeing slaves by their owners. Different approaches to manumission were developed, each specific to the time and place of a particular society. Historian Verene Shepherd states that the most widely used term is gratuitous manumission, ""the conferment of freedom on the enslaved by enslavers before the end of the slave system"".[1]
 The motivations for manumission were complex and varied. Firstly, it may present itself as a sentimental and benevolent gesture. One typical scenario was the freeing in the master's will of a devoted servant after long years of service. A trusted bailiff might be manumitted as a gesture of gratitude. For those working as agricultural labourers or in workshops, there was little likelihood of being so noticed. In general, it was more common for older slaves to be given freedom.
 Legislation under the early Roman Empire put limits on the number of slaves that could be freed in wills (lex Fufia Caninia, 2 BC), which suggests that it had been widely used. Freeing slaves could serve the pragmatic interests of the owner. The prospect of manumission worked as an incentive for slaves to be industrious and compliant. Roman slaves were paid a wage (peculium), which they could save up to buy themselves freedom. Manumission contracts, found in some abundance at Delphi (Greece), specify in detail the prerequisites for liberation.
 A History of Ancient Greece explains that in the context of Ancient Greece, affranchisement came in many forms.[2] A master choosing to free his slave would most likely do so only ""at his death, specifying his desire in his will"". In rare cases, slaves who were able to earn enough money in their labour were able to buy their own freedom and were known as choris oikointes. Two 4th-century bankers, Pasion and Phormion, had been slaves before they bought their freedom. A slave could also be sold fictitiously to a sanctuary from where a god could enfranchise him. In very rare circumstances, the city could affranchise a slave. A notable example is that Athens liberated everyone who was present at the Battle of Arginusae (406 BC).
 Even once a slave was freed, he was not generally permitted to become a citizen, but would become a metic. The master then became the metic's prostatès (guarantor or guardian).[2][3][4] The former slave could be bound to some continuing duty to the master[3] and was commonly required to live near the former master (paramone).[5] Ex-slaves were able to own property outright, and their children were free of all constraint.
 Under Roman law, a slave had no personhood and was protected under law mainly as his or her master's property. In Ancient Rome, a slave who had been manumitted was a libertus (feminine liberta) and a citizen.[6][7] Manumissions were subject to a state tax.[8][9]
 The soft felt pileus hat was a symbol of the freed slave and manumission; slaves were not allowed to wear them:[10]
 The cap was an attribute carried by Libertas, the Roman goddess of freedom, who was also recognized by the rod (vindicta or festuca),[10] used ceremonially in the act of manumissio vindicta, Latin for ""freedom by the rod"" (emphasis added):
 A freed slave customarily took the former owner's family name, which was the nomen (see Roman naming conventions) of the master's gens. The former owner became the patron (patronus) and the freed slave became a client (cliens) and retained certain obligations to the former master, who owed certain obligations in return. A freed slave could also acquire multiple patrons.
 A freed slave became a citizen. Not all citizens, however, held the same freedoms and privileges. In particular contrast, women could become citizens, but female Roman citizenship did not allow anywhere near the same protections, independence, or rights as men, either in the public or private spheres.  In reflection of unwritten, yet strictly enforced contemporary social codes, women were also legally prevented from participating in public and civic society. For example: through the illegality of women voting or holding public office.
 The freed slaves' rights were limited or defined by particular statutes. A freed male slave could become a civil servant but not hold higher magistracies (see, for instance, apparitor and scriba), serve as priests of the emperor or hold any of the other highly respected public positions.
 If they were sharp at business, however, there were no social limits to the wealth that freedmen could amass. Their children held full legal rights, but Roman society was stratified. Famous Romans who were the sons of freedmen include the Augustan poet Horace and the 2nd century emperor, Pertinax.
 A notable freedman in Latin literature is Trimalchio, the ostentatiously nouveau riche character in the Satyricon, by Petronius.
 In colonial Peru, the laws around manumission were influenced by the Siete Partidas, a Castilian law code. According to the Siete Partidas, masters who manumitted their slaves should be honored and obeyed by their former slaves for giving such a generous gift.[13] As in other parts of Latin America under the system of coartación, slaves could purchase their freedom by negotiating with their master for a purchase price and this was the most common way for slaves to be freed.[14] Manumission also occurred during baptism, or as part of an owner's last will and testament.
 In baptismal manumission, enslaved children were freed at baptism. Many of these freedoms came with stipulations which could include servitude often until the end of an owner's life.[15] Children freed at baptism were also frequently the children of still-enslaved parents. A child who was freed at baptism but continued to live with enslaved family was far more likely to be re-enslaved.[16] Baptismal manumission could be used as evidence of a person's freed status in a legal case but they did not always have enough information to serve as a carta de libertad.[17]
 Female slave owners were more likely than males to manumit their slaves at baptism.[18] The language used by women slave owners who freed their slaves also differed substantially from that of men, with many women using the phrasing “for the love I have for her” as well as other expressions of intimacy as part of the reasoning for freeing their slaves as written on the baptismal record or carta de libertad.[19] Male slave owners were far less likely to speak in intimate terms about their reasoning for freeing their slaves.[20]
 Many children manumitted at baptism were likely the illegitimate children of their male owners, though this can be difficult to determine from the baptismal record and must be assessed through other evidence.[21] Although slave owners often characterized these baptismal manumissions as a result of their generous beneficence, there are records of payments by parents or godparents to ensure the child's freedom.[22] Mothers were almost never manumitted alongside their children, even when the mothers gave birth to their master's own children. Manumitting a slave's children at baptism could be one way for owners to ensure the loyalty of the children's still-enslaved parents.[23]
 Enslaved people could also be freed as part of a slave owner's last will and testament. Testamentary manumission frequently involved expressions of affection on the part of the slave owner to the enslaved person as part of the rationale behind manumission.[14] Slave owners also frequently cited a desire to die with a clear conscience as part of their reasoning for freeing their slaves.[14] Testamentary manumission could often be disputed by heirs claiming fraud, or that an enslaved person had preyed upon a relative's weak mental or physical condition.[24] Legally testamentary manumissions were usually respected by the courts, who understood enslaved people as part of their owner's property to distribute as they wished.[25] Relatives who claimed fraud had to provide evidence of their claims or they would be dismissed.[24] As in baptismal manumission, conditions of ongoing servitude were sometimes placed upon the enslaved person, by obligating them to care for another relative.[19]
 In Iberoamerican law, a person had discretion over one-fifth of their estate[26] with the rest going to children, spouses, and other relatives.  An enslaved person could be sold in order to cover debts of the estate, but not if they had already paid part of their purchase price towards manumission as this was considered a legally binding agreement.[24] As long as a person had not disinherited his children or spouse, a slave owner could manumit their slaves as they wished.[26]
 Manumission laws varied between the various colonies in the Caribbean. For instance, the island of Barbados had some of the strictest laws, requiring owners to pay £200 for male slaves and £300 for female slaves, and show cause to the authorities. In some other colonies, no fees applied. It was not uncommon for ex-slaves to purchase family members or friends in order to free them. For example, ex-slave Susannah Ostrehan became a successful businesswoman in Barbados and purchased many of her acquaintances.[27]
 For Jamaica, manumission went largely unregulated until the 1770s, when manumitters had to post a bond in order to ensure those that they freed did not become wards of the parish. One quantitative analysis of the Jamaica manumission deeds shows that manumission was comparatively rare on the island around 1770, with only an estimated 165 slaves winning their freedom through this fashion.  While manumission had little demographic impact on the size of the enslaved population, it was important to the growth and development of the free population of colour, in Jamaica, during the second half of the eighteenth century.[28]
 African slaves were freed in the North American colonies as early as the 17th century. Some, such as  Anthony Johnson, went on to become landowners and slaveholders themselves.  Slaves could sometimes arrange manumission by agreeing to ""purchase themselves"" by paying the master an agreed amount. Some masters demanded market rates; others set a lower amount in consideration of service.
 Regulation of manumission began in 1692, when Virginia established that to manumit a slave, a person must pay the cost for them to be transported out of the colony. A 1723 law stated that slaves may not ""be set free upon any pretence whatsoever, except for some meritorious services to be adjudged and allowed by the governor and council"".[29]
 In some cases, a master who was drafted into the army would send a slave instead, with a promise of freedom if he survived the war.[30] The new government of Virginia repealed the laws in 1782, and declared freedom for slaves who had fought for the colonies during the American Revolutionary War of 1775–1783.[citation needed] Another law passed in 1782 permitted masters to free their slaves of their own accord.  Previously, a manumission had required obtaining consent from the state legislature, an arduous process which was rarely successful.[31]
 As the population of free Negroes increased, the Virginia legislature passed laws forbidding them from moving into the state (1778),[32]  and requiring newly freed slaves to leave the Commonwealth within one year unless special permission was granted (1806).[33]
 In the Upper South in the late 18th century, planters had less need for slaves, as they switched from labour-intensive tobacco cultivation to mixed-crop farming. Slave states such as Virginia made it easier for slaveholders to free their slaves. In the two decades after the American Revolutionary War, so many slaveholders accomplished manumissions by deed or in wills that the proportion of free black people to the total number of black people rose from less than 1% to 10% in the Upper South.[34] In Virginia, the proportion of free black people increased from 1% in 1782 to 7% in 1800.[35] Together with several  Northern states abolishing slavery during that period, the proportion of free black people nationally increased to ~14% of the total black population. New York and New Jersey adopted gradual abolition laws that kept the free children of slaves as indentured servants into their twenties.
 After the 1793 invention of the cotton gin, which enabled the development of extensive new areas for cotton cultivation, the number of manumissions decreased because of increased demand for slave labour. In the 19th century, slave revolts such as the Haitian Revolution of 1791–1804, and especially the 1831 rebellion led by Nat Turner, increased slaveholders' fears. Most Southern states passed laws making manumission nearly impossible until the passage of the 1865 Thirteenth Amendment to the United States Constitution, which abolished slavery  ""except as a punishment for crime whereof the party shall have been duly convicted,"" after the American Civil War. In South Carolina, to free a slave required permission of the state legislature; Florida law prohibited manumission altogether.[36]
 Slavery in the Ottoman Empire gradually became less central to the functions of Ottoman society throughout the late 19th and early 20th centuries. Responding to the influence and pressure of European countries in the 19th century, the Ottoman Empire began taking steps to curtail the slave trade, which had been legally valid under Ottoman law since the beginning of the empire. A number of reforms where introduced; such as the firman of 1830, the firman of 1854 and the Kanunname of 1889. 
 Ottoman Empire policy encouraged manumission of male slaves, but not female slaves.[37] The most telling evidence for this is found in the gender ratio; among slaves traded in Islamic empire across the centuries, there were roughly two females to every male.[38]
 Sexual slavery was a central part of the Ottoman slave system throughout the history of the institution, managed in accordance with the Islamic Law of concubinage, and the most resistant to change.
Outside of explicit sexual slavery, most female slaves had domestic occupations, and often, this also included sexual relations with their masters. This was a lawful motive for their purchase, and the most common one. It was similarly a common motivation for their retention.[39][40]
 The Ottoman Empire and 16 other countries signed the 1890 Brussels Conference Act for the suppression of the slave trade. However, clandestine slavery persisted well into the 20th century. Gangs were also organized to facilitate the illegal importation of slaves.[41] Slave raids and the taking of women and children as ""spoils of war"" lessened but did not stop entirely, despite the public denial of their existence, such as the enslavement of girls during the Armenian Genocide. Armenian girls were sold as slaves during the Armenian genocide of 1915.[42][43] Turkey waited until 1933 to ratify the 1926 League of Nations convention on the suppression of slavery. However, illegal sales of girls were reported in the 1930s. Legislation explicitly prohibiting slavery was adopted in 1964.[44]
"
American culture,https://en.wikipedia.org/wiki/American_culture,"


 The culture of the United States encompasses various social behaviors, institutions, and norms, including forms of speech, literature, music, visual arts, performing arts, food, sports, religion, law, technology, as well as other customs, beliefs, and forms of knowledge. American culture has been shaped by the history of the United States, its geography, and various internal and external forces and migrations.[1]
 America's foundations were initially Western-based, and primarily English-influenced, but also with prominent French, German, Greek, Irish, Italian, Jewish, Polish, Scandinavian, and Spanish regional influences. However, non-Western influences, including African and Indigenous cultures, and more recently, Asian cultures, have firmly established themselves in the fabric of American culture as well. Since the United States was established in 1776, its culture has been influenced by successive waves of immigrants, and the resulting ""melting pot"" of cultures has been a distinguishing feature of its society. Americans pioneered or made great strides in musical genres such as heavy metal, rhythm and blues, jazz, gospel, country, hip hop, and rock 'n' roll. The ""big four sports"" are American football, baseball, basketball, and ice hockey. In terms of religion, the majority of Americans are Protestant or Catholic. The irreligious element is growing. American cuisine includes popular tastes such as hot dogs, milkshakes, and barbecue, as well as many other class and regional preferences. The most commonly used language is English, though the United States does not have an official language.[2] Distinct cultural regions include New England, Mid-Atlantic, the South, Midwest, Southwest, Mountain West, and Pacific Northwest.[3]
 Politically, the country takes its values from the American Revolution and American Enlightenment, with an emphasis on liberty, individualism, and limited government, as well as the Bill of Rights and Reconstruction Amendments. Under the First Amendment, the United States has the strongest protections of free speech of any country.[4][5][6][7] American popular opinion is also the most supportive of free expression and the right to use the Internet.[8][9] The large majority of the United States has a legal system that is based upon English common law.[10] According to the Inglehart–Welzel cultural map, it leans greatly towards ""self-expression values"", while also uniquely blending aspects of ""secular-rational"" (with a strong emphasis on human rights, the individual, and anti-authoritarianism) and ""traditional"" (with high fertility rates, religiosity, and patriotism) values together.[11][12][13] Its culture can vary by factors such as region, race and ethnicity, age, religion, socio-economic status, or population density, among others. Different aspects of American culture can be thought of as low culture or high culture, or belonging to any of a variety of subcultures. The United States exerts major cultural influence on a global scale and is considered a cultural superpower.[14][15]
 The European roots of the United States originate with the English and Spanish settlers of colonial North America during British and Spanish rule. The varieties of English people, as opposed to the other peoples on the British Isles, were the overwhelming majority ethnic group in the 17th century (the population of the colonies in 1700 was 250,000) and were 47.9% of percent of the total population of 3.9 million. They constituted 60% of the whites at the first census in 1790 (%: 3.5 Welsh, 8.5 Scotch Irish, 4.3 Scots, 4.7 Irish, 7.2 German, 2.7 Dutch, 1.7 French, and 2 Swedish).[16] [citation needed]The English ethnic group contributed to the major cultural and social mindset and attitudes that evolved into the American character. Of the total population in each colony, they numbered from 30% in Pennsylvania to 85% in Massachusetts.[17] Large non-English immigrant populations from the 1720s to 1775, such as the Germans (100,000 or more), Scotch Irish (250,000), added enriched and modified the English cultural substrate.[18] The religious outlook was some versions of Protestantism (1.6% of the population comprised English, German, and Irish Catholics).[citation needed]
 Jeffersonian democracy was a foundational American cultural innovation, which is still a core part of the country's identity.[19] Thomas Jefferson's Notes on the State of Virginia was perhaps the first influential domestic cultural critique by an American and was written in reaction to the views of some influential Europeans that America's native flora and fauna (including humans) were degenerate.[19]
 Non-indigenous cultural influences have been brought by historical immigration, especially from Germany in much of the country,[20] Ireland and Italy in the Northeast, and Japan in Hawaii. Latin American culture is especially pronounced in former Spanish areas but has also been introduced by immigration, as have Asian American cultures (especially in the Northeast and West Coast regions). Caribbean culture has been increasingly introduced by immigration and is pronounced in many urban areas. Since the abolition of slavery, the Caribbean has been the source of the earliest and largest Black immigrant group, a significant source of growth of the Black population in the U.S. and has made major cultural impacts in education, music, sports and entertainment.[21]
 Indigenous cultures remains strong in both reservation and urban communities, including traditional government and communal organization of property now legally managed by Indian reservations (large reservations are mostly in the West, especially Oklahoma, Arizona and South Dakota). The fate of indigenous cultures after contact with Europeans is quite varied. For example, Taíno culture in U.S. Caribbean territories is undergoing cultural revitalization and like many Native American languages, the Taíno language is no longer spoken. By contrast, the Hawaiian language and culture of the Native Hawaiians has survived in Hawaii alongside that of immigrants from the mainland U.S. (starting before the 1898 annexation) and to some degree Asian immigrants. Indigenous Hawaiian influences on mainstream American culture include surfing and Hawaiian shirts. Most languages native to what is now U.S. territory are endangered,[22] and the economic and mainstream cultural dominance of the English language threatens the surviving ones in most places. Some of the most common native languages include Samoan, Hawaiian, Navajo, Cherokee, Sioux, and a spectrum of Inuit languages. (See Indigenous languages of the Americas for a fuller listing, plus Chamorro, and Carolinian in the Pacific territories.)[23][better source needed] Ethnic Samoans are a majority in American Samoa; Chamorro are still the largest ethnic group in Guam (though a minority), and along with Refaluwasch are smaller minorities in the Northern Mariana Islands.[citation needed]
 American culture includes both conservative and liberal elements, scientific and religious competitiveness, political structures, risk taking and free expression, materialist and moral elements. Despite certain consistent ideological principles (e.g. individualism, egalitarianism, and faith in freedom and republicanism), American culture has a variety of expressions due to its geographical scale and demographics.[24]
 As a melting pot of cultures and ethnicities, the U.S. has been shaped by the world's largest immigrant population. The country is home to a wide variety of ethnic groups, traditions, and values,[25][26] and exerts major cultural influence on a global scale, with the phenomenon being termed Americanization.[27][28][14][15]
 Semi-distinct cultural regions of the United States include New England, the Mid-Atlantic, the South, the Midwest, the Southwest, and the West—an area that can be further subdivided into the Pacific States and the Mountain States.[citation needed]
 According to cultural geographer Colin Woodward there are as many as eleven cultural areas of the United States, which spring from their settlement history. In the east, from north to south: there are Puritan areas (""Yankeedom"") of New England which spread across the northern Great Lakes to the northern reaches of the Mississippi and Missouri Rivers; the New Netherlands area in the densely populated New York metropolitan area; the Midland area which spread from Pennsylvania to the lower Great Lakes and the trans-Mississippi upper midwest; Greater Appalachia which angles from West Virginia through the lower midwest and upper-south to trans-Mississippi Arkansas, and southern Oklahoma; the Deep South from the Carolinas to Florida and west to Texas. In the west, there is the southwestern ""El Norte"" areas originally colonized by Spain, the ""Left Coast"" colonized quickly on the 19th century by a mix of Yankees and upper Appalachians, and the large but sparsely populated interior West.[29][30]
 The west coast of the continental United States, consisting of California, Oregon, and Washington state, is also sometimes referred to as the Left Coast, indicating its left-leaning political orientation and tendency towards social liberalism.[citation needed]
 The South is sometimes informally called the ""Bible Belt"" due to socially conservative evangelical Protestantism, which is a significant part of the region's culture. Christian church attendance across all denominations is generally higher there than the national average. This region is usually contrasted with the mainline Protestantism and Catholicism of the Northeast, the religiously diverse Midwest and Great Lakes, the Mormon Corridor in Utah and southern Idaho, and the relatively secular West. The percentage of non-religious people is the highest in the northeastern and New England state of Vermont at 34%, compared to 6% in the Bible Belt state of Alabama.[31]
 Strong cultural differences have a long history in the U.S., with the southern slave society in the antebellum period serving as a prime example. Social and economic tensions between the Northern and Southern states were so severe that they eventually caused the South to declare itself an independent nation, the Confederate States of America; thus initiating the American Civil War.[32]
 Although the United States has no official language at the federal level, 28 states have passed legislation making English the official language, and it is considered to be the de facto national language. According to the 2000 U.S. Census, more than 97% of Americans can speak English well, and for 81%, it is the only language spoken at home. The national dialect is known as American English, which itself consists of numerous regional dialects, but has some shared unifying features that distinguish it from British English and other varieties of English. There are four large dialect regions in the United States—the North, the Midland, the South, and the West—and several dialects more focused within metropolitan areas such as those of New York City, Philadelphia, and Boston. A standard dialect called ""General American"" (analogous in some respects to the received pronunciation elsewhere in the English-speaking world), lacking the distinctive noticeable features of any particular region, is believed by some to exist as well; it is sometimes regionally associated with the Midwest. American Sign Language, used mainly by the deaf, is also native to the United States.[citation needed]
 More than 300 languages nationwide, and up to 800 languages in New York City, besides English, have native speakers in the United States—some are spoken by indigenous peoples (about 150 living languages) and others imported by immigrants. English is not the first language of most immigrants in the US, though many do arrive knowing how to speak it, especially from countries where English is broadly used.[33] This not only includes immigrants from countries such as Canada, Jamaica, and the UK, where English is the primary language, but also countries where English is an official language, such as India, Nigeria, and the Philippines.[33]
 According to the 2000 census, there were nearly 30 million native speakers of Spanish in the United States. Spanish has official status in the Commonwealth of Puerto Rico, where it is the primary language spoken, and the state of New Mexico; numerous Spanish enclaves exist around the country as well.[34] Bilingual speakers may use both English and Spanish reasonably well and may code-switch according to their dialog partner or context, a phenomenon known as Spanglish.[citation needed]
 Indigenous languages of the United States include the Native-American languages (including Navajo, Yupik, Dakota, and Apache), which are spoken on the country's numerous Indian reservations and at cultural events such as pow wows; Hawaiian, which has official status in the state of Hawaii; Chamorro, which has official status in the commonwealths of Guam and the Northern Mariana Islands; Carolinian, which has official status in the commonwealth of the Northern Mariana Islands; and Samoan, which has official status in the commonwealth of American Samoa.
 The cuisine of the United States is extremely diverse, owing to the vastness of the country, the relatively large population (.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄3 of a billion people) and the significant number of native and immigrant influences. Mainstream American culinary arts are similar to those in other Western countries. Wheat and corn are the primary cereal grains.[citation needed] Traditional American cuisine uses ingredients such as turkey, potatoes, sweet potatoes, corn (maize), squash, and maple syrup, as well as indigenous foods employed by American Indians and early European settlers, African slaves, and their descendants.[citation needed]
 
Iconic American dishes such as apple pie, donuts, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants and domestic innovations.[36][37] French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are consumed.[38] The types of food served at home vary greatly and depend upon the region of the country and the family's own cultural heritage. Recent immigrants tend to eat food similar to that of their country of origin, and Americanized versions of these cultural foods, such as Chinese American cuisine or Italian American cuisine often eventually appear. Vietnamese cuisine, Korean cuisine, and Thai cuisine in authentic forms are often readily available in large cities. German cuisine has a profound impact on American cuisine, especially Midwestern cuisine; potatoes, noodles, roasts, stews, cakes, and other pastries are the most iconic ingredients in both cuisines.[39] Dishes such as the hamburger, pot roast, baked ham, and hot dogs are examples of American dishes derived from German cuisine.[40][41]
 Different regions of the United States have their own cuisine and styles of cooking. The states of Louisiana and Mississippi, for example, are known for their Cajun and Creole cooking. Cajun and Creole cooking are influenced by French, Acadian, and Haitian cooking, although the dishes themselves are original and unique. Examples include Crawfish Étouffée, Red beans and rice, seafood or chicken gumbo, jambalaya, and boudin. Italian, German, Hungarian, and Chinese influences, traditional Native American, Caribbean, Mexican, and Greek dishes have also diffused into the general American repertoire. It is not uncommon for a middle-class family from Middle America to eat, for example, restaurant pizza, home-made pizza, enchiladas con carne, chicken paprikash, beef stroganoff, and bratwurst with sauerkraut for dinner throughout a single week.[citation needed]
 Soul food, mostly the same as food eaten by white southerners, developed by southern African slaves, and their free descendants, is popular around the South and among many African Americans elsewhere. Syncretic cuisines such as Louisiana Creole, Cajun, Pennsylvania Dutch, and Tex-Mex are regionally important.
 Americans generally prefer coffee over tea, and more than half the adult population drinks at least one cup of coffee per day.[42] Marketing by U.S. industries is largely responsible for making orange juice and milk (now often fat-reduced) ubiquitous breakfast beverages.[43] During the 1980s and 1990s, the caloric intake of Americans rose by 24%;[38] and frequent dining at fast food outlets is associated with what health officials call the American ""obesity epidemic."" Highly sweetened soft drinks are popular; sugared beverages account for 9% of the average American's daily caloric intake.[44]
 The American fast food industry, the world's first and largest, is also often viewed as being a symbol of U.S. marketing dominance. Companies such as McDonald's,[45] Burger King, Pizza Hut, Kentucky Fried Chicken, and Domino's Pizza among others, have numerous outlets around the world,[46] and pioneered the drive-through format in the 1940s.[47]
 In the 1800s, colleges were encouraged to focus on intramural sports, particularly track and field, and, in the late 1800s, American football. Physical education was incorporated into primary school curriculums in the 20th century.[48]
 Baseball is the oldest of the major American team sports. Professional baseball dates from 1869 and had no close rivals in popularity until the 1960s. Though baseball is no longer the most popular sport,[49] it is still referred to as ""the national pastime."" Also unlike the professional levels of the other popular spectator sports in the U.S., Major League Baseball teams play almost every day. The Major League Baseball regular season consists of each of the 30 teams playing 162 games from late March to early October. The season ends with the postseason and World Series in October. Unlike most other major sports in the country, professional baseball draws most of its players from a ""minor league"" system, rather than from university athletics.
 American football, known in the United States as simply ""football"", now attracts more television viewers than any other sport and is considered to be the most popular sport in the United States.[50] The 32-team National Football League (NFL) is the most popular professional American football league.
The National Football League differs from the other three major pro sports leagues in that each of its 32 teams plays one game a week over 18 weeks, for a total of 17 games with one bye week for each team. The NFL season lasts from September to December, ending with the playoffs and Super Bowl in January and February.
Its championship game, the Super Bowl, has often been the highest rated television show, and it has an audience of over 100 million viewers annually.[citation needed]
 College football also attracts audiences of millions. Some communities, particularly in rural areas, place great emphasis on their local high school football team. American football games usually include cheerleaders and marching bands, which aim to raise school spirit and entertain the crowd at halftime.
 Basketball is another major sport, represented professionally by the National Basketball Association. It was invented in Springfield, Massachusetts, in 1891, by Canadian-born physical education teacher James Naismith. College basketball is also popular, due in large part to the NCAA men's Division I basketball tournament in March, colloquially known as ""March Madness"".
 Ice hockey is the fourth-leading professional team sport. Always a mainstay of Great Lakes and New England-area culture, the sport gained tenuous footholds in regions like the American South since the early 1990s, as the National Hockey League pursued a policy of expansion.[51]
 Lacrosse is a team sport of American and Canadian Native American origin and is most popular on the East Coast. NLL and MLL are the national box and outdoor lacrosse leagues. Many of the top Division I college lacrosse teams draw upwards of 7–10,000 for a game, especially in the Mid-Atlantic and New England areas.
 Soccer is very popular as a participation sport, particularly among youth, and the US national teams are competitive internationally. A twenty-six-team (with four more confirmed to be added within the next few years) professional league, Major League Soccer, plays from March to October, but its television audience and overall popularity lag behind other American professional sports.[53]
 Other popular sports are tennis, softball, rodeo, swimming, water polo, fencing, shooting sports, hunting, volleyball, skiing, snowboarding, skateboarding, ultimate, disc golf, cycling, MMA, roller derby, wrestling, weightlifting, and rugby.
 Relative to other parts of the world, the United States is unusually competitive in women's sports, a fact usually attributed to the Title IX anti-discrimination law, which requires most American colleges to give equal funding to men's and women's sports.[54] Despite that, however, women's sports are not nearly as popular among spectators as men's sports.
 The United States enjoys a great deal of success both in the Summer Olympics and Winter Olympics, constantly finishing among the top medal winners.
 Homecoming is an annual tradition in the United States. People, towns, high schools and colleges come together, usually in late September or early October, to welcome back former residents and alumni. It is built around a central event, such as a banquet, a parade, and most often, a game of American football, or, on occasion, basketball, wrestling or ice hockey. When celebrated by schools, the activities vary. However, they usually consist of a football game, played on the school's home football field, activities for students and alumni, a parade featuring the school's marching band and sports teams, and the coronation of a Homecoming Queen.
 American high schools commonly field football, basketball, baseball, softball, volleyball, soccer, golf, swimming, track and field, and cross-country teams as well.
 The United States observes holidays derived from events in American history, Christian traditions, and national patriarchs.
 Thanksgiving is the principal traditionally-American holiday, evolving from the English Pilgrim's custom of giving thanks for one's welfare. Thanksgiving is generally celebrated as a family reunion with a large afternoon feast. Independence Day (or the Fourth of July) celebrates the anniversary of the country's Declaration of Independence from Great Britain, and is generally observed by parades throughout the day and the shooting of fireworks at night.
 Christmas Day, celebrating the birth of Jesus Christ, is widely celebrated and a federal holiday, though a fair amount of its current cultural importance is due to secular reasons. European colonization has led to some other Christian holidays such as Easter and St. Patrick's Day to be observed, though with varying degrees of religious fidelity.
 Halloween, also known as All Hallows' Eve, is a holiday of diverse origins. It has become a holiday that is celebrated by children and teens who traditionally dress up in costumes and go door to door trick-or-treating for candy. It also brings about an emphasis on eerie and frightening urban legends and movies. Mardi Gras, which evolved from the Catholic tradition of Carnival, is observed in the state of Louisiana.
 The United States has few laws governing given names. Traditionally, the right to name your child or yourself as you choose has been upheld by court rulings and is rooted in the Due Process Clause of the Fourteenth Amendment of the U.S. Constitution and the Free Speech Clause of the First Amendment. This freedom, along with the cultural diversity within the United States has given rise to a wide variety of names and naming trends.
 Creativity has also long been a part of American naming traditions and names have been used to express personality, cultural identity, and values.[61][62] Naming trends vary by race, geographic area, and socioeconomic status. African Americans, for instance, have developed a very distinct naming culture.[62] Both religious names and those inspired by popular culture are common.[63]
 A few restrictions do exist, varying by state, mostly for the sake of practicality (e.g., limiting the number of characters due to limitations in record-keeping software).
 Fashion in the United States is eclectic and predominantly informal. While the diverse cultural roots of Americans are reflected in their clothing, particularly those of recent immigrants, cowboy hats and boots, and leather motorcycle jackets are emblematic of specifically-American styles.[citation needed]
 Blue jeans were popularized as work clothes in the 1850s by merchant Levi Strauss, a German-Jewish immigrant in San Francisco, and adopted by many American teenagers a century later. They are worn in every state by people of all ages and social classes. Along with mass-marketed informal wear in general, blue jeans are arguably one of US culture's primary contributions to global fashion.[64]
 Though the informal dress is more common, certain professionals, such as bankers and lawyers, traditionally dress formally for work, and some occasions, such as weddings, funerals, dances, and some parties, typically call for formal wear.[citation needed] The annual Met Gala in Manhattan is known worldwide as ""fashion's biggest night"".[65][66]
 Some cities and regions have specialties in certain areas. For example, Miami for swimwear, Boston and the general New England area for formal menswear, Los Angeles for casual attire and womenswear, and cities like Seattle and Portland for eco-conscious fashion. Chicago is known for its sportswear, and is the premier fashion destination in the middle American market. Dallas, Houston, Austin, Nashville, and Atlanta are big markets for the fast fashion and cosmetics industries, alongside having their own distinct fashion sense that mainly incorporates cowboy boots and workwear, greater usage of makeup, lighter colors and pastels, ""college prep"" style, sandals, bigger hairstyles, and thinner, airier fabrics due to the heat and humidity of the region.
 Family arrangements in the United States reflect the nature of contemporary American society. The classic nuclear family is a man and a woman, united in marriage, with one or more biological children.[67] Today, a person may grow up in a single-parent family, go on to marry and live in a childfree couple arrangement, then get divorced, live as a single for a couple of years, remarry, have children and live in a nuclear family arrangement.[26][68]
 Exceptions to the longstanding American custom of leaving home when one reaches legal adulthood at age eighteen can occur especially among Italian and Hispanic Americans, and in expensive urban real estate markets such as New York City,[69] California,[70] and Honolulu,[71] where monthly rents can be prohibitively high.
 Marriage laws are established by individual states. The typical wedding involves a couple proclaiming their commitment to one another in front of their close relatives and friends, often presided over by a religious figure such as a minister, priest, or rabbi, depending upon the faith of the couple. In traditional Christian ceremonies, the bride's father will ""give away"" (handoff) the bride to the groom. Secular weddings are also common, often presided over by a judge, Justice of the Peace, or other municipal officials. Same-sex marriage is legal in all states since June 26, 2015.[citation needed]
 Divorce is the province of state governments, so divorce law varies from state to state. Prior to the 1970s, divorcing spouses had to allege that the other spouse was guilty of a crime or sin like abandonment or adultery; when spouses simply could not get along, lawyers were forced to manufacture ""uncontested"" divorces. The no-fault divorce revolution began in 1969 in California; New York and South Dakota were the last states to begin allowing no-fault divorce. No-fault divorce on the grounds of ""irreconcilable differences"" is now available in all states. However, many states have recently required separation periods prior to a formal divorce decree.
 State law provides for child support where children are involved, and sometimes for alimony. ""Married adults now divorce two-and-a-half times as often as adults did 20 years ago and four times as often as they did 50 years ago... between 40% and 60% of new marriages will eventually end in divorce. The probability within... the first five years is 20%, and the probability of its ending within the first 10 years is 33%... Perhaps 25% of children (ages 16 and under) live with a stepparent.""[72] The median length for a marriage in the U.S. today is 11 years with 90% of all divorces being settled out of court.[citation needed]
 Historically, Americans mainly lived in a rural environment, with a few important cities of moderate size. The Industrial Revolution brought a period of urbanization accelerated by the GI Bill that incentivized soldiers returning from WWII to purchase a house in the suburbs.
 American cities with housing prices near the national median have also been losing the middle income neighborhoods, those with median income between 80% and 120% of the metropolitan area's median household income. Here, the more affluent members of the middle-class, who are also often referred to as being professional or upper-middle-class, have left in search of larger homes in more exclusive suburbs. This trend is largely attributed to the middle-class squeeze, which has caused a starker distinction between the statistical middle class and the more privileged members of the middle class.[73] In more expensive areas such as California, however, another trend has been taking place where an influx of more affluent middle-class households has displaced those in the actual middle of society and converted former American middle-middle-class neighborhoods into upper-middle-class neighborhoods.[74]
 Alexis de Tocqueville first noted, in 1835, the American attitude towards helping others in need. A 2011 Charities Aid Foundation study found that Americans were the first most willing to help a stranger and donate time and money in the world at 60%. Many low-level crimes are punished by assigning hours of ""community service"", a requirement that the offender perform volunteer work;[75] some high schools also require community service to graduate. Since US citizens are required to attend jury duty, they can be jurors in legal proceedings.
 American attitudes towards drugs and alcoholic beverages have evolved considerably throughout the country's history. In the 19th century, alcohol was readily available and consumed, and no laws restricted the use of other drugs. Attitudes on drug addiction started to change, resulting in the Harrison Act, which eventually became proscriptive.
 A movement to ban alcoholic beverages called the Temperance movement, emerged in the late 19th century. Several American Protestant religious groups and women's groups, such as the Women's Christian Temperance Union, supported the movement. In 1919, Prohibitionists succeeded in amending the Constitution to prohibit the sale of alcohol. Although the Prohibition period did result in a 50% decrease in alcohol consumption,[76] banning alcohol outright proved to be unworkable, as the previously legitimate distillery industry was replaced by criminal gangs that trafficked in alcohol. Prohibition was repealed in 1933. States and localities retained the right to remain ""dry"", and to this day, a handful still do.
 During the Vietnam War era, attitudes swung well away from prohibition.[clarification needed] Commentators noted that an 18-year-old could be drafted to war but could not buy a beer.[citation needed]
 Since 1980, the trend has been toward greater restrictions on alcohol and drug use. The focus this time, however, has been to criminalize behaviors associated with alcohol, rather than attempt to prohibit consumption outright. New York was the first state to enact tough drunk-driving laws in 1980; since then all other states have followed suit. All states have also banned the purchase of alcoholic beverages by individuals under 21.
 A ""Just Say No to Drugs"" movement replaced the more liberal ethos of the 1960s. This led to stricter drug laws and greater police latitude in drug cases. Drugs are, however, widely available, and 16% of Americans 12 and older used an illicit drug in 2012.[77]
 Since the 1990s, marijuana use has become increasingly tolerated in America, and a number of states allow the use of marijuana for medical purposes. In most states marijuana is still illegal without a medical prescription. Since the 2012 general election, voters in the District of Columbia and the states of Alaska, California, Colorado, Maine, Massachusetts, Nevada, Oregon, and Washington approved the legalization of marijuana for recreational use. Marijuana is illegal under federal law.
 It is customary for Americans to hold a wake in a funeral home within a couple of days of the death of a loved one. The body of the deceased may be embalmed and dressed in fine clothing if there will be an open-casket viewing. Traditional Jewish and Muslim practices include a ritual bath and no embalming. Friends, relatives and acquaintances gather, often from distant parts of the country, to ""pay their last respects"" to the deceased. Flowers are brought to the coffin and sometimes eulogies, elegies, personal anecdotes or group prayers are recited. Otherwise, the attendees sit, stand or kneel in quiet contemplation or prayer. Kissing the corpse on the forehead is typical among Italian Americans[78] and others. Condolences are also offered to the widow or widower and other close relatives.
 A funeral may be held immediately afterward or the next day. The funeral ceremony varies according to religion and culture. American Catholics typically hold a funeral mass in a church, which sometimes takes the form of a Requiem mass. Jewish Americans may hold a service in a synagogue or temple. Pallbearers carry the coffin of the deceased to the hearse, which then proceeds in a procession to the place of final repose, usually a cemetery. The unique Jazz funeral of New Orleans features joyous and raucous music and dancing during the procession.
 Mount Auburn Cemetery (founded in 1831) is known as ""America's first garden cemetery.""[79] American cemeteries created since are distinctive for their park-like setting. Rows of graves are covered by lawns and are interspersed with trees and flowers. Headstones, mausoleums, statuary or simple plaques typically mark off the individual graves. Cremation is another common practice in the United States, though it is frowned upon by various religions. The ashes of the deceased are usually placed in an urn, which may be kept in a private house, or they are interred. Sometimes the ashes are released into the atmosphere. The ""sprinkling"" or ""scattering"" of the ashes may be part of an informal ceremony, often taking place at a scenic natural feature (a cliff, lake or mountain) that was favored by the deceased.
 Architecture in the United States is regionally diverse and has been shaped by many external forces. U.S. architecture can therefore be said to be eclectic.[80] Traditionally American architecture has influences from English architecture[81] to Greco Roman architecture.[82] The overriding theme of city American Architecture is modernity, as manifest in the skyscrapers of the 20th century, with domestic and residential architecture greatly varying according to local tastes and climate, rural American and suburban architecture tends to be more traditional.
 In the late-18th and early-19th centuries, American artists primarily painted landscapes and portraits in a realistic style or that which looked to Europe for answers on technique: for example, John Singleton Copley was born in Boston, but most of his portraiture for which he is famous follow the trends of British painters like Thomas Gainsborough and the transitional period between Rococo and Neoclassicism. The later 18th century was a time when the United States was just an infant as a nation and as far away from the phenomenon where artists would receive training as craftsmen by apprenticeship and later seeking a fortune as a professional, ideally getting a patron: Many artists benefited from the patronage of Grand Tourists eager to procure mementos of their travels. There were no temples of Rome or grand nobility to be found in the Thirteen Colonies. Later developments of the 19th century brought America one of its earliest native homegrown movements, like the Hudson River School and portrait artists with a uniquely American flavor like Winslow Homer.
 A parallel development taking shape in rural America was the American craft movement, which began as a reaction to the Industrial Revolution. As the nation grew wealthier, it had patrons able to buy the works of European painters and attract foreign talent willing to teach methods and techniques from Europe to willing students as well as artists themselves; photography became a very popular medium for both journalism and in time as a medium in its own right with America having plenty of open spaces of natural beauty and growing cities in the East teeming with new arrivals and new buildings. Museums in New York, Boston, Philadelphia, and Washington, D.C. began to have a booming business in acquisitions, competing for works as diverse as the then more recent work of the Impressionists to pieces from ancient Egypt, all of which captured the public imaginations and further influenced fashion and architecture. Developments in modern art in Europe came to America from exhibitions in New York City such as the Armory Show in 1913. After World War II, New York emerged as a center of the art world. Painting in the United States today covers a vast range of styles. American painting includes works by Jackson Pollock, John Singer Sargent, Georgia O'Keeffe, and Norman Rockwell, among many others.
 Theater of the United States is based in the Western tradition. The United States originated stand-up comedy and modern improvisational theatre, which involves taking suggestions from the audience.
 The minstrel show, though now widely recognized as racist and offensive, is also recognized as the first uniquely American theatrical art form. Minstrel shows were developed in the 19th century and they were typically performed by white actors wearing blackface makeup for the purpose of imitating and caricaturing the speech and music of African Americans. Stephen Foster was a famous composer for minstrel shows. Many of his songs such as ""Camptown Races"", ""Oh Susanna"", and ""My Old Kentucky Home"" became popular American folk songs. Tap dancing and stand-up comedy have origins in minstrel shows.[84]
 Banjos, originally hand-made by slaves for entertainment on plantations, began to be mass-produced in the United States in the 1840s as a result of their extensive use on the minstrel stage.[85]
 American theater did not take on a unique dramatic identity until the emergence of Eugene O'Neill in the early 20th century, now considered by many to be the father of American drama.[citation needed] O'Neill is a four-time winner of the Pulitzer Prize for drama and the only American playwright to win the Nobel Prize in Literature. After O'Neill, American drama came of age and flourished with the likes of Arthur Miller, Tennessee Williams, Lillian Hellman, William Inge, and Clifford Odets during the first half of the 20th century. After this fertile period, American theater broke new ground, artistically, with the absurdist forms of Edward Albee in the 1960s.
 Social commentary has also been a preoccupation of American theater, often addressing issues not discussed in the mainstream. Writers such as Lorraine Hansbury, August Wilson, David Mamet and Tony Kushner have all won Pulitzer Prizes for their polemical plays on American society.[86]
 The United States is also the home and largest exporter of modern musical theater, producing such musical talents as Rodgers and Hammerstein, Lerner and Loewe, Cole Porter, Irving Berlin, Leonard Bernstein, George and Ira Gershwin, Kander and Ebb, and Stephen Sondheim. Broadway is one of the largest theater communities in the world and is the epicenter of American commercial theater.
 American music styles and influences (such as country, jazz, blues, rock, pop, techno, soul, and hip hop) and music based on them can be heard all over the world. Music in the U.S. is very diverse, and the country has the world's largest music market with a total retail value of $4.9 billion in 2014.[87]
 The rhythmic and lyrical styles of African-American music have significantly influenced American music at large, distinguishing it from European and African traditions. The Smithsonian Institution states, ""African-American influences are so fundamental to American music that there would be no American music without them.""[88] Country music developed in the 1920s, and rhythm and blues in the 1940s. Elements from folk idioms such as the blues and what is known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century.[89] Known for singing in a wide variety of genres, Aretha Franklin is considered one of the all-time greatest American singers.[90]
 Chuck Berry, Elvis Presley, and Little Richard were among the pioneers of rock and roll in the mid-1950s. Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales.[91][92][93] In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters.[94] 
 American popular music, as part of the wider U.S. pop culture, has a worldwide influence and following.[95] Mid-20th-century American pop stars such as Bing Crosby, Frank Sinatra,[96] and Elvis Presley became global celebrities,[89] as have artists of the late 20th century such as Michael Jackson, Prince, Madonna, and Whitney Houston.[97][98] American professional opera singers have reached the highest level of success in that form, including Renée Fleming, Leontyne Price, Beverly Sills, Nelson Eddy, and many others.
 As of 2022[update], Taylor Swift, Miley Cyrus, Ariana Grande, Eminem, Lady Gaga, Katy Perry, and many others contemporary artists dominate global streaming rankings.[99]
 The annual Coachella music festival in California is one of the largest, most famous, and most profitable music festivals in the United States and the world.[100][101]
 The United States movie industry has a worldwide influence and following. Hollywood, a northern district of Los Angeles, California, is the leader in motion picture production and the most recognizable movie industry in the world.[102][103][104] The major film studios of the United States are the primary source of the most commercially successful and most ticket selling movies in the world.[105][106]
 The dominant style of American cinema is classical Hollywood cinema, which developed from 1913 to 1969 and is still typical of most films made there to this day. While Frenchmen Auguste and Louis Lumière are generally credited with the birth of modern cinema,[107] American cinema soon came to be a dominant force in the emerging industry. The world's first sync-sound musical film, The Jazz Singer, was released in 1927,[108] and was at the forefront of sound-film development in the following decades. Orson Welles's Citizen Kane (1941) is frequently cited in critics' polls as the greatest film of all time.[109]
 Television constitutes a significant part of the traditional media of the United States. Household ownership of television sets in the country is 96.7%,[110] and the majority of households have more than one set. The peak ownership percentage of households with at least one television set occurred during the 1996–97 season, with 98.4% ownership.[111] As a whole, the television networks of the United States is the largest and most syndicated in the world.[112]
 As of August 2013, approximately 114,200,000 American households own at least one television set.[113]
 In 2014, due to a recent surge in the number and popularity of critically acclaimed television series, many critics have said that American television is currently enjoying a golden age.[114][115]
 Early American philosophy was heavily shaped by the European Age of Enlightenment, which promoted ideals such as reason and individual liberty.[116] Enlightenment ideals influenced the American Revolution and the Constitution of the United States. Major figures in the American Enlightenment included Thomas Jefferson, Benjamin Franklin, George Mason and Thomas Paine.
 Pragmatism and transcendentalism are uniquely American philosophical traditions founded in the 19th century by William James and Ralph Waldo Emerson respectively. Objectivism is a philosophical system founded by Ayn Rand which influenced libertarianism. John Rawls presented the theory of ""justice as fairness"" in A Theory of Justice (1971).
 Willard Van Orman Quine, Saul Kripke, and David Lewis helped advance logic and analytic philosophy in the 20th century. Thomas Kuhn revolutionized the philosophy of science with his book The Structure of Scientific Revolutions (1962), one of the most cited academic works of all time, and he coined the term paradigm shift.
 Artificial intelligence and the philosophy of mind have been heavily influenced by American philosophers such as Daniel Dennett,[117] Noam Chomsky,[118] Hilary Putnam,[119] Jerry Fodor, and John Searle, who contributed to cognitivism, the hard problem of consciousness, and the mind-body problem. The Libet experiment created by American neuroscientist Benjamin Libet raised philosophical debate regarding the neuroscience of free will. The Chinese room thought experiment presented by John Searle questions the nature of intelligence in machines, and it has been influential in cognitive science and the philosophy of artificial intelligence.
 LGBT rights in the United States are comparatively advanced by world standards.[120][121][122]
 There are about 18,000 U.S. police agencies from local to federal level in the United States.[123] Law in the United States is mainly enforced by local police departments and sheriff's offices. The state police provides broader services, and federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, such as protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws.[124] State courts conduct most civil and criminal trials,[125] and federal courts handle designated crimes and appeals from the state criminal courts.[126]
 As of 2023[update], the United States has the sixth-highest documented incarceration rate and second-largest prison population in the world.[127] In 2019, the total prison population for those sentenced to more than a year was 1,430,800, corresponding to a ratio of 419 per 100,000 residents and the lowest since 1995.[128] Various states have attempted to reduce their prison populations via government policies and grassroots initiatives.[129]
 U.S. police are comparatively violent, with deaths in custody and fatal shootings being higher than in other developed countries.[130][131]
 From the time of its inception, the military played a decisive role in the history of the United States. A sense of national unity and identity was forged out of the victorious First Barbary War, Second Barbary War, and the War of 1812. Even so, the Founders were suspicious of a permanent military force and not until the outbreak of World War II did a large standing army become officially established.[132] The National Security Act of 1947, adopted following World War II and during the onset of the Cold War, created the modern U.S. military framework;[133] the Act merged previously Cabinet-level Department of War and the Department of the Navy into the National Military Establishment (renamed the Department of Defense in 1949), headed by the Secretary of Defense; and created the Department of the Air Force and National Security Council.[134]
 Military service in the United States is voluntary, although conscription may occur in wartime through the Selective Service System.[135] The United States has the third-largest combined armed forces in the world, behind the Chinese People's Liberation Army and Indian Armed Forces.[136] The U.S. military operates about 800 bases and facilities abroad,[137] maintainaining deployments greater than 100 active duty personnel in 25 foreign countries,[138] and possesses significant capabilities in both defense and power projection.[139][140]
 In sharp contrast to most other nations, firearms laws in the United States are permissive, and private gun ownership is common; almost half of American households contain at least one firearm.[142] The Supreme Court has ruled that the Second Amendment to the United States Constitution protects an individual right to possess modern firearms, subject to reasonable regulation,[143] a view shared by the majority of Americans.
 There are more privately owned firearms in the United States than in any other country, both per capita and in total.[144] Civilians in the United States possess about 42% of the global inventory of privately owned firearms.[145] Rates of gun ownership vary significantly by region and by state; gun ownership is most common in Alaska, the Mountain States, and the South, and least prevalent in Hawaii, the island territories, California, and New England. Across the board, gun ownership tends to be more common in rural than in urban areas.[146]
 Hunting, plinking, and target shooting are popular pastimes, although ownership of firearms for purely utilitarian purposes such as personal protection is common as well. ""Personal protection"" was the most common reason given for gun ownership in a 2013 Gallup poll of gun owners, at 60%.[147] Ownership of handguns, while not uncommon, is less common than ownership of long guns. Gun ownership is much more prevalent among men than among women, with men being approximately four times more likely than women to report owning guns.[148]
 There is a regard for scientific advancement and technological innovation in American culture, resulting in the creation of many modern innovations. The great American inventors include Robert Fulton (the steamboat); Samuel Morse (the telegraph); Eli Whitney (the cotton gin, interchangeable parts); Cyrus McCormick (the reaper); and Thomas Edison (with more than a thousand inventions credited to his name). Most of the new technological innovations over the 20th and 21st centuries were either first invented in the United States, first widely adopted by Americans, or both. Examples include the lightbulb, the airplane, the transistor, the atomic bomb, nuclear power, the personal computer, the iPod, video games, online shopping, and the development of the Internet.[149] The United States also developed the Global Positioning System, which is the world's pre-eminent satellite navigation system.[150]
 The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts and the establishment of a machine tool industry enabled the U.S. to have large-scale manufacturing of sewing machines, bicycles, and other items in the late 19th century. In the early 20th century, factory electrification, the introduction of the assembly line, and other labor-saving techniques created the system of mass production. This propensity for application of scientific ideas continued throughout the 20th century with innovations that held strong international benefits. The 20th century saw the arrival of the Space Age, the Information Age, and a renaissance in the health sciences. This culminated in cultural milestones such as the Apollo Moon landings, the creation of the personal computer, and the sequencing effort called the Human Genome Project. In the 21st century, approximately two-thirds of research and development funding comes from the private sector.[151] The U.S. had 2,944 active satellites in space in December 2021, the highest number of any country.[152]
 Throughout its history, American culture has made significant gains through the open immigration of accomplished scientists. Accomplished scientists include Scottish American scientist Alexander Graham Bell, who developed and patented the telephone and other devices; German scientist Charles Steinmetz, who developed new alternating-current electrical systems in 1889; Russian scientist Vladimir Zworykin, who invented the motion camera in 1919; Serb scientist Nikola Tesla who patented a brushless electrical induction motor based on rotating magnetic fields in 1888. The rise of fascism and Nazism in the 1920s and 30s led many European scientists, such as Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States.[153]
 Thomas Edison's research laboratory developed the phonograph, the first long-lasting light bulb, and the first viable movie camera.[154] The Wright brothers in 1903 made the first sustained and controlled heavier-than-air powered flight, and the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line in the early 20th century.[155]
 Education in the United States is and has historically been provided mainly by the government. Control and funding come from three levels: federal, state, and local. School attendance is mandatory and nearly universal at the elementary and high school levels (often known outside the United States as the primary and secondary levels).[citation needed]
 Students have the option of having their education held in public schools, private schools, or home school. In most public and private schools, education is divided into three levels: elementary school, junior high school (also often called middle school), and high school. In almost all schools at these levels, children are divided by age groups into grades. Post-secondary education, better known as ""college"" in the United States, is generally governed separately from the elementary and high school systems.[citation needed]
 In the year 2000, there were 76.6 million students enrolled in schools from kindergarten through graduate schools. Of these, 72 percent aged 12 to 17 were judged academically ""on track"" for their age (enrolled in school at or above grade level). Of those enrolled in compulsory education, 5.2 million (10.4 percent) were attending private schools. Among the country's adult population, over 85 percent have completed high school and 27 percent have received a bachelor's degree or higher.[156]
 The large majority of the world's top universities, as listed by various ranking organizations, are in the United States, including 19 of the top 25, and the most prestigious – Harvard University.[158][159][160][161] The country also has by far the most Nobel Prize winners in history, with 403 (having won 406 awards).[162]
 Among developed countries, the U.S. is one of the most religious in terms of its demographics. According to a 2002 study by the Pew Global Attitudes Project, the U.S. was the only developed nation in the survey where a majority of citizens reported that religion played a ""very important"" role in their lives, an opinion similar to that found in Latin America.[164] Today, governments at the national, state, and local levels are secular institutions, with what is often called the ""separation of church and state"". The most popular religion in the U.S. is Christianity, comprising the majority of the population (73.7% of adults in 2016).[165][166]
 Although participation in organized religion has been diminishing, the public life and popular culture of the United States incorporates many Christian ideals specifically about redemption, salvation, conscience, and morality. Examples are popular culture obsessions with confession and forgiveness, which extends from reality television to twelve-step meetings.[167]
 Most of the British Thirteen Colonies were generally not tolerant of dissident forms of worship. Civil and religious restrictions were most strictly applied by the Puritans of the Massachusetts Bay Colony which saw various banishments applied to enforce conformity, including the branding iron, the whipping post, the bilboes and the hangman's noose.[169] The persecuting spirit was shared by Plymouth Colony and the colonies along the Connecticut river.[170] Mary Dyer was one of the four executed Quakers known as the Boston martyrs, and her death on the Boston gallows marked the beginning of the end of Puritan theocracy and New England independence from English rule; in 1661 Massachusetts was forbidden from executing anyone for professing Quakerism.[171] Anti-Catholic sentiment appeared in New England with the first Pilgrim and Puritan settlers.[172] The Pilgrims of New England held radical Protestant disapproval of Christmas.[173] Christmas observance was outlawed in Boston in 1659.[174] The ban by the Puritans was revoked in 1681 by an English appointed governor; however, it was not until the mid-19th century that celebrating Christmas became common in the Boston region.[175]
 The colony of Maryland, founded by the Catholic Lord Baltimore in 1634, came closest to applying freedom of religion.[176] Fifteen years later (1649), the Maryland Toleration Act, drafted by Lord Baltimore, provided: ""No person or persons...shall from henceforth be any waies troubled, molested or discountenanced for or in respect of his or her religion nor in the free exercise thereof."" The Act allowed freedom of worship for all Trinitarian Christians in Maryland, but sentenced to death anyone who denied the divinity of Jesus.
 Modeling the provisions concerning religion within the Virginia Statute for Religious Freedom, the framers of the United States Constitution rejected any religious test for office, and the First Amendment specifically denied the central government any power to enact any law respecting either an establishment of religion or prohibiting its free exercise. In the following decades, the animating spirit behind the constitution's Establishment Clause led to the disestablishment of the official religions within the member states. The framers were mainly influenced by secular, Enlightenment ideals, but they also considered the pragmatic concerns of minority religious groups who did not want to be under the power or influence of a state religion that did not represent them.[177] Thomas Jefferson, author of the Declaration of Independence said: ""The priest has been hostile to liberty. He is always in alliance with the despot.""[178]
 Gallup polls during the early 2020s found that about 81% of Americans believe in some conception of a God and 45% report praying on a daily basis.[179][180][181] According to their poll in December 2022, ""31% report attending a church, synagogue, mosque or temple weekly or nearly weekly today.""[181] In the ""Bible Belt"", which is located primarily within the Southern United States, socially conservative evangelical Protestantism plays a significant role culturally. New England and the Western United States tend to be less religious.[182] Around 6% of Americans claim a non-Christian faith;[183] the largest of which are Judaism, Islam, Hinduism, and Buddhism.[184] The United States either has the first or second-largest Jewish population in the world, and the largest outside of Israel.[185] ""Ceremonial deism"" is common in American culture.[186][187]
 Around 30% of Americans describe themselves as having no religion.[183] Membership in a house of worship fell from 70% in 1999 to 47% in 2020, much of the decline related to the number of Americans expressing no religious preference. Membership also fell among those who identified with a specific religious group.[188][189] According to Gallup, trust in ""the church or organized religion"" has declined significantly since the 1970s.[190] According to the 2022 Cooperative Election Study, younger Americans are significantly less religious. Among Generation Z, a near-majority consider themselves atheist, agnostic, or nothing in particular.[191]
 Though the majority of Americans in the 21st century identify themselves as middle class, American society has experienced increased income inequality.[26][192][193] Social class, generally described as a combination of educational attainment, income and occupational prestige, is one of the greatest cultural influences in America.[26] Nearly all cultural aspects of mundane interactions and consumer behavior in the U.S. are guided by a person's location within the country's social structure.
 Distinct lifestyles, consumption patterns and values are associated with different classes. Early sociologist-economist Thorstein Veblen, for example, said that those at the top of the societal hierarchy engage in conspicuous leisure and conspicuous consumption. Upper class Americans commonly have elite Ivy League educations and are traditionally members of exclusive clubs and fraternities with connections to high society, distinguished by their enormous incomes derived from their wealth in assets. The upper-class lifestyle and values often overlap with that of the upper middle class, but with more emphasis on security and privacy in home life and for philanthropy (i.e. the ""Donor Class"") and the arts. Due to their large wealth (inherited or accrued over a lifetime of investments) and lavish, leisurely lifestyles, the upper class are more prone to idleness. The upper middle class, or the ""working rich"",[194] commonly identify education and being cultured as prime values, similar to the upper class. Persons in this particular social class tend to speak in a more direct manner that projects authority, knowledge and thus credibility. They often tend to engage in the consumption of so-called mass luxuries, such as designer label clothing. A strong preference for natural materials, organic foods, and a strong health consciousness tend to be prominent features of the upper middle class. American middle-class individuals in general value expanding one's horizon, partially because they are more educated and can afford greater leisure and travel. Working-class individuals take great pride in doing what they consider to be ""real work"" and keep very close-knit kin networks that serve as a safeguard against frequent economic instability.[26][195][196]
 Working-class Americans and many of those in the middle class may also face occupation alienation. In contrast to upper-middle-class professionals who are mostly hired to conceptualize, supervise, and share their thoughts, many Americans have little autonomy or creative latitude in the workplace.[197] As a result, white collar professionals tend to be significantly more satisfied with their work.[198][199] In 2006, Elizabeth Warren presented her article entitled ""The Middle Class on the Precipice"", stating that individuals in the center of the income strata, who may still identify as middle class, have faced increasing economic insecurity,[200] supporting the idea of a working-class majority.[201] Additionally, working-class Americans who work in the public sector, excluding politicians, are respected and generally respected in the culture, notably postal workers.[202][203]
 Political behavior is affected by class; more affluent individuals are more likely to vote, and education and income affect whether individuals tend to vote for the Democratic or Republican party. Income also had a significant impact on health as those with higher incomes had better access to health care facilities, higher life expectancy, lower infant mortality rate and increased health consciousness.[205][206][207] This is particularly noticeable with black voters who are often socially conservative, yet overwhelmingly vote Democratic.[208][209]
 In the United States, occupation is one of the prime factors of social class and is closely linked to an individual's identity. The average workweek in the U.S. for those employed full-time was 42.9 hours long with 30% of the population working more than 40 hours a week.[210] The Average American worker earned $16.64 an hour in the first two quarters of 2006.[211] Overall Americans worked more than their counterparts in other developed post-industrial nations. While the average worker in Denmark enjoyed 30 days of vacation annually, the average American had 16 annual vacation days.[212]
 In 2000, the average American worked 1,978 hours per year, 500 hours more than the average German, yet 100 hours less than the average Czech. Overall, the U.S. labor force is one of the most productive in the world, largely due to its workers working more than those in any other post-industrial country, except for South Korea.[213] Americans generally hold working and being productive in high regard.[196] Individualism,[214] having a strong work ethic,[215] competitiveness,[216] and altruism[217][218][219] are among the most cited American values. According to a 2016 study by the Charities Aid Foundation, Americans donated 1.44% of total GDP to charity, the highest in the world by a large margin.[220]
 The United States has an ethnically diverse population, and 37 ancestry groups have more than one million members.[223] White Americans with ancestry from Europe, the Middle East or North Africa, form the largest racial and ethnic group at 57.8% of the U.S. population.[224][225] Hispanic and Latino Americans form the second-largest group and are 18.7% of the U.S. population. African Americans constitute the nation's third-largest ancestry group and are 12.1% of the total U.S. population.[223] Asian Americans are the country's fourth-largest group, composing 5.9% of the U.S. population, while the country's 3.7 million Native Americans account for about 1%.[223] In 2020, the median age of the U.S. population was 38.5 years.[226]
 According to the United Nations, the U.S. has the highest number of immigrant population in the world, with 50,661,149 people.[227][228] In 2018, there were almost 90 million immigrants and U.S.-born children of immigrants in the U.S., accounting for 28% of the overall U.S. population.[229] In 2017, out of the U.S. foreign-born population, some 45% (20.7 million) were naturalized citizens, 27% (12.3 million) were lawful permanent residents, 6% (2.2 million) were temporary lawful residents, and 23% (10.5 million) were unauthorized immigrants.[230] The U.S. led the world in refugee resettlement for decades, admitting more refugees than the rest of the world combined.[231]
 Race in the U.S. is based on physical characteristics, such as skin color, and has played an essential part in shaping American society even before the nation's conception.[26] Until the civil rights movement of the 1960s, racial minorities in the U.S. faced institutional discrimination and both social and economic marginalization.[232] The U.S. Census Bureau currently recognizes five racial groupings: White, African, Native, Asian, and Pacific Islander. According to the U.S. government, Hispanic Americans do not constitute a race, but rather an ethnic group. During the 2000 U.S. census, Whites made up 75.1% of the population; those who are Hispanic or Latino constituted the nation's prevalent minority with 12.5% of the population. African Americans made up 12.3% of the total population, 3.6% were Asian American, and 0.7% were Native American.[233]
 With its ratification on December 6, 1865, the Thirteenth Amendment to the U.S. Constitution abolished slavery in the U.S. The Northern states had outlawed slavery in their territory in the late 18th and early 19th centuries, though their industrial economies relied on raw materials produced by slaves in the South. Following the Reconstruction period in the 1870s, racist legislation emerged in the Southern states named the Jim Crow laws that provided for legal segregation. Lynching was practiced throughout the U.S., including in the Northern states, until the 1930s, while continuing well into the civil rights movement in the South.[232]
 Chinese Americans were earlier marginalized as well during a significant proportion of U.S. history. Between 1882 and 1943, the U.S. instituted the Chinese Exclusion Act barring all Chinese immigrants from entering the U.S. During the Second World War against the Empire of Japan, roughly 120,000 Japanese Americans, 62% of whom were U.S. citizens,[234] were imprisoned in Japanese internment camps by the U.S. government following the attack on Pearl Harbor, an American military base, by Japanese forces in December 1941.
 Due to exclusion from or marginalization by earlier mainstream society, there emerged a unique subculture among the racial minorities in the U.S. During the 1920s, Harlem, New York City became home to the Harlem Renaissance. Music styles such as jazz, blues, rap, rock and roll, and numerous folk songs such as Blue Tail Fly (Jimmy Crack Corn) originated within the realms of African American culture and were later adopted by the mainstream.[232] Chinatowns can be found in many cities across the country and Asian cuisine has become a common staple in mainstream America. The Hispanic community has also had a dramatic impact on American culture. Today, Catholics are the largest religious denomination in the U.S. and outnumber Protestants in the Southwest and California.[235] Mariachi music and Mexican cuisine are commonly found throughout the Southwest, and some Latin dishes, such as burritos and tacos, are found practically everywhere in the nation.
 Asian Americans have median household income and educational attainment exceeding that of other races. African Americans, Hispanics, and Native Americans have considerably lower income and education than do White Americans or Asian Americans.[236][237]
 White Americans (non-Hispanic/Latino and Hispanic/Latino) are the racial majority and have a 72% share of the U.S. population, according to the 2010 U.S. census.[238] Hispanic and Latino Americans comprise 15% of the population, making up the largest ethnic minority.[239] Black Americans are the largest racial minority, comprising nearly 13% of the population.[238][239] The White, non-Hispanic or Latino population comprises 63% of the nation's total.[239]
 Throughout most of the country's history before and after its independence, the majority race in the United States has been Caucasian—aided by historic restrictions on citizenship and immigration—and the largest racial minority has been African Americans, most of whom are descended from slaves smuggled to the Americas by the European colonial powers. This relationship has historically been the most important one since the founding of the United States. Slavery existed in the United States at the time of the country's formation in the 1770s. The Missouri Compromise declared a policy of prohibiting slavery in the remaining Louisiana lands north of the 36°30′ parallel. De facto, it sectionalized the country into two factions: free states, which forbid the institution of slavery; and slave states, which protected the institution. The Missouri Compromise was controversial, seen as lawfully dividing the country along sectarian lines. Although the federal government outlawed American participation in the Atlantic slave trade in 1807, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the use of slave labor.[240][241][242] The Second Great Awakening, especially in the period 1800–1840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism;[243] in the South, Methodists and Baptists proselytized among slave populations.[244]
 Slavery was partially abolished by the Emancipation Proclamation issued by the president Abraham Lincoln in 1862 for slaves in the Southeastern United States during the Civil War. With the United States' victory and preservation, slavery was abolished nationally by the Thirteenth Amendment. Jim Crow laws prevented full use of African American citizenship until the civil rights movement in the 1960s, and the Civil Rights Act of 1964 outlawed official or legal segregation at any level and forbid placing limitations on minorities' access to public places.
 In 1882, in response to Chinese immigration due to the Gold Rush and the labor needed for the transcontinental railroad, the government signed into law the Chinese Exclusion Act which banned immigration by Chinese people into the U.S. In the late 19th century, the growth of the Hispanic population in the U.S., fueled largely by Mexican immigration, generated debate over policies such as English as the official language and reform to immigration policies. The Immigration Act of 1924 established the National Origins Formula as the basis of U.S. immigration policy, largely to restrict immigration from Asia, Southern Europe, and Eastern Europe. According to the Office of the Historian of the U.S. Department of State, the purpose of the 1924 Act was ""to preserve the ideal of U.S. homogeneity"".[245] In 1924, Indian-born Bhagat Singh Thind was twice denied citizenship as he was not deemed white.[246] Marking a radical break from U.S. immigration policies of the past, the Immigration and Nationality Act of 1965 opened entry to the U.S. to non-Germanic groups.[247] This Act significantly altered the demographic mix in the U.S. as a result, creating a modern, diverse America.[247]
 A huge majority of Americans of all races disapprove of racism. Nevertheless, some Americans continue to hold negative racial/ethnic stereotypes about various racial and ethnic groups. Professor Imani Perry, of Princeton University, has argued that contemporary racism in the United States ""is frequently unintentional or unacknowledged on the part of the actor"",[248] believing that racism mostly stems unconsciously from below the level of cognition.[249]
 Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads, making it the longest network in the world.[250][251] In 2001, 90% of Americans drove to work by car.[252] As of 2022, the United States is the second-largest manufacturer of motor vehicles[253] and is home to Tesla, the world's most valuable car company.[254] General Motors held the title of the world's best-selling automaker from 1931 to 2008.[255] Currently, the U.S. has the world's second-largest automobile market by sales[256] and the highest vehicle ownership per capita in the world, with 816.4 vehicles per 1,000 Americans (2014).[257] In 2017, there were 255 million non-two wheel motor vehicles, or about 910 vehicles per 1,000 people.[258]
 Beginning in the 1990s, lower energy and land costs favor the production of relatively larger cars, leading to a decline in economy cars. The culture in the 1950s and 1960s often catered to the automobile with motels and drive-in restaurants. Outside of the relatively few urban areas, it is considered a necessity for most Americans to own and drive cars. New York City is the only locality in the United States where more than half of all households do not own a car.[252] In a car-dependent America, there is a common dislike of car dealerships and car salesmen, with only 10 percent of U.S. citizens in a Gallup poll rating them highly honest.[259] Matilda by Roald Dahl gives an example of this stereotype: Matilda's father sells used cars by filling their engines with sawdust or reversing their odometers with a drill.
 The United States emerged as a pioneer of the automotive industry in the early 20th century. General Motors Corporation (GM), the company that would soon become the world's largest automaker, was founded in 1908 by William Durant.[260] The U.S. also became the first country in the world to have a mass market for vehicle production and sales, and mass market production process.[261][262] In the 1950s and 1960s, subcultures began to arise around the modification and racing of American automobiles and converting them into hot rods. Later, in the late-1960s and early-1970s Detroit manufacturers began making muscle cars and pony cars to cater to wealthier Americans seeking hot rod style & performance. This culture has evolved into a worldwide phenomenon for car enthusiasts of today, and the project car is a common sight in American suburbs.[citation needed]
 The United States government does not have a ministry of culture, but there are a number of government institutions with cultural responsibilities, including the President's Committee on the Arts and Humanities, the Federal Communications Commission, the Corporation for Public Broadcasting, the National Endowment for the Humanities, the National Endowment for the Arts, the Institute of Museum and Library Services, the U.S. Commission of Fine Arts, the Library of Congress, the Smithsonian Institution, and the National Gallery of Art.
 Many state and city governments have a department dedicated to cultural affairs.
 The National Register of Historic Places (NRHP) is the United States federal government's official list of districts, sites, buildings, structures, and objects deemed worthy of preservation for their historical significance or ""great artistic value."" For most of its history, the National Register has been administered by the National Park Service (NPS), an agency within the U.S. Department of the Interior.
 Major private US-based culture institutions include the Poetry Foundation, the Solomon R. Guggenheim Foundation, the J. Paul Getty Trust, and the Andrew W. Mellon Foundation.
 In the United States, there are many museums, both public and private. Major museums in the US include the Metropolitan Museum of Art, the Museum of Modern Art, the museums of the Smithsonian Institution, the American Museum of Natural History, the Art Institute of Chicago, and The Getty Museum.
 There are various archives in the United States for the preservation of history and culture, such as the National Archives and Records Administration.
"
List of memorials to George Washington,https://en.wikipedia.org/wiki/List_of_memorials_to_George_Washington,"
 This is a list of memorials to George Washington, the commander-in-chief of the Continental Army during the American Revolutionary War and first president of the United States.
 Washington's Birthday has been a federal holiday in the United States since 1879.
 One of the United States, 31 counties, and 241 civil townships are named for George Washington.
 Some of the locations below are named for George Washington:
 Places named for George Washington outside of the United States include:
"
"Washington, D.C.","https://en.wikipedia.org/wiki/Washington,_D.C.","



 Washington, D.C., formally the District of Columbia and commonly known as Washington or D.C., is the capital city and federal district of the United States. The city is on the Potomac River, across from Virginia, and shares land borders with Maryland to its north and east. It was named after George Washington, the first president of the United States. The district is named for Columbia, the female personification of the nation.
 The U.S. Constitution in 1789 called for the creation of a federal district under the exclusive jurisdiction of the U.S. Congress. As such, Washington, D.C., is not part of any state, and is not one itself. The Residence Act, adopted on July 16, 1790, approved the creation of the capital district along the Potomac River. The city was founded in 1791, and the 6th Congress held the first session in the unfinished Capitol Building in 1800 after the capital moved from Philadelphia. In 1801, the District of Columbia, formerly part of Maryland and Virginia and including the existing settlements of Georgetown and Alexandria, was officially recognized as the federal district; initially, the city was a separate settlement within the larger district. In 1846, Congress reduced the size of the district when it returned the land originally ceded by Virginia, including the city of Alexandria. In 1871, it created a single municipality for the district. There have been several unsuccessful efforts to make the district into a state since the 1880s; a statehood bill passed the House of Representatives in 2021 but was not adopted by the U.S. Senate. To become law it would have to be passed by the Senate and signed by president; it would have renamed the city Washington, Douglass Commonwealth and shrunk the Federal District to about the size of the National Mall.
 Designed in 1791 by Pierre Charles L'Enfant, the city is divided into quadrants, which are centered around the Capitol Building and include 131 neighborhoods. As of the 2020 census, the city had a population of 689,545.[3] Commuters from the city's Maryland and Virginia suburbs raise the city's daytime population to more than one million during the workweek.[12] The Washington metropolitan area, which includes parts of Maryland, Virginia, and West Virginia, is the country's seventh-largest metropolitan area, with a 2023 population of 6.3 million residents.[6] A locally elected mayor and 13-member council have governed the district since 1973, though Congress retains the power to overturn local laws. Washington, D.C. residents do not have voting representation in Congress, but elect a single non-voting congressional delegate to the U.S. House of Representatives. The city's voters choose three presidential electors in accordance with the Twenty-third Amendment, passed in 1961.
 Washington, D.C. anchors the southern end of the Northeast megalopolis. As the seat of the U.S. federal government, the city is an important world political capital.[13] The city hosts the buildings that house federal government headquarters, including the White House, the Capitol, the Supreme Court Building, and multiple federal departments and agencies. The city is home to many national monuments and museums, located most prominently on or around the National Mall, including the Jefferson Memorial, the Lincoln Memorial, and the Washington Monument. It hosts 177 foreign embassies and serves as the headquarters for the World Bank, the International Monetary Fund, the Organization of American States, and other international organizations. Home to many of the nation's largest industry associations, non-profit organizations, and think tanks, D.C. is known as a lobbying hub, with K Street as the industry center.[14] It is also among the country's top tourist destinations; in 2022, it drew an estimated 20.7 million domestic[15] and 1.2 million international visitors, seventh-most among U.S. cities.[16]
 The Algonquian-speaking Piscataway people, also known as the Conoy, inhabited the lands around the Potomac River and present-day Washington, D.C., when Europeans first arrived and colonized the region in the early 17th century. The Nacotchtank, also called the Nacostines by Catholic missionaries, maintained settlements around the Anacostia River in present-day Washington, D.C. Conflicts with European colonists and neighboring tribes ultimately displaced the Piscataway people, some of whom established a new settlement in 1699 near Point of Rocks, Maryland.[17]
 Nine cities served as capitals to the Continental Congress and under the Articles of Confederation. New York City was the first capital upon the adoption of the Constitution, succeeded by Philadelphia, which was capital from 1790 to 1800.[18]
 On October 6, 1783, after the capital was forced by the Pennsylvania Mutiny of 1783 to move to Princeton, Congress resolved to consider a new location for it.[19] The following day, Elbridge Gerry of Massachusetts moved ""that buildings for the use of Congress be erected on the banks of the Delaware near Trenton, or of the Potomac, near Georgetown, provided a suitable district can be procured on one of the rivers as aforesaid, for a federal town"".[20]
 In Federalist No. 43, published January 23, 1788, James Madison argued that the new federal government would need authority over a national capital to provide for its own maintenance and safety.[21] The Pennsylvania Mutiny of 1783 emphasized the need for the national government not to rely on any state for its own security.[22]
 Article One, Section Eight of the Constitution permits the establishment of a ""District (not exceeding ten miles square) as may, by cession of particular states, and the acceptance of Congress, become the seat of the government of the United States"".[23] However, the constitution does not specify a location for the capital. In the Compromise of 1790, Madison, Alexander Hamilton, and Thomas Jefferson agreed that the federal government would pay each state's remaining Revolutionary War debts in exchange for establishing the new national capital in the Southern United States.[24][a]
 On July 9, 1790, Congress passed the Residence Act, which approved the creation of a national capital on the Potomac River. Under the Residence Act, the exact location was to be selected by President George Washington, who signed the bill into law on July 16, 1790. Formed from land donated by Maryland and Virginia, the initial shape of the federal district was a square measuring 10 miles (16 km) on each side and totaling 100 square miles (259 km2).[25][b]
 Two pre-existing settlements were included in the territory, the port of Georgetown, founded in 1751,[26] and the port city of Alexandria, Virginia, founded in 1749.[27] In 1791 and 1792, a team led by Andrew Ellicott, including Ellicott's brothers Joseph and Benjamin and African American astronomer Benjamin Banneker, whose parents had been enslaved, surveyed the borders of the federal district and placed boundary stones at every mile point; many of these stones are still standing.[28][29] Both Maryland and Virginia were slave states, and slavery existed in the District from its founding. The building of Washington likely relied in significant part on slave labor, and slave receipts have been found for the White House, Capitol Building, and establishment of Georgetown University. The city became an important slave market and a center of the nation's internal slave trade.[30][31]
 After its survey, the new federal city was constructed on the north bank of the Potomac River, to the east of Georgetown centered on Capitol Hill. On September 9, 1791, three commissioners overseeing the capital's construction named the city in honor of President Washington. The same day, the federal district was named Columbia, a feminine form of Columbus, which was a poetic name for the United States commonly used at that time.[32][33] Congress held its first session there on November 17, 1800.[34]
 Congress passed the District of Columbia Organic Act of 1801, which officially organized the district and placed the entire territory under the exclusive control of the federal government. The area within the district was organized into two counties, the County of Washington to the east and north of the Potomac and the County of Alexandria to the west and south.[35] After the Act's passage, citizens in the district were no longer considered residents of Maryland or Virginia, which ended their representation in Congress.[36]
 On August 24, 1814, during the War of 1812, British forces invaded and occupied the city after defeating an American force at Bladensburg. In retaliation for acts of destruction by American troops in the Canadas, the British set fire to government buildings in the city, gutting the United States Capitol, the Treasury Building, and the White House in what became known as the burning of Washington. However, a storm forced the British to evacuate the city after just 24 hours.[37] Most government buildings were repaired quickly, but the Capitol, which was largely under construction at the time, would not be completed in its current form until 1868.[38]
 In the 1830s, the district's southern territory of Alexandria declined economically, due in part to its neglect by Congress.[39] Alexandria was a major market in the domestic slave trade and pro-slavery residents feared that abolitionists in Congress would end slavery in the district. Alexandria's citizens petitioned Virginia to retake the land it had donated to form the district, a process known as retrocession.[40]
 The Virginia General Assembly voted in February 1846, to accept the return of Alexandria. On July 9, 1846, Congress went further, agreeing to return all territory that Virginia had ceded to the district during its formation. This left the district's area consisting only of the portion originally donated by Maryland.[39] Confirming the fears of pro-slavery Alexandrians, the Compromise of 1850 outlawed the slave trade in the district, although not slavery itself.[41]
 The outbreak of the American Civil War in 1861 led to the expansion of the federal government and notable growth in the city's population, including a large influx of freed slaves.[42] President Abraham Lincoln signed the Compensated Emancipation Act in 1862, which ended slavery in the district, freeing about 3,100 slaves in the district nine months before the Emancipation Proclamation.[43] In 1868, Congress granted the district's African American male residents the right to vote in municipal elections.[42]
 By 1870, the district's population had grown 75% in a decade to nearly 132,000 people,[44] yet the city still lacked paved roads and basic sanitation. Some members of Congress suggested moving the capital farther west, but President Ulysses S. Grant refused to consider the proposal.[45]
 In the Organic Act of 1871, Congress repealed the individual charters of the cities of Washington and Georgetown, abolished Washington County, and created a new territorial government for the whole District of Columbia.[46] These steps made ""the city of Washington...legally indistinguishable from the District of Columbia.""[47]
 In 1873, President Grant appointed Alexander Robey Shepherd as Governor of the District of Columbia. Shepherd authorized large projects that modernized the city but bankrupted its government. In 1874, Congress replaced the territorial government with an appointed three-member board of commissioners.[48]
 In 1888, the city's first motorized streetcars began service. Their introduction generated growth in areas of the district beyond the City of Washington's original boundaries, leading to an expansion of the district over the next few decades.[49] Georgetown's street grid and other administrative details were formally merged with those of the City of Washington in 1895.[50] However, the city had poor housing and strained public works, leading it to become the first city in the nation to undergo urban renewal projects as part of the City Beautiful movement in the early 20th century.[51]
 The City Beautiful movement built heavily upon the already-implemented L'Enfant Plan, with the new McMillan Plan leading urban development in the city throughout the movement. Much of the old Victorian Mall was replaced with modern Neoclassical and Beaux-Arts architecture; these designs are still prevalent in the city's governmental buildings today.
 Increased federal spending under the New Deal in the 1930s led to the construction of new government buildings, memorials, and museums in the district,[52] though the chairman of the House Subcommittee on District Appropriations, Ross A. Collins of Mississippi, justified cuts to funds for welfare and education for local residents by saying that ""my constituents wouldn't stand for spending money on niggers.""[53]
 World War II led to an expansion of federal employees in the city;[54] by 1950, the district's population reached its peak of 802,178 residents.[44]
 The Twenty-third Amendment to the United States Constitution was ratified in 1961, granting the district three votes in the Electoral College for the election of president and vice president, but still not affording the city's residents representation in Congress.[55]
 After the assassination of civil rights leader Martin Luther King Jr. on April 4, 1968, riots broke out in the city, primarily in the U Street, 14th Street, 7th Street, and H Street corridors, which were predominantly black residential and commercial areas. The riots raged for three days until more than 13,600 federal troops and Washington, D.C., Army National Guardsmen stopped the violence. Many stores and other buildings were burned, and rebuilding from the riots was not completed until the late 1990s.[56]
 In 1973, Congress enacted the District of Columbia Home Rule Act providing for an elected mayor and 13-member council for the district.[57] In 1975, Walter Washington became the district's first elected and first black mayor.[58]
 Since the 1980s, the D.C. statehood movement has grown in prominence. In 2016, a referendum on D.C. statehood resulted in an 85% support among Washington, D.C., voters for it to become the nation's 51st state. In March 2017, the city's congressional delegate Eleanor Holmes Norton introduced a bill for statehood. Reintroduced in 2019 and 2021 as the Washington, D.C., Admission Act, the U.S. House of Representatives passed it in April 2021.[citation needed] After not progressing in the Senate, the statehood bill was introduced again in January 2023.[59]
The bill would have made D.C. into a state with one representative and two senators, with the name Washington, Douglass Commonwealth (thus keeping the same abbreviation Washington, D.C.).[60] The legalities, reasons, and impact of statehood have been heavily debated in the 2020s.[61]
 Washington, D.C., is located in the Mid-Atlantic region of the U.S. East Coast. The city has a total area of 68.34 square miles (177 km2), of which 61.05 square miles (158.1 km2) is land and 7.29 square miles (18.9 km2) (10.67%) is water.[62] The district is bordered by Montgomery County, Maryland, to the northwest; Prince George's County, Maryland, to the east; Arlington County, Virginia, to the west; and Alexandria, Virginia, to the south.
 The south bank of the Potomac River forms the district's border with Virginia and has two major tributaries, the Anacostia River and Rock Creek.[63] Tiber Creek, a natural watercourse that once passed through the National Mall, was fully enclosed underground during the 1870s.[64] The creek also formed a portion of the now-filled Washington City Canal, which allowed passage through the city to the Anacostia River from 1815 until the 1850s.[65] The Chesapeake and Ohio Canal starts in Georgetown and was used during the 19th century to bypass the Little Falls of the Potomac River, located at the northwest edge of the city at the Atlantic Seaboard fall line.[66]
 The highest natural elevation in the district is 409 feet (125 m) above sea level at Fort Reno Park in upper northwest Washington, D.C.[67] The lowest point is sea level at the Potomac River.[68] The geographic center of Washington is near the intersection of 4th and L streets NW.[69][70][71]
 The district has 7,464 acres (30.21 km2) of parkland, about 19% of the city's total area, the second-highest among high-density U.S. cities after Philadelphia.[72] The city's sizable parkland was a factor in the city being ranked as third in the nation for park access and quality in the 2018 ParkScore ranking of the park systems of the nation's 100 most populous cities, according to Trust for Public Land, a non-profit organization.[73]
 The National Park Service manages most of the 9,122 acres (36.92 km2) of city land owned by the U.S. government.[74] Rock Creek Park is a 1,754-acre (7.10 km2) urban forest in Northwest Washington, which extends 9.3 miles (15.0 km) through a stream valley that bisects the city. Established in 1890, it is the country's fourth-oldest national park and is home to a variety of plant and animal species, including raccoon, deer, owls, and coyotes.[75] Other National Park Service properties include the Chesapeake and Ohio Canal National Historical Park, the National Mall and Memorial Parks, Theodore Roosevelt Island, Columbia Island, Fort Dupont Park, Meridian Hill Park, Kenilworth Park and Aquatic Gardens, and Anacostia Park.[76] The District of Columbia Department of Parks and Recreation maintains the city's 900 acres (3.6 km2) of athletic fields and playgrounds, 40 swimming pools, and 68 recreation centers.[77] The U.S. Department of Agriculture operates the 446-acre (1.80 km2) United States National Arboretum in Northeast Washington, D.C.[78]
 Washington's climate is humid subtropical (Köppen: Cfa), or oceanic (Trewartha: Do bordering Cf downtown).[79][80] Winters are cool to cold with some snow of varying intensity, while summers are hot and humid. The district is in plant hardiness zone 8a near downtown, and zone 7b elsewhere in the city.[81][82]
 Summers are hot and humid with a July daily average of 79.8 °F (26.6 °C) and average daily relative humidity around 66%, which can cause moderate personal discomfort. Heat indices regularly approach 100 °F (38 °C) at the height of summer.[83] The combination of heat and humidity in the summer brings very frequent thunderstorms, some of which occasionally produce tornadoes in the area.[84]
 Blizzards affect Washington once every four to six years on average. The most violent storms, known as nor'easters, often impact large regions of the East Coast.[85] From January 27 to 28, 1922, the city officially received 28 inches (71 cm) of snowfall, the largest snowstorm since official measurements began in 1885.[86] According to notes kept at the time, the city received between 30 and 36 inches (76 and 91 cm) from a snowstorm in January 1772.[87]
 Hurricanes or their remnants occasionally impact the area in late summer and early fall. However, they usually are weak by the time they reach Washington, partly due to the city's inland location.[88] Flooding of the Potomac River, however, caused by a combination of high tide, storm surge, and runoff, has been known to cause extensive property damage in the Georgetown neighborhood of the city.[89] Precipitation occurs throughout the year.[90]
 The highest recorded temperature was 106 °F (41 °C) on August 6, 1918, and on July 20, 1930.[91] The lowest recorded temperature was −15 °F (−26 °C) on February 11, 1899, right before the Great Blizzard of 1899.[85] During a typical year, the city averages about 37 days at or above 90 °F (32 °C) and 64 nights at or below the freezing mark (32 °F or 0 °C).[92] On average, the first day with a minimum at or below freezing is November 18 and the last day is March 27.[93][94]
 Washington, D.C., was a planned city, and many of the city's street grids were developed in that initial plan. In 1791, President George Washington commissioned Pierre Charles L'Enfant, a French-born military engineer and artist, to design the new capital. He enlisted the help of Isaac Roberdeau, Étienne Sulpice Hallet and Scottish surveyor Alexander Ralston to help lay out the city plan.[97] The L'Enfant Plan featured broad streets and avenues radiating out from rectangles, providing room for open space and landscaping.[98]
 L'Enfant was also provided a roll of maps by Thomas Jefferson depicting Frankfurt, Amsterdam, Strasbourg, Paris, Orleans, Bordeaux, Lyon, Marseille, Turin, and Milan.[99] L'Enfant's design also envisioned a garden-lined grand avenue about 1 mile (1.6 km) long and 400 feet (120 m) wide in an area that is now the National Mall inspired by the grounds at Versailles and Tuileries Gardens.[100] In March 1792, President Washington dismissed L'Enfant due to conflicts with the three commissioners appointed to supervise the capital's construction. Andrew Ellicott, who worked with L'Enfant in surveying the city, was then tasked with completing its design. Though Ellicott revised the original L'Enfant plans, including changing some street patterns, L'Enfant is still credited with the city's overall design.[101]
 By the early 20th century, however, L'Enfant's vision of a grand national capital was marred by slums and randomly placed buildings in the city, including a railroad station on the National Mall. Congress formed a special committee charged with beautifying Washington's ceremonial core.[51] What became known as the McMillan Plan was finalized in 1901 and included relandscaping the Capitol grounds and the National Mall, clearing slums, and establishing a new citywide park system. The plan is thought to have largely preserved L'Enfant's intended design for the city.[98]
 By law, the city's skyline is low and sprawling. The federal Height of Buildings Act of 1910 prohibits buildings with height exceeding the width of the adjacent street plus 20 feet (6.1 m).[102] Despite popular belief, no law has ever limited buildings to the height of the United States Capitol or the 555-foot (169 m) Washington Monument,[71] which remains the district's tallest structure. City leaders have cited the height restriction as a primary reason that the district has limited affordable housing and its metro area has suburban sprawl and traffic problems.[102] Washington, D.C., still has a relatively high homelessness rate, despite its high living standard compared to many American cities.[103]
 Washington, D.C., is divided into four quadrants of unequal area: Northwest (NW), Northeast (NE), Southeast (SE), and Southwest (SW). The axes bounding the quadrants radiate from the U.S. Capitol.[104] All road names include the quadrant abbreviation to indicate their location. House numbers generally correspond with the number of blocks away from the Capitol. Most streets are set out in a grid pattern with east–west streets named with letters (e.g., C Street SW), north–south streets with numbers (e.g., 4th Street NW), and diagonal avenues, many of which are named after states.[104]
 The City of Washington was bordered on the north by Boundary Street (renamed Florida Avenue in 1890), Rock Creek to the west, and the Anacostia River to the east.[49][98] Washington, D.C.'s street grid was extended, where possible, throughout the district starting in 1888.[105] Georgetown's streets were renamed in 1895.[50] Some streets are particularly noteworthy, including Pennsylvania Avenue, which connects the White House to the Capitol; and K Street, which houses the offices of many lobbying groups.[106] Constitution Avenue and Independence Avenue, located on the north and south sides of the National Mall, respectively, are home to many of Washington's iconic museums, including many Smithsonian Institution buildings and the National Archives Building. Washington hosts 177 foreign embassies; these maintain nearly 300 buildings and more than 1,600 residential properties, many of which are on a section of Massachusetts Avenue informally known as Embassy Row.[107]
 The architecture of Washington, D.C., varies greatly and is generally popular among tourists and locals. In 2007, six of the top ten buildings in the American Institute of Architects' ranking of America's Favorite Architecture were in the city:[108] the White House, Washington National Cathedral, the Jefferson Memorial, the United States Capitol, the Lincoln Memorial, and the Vietnam Veterans Memorial. The neoclassical, Georgian, Gothic, and Modern styles are reflected among these six structures and many other prominent edifices in the city.[citation needed]
 Many government buildings, monuments, and museums along the National Mall and surrounding areas are heavily inspired by classical Roman and Greek architecture. The designs of the White House, the U.S. Capitol, Supreme Court Building, Washington Monument, National Gallery of Art, Lincoln Memorial, and Jefferson Memorial are all heavily drawn from these classical architectural movements and feature large pediments, domes, columns in classical order, and heavy stone walls. Notable exceptions to the city's classical-style architecture include buildings constructed in the French Second Empire style, including the Eisenhower Executive Office Building, and the modernist Watergate complex.[109] The Thomas Jefferson Building, the main Library of Congress building, and the historic Willard Hotel are built in Beaux-Arts style, popular throughout the world in the late nineteenth and early twentieth centuries.[110][111] Meridian Hill Park contains a cascading waterfall with Italian Renaissance-style architecture.[112]
 Modern, Postmodern, contemporary, and other non-classical architectural styles are also seen in the city. The National Museum of African American History and Culture deeply contrasts the stone-based neoclassical buildings on the National Mall with a design that combines modern engineering with heavy inspiration from African art.[113] The interior of the Washington Metro stations and the Hirshhorn Museum and Sculpture Garden are designed with strong influence from the 20th-century Brutalism movement.[114] The Smithsonian Institution Building is built of Seneca red sandstone in the Norman Revival style.[115] The Old Post Office building, located on Pennsylvania Avenue and completed in 1899, was the first building in the city to have a steel frame structure and the first to use electrical wiring in its design.[116]
 Notable contemporary residential buildings, restaurants, shops, and office buildings in the city include the Wharf on the Southwest Waterfront, Navy Yard along the Anacostia River, and CityCenterDC in Downtown. The Wharf has seen the construction of several high-rise office and residential buildings overlooking the Potomac River. Additionally, restaurants, bars, and shops have been opened at street level. Many of these buildings have a modern glass exterior and heavy curvature.[117][118] CityCenterDC is home to Palmer Alley, a pedestrian-only walkway, and houses several apartment buildings, restaurants, and luxury-brand storefronts with streamlined glass and metal facades.[119]
 Outside Downtown D.C., architectural styles are more varied. Historic buildings are designed primarily in the Queen Anne, Châteauesque, Richardsonian Romanesque, Georgian Revival, Beaux-Arts, and a variety of Victorian styles.[citation needed] Rowhouses are prominent in areas developed after the Civil War and typically follow Federal and late Victorian designs.[120] Georgetown's Old Stone House, built in 1765, is the oldest-standing building in the city.[121] Founded in 1789, Georgetown University features a mix of Romanesque and Gothic Revival architecture.[109] The Ronald Reagan Building is the largest building in the district with a total area of about 3.1 million square feet (288,000 m2).[122] Washington Union Station is designed in a combination of architectural styles. Its Great Hall has elaborate gold leaf designs along the ceilings and the hall includes several decorative classical-style statues.[123]
 The U.S. Census Bureau estimates that the district's population was 705,749 as of July 2019, up more than 100,000 people since the 2010 United States Census. When measured decade-over-decade, this shows growth since 2000, following a half-century of population decline.[130] Washington was the 24th-most populous place in the United States as of 2010[update].[131] According to data from 2010, commuters from the suburbs boost the district's daytime population past one million.[132] If the district were a state, it would rank 49th in population, ahead of Vermont and Wyoming.[133]
 The Washington metropolitan area, which includes the district and surrounding suburbs, is the sixth-largest metropolitan area in the U.S., with an estimated six million residents as of 2016.[134] When the Washington area is included with Baltimore and its suburbs, it forms the vast Washington–Baltimore combined statistical area. With a population exceeding 9.8 million residents in 2020, it is the third-largest combined statistical area in the country.[135]
 According to Department of Housing and Development's Annual Homeless Assessment Report in 2022, there were an estimated 4,410 homeless people in Washington, D.C.[136][137]
 According to 2020 Census Bureau data, the population of Washington, D.C., was 41.4% Black or African American, 39.6% White (37.9% non-Hispanic White), 4.9% Asian, 0.5% American Indian or Alaska Native, 0.1% Native Hawaiian or Other Pacific Islander, and 5.4% Some Other Race. Individuals from two or more races made up 8.1% of the population. Hispanics of any race made up 11.3% of the district's population.[124]
 Washington, D.C. has had a relatively large African American population since the city's foundation.[138] African American residents composed about 30% of the district's total population between 1800 and 1940.[44] The black population reached a peak of 70% by 1970 and has since declined as African Americans moved to the surrounding suburbs. Partly as a result of gentrification, there was a 31.4% increase in the non-Hispanic white population and an 11.5% decrease in the black population between 2000 and 2010.[139] According to a study by the National Community Reinvestment Coalition, D.C. has experienced more ""intense"" gentrification than any other American city, with 40% of neighborhoods gentrified.[140]
 As of 2010, about 17% of Washington, D.C. residents were age 18 or younger, which is lower than the U.S. average of 24%. However, at 34 years old, the district had the lowest median age compared to the 50 states as of 2010.[141] As of 2010[update], there were an estimated 81,734 immigrants living in Washington, D.C.[142] Major sources of immigration include El Salvador, Ethiopia, Mexico, Guatemala, and China, with a concentration of Salvadorans in the Mount Pleasant neighborhood.[143]
 As of 2010, there were 4,822 same-sex couples in the city, about 2% of total households, according to Williams Institute.[144] Legislation authorizing same-sex marriage passed in 2009, and the district began issuing marriage licenses to same-sex couples in March 2010.[145]
 As of 2007, about one-third of Washington, D.C., residents were functionally illiterate, more than the national rate of about one in five. The city's relatively high illiteracy rate is attributed in part to immigrants who are not proficient in English.[146] As of 2011[update], 85% of D.C. residents age 5 and older spoke English at home as a primary language.[147] Half of residents had at least a four-year college degree in 2006.[142] In 2017, the median household income in D.C. was $77,649;[148] also in 2017, D.C. residents had a personal income per capita of $50,832 (higher than any of the 50 states).[148][149] However, 19% of residents were below the poverty level in 2005, higher than any state except Mississippi. In 2019, the poverty rate stood at 14.7%.[150][f][152]
 As of 2010[update], more than 90% of Washington, D.C., residents had health insurance coverage, the second-highest rate in the nation. This is due in part to city programs that help provide insurance to low-income individuals who do not qualify for other types of coverage.[153][better source needed] A 2009 report found that at least three percent of Washington, D.C., residents have HIV or AIDS, which the Centers for Disease Control and Prevention (CDC) characterizes as a ""generalized and severe"" epidemic.[154]
 As of 2020, according to the Association of Statisticians of American Religious Bodies, 56% of the city's residents were adherents[g] of a religious body. The largest tradition represented was Evangelical Protestantism (15% of total population), followed by Catholicism (12%), Black Protestantism (10%), Mainline Protestantism (10%), Judaism (3%), Orthodox Christianity (2%), Buddhism (1%), and Islam (1%), with several other groups numbering less than 1%. Mainline Protestants were the largest group in 2010, Catholics in 2000, and Black Protestants in 1990.[155] The city is populated with many religious buildings, including the Washington National Cathedral, the Basilica of the National Shrine of the Immaculate Conception, which comprises the largest Catholic church building in the United States, and the Islamic Center of Washington, which was the largest mosque in the Western Hemisphere when opened in 1957. St. John's Episcopal Church, located off Lafayette Square, has held services for every U.S. president since James Madison. The Sixth & I Historic Synagogue, built in 1908, is a synagogue located in the Chinatown section of the city. The Washington D.C. Temple is a large Mormon temple located just outside the city in Kensington, Maryland. Viewable from the Capital Beltway, the temple is the tallest Mormon temple in existence and the third-largest by square footage.[156][157]
 As of 2023,[update] the Washington metropolitan area, including the District of Columbia as well as parts of Virginia, Maryland, and West Virginia, was one of the nation's largest metropolitan economies. Its growing and diversified economy has an increasing percentage of professional and business service jobs in addition to more traditional jobs rooted in tourism, entertainment, and government.[158][obsolete source]
 Between 2009 and 2016, gross domestic product per capita in Washington, D.C., consistently ranked at the very top among U.S. states.[159] In 2016, at $160,472, its GDP per capita was almost three times greater than that of Massachusetts, which was ranked second in the nation (see List of U.S. states and territories by GDP).[159]
As of 2022[update], the metropolitan statistical area's unemployment rate was 3.1%, ranking 171 out of the 389 metropolitan areas as defined by the U.S. Bureau of Economic Analysis.[160] The District of Columbia itself had an unemployment rate of 4.6% during the same time period.[161] In 2019, Washington, D.C., had the highest median household income in the U.S. at $92,266.[162]
 According to the District's comprehensive annual financial reports, the top employers by number of employees in 2022 included Georgetown University, Children's National Medical Center, Washington Hospital Center, George Washington University, American University, Georgetown University Hospital, Booz Allen & Hamilton, Insperity PEO Services, Universal Protection Service, Howard University, Medstar Medical Group, George Washington University Hospital, Catholic University of America, and Sibley Memorial Hospital.[163]
 As of July 2022, 25% of people employed in Washington, D.C., were employed by the federal government.[164] Many of the region's residents are employed by companies and organizations that do work for the federal government, seek to influence federal policy, or are otherwise related to its work, including law firms, defense contractors, civilian contractors, nonprofit organizations, lobbying firms, trade unions, industry trade groups, and professional associations, many of which have their headquarters in or near the city for proximity to the federal government.[citation needed]
 As the national capital, Washington, D.C. hosts about 185 foreign missions, including embassies, ambassador's residences, and international cultural centers.[165] Many are concentrated along a stretch of Massachusetts Avenue known informally as Embassy Row.[166] Washington, D.C. is one of the most culturally diverse cities in the world; it hosts a number of internationally themed festivals and events, often in collaboration with foreign missions or delegations.[citation needed] The city government maintains an Office of International Affairs to liaise with the diplomatic community and foreign delegations.[167]
 Washington, D.C., is a leading center for national and international research organizations, especially think tanks engaged in public policy.[168] As of 2020, 8% of the country's think tanks are based in the city, including many of the largest and most widely cited,[169] including the Carnegie Endowment for International Peace, Center for Strategic and International Studies, Peterson Institute for International Economics, The Heritage Foundation, and Urban Institute.[170]
 Washington, D.C. is home to many non-profit organizations that engage with issues of domestic and global importance by conducting advanced research, running programs, or public advocacy. Among these organizations are the UN Foundation, Human Rights Campaign, Amnesty International, and the National Endowment for Democracy.[171] Major medical research institutions include the MedStar Washington Hospital Center and the Children's National Medical Center.[172]
 The city is the country's primary location for international development firms, many of which contract with the D.C.-based United States Agency for International Development (USAID), the U.S. federal government's aid agency. The American Red Cross, a humanitarian agency focused on emergency relief, is also based in the city.[173]
 According to statistics compiled in 2011, four of the largest 500 companies in the country were based in Washington, D.C.[174] In the 2023 Global Financial Centres Index, Washington was ranked as having the 8th most competitive financial center in the world, and fourth most competitive in the United States (after New York City, San Francisco, and Los Angeles).[175] Among the largest companies based in Washington, D.C., are Fannie Mae, Amtrak, Danaher Corporation, FTI Consulting, and Hogan Lovells.[176][better source needed]
 Tourism is the city's second-largest industry, after the federal government. In 2012, some 18.9 million visitors contributed an estimated $4.8 billion to the local economy.[177] In 2019, the city saw 24.6 million tourists, including 1.8 million from foreign countries, who collectively spent $8.15 billion during their stay.[178] Tourism helps many of the region's other industries, such as lodging, food and beverage, entertainment, shopping, and transportation.[178]
 The city and the larger Washington metropolitan area have an array of attractions for tourists, including monuments, memorials, museums, sports events, and trails. Within the city, the National Mall serves as the center of the tourism industry. It is there that many of the city's museums and monuments are located. Adjacent to the mall sits the Tidal Basin, where several major national memorials and monuments are located, including the popular Jefferson Memorial. Washington Union Station is a popular tourist spot with its multitude of restaurants and shops.[citation needed]
 Washington, D.C., is a national center for the arts, home to several concert halls and theaters. The John F. Kennedy Center for the Performing Arts is home to the National Symphony Orchestra, the Washington National Opera, and the Washington Ballet. The Kennedy Center Honors are awarded each year to those in the performing arts who have contributed greatly to the cultural life of the United States. This ceremony is often attended by the sitting U.S. president and other dignitaries and celebrities.[180] The Kennedy Center also awards the annual Mark Twain Prize for American Humor.[181]
 The historic Ford's Theatre, the site of the assassination of President Abraham Lincoln on April 14, 1865, continues to function as a theatre and as a museum.[182]
 The Marine Barracks near Capitol Hill houses the United States Marine Band; founded in 1798, it is the country's oldest professional musical organization.[183] American march composer and Washington-native John Philip Sousa led the Marine Band from 1880 until 1892.[184] Founded in 1925, the United States Navy Band has its headquarters at the Washington Navy Yard and performs at official events and public concerts around the city.[185]
 Founded in 1950, Arena Stage achieved national attention and spurred growth in the city's independent theater movement that now includes organizations such as the Shakespeare Theatre Company, Woolly Mammoth Theatre Company, and the Studio Theatre.[186] Arena Stage reopened after a renovation and expansion in the city's emerging Southwest waterfront area in 2010.[187] The GALA Hispanic Theatre, now housed in the historic Tivoli Theatre in Columbia Heights, was founded in 1976 and is a National Center for the Latino Performing Arts.[188]
 Other performing arts spaces in the city include the Andrew W. Mellon Auditorium in Federal Triangle, the Atlas Performing Arts Center on H Street, the Carter Barron Amphitheater in Rock Creek Park, Constitution Hall in Downtown, the Keegan Theatre in Dupont Circle, the Lisner Auditorium in Foggy Bottom, the Sylvan Theater on the National Mall, and the Warner Theatre in Penn Quarter.[citation needed] The National Theatre in Downtown D.C. is the second-oldest continuously operating theater in the United States, having first opened in 1835.[189]
 The U Street Corridor in Northwest D.C., once known as ""Washington's Black Broadway"", is home to institutions like Howard Theatre and Lincoln Theatre, which hosted music legends such as Washington-native Duke Ellington, John Coltrane, and Miles Davis.[190] Just east of U Street is Shaw, which also served as a major cultural center during the jazz age. Intersecting with U Street is Fourteenth Street, which was an extension of the U Street cultural corridor during the 1920s through the 1960s. The collection of Fourteenth Street, U Street, and Shaw was the location of the Black Renaissance in D.C., which was part of the larger Harlem Renaissance. Today, the area starting at Fourteenth Street downtown going north through U Street and east to Shaw boasts a high concentration of bars, restaurants, and theaters, and is among the city's most notable cultural and artistic areas.[citation needed]
 The Washington D.C. Area Film Critics Association (WAFCA), a group of more than 65 film critics, holds an annual awards ceremony.[191]
 Columbia Records, a major music record label in the US, was founded in Washington, D.C. in 1889.[192]: 105 
 The city grew into being one of America's most important music cities in the early jazz age. Duke Ellington, among the most prominent jazz composers and musicians of his time, was born and raised in Washington, and began his music career in the city. The center of the city's jazz scene during those years was U street and Shaw. Among the city's major jazz locations were the Lincoln Theatre and the Howard Theatre.[193]
 Washington has its own native music genre called go-go; a post-funk, percussion-driven flavor of rhythm and blues that was popularized in the late 1970s by D.C. band leader Chuck Brown.[194]
 The district is an important center for indie culture and music in the United States. The DC-based label Dischord Records, formed by Ian MacKaye, frontman of Fugazi, was one of the most crucial independent labels in the genesis of 1980s punk and eventually indie rock in the 1990s.[195] Modern alternative and indie music venues like The Black Cat and the 9:30 Club bring popular acts to the U Street area.[196] The hardcore punk scene in the city, known as D.C. hardcore, is an important genre of D.C.'s contemporary music scene. Starting in the 1970s and flourishing in the Adams Morgan neighborhood, it is considered to be one of the most influential punk music movements in the country.[197]
 Washington, D.C., is rich in fine and casual dining; some consider it among the country's best cities for dining.[198] The city has a diverse range of restaurants, including a wide variety of international cuisines. The city's Chinatown, for example, has more than a dozen Chinese-style restaurants. The city also has many Middle Eastern, European, African, Asian, and Latin American cuisine options.[citation needed] D.C. is known as one of the best cities in the world for Ethiopian cuisine, due largely to Ethiopian immigrants who arrived in the 20th century.[199] A part of the Shaw neighborhood in central D.C. is known as ""Little Ethiopia"" and has a high concentration of Ethiopian restaurants and shops.[200] The diversity of cuisine is also reflected in the city's many food trucks, which are particularly heavily concentrated along the National Mall, which has few other dining options.[citation needed]
 Among the most notable Washington, D.C.-born foods is the half-smoke, a half-beef, half-pork sausage placed in a hotdog-style bun and topped with onion, chili, and cheese.[201] The city is also the birthplace of mumbo sauce, a condiment similar to barbecue sauce but sweeter in flavor, often used on meat and french fries.[202][203] Washington, D.C. is known for popularizing the jumbo slice pizza, a large New York-style pizza[204][205][206] with roots in the Adams Morgan neighborhood.[207]
 Among the city's signature restaurants is Ben's Chili Bowl, located on U Street since its founding in 1958. The restaurant rose to prominence as a peaceful escape during the violent 1968 race riots in the city. Famous for its chili dogs and half-smokes, it has been visited by numerous presidents and celebrities over the years.[208] The Georgetown Cupcake bakery became famous through its appearance on the reality T.V. show DC Cupcakes. Another culinary hotspot is Union Market in Northeast D.C., a former farmer's market and wholesale that now houses a large, gourmet food hall.[209]
 As of 2024, 25 restaurants (including one in Virginia, The Inn at Little Washington) have received stars in the D.C. Michelin Guide.[210] This represents the most starred restaurants per capita for any U.S. city, and the third-most in the world.[211] Several celebrity chefs have opened restaurants in the city, including José Andrés,[212] Kwame Onwuachi,[213] Gordon Ramsay,[214][215] and previously Michel Richard.[216]
 Washington, D.C. is home to several of the country's and world's most visited museums. In 2022, the National Museum of Natural History and the National Gallery of Art were the two most visited museums in the country. Overall, Washington had eight of the 28 most visited museums in the U.S. in 2022. That year, the National Museum of Natural History was the fifth-most-visited museum in the world; the National Gallery of Art was the eleventh.[217]
 The Smithsonian Institution, an educational foundation chartered by Congress in 1846 and the world's largest research and museum complex, is responsible for maintaining most of the city's official museums and galleries.[218] The U.S. government partially funds the Smithsonian, and its collections are open to the public free of charge.[219] The Smithsonian's locations had a combined total of 30 million visits in 2013. The most visited museum is the National Museum of Natural History on the National Mall.[220] Other Smithsonian Institution museums and galleries on the Mall include the National Air and Space Museum; the National Museum of African Art; the National Museum of American History; the National Museum of the American Indian; the Sackler and Freer galleries, which focus on Asian art and culture; the Hirshhorn Museum and Sculpture Garden; the Arts and Industries Building; the S. Dillon Ripley Center; and the Smithsonian Institution Building, which serves as the institution's headquarters.[221]
 The Smithsonian American Art Museum and the National Portrait Gallery are housed in the Old Patent Office Building near Washington's Chinatown.[222] Renwick Gallery is part of the Smithsonian American Art Museum and is located in a separate building near the White House. Other Smithsonian museums and galleries include Anacostia Community Museum in Southeast Washington, the National Postal Museum near Washington Union Station, and the National Zoo in Woodley Park.[221]
 The National Gallery of Art is on the National Mall near the Capitol and features American and European artworks. The U.S. government owns the gallery and its collections. However, they are not a part of the Smithsonian Institution.[223] The National Building Museum, which occupies the former Pension Building near Judiciary Square, was chartered by Congress and hosts exhibits on architecture, urban planning, and design.[224] The Botanic Garden is a botanical garden and museum operated by the U.S. Congress that is open to the public.[225]
 There are several private art museums in Washington, D.C., that house major collections and exhibits open to the public, such as the National Museum of Women in the Arts and The Phillips Collection in Dupont Circle, the first museum of modern art in the United States.[226] Other private museums in Washington include the O Street Museum, the International Spy Museum, the National Geographic Society Museum, and the Museum of the Bible. The United States Holocaust Memorial Museum near the National Mall maintains exhibits, documentation, and artifacts related to the Holocaust.[227]
 The National Mall is a park near Downtown Washington that stretches nearly two miles from the Lincoln Memorial to the United States Capitol. The mall often hosts political protests, concerts, festivals, and presidential inaugurations. The Capitol grounds host the National Memorial Day Concert, held each Memorial Day, and A Capitol Fourth, a concert held each Independence Day. Both concerts are broadcast across the country on PBS. In the evening on the Fourth of July, the park hosts a large fireworks show.[228]
 The Washington Monument and the Jefferson Pier are near the center of the mall, south of the White House. Directly northwest of the Washington Monument is Constitution Gardens, which includes a garden, park, pond, and a memorial to the signers of the United States Declaration of Independence.[229] Just north of Constitution Gardens is the Lockkeeper's House, which is the second-oldest building on the mall after the White House. The house is operated by the National Park Service (NPS) and is open to the public. Also on the mall is the National World War II Memorial at the east end of the Lincoln Memorial Reflecting Pool; the Korean War Veterans Memorial; and the Vietnam Veterans Memorial.[230]
 South of the mall is the Tidal Basin, a human-made reservoir surrounded by pedestrian paths lined by Japanese cherry trees. Every spring, millions of cherry blossoms bloom, attracting visitors from across the world as part of the annual National Cherry Blossom Festival.[231] The Franklin Delano Roosevelt Memorial, George Mason Memorial, Jefferson Memorial, Martin Luther King Jr. Memorial, and the District of Columbia War Memorial are around the Tidal Basin.[230]
 Numerous historic landmarks are located outside the National Mall. Among these are the Old Post Office,[232] the Treasury Building,[233] Old Patent Office Building,[234] the National Cathedral,[235] the Basilica of the National Shrine of the Immaculate Conception,[236] the National World War I Memorial,[237] the Frederick Douglass National Historic Site,[238] Lincoln's Cottage,[239] the Dwight D. Eisenhower Memorial, and the United States Navy Memorial.[240] The Octagon House, which was the building that President James Madison and his administration moved into following the burning of the White House during the War of 1812, is now a historic museum and popular tourist destination.[241]
 The National Archives is headquartered in a building just north of the National Mall and houses thousands of documents important to American history, including the Declaration of Independence, the Constitution, and the Bill of Rights.[242] Located in three buildings on Capitol Hill, the Library of Congress is the largest library complex in the world with a collection of more than 147 million books, manuscripts, and other materials.[243] The United States Supreme Court is located immediately north of the Library of Congress. The United States Supreme Court Building was completed in 1935; before then, the court held sessions in the Old Senate Chamber of the Capitol.[244]
 Chinatown, located just north of the National Mall, houses Capital One Arena, which serves as the home arena to the Washington Capitals of the National Hockey League and the Washington Wizards of the National Basketball Association, and serves as the city's primary indoor entertainment arena. Chinatown includes several Chinese restaurants and shops. The Friendship Archway is one of the largest Chinese ceremonial archways outside of China and bears the Chinese characters for ""Chinatown"" below its roof.[245]
 The Southwest Waterfront along the Potomac River has been redeveloped in recent years and now serves as a popular cultural center. The Wharf, as it is called, contains the city's historic Maine Avenue Fish Market. This is the oldest fish market currently in operation in the entire United States.[246] The Wharf also has many hotels, residential buildings, restaurants, shops, parks, piers, docks and marinas, and live music venues.[117][118]
 As a result of its central role in United States history, the District of Columbia has many sites listed on the National Register of Historic Places.[citation needed]
 There are many parks, gardens, squares, and circles throughout Washington. The city has 683 parks and greenspaces, comprising almost a quarter of its land area.[247] Consequently, 99% of residents live within a 10-minute walk of a park.[248] According to the nonprofit Trust for Public Land, Washington ranked first among the 100 largest U.S. cities for its public parks, based on indicators such as accessibility, the share of land reserved for parks, and the amount invested in green spaces.[248]
 Rock Creek Park, located in Northwest D.C., is the largest park in the city and is administered by the National Park Service.[249] Located on the northern side of the White House, Lafayette Square is a historic public square. Named after the Marquis de Lafayette, a Frenchman who served as a commander during the American Revolutionary War, the square has been the site of many protests, marches, and speeches. The houses bordering Lafayette Square have served as the home to many notable figures, such as First Lady Dolley Madison and Abraham Lincoln's Secretary of State William H. Seward, who was stabbed by an intruder in his Lafayette Square house on the evening of President Lincoln's assassination.[250] Located next to the square and on Pennsylvania Avenue across from the White House is the Blair House, which serves as the primary state guest house for the U.S. president.[251]
 There are several river islands in Washington, D.C., including Theodore Roosevelt Island in the Potomac River, which hosts the Theodore Roosevelt National Memorial and a number of trails.[252] Columbia Island, also in the Potomac, is home to the Lyndon Baines Johnson Memorial Grove, the Navy – Merchant Marine Memorial, and a marina. Kingman Island, in the Anacostia River, is home to Langston Golf Course and a public park with trails.[253]
 West Potomac Park includes the parkland that extends south of the Lincoln Memorial Reflecting Pool, from the Lincoln Memorial to the grounds of the Washington Monument. The park is the site of several national landmarks including the Korean War Veterans Memorial, Jefferson Memorial, Franklin Delano Roosevelt Memorial, George Mason Memorial, and the Martin Luther King Jr. Memorial.[254]
 Other parks, gardens, and squares include Dumbarton Oaks, Meridian Hill Park, the Yards, Anacostia Park, Lincoln Park, Kenilworth Park and Aquatic Gardens, Franklin Square, McPherson Square, Farragut Square, and Chesapeake and Ohio Canal National Historical Park.[255] There are a large number of traffic circles and circle parks in Washington, D.C., including Dupont Circle, Logan Circle, Scott Circle, Sheridan Circle, Thomas Circle, Washington Circle, and others.[citation needed]
 The United States National Arboretum is a dense arboretum in Northeast D.C. filled with gardens and trails. Its most notable landmark is the National Capitol Columns monument.[256]
 Washington, D.C. is one of 13 cities in the United States with teams from the primary four major professional men's sports and is home to two major professional women's teams.[258] The Washington Nationals of Major League Baseball are the most popular sports team in the District, as of 2019.[259] They play at Nationals Park, which opened in 2008. The Washington Commanders of the National Football League play at Northwest Stadium in nearby Landover, Maryland. The Washington Wizards of the National Basketball Association and the Washington Capitals of the National Hockey League play at Capital One Arena in the city's Penn Quarter neighborhood. The Washington Mystics of the Women's National Basketball Association play at CareFirst Arena. D.C. United of Major League Soccer and the Washington Spirit of the National Women's Soccer League play at Audi Field.[citation needed]
 The city's teams have won a combined 14 professional league championships over their respective histories. The Washington Commanders (named the Washington Redskins until 2020), have won two NFL Championships and three Super Bowls;[260] D.C. United has won four;[261] and the Washington Wizards, then named the Washington Bullets, Washington Capitals, Washington Mystics, Washington Nationals, and Washington Spirit have each won a single championship.[262][263]
 Other professional and semi-professional teams in Washington, D.C. include DC Defenders of the XFL, Old Glory DC of Major League Rugby, the Washington Kastles of World TeamTennis, and the D.C. Divas of the Independent Women's Football League. The William H.G. FitzGerald Tennis Center in Rock Creek Park hosts the Washington Open, a joint men's ATP Tour 500- and women's WTA Tour 500-level tennis tournament, every summer in late July and early August. Washington, D.C. has two major annual marathon races, the Marine Corps Marathon, held every autumn, and the Rock 'n' Roll USA Marathon, held each spring. The Marine Corps Marathon began in 1976 and is sometimes called ""The People's Marathon"" because it is the largest marathon that does not offer prize money to participants.[264]
 The district's four NCAA Division I teams are the American Eagles of American University, George Washington Revolutionaries of George Washington University, the Georgetown Hoyas of Georgetown University, and the Howard Bison and Lady Bison of Howard University. The Georgetown men's basketball team is the most notable and also plays at Capital One Arena. Washington, D.C. area's regional sports television network is Monumental Sports Network, and was known as NBC Sports Washington until September 2023.[265]
 Article One, Section Eight of the United States Constitution grants the United States Congress ""exclusive jurisdiction"" over the city. The district did not have an elected local government until the passage of the 1973 Home Rule Act. The Act devolved certain Congressional powers to an elected mayor and the thirteen-member Council of the District of Columbia. However, Congress retains the right to review and overturn laws created by the council and intervene in local affairs.[266] Washington, D.C., is overwhelmingly Democratic, having voted for the Democratic presidential candidate solidly since it was granted electoral votes in 1964.[267]
 Each of the city's eight wards elects a single member of the council and residents elect four at-large members to represent the district as a whole. The council chair is also elected at-large.[268] There are 37 Advisory Neighborhood Commissions (ANCs) elected by small neighborhood districts. ANCs can issue recommendations on all issues that affect residents; government agencies take their advice under careful consideration.[269] The attorney general of the District of Columbia is elected to a four-year term.[270]
 Washington, D.C., observes all federal holidays and also celebrates Emancipation Day on April 16, which commemorates the end of slavery in the district.[43] The flag of Washington, D.C., was adopted in 1938 and is a variation on George Washington's family coat of arms.[271]
 Washington, D.C., has been a member state of the Unrepresented Nations and Peoples Organization (UNPO) since 2015.[272]
 The idiom ""Inside the Beltway"" is a reference used by media to describe discussions of national political issues inside of Washington, by way of geographical demarcation regarding the region within the Capital's Beltway, Interstate 495, the city's highway loop (beltway) constructed in 1964. The phrase is used as a title for a number of political columns and news items by publications like The Washington Times.[273]
 The mayor and council set local taxes and a budget, which Congress must approve. The Government Accountability Office and other analysts have estimated that the city's high percentage of tax-exempt property and the Congressional prohibition of commuter taxes create a structural deficit in the district's local budget of anywhere between $470 million and over $1 billion per year. Congress typically provides additional grants for federal programs such as Medicaid and the operation of the local justice system; however, analysts claim that the payments do not fully resolve the imbalance.[274][275]
 The city's local government, particularly during the mayoralty of Marion Barry, has been criticized for mismanagement and waste.[276] During his administration in 1989, Washington Monthly magazine labeled the district ""the worst city government in America"".[277] In 1995, at the start of Barry's fourth term, Congress created the District of Columbia Financial Control Board to oversee all municipal spending.[278] Mayor Anthony Williams won election in 1998 and oversaw a period of urban renewal and budget surpluses.[citation needed]
 The district regained control over its finances in 2001 and the oversight board's operations were suspended.[279]
 The district has a federally funded ""Emergency Planning and Security Fund"" to cover security related to visits by foreign leaders and diplomats, presidential inaugurations, protests, and terrorism concerns. During the Trump administration, the fund has run with a deficit. Trump's January 2017 inauguration cost the city $27 million; of that, $7 million was never repaid to the fund. Trump's 2019 Independence Day event, ""A Salute to America"", cost six times more than Independence Day events in past years.[280]
 The city passed a law that requires shelter to be provided to everyone in need when the temperature drops below freezing.[281] Since D.C. does not have enough shelter units available, every winter it books hotel rooms in the suburbs with an average cost of around $100 for a night. According to the D.C. Department of Human Services, during the winter of 2012 the city spent $2,544,454 on putting homeless families in hotels,[282] and budgeted $3.2 million on hotel beds in 2013.[283]
 Congress controlled the Federal District from its establishment, and did not make a provision for federal representation of the people living there. That changed in 1961, when the 23rd amendment was ratified by the states, and Washington, D.C. was granted three electoral college votes in each presidential election. In 1978 another amendment was passed but not ratified by the states to grant D.C. congressional representation. In 2021, a bill to make D.C. a state passed the House of Representatives but not Senate. Congress has the power to add a state, but an amendment must be ratified by the states.
 Washington, D.C. is not a state and therefore has no federal voting representation in Congress. The city's residents elect a non-voting delegate to the House of Representatives (D.C. at-large), who may sit on committees, participate in debate, and introduce legislation, but cannot vote on the House floor. The district has no official representation in the United States Senate. Neither chamber seats the district's elected ""shadow"" representative or senators. Unlike residents of U.S. territories such as Puerto Rico or Guam, which also have non-voting delegates, D.C. residents are subject to all federal taxes.[284] In the financial year 2012, D.C. residents and businesses paid $20.7 billion in federal taxes, more than the taxes collected from 19 states and the highest federal taxes per capita.[285]
 A 2005 poll found that 78% of Americans did not know residents of Washington, D.C., have less representation in Congress than residents of the 50 states.[286] Efforts to raise awareness about the issue have included campaigns by grassroots organizations and featuring the city's unofficial motto, ""End Taxation Without Representation"", on D.C. vehicle license plates.[287] There is evidence of nationwide approval for D.C. voting rights; various polls indicate that 61 to 82% of Americans believe D.C. should have voting representation in Congress.[286][288]
 Opponents to federal voting rights for Washington, D.C., propose that the Founding Fathers never intended for district residents to have a vote in Congress since the Constitution makes clear that representation must come from the states. Those opposed to making the District of Columbia a state say such a move would destroy the notion of a separate national capital and that statehood would unfairly grant Senate representation to a single city.[289]
 The District was granted presidential voting rights by the 23rd Amendment in 1961.[290] The 23rd Amendment was ratified which granted the people of the Washington, D.C., the right to vote for the president. This was done by giving them electoral college votes they would get if they were a state, but it must be no more than the least a state has; this works out to three electoral college votes. The amendment reads, "".. A number of electors of President and Vice President equal to the whole number of Senators and Representatives in Congress to which the District would be entitled if it were a State, but in no event more than the least populous State"".[291] The 23rd Amendment could complicate statehood, because it would apply even if the federal district was shrunk, and undoing the amendment requires another amendment.[61] Congress must operate from a district it controls, but it can be no larger than ten miles on a side; the 2021 statehood bill got around this by proposing the federal district be shrunk to an area roughly the size of the national mall.[61]
 In 2021, Senator Joe Manchin made known his non-support of the D.C. Statehood bill that had passed the House of Representatives, and suggested that D.C. could instead be given statehood by constitutional amendment.[292] He was concerned about complications from the 23rd amendment and said the bill would likely end up in court, adding that the correct way was to propose a constitutional amendment and let the nation vote on it.[292]
 In 1978, the District of Columbia Voting Rights Amendment was passed, which would have granted D.C. Congressional representation, but it expired in 1986 without being ratified into law.[293]
 In 2021, a bill was introduced to congress for retroceding the district to Maryland.[294] The idea was that by returning the area to Maryland, the residents would have normal State representation.[295]
 District of Columbia Public Schools (DCPS), the sole public school district in the city,[296] operates the city's 123 public schools.[297] The number of students in DCPS steadily decreased for 39 years until 2009. In the 2010–11 school year, 46,191 students were enrolled in the public school system.[298] DCPS has one of the highest-cost, yet lowest-performing school systems in the country, in terms of both infrastructure and student achievement.[299] Mayor Adrian Fenty's administration made sweeping changes to the system by closing schools, replacing teachers, firing principals, and using private education firms to aid curriculum development.[300]
 The District of Columbia Public Charter School Board monitors the 52 public charter schools in the city.[301] Due to the perceived problems with the traditional public school system, enrollment in public charter schools had by 2007 steadily increased.[302] As of 2010, D.C., charter schools had a total enrollment of about 32,000, a 9% increase from the prior year.[298] The district is also home to 92 private schools, which enrolled approximately 18,000 students in 2008.[303]
 The University of the District of Columbia (UDC) is a public land-grant university providing undergraduate and graduate education.[304] Federally chartered universities include American University (AU), Gallaudet University, George Washington University (GWU), Georgetown University (GU), and Howard University (HU). Other private universities include the Catholic University of America (CUA), the Johns Hopkins University Paul H. Nitze School of Advanced International Studies (SAIS), and Trinity Washington University. The Corcoran College of Art and Design, the oldest art school in the capital, was absorbed into the George Washington University in 2014, now serving as its college of arts.[305]
 The city's medical research institutions include Washington Hospital Center and Children's National Medical Center. The city is home to three medical schools and associated teaching hospitals: George Washington, Georgetown, and Howard universities.[306]
 Washington, D.C., has dozens of public and private libraries and library systems, including the District of Columbia Public Library system.[citation needed] Folger Shakespeare Library, a research library and museum located in the Capitol Hill neighborhood, houses the world's largest collection of material related to William Shakespeare.[307]
 The Library of Congress is the research library that officially serves the United States Congress and is the de facto national library of the United States. It is a complex of three buildings: Thomas Jefferson Building, John Adams Building and James Madison Memorial Building, all located in the Capitol Hill neighborhood. The Jefferson Building houses the library's reading room, a copy of the Gutenberg Bible, Thomas Jefferson's original library, and several museum exhibits.[citation needed]
 The District of Columbia Public Library operates 26 neighborhood locations including the landmark Martin Luther King Jr. Memorial Library.[309]
 Washington, D.C., is a prominent center for national and international media. The Washington Post, founded in 1877, is the city's most-read local daily newspaper[citation needed] and one of the preeminent newspapers in the United States.[310] It had the sixth-highest readership of all news dailies in the country in 2011.[311] The Post previously also published the Spanish-language newspaper El Tiempo Latino, which it sold to El Planeta Media in 2016.[312]
 The Washington Times is a general interest daily newspaper and popular among conservatives.[313] The alternative weekly Washington City Paper, with a circulation of 47,000, is also based in the city and has a substantial readership in the Washington area.[314][315]
 The Atlantic magazine, which has covered politics, international affairs, and cultural issues since 1857, was previously headquartered at the Watergate complex but is now headquartered in a building at the Wharf in Washington.[316]
 Several community and specialty papers focus on neighborhood and cultural issues, including the weekly Washington Blade and Metro Weekly, which focus on LGBT issues; the Washington Informer and The Washington Afro American, which highlight topics of interest to the black community; and neighborhood newspapers published by The Current Newspapers. Congressional Quarterly, The Hill, Politico, and Roll Call newspapers focus exclusively on issues related to Congress and the federal government. Other publications based in Washington include the National Geographic magazine and political publications such as The Washington Examiner, The New Republic, and Washington Monthly.[317] USA Today, which is the largest newspaper in the country as measured by circulation, is headquartered in nearby Tysons, Virginia.[318][319]
 The Washington metropolitan area is the ninth-largest television media market in the nation, with two million homes, representing approximately 2% of the country's television market.[320] Several media companies and cable television channels have their headquarters in the area, including C-SPAN, Radio One, the National Geographic Channel, Smithsonian Networks, National Public Radio (NPR), Travel Channel (in Chevy Chase, Maryland), Discovery Communications (in Silver Spring, Maryland), and PBS (in Arlington County, Virginia). The headquarters of Voice of America, the U.S. government's international news service, is near the Capitol in Southwest Washington, D.C.[321]
 The city is served by two local NPR affiliates, WAMU and WETA.[322]
 There are 1,500 miles (2,400 km) of streets, parkways, and avenues in the district.[323] Due to the freeway revolts of the 1960s, much of the proposed interstate highway system through the middle of Washington was never built. Interstate 95 (I-95), the nation's major east coast highway, therefore bends around the district to form the eastern portion of the Capital Beltway. A portion of the proposed highway funding was directed to the region's public transportation infrastructure instead.[324] The interstate highways that continue into Washington, including I-66 and I-395, both terminate shortly after entering the city.[325]
 According to a 2010 study, Washington-area commuters spent 70 hours a year in traffic delays, which tied with Chicago for having the nation's worst road congestion.[326] However, 37% of Washington-area commuters take public transportation to work, the second-highest rate in the country.[327] An additional 12% of D.C. commuters walked to work, 6% carpooled, and 3% traveled by bicycle in 2010.[328]
 In May 2022, the city celebrated the expansion of its bike lane network to 104 miles (167 km), a 60 percent increase from 2015. Of those miles, 24 miles (39 km) were protected bike lanes. It also boasted 62 miles (100 km) of bike trails.[329] As of March 2023[update], the city has 108 miles (174 km) of bike lanes, with 30 miles (48 km) of them protected bike lanes.[330]
 D.C. is part of the regional Capital Bikeshare program. Started in 2010, it is one of the largest bicycle sharing systems in the country. As of February 2024[update], the program had 6,372 bicycles and 395 stations.[331] A preceding SmartBike DC pilot program had begun in 2008.[332]
 A 2021 study by Walk Score ranked Washington, D.C. the fifth-most walkable city in the country. According to the study, the most walkable neighborhoods are U Street, Dupont Circle, and Mount Vernon Square.[333] In 2013, the Washington Metropolitan Area had the eighth lowest percentage of workers who commuted by private automobile (75.7 percent), with 8 percent of area workers traveling via rail transit.[334]
 There are multiple transportation methods to cross the city's two rivers, the Potomac River and the Anacostia River. There are numerous bridges that take cars, trains, pedestrians, and bikers across the rivers. Among these are Arlington Memorial Bridge, the 14th Street Bridges, Francis Scott Key Bridge, Theodore Roosevelt Bridge, Woodrow Wilson Bridge, and Frederick Douglass Bridge.[335]
 There are also ferries and water cruises that cross the Potomac River. One of these is the Potomac Water Taxi, operated by Hornblower Cruises, which goes between the Georgetown Waterfront, the Wharf, the Old Town Alexandria Waterfront, and National Harbor.[336]
 The Washington Metropolitan Area Transit Authority (WMATA) operates the Washington Metro, the city's rapid transit rail system. The system serves Washington, D.C. and its Maryland and Northern Virginia suburbs. Metro opened on March 27, 1976, and consists of six lines (each one color coded), 98 stations, and 129 miles (208 km) of track.[337] Metro is the second-busiest rapid transit system in the country and fifth-busiest in North America.[338] It operates mostly as a deep-level subway in more densely populated parts of the D.C. metropolitan area (including most of the District itself), while most of the suburban tracks are at surface level or elevated. Metro is known for its iconic brutalist-style vaulted ceilings in the interior stations. It is also known for having long escalators in some of its underground stations. The longest single-tier escalator in the Western Hemisphere, spanning 230 feet (70 m), is located at Metro's Wheaton station in Maryland.[339]
 Washington Union Station is the city's main train station and serves approximately 70,000 people each day. It is Amtrak's second-busiest station with 4.6 million passengers annually and is the southern terminus for the Northeast Corridor, which carries long-distance and regional services to New York Penn Station and points in New England. As of 2023, Union Station is the ninth-busiest rail station in the nation and tenth-busiest in North America.[citation needed]
 Maryland's MARC and Virginia's VRE commuter trains and the Metrorail Red Line also provide service into Union Station.[340] Following renovations in 2011, Union Station became Washington's primary intercity bus transit center.[341]
 Although Washington, D.C. was known throughout the 19th and early- to mid-20th centuries for its streetcars, these lines were dismantled in the 1960s. In 2016, however, the city brought back a streetcar line, DC Streetcar, which is a single line system in Northeast Washington, D.C., along H Street and Benning Road, known as the H Street/Benning Road Line.[342]
 Two main public bus systems operate in Washington, D.C. Metrobus, operated by the Washington Metropolitan Area Transit Authority (WMATA), is the primary public bus system in Washington, D.C. Serving more than 400,000 riders each weekday, it is one of the nation's largest bus systems by annual ridership.[343] The city also operates its own DC Circulator bus system, which connects commercial and touristic areas within central Washington.[344] The DC Circulator costs only $1 to ride and is composed of six distinct routes that cover central D.C. and suburban Rosslyn, Virginia. The DC Circulator is run via a public-private partnership between the District of Columbia Department of Transportation, WMATA, and DC Surface Transit, Inc. (DCST). The bus system services each stop approximately every 10 minutes.[345]
 Many other public bus systems operate in the various jurisdictions of the Washington region outside of the city in suburban Maryland and Virginia. Among these are the Fairfax Connector in Fairfax County, Virginia; DASH in Alexandria, Virginia; and TheBus in Prince George's County, Maryland.[346] There are also numerous commuter buses that residents of the wider Washington region take to commute into the city for work or other events. Among these are the Loudoun County Transit Commuter Bus and the Maryland Transit Administration Commuter Bus.[347]
 The city also has several bus lines used by tourists and others visiting the city, including Big Bus Tours, Old Town Trolley Tours, and DC Trails. Many tourists also arrive via charter buses.[citation needed]
 Three major airports serve the district, though none are within the city's borders. Two of these major airports are located in suburban Northern Virginia and one in suburban Maryland. The closest is Ronald Reagan Washington National Airport, which is located in Arlington County, Virginia, just across the Potomac River about 5 miles (8 km) from downtown Washington, D.C. This airport provides primarily domestic flights and has the lowest number of passengers of the three airports in the region. The busiest by number of total passengers is Baltimore/Washington International Airport (BWI), located in Anne Arundel County, Maryland about 30 miles (48 km) northeast of the city.[348] The busiest by international flights and the largest by land size and amount of facilities is Washington Dulles International Airport, located in Dulles, Virginia, about 24 miles (39 km) west of the city.[349] Dulles has the most international passenger traffic of any airport in the Mid-Atlantic outside the New York metropolitan area, including approximately 90% of the international passenger traffic in the Washington-Baltimore region.[350] Each of these three airports also serves as a hub for a major American airline: Reagan National Airport is a hub for American Airlines,[351] Dulles is a major hub for United Airlines and Star Alliance partners,[352] and BWI is an operating base for Southwest Airlines.[353] In 2018, the Washington, D.C. area was the 18th-busiest airport system in the world by passenger traffic, accumulating over 74 million passengers between its three main commercial airports; by 2022 it had climbed to 13th-busiest for passenger traffic, even though passenger numbers decreased to less than 69 million.[citation needed]
 The President of the United States does not use any of these airports for travel. Instead, the U.S. president typically travels by Marine One from the South Lawn of the White House to Joint Base Andrews in suburban Maryland. From there, he takes Air Force One to his destination.[354]
 The District of Columbia Water and Sewer Authority, also known as WASA or D.C. Water, is an independent authority of the Washington, D.C., government that provides drinking water and wastewater collection in the city. WASA purchases water from the historic Washington Aqueduct, which is operated by the Army Corps of Engineers. The water, sourced from the Potomac River, is treated and stored in the city's Dalecarlia, Georgetown, and McMillan reservoirs. The aqueduct provides drinking water for a total of 1.1 million people in the district and Virginia, including Arlington, Falls Church, and a portion of Fairfax County.[356] The authority also provides sewage treatment services for an additional 1.6 million people in four surrounding Maryland and Virginia counties.[357]
 Pepco is the city's electric utility and services 793,000 customers in the district and suburban Maryland.[358] An 1889 law prohibits overhead wires within much of the historic City of Washington. As a result, all power lines and telecommunication cables are located underground in downtown Washington, and traffic signals are placed at the edge of the street.[359] A 2013 plan would bury an additional 60 miles (97 km) of primary power lines throughout the district.[360]
 Washington Gas is the city's natural gas utility and serves more than a million customers in the district and its suburbs. Incorporated by Congress in 1848, the company installed the city's first gas lights in the Capitol, White House, and along Pennsylvania Avenue.[361]
 Washington has historically endured high crime, particularly violent offences. The city was once described as the ""murder capital"" of the United States during the early 1990s.[362] The number of murders peaked in 1991 at 479, but then began to decline,[363] reaching a historic low of 88 in 2012, the lowest total since 1961.[364] In 2016, the district's Metropolitan Police Department tallied 135 homicides, a 53% increase from 2012 but a 17% decrease from 2015.[365] By 2019, citywide reports of both property and violent crimes declined from their most recent highs in the mid-1990s.[366][better source needed] However, both 2021 and 2022 saw over 200 homicides each, reflecting an upward trends from prior decades.[367] In 2023, D.C. recorded 274 homicides, a 20-year high and the fifth-highest murder rate among the nation's largest cities.[368]
 Many D.C. residents began to press the city government for refusing to prosecute nearly 70% of arrested offenders in 2022. After months of criticism, the rate of unprosecuted cases dropped to 56% by October 2023—albeit still higher than nine of the past 10 years and almost twice what it was in 2013.[369] In February 2024, the Council of the District of Columbia passed a major bill meant to reduce crime in the city by introducing harsher penalties for arrested offenders.[370] Rising crime and gang activities contributed to some local businesses leaving the city.[371][372]
 According to a 2018 report, 67,000 residents, or about 10% of the population, are ex-convicts.[373] An estimated 2,000–2,500 offenders return to the city from prison every year.[374]
 On June 26, 2008, the Supreme Court of the United States held in District of Columbia v. Heller that the city's 1976 handgun ban violated the right to keep and bear arms as protected under the Second Amendment.[375] However, the ruling does not prohibit all forms of gun control; laws requiring firearm registration remain in place, as does the city's assault weapon ban.[376]
 In addition to the Metropolitan Police Department, several federal law enforcement agencies have jurisdiction in the city, including the U.S. Park Police, founded in 1791.[377] Because the D.C. National Guard serves a federal district, the president of the United States—and not city officials—has power to deploy it. The president also has the power to take over the police force in emergency situations.[378]
 Washington, D.C., has fifteen official sister city agreements. Each of the listed cities is a national capital except for Sunderland, which includes the town of Washington, the ancestral home of George Washington's family.[379] Paris and Rome are each formally recognized as a partner city due to their special one sister city policy.[380] Listed in the order each agreement was first established, they are:
"
State of Washington,https://en.wikipedia.org/wiki/State_of_Washington,"
 

 Washington, officially the State of Washington,[5] is a state in the Pacific Northwest region of the United States. It is often referred to as Washington state[a] to distinguish it from the national capital,[6] both named after George Washington (the first U.S. president). Washington borders the Pacific Ocean to the west, Oregon to the south, Idaho to the east, and shares an international border with the Canadian province of British Columbia to the north. Olympia is the state capital, and the most populous city is Seattle.
 Washington is the 18th-largest state, with an area of 71,362 square miles (184,830 km2), and the 13th-most populous state, with a population of just less than 8 million.[7] The majority of Washington's residents live in the Seattle metropolitan area, the center of transportation, business, and industry on Puget Sound,[8][9] an inlet of the Pacific Ocean consisting of numerous islands, deep fjords and bays carved out by glaciers. The remainder of the state consists of deep temperate rainforests in the west; mountain ranges in the west, center, northeast, and far southeast; and a semi-arid basin region in the east, center, and south, given over to intensive agriculture. Washington is the second most populous state on the West Coast and in the Western United States, after California. Mount Rainier, an active stratovolcano, is the state's highest elevation at 14,411 feet (4,392 meters), and is the most topographically prominent mountain in the contiguous U.S.
 Washington is a leading lumber producer, the largest producer of apples, hops, pears, blueberries, spearmint oil, and sweet cherries in the U.S., and ranks high in the production of apricots, asparagus, dry edible peas, grapes, lentils, peppermint oil, and potatoes.[10][11] Livestock, livestock products, and commercial fishing—particularly of salmon, halibut, and bottomfish—are also significant contributors to the state's economy.[12] Washington ranks second only to California in wine production. Manufacturing industries in Washington include aircraft, missiles, shipbuilding, and other transportation equipment, food processing, metals, and metal products, chemicals, and machinery.[13]
 The state was formed from the western part of the Washington Territory, which was ceded by the British Empire in the Oregon Treaty of 1846. It was admitted to the Union as the 42nd state in 1889. One of the wealthiest and most socially liberal states in the country,[14] Washington consistently ranks among the top states for highest life expectancy and employment rates.[15] It was one of the first states (alongside Colorado) to legalize medicinal and recreational cannabis,[16] was among the first states to introduce same-sex marriage,[17] and was one of only four states to have provided legal abortions on request before Roe v. Wade in 1973.[18] Washington voters also approved a 2008 referendum on the legalization of physician-assisted suicide,[19] making it one of 10 states to have legalized the practice.[20]
 Washington was named after President George Washington by an act of the United States Congress during the creation of Washington Territory in 1853; the territory was to be named ""Columbia"", for the Columbia River and the Columbia District, but Kentucky representative Richard H. Stanton found the name too similar to the District of Columbia (the national capital, itself containing the city of Washington), and proposed naming the new territory after President Washington.[21][22][23] Washington is the only U.S. state named after a president.[24]
 Confusion between the state of Washington and the city of Washington, D.C., led to renaming proposals during the statehood process for Washington in 1889, including David Dudley Field II's suggestion to name the new state ""Tacoma""; these proposals failed to garner support.[25] Washington, D.C.'s, own statehood movement in the 21st century has included a proposal to use the name ""State of Washington, Douglass Commonwealth"", which would conflict with the current state of Washington.[5] Residents of Washington (known as ""Washingtonians"") and the Pacific Northwest simply refer to the state as ""Washington"", and the nation's capital ""Washington, D.C."", ""the other Washington"",[26] or simply ""D.C.""
 The 9,300-year-old skeletal remains of Kennewick Man, one of the oldest and most complete human remains found in North America, were discovered in Washington in the 1990s.[27] The region has been home to many established tribes of indigenous peoples for thousands of years. They are notable for their ornately carved welcome figures, canoes, long houses and masks. Prominent among their industries were salmon fishing and, notably among the Makah, whale hunting.[28][29] The peoples of the Interior had a different subsistence-based culture based on hunting, food-gathering and some forms of agriculture, as well as a dependency on salmon from the Columbia and its tributaries.
 The area has been known to host megathrust earthquakes in the past, the last being the Cascadia earthquake of 1700.[30]
 The first recorded European landing on the Washington coast was by Spanish Captain Don Bruno de Heceta in 1775,[31] on board the Santiago, part of a two-ship flotilla with the Sonora. He claimed the coastal lands up to Prince William Sound for Spain as part of their claimed rights under the Treaty of Tordesillas, which they maintained made the Pacific a ""Spanish lake"" and all its shores part of the Spanish Empire. Soon thereafter, the smallpox epidemic of the 1770s devastated the Native American population.[32]
 In 1778, British explorer Captain James Cook sighted Cape Flattery, at the entrance to the Strait of Juan de Fuca, but Cook did not realize the strait existed.[33] It was not discovered until Charles William Barkley, captain of the Imperial Eagle, sighted it in 1787.[34] The straits were further explored by Spanish explorers Manuel Quimper in 1790 and Francisco de Eliza in 1791,[35][36] and British explorer George Vancouver in 1792.[37]
 The British–Spanish Nootka Convention of 1790 ended Spanish claims of exclusivity and opened the Northwest Coast to explorers and traders from other nations, most notably Britain and Russia as well as the fledgling United States.[38][39] American captain Robert Gray (for whom Grays Harbor County is named) then discovered the mouth of the Columbia River. He named the river after his ship, the Columbia.[40] Beginning in 1792, Gray established trade in sea otter pelts. The Lewis and Clark Expedition entered the state on October 10, 1805.[41]
 Explorer David Thompson, on his voyage down the Columbia River, camped at the confluence with the Snake River on July 9, 1811,[42] and erected a pole and a notice claiming the territory for Great Britain and stating the intention of the North West Company to build a trading post at the site.
 Britain and the United States agreed to what has since been described as ""joint occupancy"" of lands west of the Continental Divide to the Pacific Ocean as part of the Anglo-American Convention of 1818, which established the 49th parallel as the international boundary west from Lake of the Woods to the Rocky Mountains.[43] Resolution of the territorial and treaty issues west to the Pacific was deferred until a later time. In 1819, Spain ceded its rights north of the 42nd parallel to the United States.[44]
 Negotiations with Great Britain over the next few decades failed to settle upon a compromise boundary and the Oregon boundary dispute was highly contested between Britain and the United States. Disputed joint occupancy by Britain and the U.S. lasted for several decades. With American settlers pouring into Oregon Country, Hudson's Bay Company, which had previously discouraged settlement because it conflicted with the fur trade, reversed its position in an attempt to maintain British control of the Columbia District.[45]
 Fur trapper James Sinclair, on orders from Sir George Simpson, Governor of the Hudson's Bay Company, led some 200 settlers from the Red River Colony west in 1841 to settle on Hudson Bay Company farms near Fort Vancouver.[46] The party crossed the Rockies into the Columbia Valley, near present-day Radium Hot Springs, British Columbia, then traveled south-west down the Kootenai River and Columbia River. Despite such efforts, Britain eventually ceded all claims to land south of the 49th parallel to the United States in the Oregon Treaty on June 15, 1846.[47]
 In 1836, a group of missionaries, including Marcus Whitman, established several missions and Whitman's own settlement Waiilatpu, in what is now southeastern Washington state, near present-day Walla Walla County, in the territory of both the Cayuse and the Nez Perce Indian tribes.[48] Whitman's settlement would in 1843 help the Oregon Trail, the overland emigration route to the west, get established for thousands of emigrants in the following decades. Whitman provided medical care for the Native Americans, but when Indian patients—lacking immunity to new, ""European"" diseases—died in striking numbers, while at the same time many white patients recovered, they held ""medicine man"" Marcus Whitman personally responsible, and executed Whitman and twelve other white settlers. This was called the Whitman massacre in 1847.[49] This event triggered the Cayuse War between settlers and Indians.
 Fort Nisqually, a farm and trading post of the Hudson's Bay Company and the first European settlement in the Puget Sound area, was founded in 1833.[50] Black pioneer George Washington Bush and his Caucasian wife, Isabella James Bush, from Missouri and Tennessee, respectively, led four white families into the territory and founded New Market, now Tumwater, in 1846.[51] They settled in Washington to avoid Oregon's black exclusion law, which prohibited African Americans from entering the territory while simultaneously prohibiting slavery.[52][53] After them, many more settlers, migrating overland along the Oregon Trail, wandered north to settle in the Puget Sound area.
 Spanish and Russian claims to the region were ceded in the early 19th century through a series of treaties. The Spanish signed the Adams–Onís Treaty of 1819, and the Russians the Russo-American Treaty of 1824 and 1825.
 The Oregon Question remained contested between the United Kingdom and the United States until the 1846 Oregon Treaty established the border between British North America and the United States along the 49th parallel until the Strait of Georgia.[47] Vague wording in the treaty left the ownership of the San Juan Islands in doubt; during the so-called Pig War, both nations agreed to a joint military occupation of the islands.[54] Kaiser Wilhelm I of the German Empire was selected as an arbitrator to end the dispute, with a three-man commission ruling in favor of the United States in 1872. The border established by the Oregon Treaty and finalized by the arbitration in 1872 remains the boundary between Washington and British Columbia.
 The growing population of Oregon Territory north of the Columbia River formally requested a new territory. As a result of the Monticello Convention, held in present-day Cowlitz County, the U.S. Congress passed legislation to create Washington Territory. It was signed into law by President Millard Fillmore on March 2, 1853.[55][23] The boundary of Washington Territory initially extended farther east than the present state, including what is now the Idaho panhandle and parts of western Montana, and picked up more land to the southeast that was left behind when Oregon was admitted as a state; the creation of Idaho Territory in 1863 established the final eastern border. A Washington state constitution was drafted and ratified in 1878, but it was never officially adopted.[56] Although never approved by the United States Congress, the 1878 constitution is an important historical document that shows the political thinking of the time; it was used extensively during the drafting of Washington state's 1889 constitution, the one and only official Constitution of the State of Washington. Washington became the 42nd state of the United States on November 11, 1889.[57]
 Early prominent industries in the new state included agriculture and lumber. In Eastern Washington, the Yakima River Valley became known for its apple orchards,[58] while the growth of wheat using dry farming techniques became particularly productive. Heavy rainfall to the west of the Cascade Range produced dense forests, and the ports along Puget Sound prospered from the manufacturing and shipping of lumber products, particularly the Douglas fir. Other industries that developed in the state included fishing, salmon canning and mining.[12][59]
 For a long period, Tacoma had large smelters where gold, silver, copper, and lead ores were treated.[60] Seattle was the primary port for trade with Alaska and the rest of the country, and for a time, it possessed a large shipbuilding industry. The region around eastern Puget Sound developed heavy industry during the period including World War I and World War II, and the Boeing company became an established icon in the area.[61]
 During the Great Depression, a series of hydroelectric dams were constructed along the Columbia River as part of a project to increase the production of electricity. This culminated in 1941 with the completion of the Grand Coulee Dam, the largest concrete structure in the United States and the largest dam in the world at its construction.[62]
 During World War II, the state became a focus for war industries. While the Boeing Company produced many heavy bombers, ports in Seattle, Bremerton, Vancouver, and Tacoma were available for the manufacture of warships. Seattle was the point of departure for many soldiers in the Pacific, several of whom were quartered at Fort Lawton, which later became Discovery Park.[63] In Eastern Washington, the Hanford Works atomic energy plant was opened in 1943 and played a major role in the construction of atomic bombs.[64]
 After the end of World War II, and with the beginning of the civil rights movement, the state's growing Black or African-American population's wages were 53% above the national average. The early diversification of Washington through the Great Migration led to successful efforts at reducing discrimination in the workplace.[65][66] In 1950, Seattle's first black representative for the state's legislature was elected. At the 1970 U.S. census, the black population grew to 7.13% of the total population.[67]
 In 1970, the state was one of only four U.S. states to have been providing legal abortions before the 1973 Supreme Court decision in Roe v. Wade which loosened abortion laws nationwide.[18][68]
 On May 18, 1980, following a period of heavy tremors and small eruptions, the north face of Mount St. Helens slid off in the largest landslide in recorded history before erupting violently, destroying a large part of the top of the volcano. The eruption flattened the forest up to 12 mi (20 km) north of the volcano, killed 57 people, flooded the Columbia River and its tributaries with ash and mud, and blanketed large parts of Washington eastward and other surrounding states in ash, making day look like night.[69][70]
 Washington is the northwesternmost state of the contiguous United States. It borders Idaho to the east, bounded mostly by the meridian running north from the confluence of the Snake River and Clearwater River (about 117°02'23"" west), except for the southernmost section where the border follows the Snake River. Oregon is to the south, with the Columbia River forming the western part and the 46th parallel forming the eastern part of the Oregon–Washington border. During Washington's partition from Oregon, the original plan for the border followed the Columbia River east until the confluence with the Snake, and then would have followed the Snake River east; this was changed to keep Walla Walla's fertile farmland in Washington.
 To the west of Washington lies the Pacific Ocean.[71] Its northern border lies mostly along the 49th parallel, and then via marine boundaries through the Strait of Georgia, Haro Strait, and Strait of Juan de Fuca, with the Canadian province of British Columbia to the north.[72]
 Washington is part of a region known as the Pacific Northwest, a term which always refers to at least Washington and Oregon, and may or may not include some or all the following, depending on the user's intent: Idaho, western Montana, northern California, British Columbia, and Alaska.
 The high mountains of the Cascade Range run north–south, bisecting the state. In addition to Western Washington and Eastern Washington, residents call the two parts of the state the ""Westside"" and the ""Eastside"", ""Wet side"" and ""Dry side"", or ""Timberland"" and ""Wheatland"", the latter pair more commonly in the names of region-specific businesses and institutions. These terms reflect the geography, climate, and industry of the land on both sides of the Cascades.
 From the Cascade Mountains westward, Western Washington has a mostly Mediterranean climate, with mild temperatures and wet winters, autumns and springs, and relatively dry summers. The Cascade Range has several volcanoes, which reach altitudes significantly higher than the rest of the mountains. From north to south, these major volcanoes are Mount Baker, Glacier Peak, Mount Rainier, Mount St. Helens, and Mount Adams. All are active volcanoes.[73]
 Mount Rainier—the tallest mountain in the state[74]—is 50 miles (80 km) south of the city of Seattle, from which it is prominently visible. The United States Geological Survey considers 14,411-foot-tall (4,392 m) Mount Rainier the most dangerous volcano in the Cascade Range, due to its proximity to the Seattle metropolitan area, and most dangerous in the continental U.S. according to the Decade Volcanoes list.[75] It is also covered with more glacial ice than any other peak in the contiguous 48 states.[76]
 Western Washington also is home of the Olympic Mountains, far west on the Olympic Peninsula, which support dense forests of conifers and areas of temperate rainforest. These deep forests, such as the Hoh Rainforest, are among the only rainforests in the continental United States.[77] While Western Washington does not always experience a high amount of rainfall as measured in total inches of rain per year, it does consistently have more rainy days per year than most other places in the country.[78]
 Eastern Washington—the part of the state east of the Cascades—has a relatively dry climate, in distinct contrast to the west side. It includes large areas of semiarid steppe and a few truly arid deserts in the rain shadow of the Cascades; the Hanford reservation receives an average annual precipitation of 6 to 7 inches (150 to 180 mm). Despite the limited amount of rainfall, agriculture is an extremely important business throughout much of Eastern Washington, as the soil is highly productive and irrigation, aided by dams along the Columbia River, is fairly widespread.[79] The spread of population in Eastern Washington is dominated by access to water, especially rivers. The main cities are all located alongside rivers or lakes; most of them are named after the river or lake they adjoin.
 Farther east, the climate becomes less arid, with annual rainfall increasing as one goes east to 21.2 inches (540 mm) in Pullman, near the Washington–Idaho border.[80] The Okanogan Highlands and the rugged Kettle River Range and Selkirk Mountains cover much of the state's northeastern quadrant. The Palouse southeast region of Washington was grassland that has been mostly converted into farmland, and extends to the Blue Mountains.[81]
 The state of Washington has a temperate climate. The eastern half of Washington has a semi-arid to warm-summer mediterranean climate, while the western side of Washington as well as the coastal areas of the state have a cool oceanic climate or warm-summer mediterranean climate. Major factors determining Washington's climate include the large semi-permanent low pressure and high pressure systems of the north Pacific Ocean, the continental air masses of North America, and the Olympic and Cascade mountains. In the spring and summer, a high-pressure anticyclone system dominates the north Pacific Ocean, causing air to spiral out in a clockwise fashion. For Washington, this means prevailing winds from the northwest bring relatively cool air and a predictably dry season.[83][failed verification]
 In the autumn and winter, a low-pressure cyclone system takes over in the north Pacific Ocean. The air spiraling inward in a counter-clockwise fashion causes Washington's prevailing winds to come from the southwest, and bring cool and overcast weather and a predictably wet season. The term ""Pineapple Express"" is used colloquially to describe atmospheric river events, where repeated storm systems are directed by this persistent cyclone from the tropical Pacific regions a great distance into the Pacific Northwest. Western Washington is very cloudy during much of fall, winter, and early spring. Seattle averages the least sunshine hours of any major city in the United States.[84]
 Despite Western Washington's marine climate similar to many coastal cities of Europe, there are exceptions such as the ""Big Snow"" events of 1880, 1881, 1893, and 1916,[85][86] and the ""deep freeze"" winters of 1883–1884, 1915–1916, 1949–1950, and 1955–1956, among others.[87] During these events, Western Washington experienced up to 6 feet (1.8 m) of snow, sub-zero (−18 °C) temperatures, three months with snow on the ground, and lakes and rivers frozen over for weeks.[86] Seattle's lowest officially recorded temperature is 0 °F (−18 °C) set on January 31, 1950, but low-altitude areas approximately three hours away from Seattle have recorded lows as cold as −48 °F (−44 °C).[88]
 The Southern Oscillation greatly influences weather during the cold season. During the El Niño phase, the jet stream enters the U.S. farther south through California, therefore late fall and winter are drier than normal with less snowpack. The La Niña phase reinforces the jet stream through the Pacific Northwest, causing Washington to have more rain and snow than average.[89]
 In 2006, the Climate Impacts Group at the University of Washington published The Impacts of Climate Change in Washington's Economy, a preliminary assessment of the risks and opportunities presented given the possibility of a rise in global temperatures and their effects on Washington state.[90]
 Rainfall in Washington varies dramatically going from east to west. The Olympic Peninsula's western side receives as much as 160 inches (4,100 mm) of precipitation annually, making it the wettest area of the 48 conterminous states and a temperate rainforest. Weeks may pass without a clear day. The western slopes of the Cascade Range receive some of the heaviest annual snowfall (in some places more than 200 inches or 5,100 millimeters water equivalent) in the country. In the rain shadow area east of the Cascades, the annual precipitation is only 6 inches (150 mm). Precipitation then increases again eastward toward the Rocky Mountains (about 120 miles (190 km) east of the Idaho border).
 The Olympic mountains and Cascades compound this climatic pattern by causing orographic lift of the air masses blown inland from the Pacific Ocean, resulting in the windward side of the mountains receiving high levels of precipitation and the leeward side receiving low levels. This occurs most dramatically around the Olympic Mountains and the Cascade Range. In both cases, the windward slopes facing southwest receive high precipitation and mild, cool temperatures. While the Puget Sound lowlands are known for clouds and rain in the winter, the western slopes of the Cascades receive larger amounts of precipitation, often falling as snow at higher elevations.[91] Mount Baker, near the state's northern border, is one of the snowiest places in the world. In 1999, it set the world record for snowfall in a single season—1,140 inches (95 ft; 29 m).[92]
 East of the Cascades, a large region experiences strong rain shadow effects. Semi-arid conditions occur in much of Eastern Washington with the strongest rain shadow effects at the relatively low elevations of the central Columbia Plateau—especially the region just east of the Columbia River from about the Snake River to the Okanagan Highland. Thus, instead of rain forests, much of Eastern Washington is covered with dry grassland, shrub-steppe, and dunes.
 The average annual temperature ranges from 51 °F (11 °C) on the Pacific coast to 40 °F (4 °C) in the northeast. The lowest temperature recorded in the state was −48 °F (−44 °C) in Winthrop and Mazama. The highest recorded temperature in the state was 120 °F (49 °C) at Hanford on June 29, 2021.[93][94] Both records were set east of the Cascades. Western Washington is known for its mild climate, considerable fog, frequent cloud cover, long-lasting drizzles in the winter and warm, temperate summers. The eastern region, which does not benefit from the general moderating effect of the Pacific Ocean, occasionally experiences extreme climate. Arctic cold fronts in the winter and heat waves in the summer are not uncommon. In the Western region, temperatures have reached as high as 118 °F (48 °C) in Maple Valley[95] during the June 2021 heat wave, and as low as −6 °F (−21 °C) in Longview,[96] and even −8 °F (−22 °C) in Sammamish.[97]
 Forests cover about half the state's land area, mostly west of the northern Cascades. Approximately two-thirds of Washington's forested area is publicly owned, including 64 percent of federal land.[108] Common trees and plants in the region are camassia, Douglas fir, hemlock, penstemon, ponderosa pine, western red cedar, and many species of ferns.[109] The state's various areas of wilderness offer sanctuary, with substantially large populations of shorebirds and marine mammals. The Pacific shore surrounding the San Juan Islands is heavily inhabited by killer, gray, and humpback whales.[110]
 In Eastern Washington, the flora is vastly different. Tumbleweeds and sagebrush dominate the landscape throughout large parts of the countryside. Russian olives and other trees are common alongside riverbanks; however, apart from the riversides, large swaths of Eastern Washington have no naturally existing trees at all (though many trees have been planted and are irrigated by people, of course). A wider variety of flora can be found in both the Blue Mountains and the eastern sides of the Cascades.
 Mammals native to the state include the bat, black bear, bobcat, cougar, coyote, deer, elk, gray wolf, hare, moose, mountain beaver, muskrat, opossum, pocket gopher, rabbit, raccoon, river otter, skunk, and tree squirrel.[111] Because of the wide range of geography, the state of Washington is home to several different ecoregions, which allow for a varied range of bird species. This range includes raptors, shorebirds, woodland birds, grassland birds, ducks, and others.[112] There have also been a large number of species introduced to Washington, dating back to the early 18th century, including horses and burros.[113] The channel catfish, lamprey, and sturgeon are among the 400 known freshwater fishes.[114][115] Along with the Cascades frog, there are several forms of snakes that define the most prominent reptiles and amphibians.[116][117] Coastal bays and islands are often inhabited by plentiful amounts of shellfish and whales. There are five species of salmon that ascend the Western Washington area, from streams to spawn.[110]
 Washington has a variety of National Park Service units. Among these are the Alta Lake State Park, Lake Roosevelt National Recreation Area, San Juan Islands National Wildlife Refuge, as well as three national parks—the Olympic National Park, North Cascades National Park, and Mount Rainier National Park.[118] The three national parks were established between 1899 and 1968. Almost 95 percent (876,517 acres, 354,714 hectares, 3,547.14 square kilometers) of Olympic National Park's area has been designated as wilderness under the National Wilderness Preservation System.[119] Additionally, there are 143 state parks and 9 national forests, run by the Washington State Park System and the United States Forest Service.[120] The Okanogan National Forest is the largest national forest on the West Coast, encompassing 1,499,023 acres (606,633 ha). It is managed together as the Okanogan–Wenatchee National Forest, encompassing a considerably larger area of around 3,239,404 acres (1,310,940 ha).[121]
 There are 39 counties within the state, and 281 incorporated municipalities which are divided into cities and towns.[122] The majority of the state's population lives within Western Washington, in the Seattle metropolitan area; the city of Seattle is the principal city of the metropolitan area, and Western Washington, with a 2020 census population of 737,015.[123]
.mw-parser-output .largestCities-table-background{background:#f9f9f9;color:#222}.mw-parser-output .largestCities-cell-background{background:#f0f0f0;color:#222}
 Washington's population was 7,705,281 in the 2020 census,[129] a 14.6% increase since the 2010 census.[130] In 2020, the state ranked 13th overall in population, and was the third most populous, after California and Texas, west of the Mississippi River.[131] Washington has the largest population among states in the Pacific Northwest, followed by Oregon and Idaho. The Washington State Office of Financial Management estimated the state population to be 7,951,150 as of April 1, 2023.[132]
 The Seattle–Tacoma–Bellevue metropolitan area's population was 4,018,762 in the 2020 census, more than half the state total.[133] The center of population of Washington in 2010 was at .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}47°20′N 121°37′W﻿ / ﻿47.33°N 121.62°W﻿ / 47.33; -121.62, in an unpopulated part of the Cascade Mountains in rural eastern King County, southeast of North Bend, northeast of Enumclaw, and west of Snoqualmie Pass.[134]
 In 2020, Washington's proportion of residents under the age of five was 5.7%, 21.8% under 18, and 16.3% 65 or older.[135]
 Four-fifths of the state's population identifies as White or European American. Washington has some of the largest Native American and Asian populations among states in the U.S.; the state also has a small proportion of African Americans. Washington's Hispanic community began growing rapidly in the late 20th century.[110] In 2018, the top countries of origin for Washington's immigrants were Mexico, India, China, the Philippines and Vietnam.[136] There are 29 federally recognized Native American tribes in the state, mostly in Western Washington, and other unrecognized groups.[137]
 According to HUD's 2022 Annual Homeless Assessment Report, there were an estimated 25,211 homeless people in Washington.[138][139]
 The racial composition of Washington's population as of the 2020 census was:
 According to the 2016 American Community Survey, 12.1% of Washington's population were of Hispanic or Latino origin (of any race): Mexican (9.7%), Puerto Rican (0.4%), Cuban (0.1%), and other Hispanic or Latino origin (1.8%).[145] The five largest ancestry groups were: German (17.8%),
Irish (10.8%), English (10.4%), Norwegian (5.4%), and American (4.6%).[146]
 In 2011, 44.3 percent of Washington's population younger than age 1 were minorities.[147]
 Note: Births in table do not add up because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.
 While the population of African Americans in the Pacific Northwest is relatively scarce overall, they are mostly concentrated in the South End and Central District areas of Seattle, and in inner Tacoma.[158] The black community of Seattle consisted of one individual in 1858, Manuel Lopes, and grew to a population of 406 by 1900.[159] It developed substantially during and after World War II when wartime industries and the U.S. Armed Forces employed and recruited tens of thousands of African Americans from the Southeastern United States. They moved west in the second wave of the Great Migration, leaving a high influence on West Coast rock music and R&B and soul in the 1960s, including Seattle native Jimi Hendrix, a pioneer in hard rock, who was of African-American and alleged Cherokee descent.
 Native Americans lived on Indian reservations or jurisdiction lands such as the Colville Indian Reservation, Makah, Muckleshoot Indian Reservation, Quinault, Salish people, Spokane Indian Reservation, and Yakama Indian Reservation. The westernmost and Pacific coasts have primarily American Indian communities, such as the Chinook, Lummi, and Salish. Urban Indian communities formed by the U.S. Bureau of Indian Affairs relocation programs in Seattle since the end of World War II brought a variety of Native American peoples to this diverse metropolis. The city was named for Chief Seattle in the very early 1850s when European Americans settled the sound.
 Asian Americans are mostly concentrated in the Seattle−Tacoma metropolitan area of the state. Seattle, Bellevue, and Redmond, which are all within King County, have sizable Chinese communities (including Taiwanese), as well as significant Indian and Japanese communities. The Chinatown–International District in Seattle has a historical Chinese population dating back to the 1860s, who mainly emigrated from Guangdong Province in southern China, and is home to a diverse East and Southeast Asian community. Koreans are heavily concentrated in the suburban cities of Federal Way and Auburn to the south, and in Lynnwood to the north. Tacoma is home to thousands of Cambodians, and has one of the largest Cambodian-American communities in the United States, along with Long Beach, California, and Lowell, Massachusetts.[160] The Vietnamese and Filipino populations of Washington are mostly concentrated within the Seattle metropolitan area.[161]
 Washington state has the second highest percentage of Pacific Islander people in the mainland U.S. (behind Utah); the Seattle–Tacoma area is home to more than 15,000 people of Samoan ancestry, who mainly reside in southeast Seattle, Tacoma, Federal Way, and in SeaTac.[162][163]
 The most numerous (ethnic, not racial, group) are Latinos at 11%, as Mexican Americans formed a large ethnic group in the Chehalis Valley, Skagit Valley, farming areas of Yakima Valley, and Eastern Washington. They were reported to at least date as far back as the 1800s.[164] But it was in the late 20th century, that large-scale Mexican immigration and other Latinos settled in the southern suburbs of Seattle, with limited concentrations in King, Pierce, and Snohomish Counties during the region's real estate construction booms in the 1980s and 1990s.
 Additionally, Washington has a large Ethiopian community, with many Eritrean residents as well.[165] Both emerged in the late 1960s, and developed since 1980.[166] An estimated 30,000 Somali immigrants reside in the Seattle area.[167]
 In 2010, 82.51% (5,060,313) of Washington residents age 5 and older spoke English at home as a primary language, while 7.79% (477,566) spoke Spanish, 1.19% (72,552) Chinese (which includes Cantonese and Standard Chinese), 0.94% (57,895) Vietnamese, 0.84% (51,301) Tagalog, 0.83% (50,757) Korean, 0.80% (49,282) Russian, and 0.55% (33,744) German. In total, 17.49% (1,073,002) of Washington's population age 5 and older spoke a mother language other than English.[168]
 Major religious affiliations of the people of Washington are:[170]
 The largest denominations by number of adherents in 2010 were the Roman Catholic Church, with 784,332; The Church of Jesus Christ of Latter-day Saints, with 282,356;[171] and the Assemblies of God, with 125,005.[172]
 Aquarian Tabernacle Church is the largest Wiccan church in the country.[173]
 Like other West Coast states, the percentage of Washington's population identifying themselves as ""non-religious"" is higher than the national average.
 Washington has a relatively strong economy, with a total gross state product of $801.5 billion in 2023, placing it eleventh in the nation and growing by 8.6 percent per year—among the fastest rates in the United States.[174] In the late 2010s, the state had the fastest-growing economy in the United States and was tenth-largest in the nation.[175] The minimum wage was set at $11 in 2017 and has increased annually based on a cost-of-living index; since January 1, 2024, it has been $16.28 an hour, the highest of any state.[176] Several cities have higher minimum wages as of 2024[update], such as Seattle at $19.97 for large employers and Tukwila at $20.29 for large employers.[177][178] As of September 2023[update], the state's unemployment rate was 3.6 percent, ranked 36th among states.[179]
 Significant business within the state include the design and manufacture of aircraft (Boeing), automotive (Paccar), computer software development (Microsoft, Bungie, Amazon, Nintendo of America, Valve, ArenaNet, Cyan Worlds), telecom (T-Mobile US), electronics, biotechnology, aluminum production, lumber and wood products (Weyerhaeuser), mining, beverages (Starbucks, Jones Soda), real estate (John L. Scott, Colliers International, Windermere Real Estate, Kidder Mathews), retail (Nordstrom, Eddie Bauer, Car Toys, Costco, R.E.I.), and tourism (Alaska Airlines, Expedia, Inc.). A Fortune magazine survey of the top 20 Most Admired Companies in the U.S. has four Washington-based companies: Amazon, Starbucks, Microsoft, and Costco.[180] At over 80 percent the state has significant amounts of hydroelectric power generation. Also, significant amounts of trade with Asia pass through the ports of the Puget Sound, leading to a number six ranking of U.S. ports (ranking combines twenty-foot equivalent units (TEUs) moved and infrastructure index).[181]
 With the passage of Initiative 1183, the Washington State Liquor Control Board (WSLCB) ended its monopoly of all-state liquor store and liquor distribution operations on June 1, 2012. The board transitioned into licensing and regulating the sale of alcohol, tobacco, and later cannabis after the passage of Initiative 502.[182][183]
 The state is home to several of the wealthiest people in the United States and the world by net worth. Microsoft co-founder Bill Gates and Amazon founder Jeff Bezos both held the title of world's richest person, as determined by Forbes, while living in Washington.[184]
 The state of Washington is one of seven states that do not levy a personal income tax. The state does not collect a corporate income tax or franchise tax either. Washington businesses are responsible for various other state levies, including the business and occupation tax (B & O), a gross receipts tax which charges varying rates for different types of businesses.
 Washington's state base sales tax is 6.5%, which is combined with a local sales tax that varies by locality. The combined state and local retail sales tax rates increase the taxes paid by consumers, depending on the variable local sales tax rates, generally between 7.5% and 10%.[185] As of 2024[update], the combined sales tax rate in Seattle was 10.25%. The Snohomish County cities of Lynnwood, Mill Creek, Mukilteo are tied for the highest sales tax rate in the state at 10.6%.[186] These taxes apply to services as well as products, but not most foods due to a 1977 ballot measure.[187][188] However, prepared foods, dietary supplements, and soft drinks remain taxable.[189]
 An excise tax applies to certain products such as gasoline, cigarettes, and alcoholic beverages. Property tax was the first tax levied in the state of Washington, and its collection accounts for about 30% of Washington's total state and local revenue. It continues to be the most important revenue source for public schools, fire protection, libraries, parks and recreation, and other special-purpose districts.
 All real property and personal property are subject to tax unless specifically exempted by law. Most personal property owned by individuals is exempt from tax. Personal property tax applies to personal property used when conducting business, or to other personal property not exempt by law. All property taxes are paid to the county treasurer's office where the property is located. Neither does the state assess any tax on retirement income earned and received from another state. Washington does not collect inheritance taxes. However, the estate tax is de-coupled from the federal estate tax laws, and therefore, the state imposes its estate tax.
 Washington state has the 18th highest per capita effective tax rate in the United States, as of 2017.[citation needed] As of June 2023[update], Washington has the highest gasoline prices in the United States, at an average of $4.97, in part due to the third-highest gasoline tax in the country.[190] Their tax policy differs from neighboring Oregon's, which levies no sales tax, but does levy a personal income tax. This leads to border economic anomalies in the Portland–Vancouver metropolitan area.[191] Additional border economies with tax disparities exist with neighboring Idaho, which has a lower sales tax rate;[192] and British Columbia, which has higher costs for goods and has residents who commute into Washington for shopping.[193] These include remote mailbox and courier services for American online retailers, which became ubiquitous in border communities in the 21st century.[194]
 Washington is a leading agricultural state. For 2018, the total value of Washington's agricultural products was $10.6 billion.[11] In 2014, Washington ranked first in the nation in production of red raspberries (90.5 percent of total U.S. production), hops (79.3 percent), spearmint oil (75 percent), wrinkled seed peas (70.4 percent), apples (71.1 percent), sweet cherries (62.3 percent), pears (45.6 percent), Concord grapes (55.1 percent), carrots for processing (30.6 percent), and green peas for processing (32.4 percent).[195]
 Washington also ranked second in the nation in the production of fall potatoes (a quarter of the nation's production), nectarines, apricots, asparagus, all raspberries, grapes (all varieties taken together), sweet corn for processing (a quarter of the nation's production), and summer onions (a fifth of the nation's production). Washington also ranked third in the nation in the production of dried peas, lentils, onions, and peppermint oil.[11]
 The apple industry is of particular importance to Washington. Because of the favorable climate of dry, warm summers and cold winters of central Washington, the state has led the U.S. in apple production since the 1920s.[196] Two areas account for the vast majority of the state's apple crop: the Wenatchee–Okanogan region (comprising Chelan, Okanogan, Douglas, and Grant counties), and the Yakima region (comprising Yakima, Benton, and Kittitas counties).[197] Washington produces seven principal varieties of apples which are exported to more than sixty countries.[198]
 Washington ranks second in the United States in the production of wine, behind only California.[199] By 2006, the state had over 31,000 acres (130 km2) of vineyards, a harvest of 120,000 short tons (109,000 t) of grapes, and exports going to more than forty countries around the world from the state's 600 wineries. By 2021, that number had grown to 1,050 wineries. While there are some viticultural activities in the cooler, wetter western half of the state, almost all (99%) of wine grape production takes place in the desert-like eastern half.[200] The rain shadow of the Cascade Range leaves the Columbia River Basin with around 8 inches (200 mm) of annual rain fall, making irrigation and water rights of paramount interest to the Washington wine industry. Viticulture in the state is also influenced by long sunlight hours (on average, two more hours a day than in California during the growing season) and consistent temperatures.[201]
 As of 2022[update], Washington has 108,542 total U.S. Department of Defense personnel, including active duty members of the military and civilian workers at United States Armed Forces bases.[202] It ranks seventh among states for most active duty personnel, at over 60,000, and seventeenth for reserve members.[203] The U.S. Navy and Marines comprise the largest branch in Washington with 45 percent of personnel, followed by the Army at 40 percent and the Air Force at 11 percent.[202] The state is also home to the 11th-largest population of retirees and veterans at over 560,000 as of 2019[update].[204]
 The state's largest military installations are centered around the Puget Sound region and include Joint Base Lewis–McChord in Pierce County, the largest military base on the West Coast with over 25,000 active duty soldiers;[205] Naval Station Everett in Snohomish County; and Naval Air Station Whidbey Island in Island County.[202][206] The Kitsap Peninsula is home to Naval Base Kitsap, which includes the Puget Sound Naval Shipyard in Bremerton and Naval Submarine Base Bangor,[206] site of the third-largest arsenal of nuclear weapons in the world with more than 1,100 warheads for submarines.[207] Fairchild Air Force Base is a major air force installation near Spokane that has the largest aerial refueling fleet in the world.[208] Washington also has several major companies that serve as defense contractors for the U.S. military who were awarded $6.9 billion in fiscal year 2022. The largest contractors in the state include Boeing, PacMed, and Microsoft.[202][209]
 From 2009 to 2014, the Washington State Broadband Project was awarded $7.3 million in federal grants, but the program was discontinued in 2014.[210] For infrastructure, another $166 million has been awarded since 2011 for broadband infrastructure projects in Washington state.[211]
 U.S. News & World Report  ranked Washington second nationally for household internet access, and sixth for online download speed, based on data from 2014 and 2015.[212]
 In 2019, Washington State Legislature established the Washington State Broadband Office with two key mandates: high-speed internet access for 100% of WA residents by 2024 and an increase to 150/150 Mbit/s by 2028.[citation needed]
 In March 2021, the Washington State Department of Commerce issued their first biennial report on the progress of these key mandates throughout 2020.[213]
 The report includes five sections: public survey results, digital adoption disparities as they relate to federal census data, a Partner-Plan-Fund-Build-Adopt model for continued progress, success stories, and a policy discussion conclusion.
 According to the report, ""...over 42,000 survey responses from nearly 32,000 unique locations, showing that 6.4 percent of respondents reported having no broadband service, and 57 percent reported service at download speeds under 25 Mbps...""
 Washington's state transportation system comprises several modes that are maintained by various government entities. The state highway system, called State Routes, includes over 7,000 miles (11,000 km) of roads and the Washington State Ferries system, the largest of its kind in the nation[214] and the third largest in the world. There are also 57,200 miles (92,100 km) of local roads maintained by cities and counties, as well as several ferries operated by local governments.[215] There are 140 public airfields in Washington, including 16 state airports owned by the Washington State Department of Transportation. Seattle–Tacoma International Airport (Sea–Tac) is the major commercial airport of greater Seattle.[216] Boeing Field in Seattle is one of the busiest primary non-hub airports in the U.S.[217]
 There are extensive waterways around Washington's largest cities, including Seattle, Bellevue, Tacoma, and Olympia. The state highways incorporate an extensive network of bridges and the largest ferry system in the United States to serve transportation needs in the Puget Sound area. Washington's marine highway constitutes a fleet of twenty-eight ferries that navigate Puget Sound and its inland waterways to 20 different ports of call, completing close to 147,000 sailings each year. Washington is home to four of the five longest floating bridges in the world: the Evergreen Point Floating Bridge, Lacey V. Murrow Memorial Bridge and Homer M. Hadley Memorial Bridge over Lake Washington, and the Hood Canal Bridge which connects the Olympic Peninsula and Kitsap Peninsula. Among its most famous bridges is the Tacoma Narrows Bridge, which collapsed in 1940 and was rebuilt. Washington has 75 port districts,[215] including several major seaports on the Pacific Ocean. Among these are ports in Seattle, Tacoma, Kalama, Anacortes, Vancouver, Everett, Longview, Grays Harbor, Olympia, and Port Angeles.[citation needed] The Columbia and Snake rivers also provide 465 miles (748 km) of inland waterways that are navigable by barges as far east as Lewiston, Idaho.[215][218]
 The Cascade Mountain Range also impedes transportation. Washington operates and maintains roads over seven[vague] major mountain passes and eight minor passes. During the winter months, some of these passes are plowed, sanded, and kept safe with avalanche control. Not all stay open through the winter. The North Cascades Highway, State Route 20, closes every year due to snowfall and avalanches in the area of Washington Pass. The Cayuse and Chinook passes east of Mount Rainier also close in winter.[219]
 Washington is crossed by several freight railroads, and Amtrak's passenger Cascade route between Eugene, Oregon, and Vancouver, BC is the eighth busiest Amtrak service in the U.S. Seattle's King Street Station, the busiest station in Washington, and the 15th busiest in the U.S.,[220] serves as the terminus for the two long-distance Amtrak routes in Washington, the Empire Builder to Chicago and the Coast Starlight to Los Angeles. The Sounder commuter rail service operates in Seattle and its surrounding cities, between Everett and Lakewood. The intercity network includes the Cascade Tunnel, the longest railroad tunnel in the United States, which is part of the Stevens Pass route on the BNSF Northern Transcom.[221]
 Sound Transit Link light rail currently operates in the Seattle area at a length of 24 miles (39 km), and in Tacoma at a length of 4 miles (6.4 km). The entire system has a funded expansion plan that will expand light rail to a total of 116 miles by 2041. Seattle also has a 3.8-mile (6.1 km) streetcar network with two lines and plans to expand further by 2025. 32 local bus transit systems exist across the state,[215] the busiest being King County Metro, located in Seattle and King County, with just above 122 million riders in 2017.[222] Clark County has historically resisted proposals to extend Portland's MAX Light Rail into Vancouver, including the rejection of two ballot measures, but light rail is slated to be included in a future replacement of the Interstate Bridge.[223]
 Some tribal governments offer free bus service on their respective reservations, including on the Muckleshoot,[224] Spokane,[225] and Yakama Indian Reservations.[226]
 Hanford Nuclear Reservation is currently the most contaminated nuclear site in the United States[227] and is the focus of the nation's largest environmental cleanup.[228] The radioactive materials are known to be leaking from Hanford into the environment.[229] Another major cleanup site is the Duwamish River basin in Seattle, among the most contaminated bodies of water in the United States due to industrial runoff.[230]
 In 2007, Washington became the first state in the nation to target all forms of highly toxic brominated flame retardants known as PBDEs for elimination from the many common household products in which they are being used. A 2004 study of 40 mothers from Oregon, Washington, British Columbia, and Montana found PBDEs in the breast milk of every woman tested.
 Three recent studies by the Washington State Department of Ecology showed toxic chemicals banned decades ago linger in the environment and concentrate in the food chain. In one of the studies, state government scientists found unacceptable levels of toxic substances in 93 samples of freshwater fish from 45 sites. The toxic substances included PCBs, dioxins, two chlorinated pesticides, DDE, dieldrin and PBDEs. As a result of the study, the department will investigate the sources of PCBs in the Wenatchee River, where unhealthy levels of PCBs were found in mountain whitefish. Based on the 2007 information and a previous 2004 Ecology study, the Washington State Department of Health advises the public not to eat mountain whitefish from the Wenatchee River from Leavenworth downstream to where the river joins the Columbia, due to unhealthy levels of PCBs. Study results also showed high levels of contaminants in fish tissue that scientists collected from Lake Washington and the Spokane River, where fish consumption advisories are already in effect.[231]
 On March 27, 2006, Governor Christine Gregoire signed into law the recently approved House Bill 2322. This bill would limit phosphorus content in dishwashing detergents statewide to 0.5 percent over the next six years. Though the ban would be effective statewide in 2010, it would take place in Whatcom County, Spokane County, and Clark County in 2008.[232] A recent discovery had linked high contents of phosphorus in water to a boom in algae population. An invasive amount of algae in bodies of water would lead to a variety of excess ecological and technological issues.[233]
 In 2020, the electricity sold by public and private suppliers for use in Washington was primarily sourced from hydroelectric dams (55%), followed by natural gas (12%), coal (8.5%), wind (6%), and nuclear (4%). A total of 86.7 million Megawatt-hours of electricity was generated statewide in 2020.[234] Washington has the second-highest rate of renewable energy generation among U.S. states, behind Texas, and accounted for 31 percent of national hydroelectric generation.[235]
 Washington's executive branch is headed by a governor elected for a four-year term. The current statewide elected officials are:
 The bicameral Washington State Legislature is the state's legislative branch. The state legislature is composed of a lower House of Representatives and an upper State Senate. The state is divided into 49 legislative districts of equal population, each of which elects two representatives and one senator. Representatives serve two-year terms, while senators serve for four years. There are no term limits. The Democratic Party has a majority in the House and Senate.
 The Washington Supreme Court is the highest court in the state and meets in Olympia. Nine justices serve on the bench and are elected statewide or appointed by the governor to fill vacancies.[236] There are 30 judicial districts, each with a superior court; these districts roughly correspond to counties, with some districts that combine rural or closely-related counties.[237]
 The two current United States senators from Washington are Patty Murray and Maria Cantwell, both Democrats. Murray has represented the state since 1993, while Cantwell assumed office in 2001. The state is one of four with two female senators.[238]
 Washington's ten representatives in the United States House of Representatives (see map of districts) as of the 2022 election are Suzan DelBene (D-1), Rick Larsen (D-2), Marie Gluesenkamp Perez (D-3), Dan Newhouse (R-4), Michael Baumgartner (R-5), Emily Randall (D-6), Pramila Jayapal (D-7), Kim Schrier (D-8), Adam Smith (D-9), and Marilyn Strickland (D-10).
 Due to Congressional redistricting as a result of the 2010 census, Washington gained one seat in the United States House of Representatives. With the extra seat, Washington also gained one electoral vote, raising its total to 12.
 The state is typically thought of as politically divided by the Cascade Mountains, with Western Washington being liberal (particularly the I-5 Corridor) and Eastern Washington being conservative.
 Although the eastern half of the state votes heavily Republican, the overwhelming Democratic dominance in the Seattle metropolitan area has turned Washington into a reliably blue state. It is considered part of the Blue wall of states that have voted Democratic in every presidential election since 1992. This voting streak began with Democrat Michael Dukakis narrowly capturing Washington in 1988. The state has since turned much more solidly blue, beginning with Obama's landslide victory in 2008, and Democrats winning the state by double digits in every subsequent presidential election.
 Washington was considered a key swing state in 1968, and it was the only western state to give its electoral votes to Democratic nominee Hubert Humphrey over his Republican opponent Richard Nixon. Washington was considered a part of the 1994 Republican Revolution, and had the biggest pick-up in the house for Republicans, who picked up seven of Washington's nine House seats.[240] However, this dominance did not last for long, as Democrats picked up one seat in the 1996 election,[241] and two more in 1998, giving the Democrats a 5–4 majority.[242]
 In 2013 and 2014, both houses of the Washington State Legislature (the Washington Senate and the Washington House of Representatives) were controlled by Democrats. The state senate was under Republican control, due to two Democrats' joining Republicans to form the Majority Coalition Caucus. After the 2014 elections, the Democrats retained control of the House, while Republicans took a majority in the Senate without the need for a coalition. In November 2017, a special election gave Democrats a one-seat majority in the Senate and complete control over state government. Since then, in the 2018 election, the Democrats have only expanded their majorities.
 The governorship is currently held by Democrat Bob Ferguson. No state has gone longer without a Republican governor than Washington. Democrats have controlled the Washington Governor's Mansion for 40 years; the last Republican governor was John Spellman, who left office in 1985. Washington has not voted for a Republican senator, governor, or presidential candidate since 1994, tying with Delaware for the longest streak in the country.[243]
 Washington uses the non-partisan blanket primary system after the approval of Initiative 872 in 2004.[244] All candidates run on the same ballot during primary elections and the top two candidates advance to the general election in November, regardless of party affiliation. This has resulted in several same-party general election match-ups. In a 2020 study, Washington was ranked as the second easiest state for citizens to vote in.[245]
 The 2023 American Values Atlas by the Public Religion Research Institute found that same-sex marriage is supported near-universally in Washington.[246]
 Washington is one of the ten states to have legalized assisted suicide. In 2008, the Washington Death with Dignity Act ballot initiative passed and became law.
 In November 2009, Washington voters approved full domestic partnerships via Referendum 71, marking the first time voters in any state expanded recognition of same-sex relationships at the ballot box. Three years later, in November 2012, same-sex marriage was affirmed via Referendum 74, making Washington one of only three states to have approved same-sex marriage by popular vote.
 Also in November 2012, Washington was one of the first two states to approve the legal sale and possession of cannabis for both recreational and medical use with Initiative 502. Although marijuana is still illegal under U.S. federal law, persons 21 and older in Washington state can possess up to one ounce of marijuana, 16 ounces of marijuana-infused product in solid form, 72 ounces of marijuana-infused product in liquid form, or any combination of all three, and can legally consume marijuana and marijuana-infused products.[247]
 In November 2016, voters approved Initiative 1433, which among other things requires employers to guarantee paid sick leave to most workers. On January 1, 2018, the law went into effect, with Washington becoming the seventh state with paid sick leave requirements.[248]
 With the passage of Initiative 1639 in the 2018 elections, Washington adopted stricter gun laws.
 Washington enacted a measure in May 2019 in favor of sanctuary cities, similar to California and Oregon laws which are among the strongest statewide mandates in the nation.[249]
 In 2019, the legislature passed the Clean Energy Transformation Act, which requires all electricity sales to be from zero-carbon sources by 2045 and net-zero by 2030.[250]
 As of the 2020–2021 school year, 1,094,330 students were enrolled in elementary and secondary schools in Washington, with 67,841 teachers employed to educate them.[251] As of August 2009, there were 295 school districts in the state, serviced by nine Educational Service Districts.[252] Washington School Information Processing Cooperative (a non-profit opt-in state agency) provides information management systems for fiscal and human resources and student data. Elementary and secondary schools are under the jurisdiction of the Washington State Office of Superintendent of Public Instruction (OSPI).[253]
 High school juniors and seniors in Washington have the option of using the state's Running Start program. Begun by the state legislature in 1990, it allows students to attend institutions of higher education at public expense, simultaneously earning high school and college credit.[254] The state has 141 schools that offer dual language programs in 14 languages, primarily Spanish, beginning in kindergarten.[255]
 The state also has several public arts-focused high schools including Tacoma School of the Arts, the Vancouver School of Arts and Academics, and The Center School. There are also four Science and Math based high schools: one in the Tri-Cities known as Delta, one in Tacoma known as SAMI, another in Seattle known as Raisbeck Aviation High School, and one in Redmond known as Tesla STEM High School.
 There are more than 40 institutions of higher education in Washington. The state has major research universities, technical schools, religious schools, and private career colleges. Colleges and universities include the University of Washington, Seattle University, Washington State University, Western Washington University, Eastern Washington University, Central Washington University, Seattle Pacific University, Saint Martin's University, Pacific Lutheran University, Gonzaga University, University of Puget Sound, Evergreen State College, Whitman College, and Walla Walla University.
 As of 2022[update], Washington has 20 daily newspapers and 96 weekly newspapers that serve local and hyperlocal markets.[256] The most-circulated newspaper in the state is The Seattle Times, which is also among the most-circulated newspapers in the United States.[257] Other major daily newspapers include The Spokesman-Review in Spokane, The News Tribune in Tacoma, The Columbian in Vancouver, The Daily Herald in Everett, the Tri-City Herald in Kennewick, and the Kitsap Sun in Bremerton.[256] Several national and regional chains own and operate a number of local weekly newspapers, including the Adams Publishing Group,[258] Sound Publishing, The Seattle Times Company, and the McClatchy Company.[259] Free weekly newspapers include The Stranger, Seattle Weekly, and the Inlander.[259]: 18  The Seattle area also has a number of publications in English and other languages for ethnic communities, including the Seattle Chinese Post, International Examiner, and Northwest Asian Weekly.[260] Since 2004, Washington has lost 37 local newspapers and seen the consolidation of smaller papers, including neighborhood and suburban papers in the Seattle metropolitan area.[256][261] Several newspapers have also switched to online-only publication, including Seattle's morning daily Post-Intelligencer in 2009.[262]
 The state is divided into four Designated Market Areas by Nielsen Media Research: Seattle–Tacoma, which also extends east to Wenatchee; Portland, which includes most of Southwestern Washington; Spokane, which also includes northern Idaho; and Yakima–Pasco–Richland–Kennewick.[263] The Seattle–Tacoma market is the largest in the Pacific Northwest and has been the 13th largest in the United States since 2009.[264] As of 2009[update], Washington had 39 full-power television stations and an additional 11 from Portland, Oregon; most are affiliated with a national or regional broadcasting network.[265] The state is home to 383 stations licensed with the Federal Communications Commission (FCC).[266][267] These radio stations broadcast to local markets as well as online, where Seattle-based music station KEXP-FM has found a worldwide following.[268]
 The top two health insurers as of 2017 were Premera Blue Cross, with 24 percent market share, followed by Kaiser Permanente at 21 percent.[269] For the individual market, Molina Healthcare had the top share at 23%.[270]
 The state adopted the Washington Healthplanfinder system in 2014 after the passage of the federal Patient Protection and Affordable Care Act (also known as ""ObamaCare"").[271] The system is used by approximately 90 percent of Washington residents who purchase or acquire their health insurance directly rather than through an employer.[272] The state's Medicaid program, named Washington Apple Health, provides healthcare coverage to people with disabilities or low incomes.[273]
 The state of Washington reformed its health care system in 1993 through the Washington Health Services Act. The legislation required individuals to obtain health insurance or face penalties, and required employers to provide insurance to employees. In addition, health insurance companies were required to sell policies to all individuals, regardless of pre-existing conditions, and cover basic benefits.[274] The act was mostly repealed in 1995 before it could go into full effect.
 Hospitals exist across the state, but many of Washington's best-known medical facilities are located in and around Seattle. The Seattle–Tacoma area has six major hospitals: Harborview Medical Center, University of Washington Medical Center, Seattle Children's, Swedish Medical Center, MultiCare Tacoma General Hospital, and St. Joseph Medical Center.[275] The Seattle-area hospitals are concentrated on First Hill, which is home to Virginia Mason Medical Center (the neighborhood has received the nickname ""Pill Hill"" owing to the high concentration of healthcare facilities).[276] As of 2023[update], the state has over 14,000 total hospital beds that are licensed for acute care in 93 facilities. Several religious healthcare providers, primarily Catholic organizations, control 49 percent of the state's hospital beds and have acquired and consolidated major systems in Washington.[277]
 Pickleball, a racquet sport invented on Bainbridge Island in 1965, was designated as Washington's official state sport in 2022.[278] For three years in a row, 2021, 2022 and 2023, the sport was named the fastest growing sport in the United States by the Sports and Fitness Industry Association (SFIA).[279]
 The Seattle Open Invitational golf tournament was part of the PGA Tour from the 1930s to the 1960s. The GTE Northwest Classic was part of the Senior PGA Tour from 1986 to 1995, and the Boeing Classic since 2005. In addition, the 2015 U.S. Open was held at Chambers Bay, and several major tournaments were held at Sahalee Country Club.
 Pacific Raceways is a motorsports venue that has hosted the Northwest Nationals of the NHRA Mello Yello Drag Racing Series and a round of the Trans-Am Series.
 The WTA Seattle tennis tournament was part of the WTA Tour from 1977 to 1982.
 Four ships of the United States Navy, including two battleships, have been named USS Washington in honor of the state. Previous ships had held that name in honor of George Washington.[citation needed]
 The state's nickname, ""The Evergreen State"",[1][280] was proposed in 1890 by Charles T. Conover of Seattle. The name proved popular as the forests were full of evergreen trees and the abundance of rain keeps the shrubbery and grasses green throughout the year.[281] Although the nickname is widely used by the state, appearing on vehicle license plates for instance, it has not been officially adopted.[1] A 2023 bill in the state legislature to formally recognize it as the state nickname was passed by the senate but was returned to committee.[282][283] The Evergreen State College, a state-funded institution in Olympia, also takes its name from this nickname.
 The state song is ""Washington, My Home"", the state bird is the American goldfinch, the state fruit is the apple, and the state vegetable is the Walla Walla sweet onion.[284] The state dance, adopted in 1979, is the square dance. The state tree is the western hemlock. The state flower is the coast rhododendron. The state fish is the steelhead.[1] The state folk song is ""Roll On, Columbia, Roll On"" by Woody Guthrie. The unofficial, but popularly accepted, state rock song is ""Louie Louie"".[285] The state grass is bluebunch wheatgrass. The state insect is the green darner dragonfly. The state gem is petrified wood. The state fossil is the Columbian mammoth. The state marine mammal is the orca. The state soil is Tokul soil.[286] The state land mammal is the Olympic marmot.[1] The state seal (featured in the state flag as well) was inspired by the unfinished portrait of President George Washington by Gilbert Stuart.[287] The state sport is pickleball.[278]
 Washington has relationships with many provinces, states, and other entities worldwide.
 47°N 120°W﻿ / ﻿47°N 120°W﻿ / 47; -120﻿ (State of Washington)
"
George Washington Birthplace National Monument,https://en.wikipedia.org/wiki/George_Washington_Birthplace_National_Monument,"The George Washington Birthplace National Monument is a national monument in Westmoreland County, Virginia, at the confluence of Popes Creek and the Potomac River. It commemorates the birthplace location of George Washington, a Founding Father and the first President of the United States, who was born here on February 22, 1732. Washington lived at the residence until age three and later returned to live there as a teenager.
 John Washington, George Washington's great-grandfather, settled this plantation in 1657 at the original property on Bridges Creek.[5] The family acquired expanded land to the south toward nearby Popes Creek.
 Prior to 1718, the first section of the house was built. His father enlarged it between 1722–1726. He added on to it by the mid-1770s, making a ten-room house known as ""Wakefield"". This house, which George Washington in 1792 would describe as ""the ancient mansion seat,""[6] was destroyed by fire and flood on Christmas Day 1779, and never rebuilt.[7]
 George Washington was born in the house on February 22, 1732. Thirty-two graves of Washington family members have been found at the Bridges Creek cemetery plot, including George's half-brother, father, grandfather, and great-grandfather.
 Washington's father held slaves and had them cultivate tobacco on his several plantations, as his ancestors had done.
 In 1858, the Commonwealth of Virginia acquired the property to preserve the homesite and cemetery. In 1882, however, Virginia donated the land to the federal government following the Civil War.
 The Wakefield National Memorial Association was formed in 1923 to restore the property. In 1930, the grounds were authorized by Congress as a U.S. National Monument. In 1931, the Wakefield Association received a grant from John D. Rockefeller Jr., to acquire and transfer a total of 394 acres (1.59 km2) of land to the Federal government.
 Since the exact appearance of the original Washington family home is not known, a Memorial House was designed by Edward Donn Jr., representing similar buildings of the era; it was constructed on the approximate site in 1931. The actual location of Washington's boyhood home is adjacent to the memorial house and its foundation is outlined in the ground by crushed oyster shells.
 The Memorial House represents a typical tobacco plantation of the period of the original's construction. The Memorial House is constructed of bricks handmade from local clay. It has a central hallway and four rooms on each floor, furnished in the 1730–1750 period style by the Wakefield National Memorial Association. Furnishings include an 18th-century tea table believed to have been in the original house. Most of the other furnishings are more than 200 years old.[7] At the entrance to the grounds, now maintained and operated by the National Park Service, is a Memorial Shaft obelisk of Vermont marble; it is a one-tenth scale replica of the Washington Monument in Washington, D.C.
 The park and Memorial House were opened by the National Park Service in 1932, on the 200th anniversary of George Washington's birth.
 In the 21st century, the Monument is part of the National Park Service's ongoing efforts to interpret historical resources.[8] In addition to the Memorial House, park facilities open to visitors include the historic birthplace home area, Kitchen House, hiking trails, and picnic grounds. In the Kitchen House, costumed re-enactors demonstrate candle- and soap-making.[7]
 A colonial herb and flower garden has been planted with herbs and flowers common to Washington's time,[7] including thyme, sage, basil, and flowers such as hollyhocks, forget-me-nots, and roses. Typical trees and bushes of Washington's time have also been added to the landscaping. The Colonial Living Farm has a barn and pasture and raises livestock, poultry, and crops that were typical in the 18th century and using farming methods that were common at the time.
 Visitors may also tour the Washington family Burial Ground, which contains the graves of 32 members of the Washington family, including George Washington's father, grandfather, and great-grandfather. Replicas of two original gravestones are visible, along with five memorial tablets placed here in the 1930s.
 The Visitors' Center contains artifacts recovered from the burned-down Washington house, including a bowl, clay figurine, wine bottle seal belonging to Augustine Washington, wine bottle, and keyhole plate.
 A 15-minute film depicting Washington family life is shown in a theater at the Visitors' Center.
 The George Washington Birthplace National Monument is 38 miles (61 km) east of Fredericksburg, Virginia, located on the Northern Neck. It can be reached via VA Route 204, the access road to the site from VA State Route 3.
"
"Westmoreland County, Virginia","https://en.wikipedia.org/wiki/Westmoreland_County,_Virginia","
 Westmoreland County is a county located in the Northern Neck of the Commonwealth of Virginia. As of the 2020 United States census, the population sits at 18,477.[1] Its county seat is Montross.[2]
 As originally established by the Virginia colony's House of Burgesses, this area was separated from Northumberland County in 1653 and named for the English county of Westmorland; both counties are coastal.[3] The territory of Westmoreland County encompassed much of what later became the various counties and cities of Northern Virginia, including the city of Alexandria, Arlington County, Fairfax County, and Prince William County. These areas comprised part of Westmoreland until the formation of Stafford County in 1664.
 Westmoreland County on Northern Neck was the birthplace of George Washington, who later became the first President of the United States (born at the former settlement of Bridges Creek, Virginia);[4] of James Monroe, the fifth President; and of Robert E. Lee, general and commander of the Confederate armies during the American Civil War of 1861–1865.
 Colonel Nicholas Spencer (1633-1689) resided in this county. He patented the land at Mount Vernon in 1674 with his friend Lt. Col. John Washington, ancestor of George Washington. Spencer, who served as President of the Council and acting Governor (in office: 1683–1684) of the Colony of Virginia, was the cousin of, and agent for, the Barons Colepeper, proprietors of the Northern Neck. Spencer lived at his plantation, Nomini, which his descendants later sold to Robert Carter I (1662/63 – 1732) .
 Robert Carter's grandson, Robert Carter III, is known for voluntarily freeing almost 500 slaves from Nomini Hall, beginning in 1791. He also provided for their settlement on land that he bought for them in Ohio Country.[citation needed] This manumission was the largest known release of slaves in North America prior to the American Civil War and involved the largest number ever manumitted by an individual in the U.S.[5]
 According to the U.S. Census Bureau, the county has a total area of 253 square miles (660 km2), of which 229 square miles (590 km2) is land and 24 square miles (62 km2) (9.3%) is water.[6] Located on the Northern Neck, the county is within the Northern Neck George Washington Birthplace AVA winemaking appellation.
 Note: the US Census treats Hispanic/Latino as an ethnic category. This table excludes Latinos from the racial categories and assigns them to a separate category. Hispanics/Latinos can be of any race.
 At the 2000 census,[14] there were 16,718 people, 6,846 households and 4,689 families residing in the county. The population density was 73/sq mi (28/km2). There were 9,286 housing units at an average density of 40/sq mi (15/km2). The racial makeup of the county was 65.41% White, 30.89% Black or African American, 0.28% Native American, 0.36% Asian, 0.01% Pacific Islander, 1.75% from other races, and 1.29% from two or more races. 3.46% of the population were Hispanic or Latino of any race.
 There were 6,846 households, of which 25.70% had children under the age of 18 living with them, 50.70% were married couples living together, 13.50% had a female householder with no husband present, and 31.50% were non-families. 26.90% of all households were made up of individuals, and 13.60% had someone living alone who was 65 years of age or older. The average household size was 2.43 and the average family size was 2.91.
 23.00% of the population were under the age of 18, 6.30% from 18 to 24, 23.90% from 25 to 44, 27.80% from 45 to 64, and 19.00% who were 65 years of age or older. The median age was 43 years. For every 100 females, there were 92.30 males. For every 100 females age 18 and over, there were 88.90 males.
 The median household income was $35,797 and the median family income was $41,357. Males had a median income of $31,333 and females $22,221. The per capita income was $19,473. About 11.20% of families and 14.70% of the population were below the poverty line, including 21.10% of those under age 18 and 12.50% of those age 65 or over.
 The county's economy is largely based on agriculture. Tourism is another significant economic driver, related to historical sites such as George Washington Birthplace National Monument and Robert E. Lee's birthplace, Stratford Hall Plantation, and the Westmoreland County Museum as well as gambling activities available in Colonial Beach. The county is also an extended exurb of Washington, D.C.
 Northern Neck Coca-Cola Bottling Inc. (makers of Northern Neck Ginger Ale) and the weekly Westmoreland News are located in Montross.
 Westmoreland County is a notable bellwether for U.S. presidential politics, having voted for the winner in every election since 1928 except 1948, 1960, and 2020.
 There are two school districts: Westmoreland County Public Schools has the majority of the county, while Colonial Beach Town Public Schools has residential areas in the town limits of Colonial Beach.[16][17]
 38°07′N 76°48′W﻿ / ﻿38.11°N 76.80°W﻿ / 38.11; -76.80
"
Augustine Washington,https://en.wikipedia.org/wiki/Augustine_Washington,"
 Augustine Washington Sr. (1694[a] – April 12, 1743)[1][2] was a Virginian planter and merchant. Born in Westmoreland, Virginia, he was the father of ten children, among them the first president of the United States, George Washington, soldier and politician Lawrence Washington, and politician Charles Washington. Born into the planter class of the British colony of Virginia, Washington owned several slave plantations, from which he derived the primary source of his wealth. He also speculated in land development and owned an iron mine. Although Washington never sat in the House of Burgesses, as did his own father and son, he served in various government positions in the counties where he owned land.
 Augustine Washington was born in Westmoreland County, Virginia, in 1694, to Mildred Warner and her husband, Capt. Lawrence Washington, a militia captain and a member of the Virginia House of Burgesses. His paternal grandparents were Lt. Col. John Washington (c. 1631–1677) and his first wife, Anne Pope. His maternal grandparents owned Warner Hall and associated plantations in Gloucester County. Augustine was four years old when his father died. His mother remarried and moved her family to England, where she died when all were still children; although their mother's will named their stepfather George Gale as their guardian, their cousin John Washington fought to have himself named the children's guardian and brought them back to Virginia.
 When Washington came of age (and into his inheritance) in 1715, he married Jane Butler, another orphan, who had inherited about 640 acres (2.6 km2) from her father, Caleb Butler. The young couple settled on the Bridges Creek property and had four children, only two of whom (Lawrence and Augustine Jr.) lived to adulthood. After Jane Butler's death in November 1728[3] or 1729,[4] Washington married Mary Ball in 1731, and the couple had five children who survived to adulthood – George, Betty, Samuel, Charles, and John Augustine – and a daughter named Mildred who died in infancy.[2]
 When he reached legal age in 1715, Augustine Washington inherited about 1,000 acres (4.0 km2) on Bridges Creek in Westmoreland County; his sister Mildred inherited what was called the Little Hunting Creek property;[5] they both inherited enslaved people. In 1718, Washington purchased land on Pope's Creek, adjoining his property on Bridges Creek, and set about establishing himself. Between 1723 and 1735, he hired a local contractor to build a house, which was probably completed about 1726 despite the contractor's death (later called Wakefield).[6] In the same year, Washington purchased the Little Hunting Creek property from his sister Mildred.[7]
 In 1725, Augustine Washington entered into an agreement with the Principio Company of England to start an iron works on Accokeek Creek in Stafford County, and he also owned a stake in their Maryland ironworks.[6] In 1735, the family moved to the Little Hunting Creek property, which was closer to the Accokeek Furnace.[5]
 In 1738, Augustine Washington purchased the 150-acre Strother property across the Rappahannock River (now known as Ferry Farm) and moved the family there at the end of that same year.[5][7]
 Augustine Washington was active in the Anglican Church, the local militia, and politics.[6] He took the oath as justice of the peace for the Westmoreland county court in July 1716,[8] and served as county sheriff.
 He died on April 12, 1743.[9]
 After he died in 1743 at the age of 48, his 11-year-old son George inherited the former Strother property and its slaves.[10] Because he had not reached legal age, his widowed mother Mary Ball Washington managed this property for him until he came of age. She lived on the property until 1772 when she was 64 when George moved her to a house in Fredericksburg.
 Lawrence Washington inherited the Little Hunting Creek property and renamed it ""Mount Vernon"" to honor Admiral Edward Vernon, with whom he had served in the Royal Navy in 1741 during the Battle of Cartagena de Indias during the War of Jenkins' Ear.
 According to Augustine Sr's will, if his son Lawrence died without children, the Little Hunting Creek property would go to Augustine Jr., and Augustine Jr, in turn, would have to give up the Popes Creek property to his brother George. If Augustine Jr. did not want the Little Hunting Creek property, George would then inherit it. Upon Lawrence's death, Augustine Jr. chose Popes Creek and its slaves over the former Little Hunting Creek property. Lawrence's only surviving child, Sarah, lived until 1754; therefore, George Washington ultimately inherited the Little Hunting Creek property, which was known as Mount Vernon by that time. At his death, Augustine Washington Sr. enslaved a total of 64 people who were assigned among the various plantations.[11]
 Lawrence Washington's widow, Ann, had a life interest in the Little Hunting Creek plantation. Because she remarried and was not living at Mount Vernon, she leased the property to George in 1754. Upon her death in 1761, George Washington inherited the plantation outright.
"
Mary Ball Washington,https://en.wikipedia.org/wiki/Mary_Ball_Washington,"Mary Washington (née Ball; c. 1707–1709 – (1789-08-25)August 25, 1789) was an American planter best known for being the mother of the first president of the United States, George Washington. The second wife of Augustine Washington, she became a prominent member of the Washington family. She spent a large part of her life in Fredericksburg, Virginia, where several monuments were erected in her honor and a university, along with other public buildings, bear her name.
 Mary Ball was born sometime between 1707 and 1709 at either Epping Forest, her family's plantation in Lancaster County, Virginia,[1] or at a plantation near the village of Simonson, Virginia.[2] She was the only child of Col. Joseph Ball (1649–1711) and his second wife, Mary Johnson Ball (1672–1721). Her paternal grandfather was William Ball (1615–1680) who left Britain for Virginia in the 1650s. His wife, Hannah Atherold (1615–1695), arrived later along with their four children, including Mary's father Joseph, who had been born in England.[3][4] Her father died when she was three and after her mother's death, at the age of twelve Mary Ball was placed under the guardianship of Jane Washington's brother, the lawyer George Eskridge.[1][5]
 Augustine Washington had sailed to England on business (and to visit his sons who had been sent to school there) but on his return, he discovered that his first wife, Jane Butler Washington, had died in the interim. George Eskridge supposedly arranged an introduction between his friend, Washington, and his ward Mary Ball, [2] with the two marrying on March 6, 1731, when she was 23. She was wealthy by the standards of the day and brought at least 1000 acres of inherited property to the marriage.[2] The couple had the following children:[6]
 Augustine died in 1743 when son George was 11 years old. Unlike most widows in Virginia at the time, Mary Ball Washington never remarried. When George was 14, his older half-brother Lawrence Washington, arranged for young George to become a midshipman in the Royal Navy.[8] However, Mary's half-brother, Joseph Ball (1689–1760), wrote in reply to her letter requesting advice, that the Navy would ""cut and staple him and use him like a negro, or rather, like a dog.""[9]
 Mary managed the family estate and 276 acres of Ferry Farm (a plantation) with the help of others until her eldest son came of age and well beyond. She lived to see that her son, George Washington, commanded the Continental Army to independence and was inaugurated as the first president of the United States in 1789. After learning that he had been elected president in April 1789, George Washington traveled from Mount Vernon to visit his mother in Fredericksburg. He knew his mother was suffering from breast cancer, the disease to which she eventually succumbed.[1]
 It is said that Mrs. Washington informed her son of her poor health and expected to die soon. Further, the story continues, that her son George said that he would need to decline to serve as president. George's mother Mary responded, saying, ""But go, George, fulfill the high destinies which Heaven appears to have intended for you for; go, my son, and may that Heaven's and a mother's blessing be with you always.""[10][11]
 After a lengthy illness, on August 25, 1789, Mary Ball Washington died of breast cancer[12] at her home in Fredericksburg, Virginia.[1]
 She is buried near Meditation Rock in an unmarked grave which was located on the grounds of the Kenmore plantation, the former home of her daughter and son-in-law. The exact location of her grave is currently unknown. Kenmore is open to the public and operated as a historic house museum by the ""George Washington Foundation."" 
 While there is a legend that Mrs. Washington was said to be openly opposed to her son's revolutionary politics and, according to French officers based in Virginia during the war she was a Loyalist sympathizer,[13] there is no credible source to support that legend. The facts are that other than her son George who was Commander in Chief of the Continental forces (Army and Navy), Mary's other three sons Samuel, John Augustine, and Charles, all served in the Virginia Militia. Her son-in-law Fielding Lewis (husband to her daughter Betty), was in charge of the Fredericksburg Gunnery or Gun Manufactory. The gunnery works made muskets for use by American Revolutionary forces, and ended up almost bankrupting Lewis in the process.[14]  During the Revolutionary War, Mary Washington met the Marquis de Lafayette at her home in Fredericksburg.  The two enjoyed a warm relationship for the remainder of her life.[citation needed]
 Another legend about Mary Washington was that she petitioned the Government of Virginia claiming, in response to a Virginia government notice to citizens to do so, asking to be reimbursed for farm animals, horses and cattle that she gave to support the American war effort.[13]  No such petition was ever presented to the Virginia House of Delegates.  Speaker Benjamin Harrison V wrote to George Washington in 1781 about a rumor that Mary Washington was going to submit a petition.  At that time Mary Washington was experiencing economic challenges and was mourning the deaths of her son-in-law Fielding Lewis and her son Samuel Washington.  George purchased a house for her in Fredericksburg, two blocks from Kenmore, where George's sister Betty (Mrs. Fielding Lewis) lived. Mary lived in her home nearby from 1772 until her death in 1789. She left George the majority of her lands and belongings, appointing him executor of her Will.[15][16]
 Her third son, John Augustine Washington, was the father of Bushrod Washington, who was nominated by President John Adams to the U.S. Supreme Court, and confirmed by the Senate in 1798. Charles Town, West Virginia, is named for her fourth son, Charles Washington.
"
Justice of the peace,https://en.wikipedia.org/wiki/Justice_of_the_peace,"
 A justice of the peace (JP) is a judicial officer of a lower court, elected or appointed by means of a commission (letters patent) to keep the peace. In past centuries the term commissioner of the peace was often used with the same meaning. Depending on the jurisdiction, such justices dispense summary justice or merely deal with local administrative applications in common law jurisdictions. Justices of the peace are appointed or elected from the citizens of the jurisdiction in which they serve, and are (or were) usually not required to have any formal legal education in order to qualify for the office. Some jurisdictions have varying forms of training for JPs.
 In 1195, Richard I of England and his minister Hubert Walter commissioned certain knights to preserve the peace in unruly areas. They were responsible to the King in ensuring that the law was upheld and preserving the ""King's peace"". Therefore, they were known as ""keepers of the peace"".[1]
 An act of 1327 had referred to ""good and lawful men"" to be appointed in every county in the land to ""guard the peace""; such individuals were first referred to as conservators of the peace,[2] or wardens of the peace. The title justice of the peace derives from 1361,[3] in the reign of Edward III. The ""peace"" to be guarded is the sovereign's, the maintenance of which is the duty of the Crown under the royal prerogative. Justices of the peace still use the power conferred or re-conferred on them since 1361 to bind over unruly persons ""to be of good behaviour"". The bind over is not a punishment, but a preventive measure, intended to ensure that people thought likely to offend will not do so. The justices' alternative title of ""magistrate"" dates from the 16th century, although the word had been in use centuries earlier to describe some legal officials of Roman times.[4]
 In the centuries from the Tudor period until the onset of the Industrial Revolution, the JPs constituted a major element of the English (later British) governmental system, which in modern times has sometimes been termed a squirearchy (i.e., dominance of the land-owning gentry). For example, historian Tim Blanning notes[5] that while in Britain the royal prerogative was decisively curbed by the Bill of Rights 1689, in practice the central government in London had a greater ability to get its policies implemented in the rural outlying regions than could contemporary absolute monarchies such as France – a paradox due especially to JPs belonging to the same social class as the Members of Parliament and thus having a direct interest in getting laws actually enforced and implemented on the ground.
 Being an unpaid office, undertaken voluntarily and sometimes more for the sake of renown or to confirm the justice's standing within the community, the justice was typically a member of the gentry. The justices of the peace conducted arraignments in all criminal cases, and tried misdemeanours and infractions of local ordinances and bylaws. Towns and boroughs with enough burdensome judicial business that could not find volunteers for the unpaid role of justice of the peace had to petition the Crown for authority to hire a paid stipendiary magistrate.
 The Municipal Corporations Act 1835 stripped the power to appoint normal JPs from those municipal corporations that had it. This was replaced by the present system, where the Lord Chancellor nominates candidates with local advice, for appointment by the Crown.
 Until the introduction of elected county councils in the 19th century, JPs, in quarter sessions, also administered the county at a local level. Their many roles included regulating wages and food supplies, managing roads, bridges, prisons and workhouses and they undertook to provide and supervise locally those services mandated by the Crown and Parliament for the welfare of the county. To this end they set the County Rate, where one was set at all.
 Women were not allowed to become JPs in the United Kingdom until 1919, the first woman being Ada Summers, the Mayor of Stalybridge, who was a JP by virtue of her office. In October 1920 Summers was appointed a JP in her own right, alongside other pioneers including Edith Sutton[6] and Miriam Lightowler OBE in Halifax.[7] Emily Murphy of Edmonton, Canada, preceded her by some three and a half years.[8][9] As at 2018 in England and Wales, about one-third of JPs are women.[10]
 In special circumstances, a justice of the peace can be the highest governmental representative, so in fact 'gubernatorial', in a colonial entity. This was the case in the Tati Concessions Land, a gold-mining concession (territory) in the Matabele kingdom, until its annexation by the British Bechuanaland protectorate.
 A justice of the peace in Australia is typically someone of good stature in the community who is authorised to witness and sign statutory declarations and affidavits and to certify copies of original documents.[11]
 There are no Federal level JPs in Australia, as this power is devolved to the State and Territory Governments. Where a Federal Government document or task requires the services of a JP, the rules of each individual State or Territory government will dictate if they have the authority to assist. Criteria for appointment vary widely, depending on the state.
 In the Australian Capital Territory (Colloquially, ""The ACT""), there is only the single level of 'Justice of the Peace'. They are appointed on an as-needed basis, and a potential appointee must be an Australian Citizen, and both a resident of, and enrolled on the electoral roll, of the territory. They must also not be an undischarged bankrupt, and consent to criminal history checks being undertaken prior to appointment.
 Appointment is for life, unless a JP resigns, is suspended/dismissed from office, or resides outside of the ACT for a period of more than 12 consecutive months.
 JPs for the ACT also cover the Australian External Territory of Norfolk Island, and the Internal Jervis Bay Territory, subject to local law variations in those two jurisdictions.
 In the state of Queensland, a ""justice of the peace (qualified)"" has the additional powers to issue search warrants and arrest warrants and, in conjunction with another justice of the peace (qualified) constitute a magistrates' court for exercising powers to remand defendants in custody, grant bail, and adjourn court hearings.[12][13][14]
 Some justices are appointed as justice of the peace (magistrates' court), usually in remote Aboriginal communities, to perform many of the functions that might otherwise fall to a stipendiary magistrate.
 In Queensland, a lawyer may be appointed as a Justice of the Peace without further education or qualification and has the full powers of a JP (Magistrate's Court). A commissioner for declarations (C.dec) has powers limited to witnessing documents, witnessing statutory declarations, witnessing affidavits, witnessing and administering oaths and affirmations.[12]
 The first woman to become a JP in Queensland was Matilda (Maud) Hennessey of Mackay on 24 April 1918.[15][16]
 Justices of the peace and bail justices, who are also volunteers, are appointed to serve a semi-judicial function in all areas of the Victorian community. The main official roles in the Victorian community include witnessing statutory declarations, witnessing affidavits and hearing bail matters outside court hours (bail justices only).[17][18]
 The first woman to become a JP in Victoria was Mary Catherine Rogers who, in 1920, became the first woman councillor in Australia when she was elected in the City of Richmond.[19]
 Justices of the peace provide a service to the community as independent witnesses of statutory declarations, powers of attorney and affidavits. JPs, who are also volunteers, are selected through an extensive interview, written exam and practical testing. They are recommended by the state attorney-general and appointed by the governor-in-council, and it is their job to authorise and witness statutory declarations and affidavits within the state of Victoria. As of August 2022, there are currently around 3500 JPs and bail justices in Victoria, who collectively sign more than 1.5 million documents and assist more than 350,000 people each year.[20]
 Justices of the Peace and Bail Justices may use the post-nominals JP and BJ respectively after their names.[21]
 The primary role of a bail justice is to hear bail applications, including after-hours bail, (under the Bail Act 1977 (Vic)) and to hear applications for Interim Accommodation Orders for children (under the Children, Youth and Families Act 2005 (Vic)) within Victoria. Bail justices can also witness Victorian statutory declarations and affidavits.[18] Bail justices are appointed for terms of four years and may be re-appointed repeatedly until they attain 70 years of age.[22] They are often required to attend call outs and rule on bail applications or protection applications for children in danger on weekends and late at night when the courts are closed.[23] Candidates must successfully complete a three-day training course run by the Department of Justice. Bail justices, also have some limited powers under federal legislation, including the power to conduct interstate extradition hearings and extending question time for federal police.
 The most common functions performed by a justice of the peace in New South Wales are to witness the signing of a statutory declaration, witness the signing of an affidavit and certify that a copy of an original document is a true copy.[24]
 JPs are appointed by the Governor of New South Wales for five-year terms. They are volunteers, who come from all walks of life and all sections of the community. JPs are people who are trusted to be honest, careful and impartial when performing the functions of a JP. They must not charge a fee or accept a gift for providing JP services, tell people what to write in a statutory declaration or affidavit or write it for them or give them legal advice.[24]
 Ways to find a JP in New South Wales include:
1. Search the JP Public Register. The register lists all JPs for each postcode area and provides a telephone contact number for JPs who serve the community directly. 
2. Check a public listing of scheduled JP services to find when JPs are available at scheduled times and locations across the state.[25]
 In the early years of the Colony of New South Wales, justices of the peace had far greater responsibilities and broader roles in the administration of justice than now.[26]
 In South Australia, there are two types of justices: justice of the peace and special justices.
 A justice of the peace in South Australia is typically someone of good stature in the community who is authorised to witness and sign statutory declarations, affidavits, waiver rights, search warrants, drug warrants, divorce documents, and to certify copies of original documents and to witness the signing of power of attorney and guardianship documents, providing the JP is satisfied with the capability of the signatory.
 A Special Justice (SJ) is a higher level of justice of the peace in South Australia; they sit on the bench of the magistrates' court hearing cases in the petty sessions division.
 The South Australian Attorney-General has set up a web site to locate justices of the peace.[27] The majority of metropolitan and many regional Councils (Local Government authorities) have a rotational justice of the peace in residence at nominated times.
 South Australia's first women justices were appointed in July 1915.[28]
 Justices of the peace in Western Australia are appointed by the Governor who authorises them to carry out a wide range of official administrative and judicial duties in the community.
 As well as presiding in the Magistrates Court, justices of the peace are regularly called upon by the WA Police to sign search warrants and authorise the issuing of summonses. The administrative tasks include witnessing affidavits and documents such as wills and statutory declarations.
 ""Visiting justices"" are a special group of justices of the peace, appointed to preside over cases within the prison system.[29]
 JPs for Western Australia also cover the Australian External Territories of Cocos (Keeling) Islands and Christmas Island.
 In Belgium, the justices of the peace (Dutch: vredegerecht, French: justice de paix, German: Friedensgericht) function as the small claims courts in the country's judicial system; they stand at the bottom of the Belgian judicial hierarchy and only handle civil cases. There is a justice of the peace in each judicial canton of Belgium, of which there are 187 in total as of 2017. The justices of the peace have original jurisdiction over cases in which the disputed amount does not exceed 5,000 euro (as of September 2018), except for the matters over which another court or tribunal has exclusive jurisdiction. In addition, the justices of the peace have original jurisdiction over a number of matters irrespective of the disputed amount, such as cases involving the renting or leasing of real estate, evictions, easement, land consolidation, consumer credit or unpaid utility bills. The justices of the peace also have original jurisdiction in certain aspects of family law, most notably legal guardianships for incapacitated seniors, and the involuntary commitment of the mentally ill to psychiatric facilities. The judgments made by the justices of the peace can, with some exceptions, be appealed to the tribunals of first instance.[30][31]
 In Canada, justices of the peace play a role in the administration of justice at the provincial level. Justices are generally appointed by the lieutenant governors of Canada's provinces, and by the commissioners of Canada's territories, on the advice of their relevant premier or Attorney General. Canada made the second (first was in South Australia a year earlier) appointment in the then British Empire of a woman as a magistrate, namely Emily Murphy, who was sworn in as a police magistrate in the Women's Court of the City of Edmonton (Alberta) on 19 June 1916.
 In British Columbia, pursuant to the Provincial Court Act, all judges are justices of the peace, and hence all of them are peace officers.[citation needed]
 In the Northwest Territories, justices may hear summary conviction matters, municipal by-laws, and certain criminal matters.[32] However, in more populated provinces justices usually preside over bail hearings and provincial offences courts. When not in a court session, a justice can perform other judicial functions, such as issuing search warrants.
 In Ontario, justices of the peace can preside over judicial interim release (bail) hearings and other criminal hearings. JPs can also exercise jurisdiction over provincial regulatory offences and municipal by-law prosecutions. JPs must retire by reaching the age of 65, but may continue working until 75 subject to the approval of the Chief Justice of the Ontario Court of Justice.[33]
 In Quebec, there are two type of justices of the peace, administrative justice of the peace and presiding justice of the peace.
 Administrative justice of the peace are court officers appointed by the Minister of Justice, and perform duties such as receiving criminal informations and issuing warrants. Presiding justice of the peace are appointed by commission under the Great Seal, and can try some criminal matters and issue warrants. They are appointed from advocates of at least ten years' standing and serve full-time until the age of 70.
 In Yukon, justices of the peace are lay officers of the court. They sit in the Justice of the Peace Court, which is part of the Territorial Court of Yukon.
 The Code of Criminal Procedure, 1898
( ACT NO. V OF 1898 )[2]
 Chapter II OF THE CONSTITUTION OF CRIMINAL COURTS AND OFFICES[3]
 25. In virtue of their respective offices, the Judges of the Supreme Court are Justices of the Peace within and for of the whole of Bangladesh, Sessions Judges, Chief Judicial Magistrate and Metropolitan Magistrates are Justices of the Peace within their respective jurisdictions.
 (Justice of the peace for the mafassal[34])
22. The Government may, by notification in the official Gazette, appoint such persons resident within Bangladesh and not being the subjects of any foreign State as it thinks fit to be Justices of the Peace within and for the local area mentioned in such notification.
 In Hong Kong, the historical functions of justices of the peace have been replaced by full-time, legally qualified magistrates. Nowadays, justices of the peace are essentially titles of honour given by the Government to community leaders, and to certain officials while they are in their terms of offices. They have no judicial functions, and their main duties include visiting prisons, institutions for young offenders and drug addicts, psychiatric hospitals, remand homes, places of refuge, reception and detention centres,[35] administering statutory declarations, and serving as members of advisory panels. They also monitor the drawing of the Mark Six to ensure fairness.
 In India, justices of the peace exist,[36] but no longer occupy a prominent post. One of the famous justices in India was Kavasji Jamshedji Petigara.
 'Justices of the peace' existed in Ireland prior to 1922, sitting in a bench under the supervision of resident magistrates at petty sessions to try minor offences summarily, and with a county court judge (in his capacity of chairman of quarter sessions) and jury to try more serious offences at quarter sessions. In the Irish Free State the position was effectively abolished by the District Justices (Temporary Provisions) Act 1923[37] and permanently abolished by the Courts of Justice Act 1924. Their judicial powers were replaced by full-time, salaried, legally qualified district justices (now called district judges) and their quasi-judicial powers by unpaid lay peace commissioners. However, the power of Peace Commissioners has been reduced following a number of Supreme Court Challenges. Even one Government Department stopped accepting Peace Commissioner signatures because ""there is no available updated register or reliable data base to confirm that the person signing the form as a witness is in fact a Peace Commissioner. In the absence of such verification being possible, the practice was changed to remove the risk of fraudulent activity and maintain the integrity of the process."" In general, Peace commissioners may sign statutory declarations, and may rarely issue summons and search warrants to the Garda Síochána (Irish police).[38] A peace commissioner can witness the signature of an affidavit. In addition Peace Commissioners can sign custody agreements between legal guardians in relation to visitation etc. These agreements are legally binding and can be altered only by a Judge in the regular courts. Peace Commissioners are appointed on the basis of good character and usually prominent standing in their local communities.
 A justice of the peace, according to the Ministry of Justice, is a person of unquestionable integrity who seeks to promote and protect the rights of the individual and helps to provide justice to persons in a particular community. Additionally, the JP serves as a justice in petty court sessions, attends juvenile court sessions, issues summonses, considers applications for bail, explains and signs legal documents, sits on licensing panels, and gives counsel/advice. Any Jamaican citizen that can speak and write English is eligible to become a JP. Any club/organisation/citizen can recommend someone to become JP for a community. JPs are chosen under the Governor-General's discretion.
 In Malaysia, justices of the peace (jaksa pendamai in Malay, also abbreviated JP) have largely been replaced in magistrates' courts by legally qualified (first-class) stipendiary magistrates. However, state governments continue to appoint justices of the peace as honours. In 2004, some associations of justices of the peace pressed the federal government to allow justices of the peace to sit as second-class magistrates in order to reduce the backlog of cases in the courts.
 The legal framework for the office of Justice of the Peace within New Zealand is derived from the Act of Parliament 'Justices of the Peace Act 1957', and subsequent amendments.[39]
 There are two levels of this position within New Zealand: The standard level of 'Justice of the Peace', and a separate 'Judicial Justice of the Peace'. Persons so appointed may use the post-nominals JP and JJP respectively. A JP who is retired may apply to the Secretary for Justice for permission to use the post-nominals 'JP (retired)'.[40]
 Appointment as a JP is for life, unless a voluntary resignation is tendered in writing, or a JP is suspended or dismissed from office due to misconduct, bankruptcy, or other specific reasons.[41]
 A JP in New Zealand is someone of good stature in the community who is authorised to witness and sign a number of documents, including statutory declarations, affidavits, and producing certified copies of documents, amongst others. In some limited circumstances they may also perform citizenship ceremonies, and act as a 'Visiting Justice' in prisons. They are nominated for office by local Members of Parliament and appointed by the Governor-General. They must take both the Oath of Allegiance and the Judicial Oath.
 If a local JP Federation determines that a need for one or more JJPs exists, they will issue a call for nominations to JPs in their territory. Following closure of nominations, a shortlist will be drawn up, and interviews undertaken. If a prospective applicant passes the interview stage, and the relevant training, exams, and assignments are successfully completed, they can be sworn in as a JJP. This means they can then be assigned to the bench in the relevant District Court to oversee minor criminal cases. These would involve tasks such as the exercise of powers to remand defendants in custody, grant bail, and adjourn court hearings.
 Appointment as a JJP is at the pleasure of the Secretary for Justice, generally for a minimum of 5 years, and for as long as there is a need in the area an individual resides. Should an individual move abroad, to an area of New Zealand where the service is no longer required, or the local area no longer requires it, the JJP appointment will be terminated.
 Prior to 2012, all JPs were able to issue search and arrest warrants, but with the passage of the NZ Act of Parliament ""Search and Surveillance Act 2012"", this power was spun off to the separate position of Issuing Officer.[42]
 Under the law, an Issuing Officer does not have to be a Justice of the Peace - Holders of some positions within the judicial system (e.g. Registrar or Deputy Registrar of a court) are Issuing Officers automatically under the law while serving in those roles, and cease to be so when they are not. For a JP or JJP to exercise this power after 2012, they must apply to become an Issuing Officer separately. The application process involves a further training course and exam, followed by a vetting and approval process by the Attorney-General. Appointment as an Issuing Officer is for a maximum of three years, and may be renewed for a further three years at expiry as needed.
 Sections 22, 22-A and 22-B of the Code of Criminal Procedure Code, 1898 provide for the appointment of justices of the peace by the provincial governments, their powers and duties respectively. However, seldom are justices of the peace appointed in Pakistan outside the judiciary. Session and additional session judges act as ex-officio justices of the peace as per Section 25 of the Code of Criminal Procedure, 1898. An Ex-officio Justice of the Peace may issue appropriate directions to the police authorities concerned on a complaint regarding-
(i) non-registration of a criminal case;
(ii) transfer of investigation from one police officer to another; and
(iii) neglect, failure or excess committed by a police authority in relation to its functions and duties.
Such functions being quasi-judicial in nature could not be termed as executive, administrative or ministerial.(PLD 2016 Supreme Court 581)
 It is pertinent to note however, as many academics have pointed out, that there is great utility in the appointment of such justices especially in rural areas where enmity between rival groups can lead to the inability of registration of cognizable offences and biased judicial proceedings.
 In Singapore, the functions of Justices of the Peace have been replaced by full-time, legally qualified magistrates. JP in Singapore are appointed amongst outstanding individuals in Singapore who have made significant contributions in their professions, the public service, social services and the community at large by the President of the Republic of Singapore, under section 11(1) of the State Courts Act 1970 for a renewable five-year term.[43] 
 JP do not serve any judicial roles, rather they derive their functions from statute.[44] Some examples of the functions, powers and duties of a Justice of the Peace includes:
 Other roles of a JP includes being mediators in the State Courts of Singapore or marriage solemnisers in the Registry of Marriages.[45] 
 Newly appointed justices of the peace are required by Section 17 of the State Courts Act to take the oath of office and allegiance as set out in the schedule to the State Courts Act, before exercising the functions of their good office. The President may also revoke the appointment of any Justice of the Peace.
 In Sri Lanka, Justice of the Peace is an honorary post, with authorisation to witness and sign statutory declarations and affidavits as well as certify documents. Persons appointed as a Justice of the Peace may use the post-nominal JP. Current appointments are made under the Judicature Act No 02 of 1978, by the Minister of Justice at his/her discretion by publishing a list in The Gazette and appointee taking oaths before a high court, district court judge or magistrate with registrar of the supreme court recording it. There are four types of appointments of Justice of the Peace;
 Senior Attorney at laws are appointed as Justice of the Peace and Unofficial magistrates to preside in the absence of a sitting Magistrate.[46] Any citizen of Sri Lanka can apply to the Ministry of Justice giving his or her credentials to be appointed as a justice of the peace. However, the applicant should be one who has served the public and carries out social service and should be of good standing. These JPs would be appointed with legal authority in all parts of the island or limited to a judicial district. The President of Sri Lanka and his/her officers are ex officio justices of the peace. There about 100,000 JPs in the island.[47]
 The post was introduced in the island during the British colonial era by the Governor Frederick North in 1801 and was later reformed in 1871.[47] Until 1938, appointments were made by the Governor, after which appointments were made by the Legal Secretary until 1947. After Ceylon gained its independence in 1948, appointments were made by the Governor General and the Minister of Justice. Justice of the Peace had the power to administer oaths and affirmations per the Courts Ordinance No. 1 on 1889 section 84 and they could formally appoint members of the public to act as special police officers in times of turmoil and riots. Since certain government officers were ex-officio justices of the peace, this allowed British colonial officers to appoint special police officers from the European planters in times of crisis such as the 1915 riots.[48] The Village Councils Law (No. 6 of 1964) made the Chairman of the Village Council an ex officio justices of the peace for that village area.[49]
 In 2014, for the first time, Justices of the Peace were authorised in Tonga. JPs are appointed by the Crown, but the Lord Chief Justice regulates their duties and defines their powers. The first JPs were warranted with duties including granting bail; issuing search warrants and subpoenas; taking affidavits, declarations and oaths; and having the power to witness documents. Term of office is one year and officials can be reappointed. The initial 19 JPs appointed were: ‘Aisea Ta’ofi and Sione Hinakau of Niuatoputapu; ‘Inoke Tuaimei’api of Niuafo'ou; Siosiua Hausia from ʻEua; Sione Palu, Sione Fakahua, Me’ite Fukofuka and Kisione Taulani of Ha’apai; Salesi Kauvaka, Viliami Pasikala, Haniteli Fa’anunu, Meli Taufaeteau and Moleni Taufa from Vava’u; and Salote Fukofuka, ‘Amelia Helu, ‘Ofa Likiliki, Tevita Fakatou, Sioape Tu’iono and Semisi Tongia of Tongatapu.[50]
 A magistrates' court in England and Wales is typically composed of a bench of (usually three) Justices of the Peace (otherwise known as magistrates) who dispense summary justice. They decide on offences which carry a community sentence, a prison sentence (maximum of six months for any one offence, up to one year for multiple offences), or an unlimited fine.[51][52] They are advised on points of law and procedure by a legally qualified justices' clerk and their assistants.[53] In practice, JPs have a wide range of sentencing options, which include issuing fines, imposing community orders, or dealing with offences by means of a discharge. In more serious cases, where magistrates' consider that their sentencing powers are insufficient, they can send 'either-way' offenders to the Crown Court for sentencing.[54]
 Justices of the Peace are trained volunteers.[55] No formal qualifications are required but magistrates need intelligence, common sense, integrity and the capacity to act fairly.[55]  Membership is widely spread throughout the local area and drawn from all walks of life following a rigorous selection process undertaken by a local advisory committee, who recommends to the Lord Chancellor those individuals who have demonstrated the five key qualities for appointment which are: (1) awareness of social issues; (2) maturity and understanding; (3) reliability and commitment; (4) understanding of documents and effective communication; and (5) logical thinking.[56] Justices of the Peace are trained volunteers, but those who are employed in some occupations (e.g. Police Officers) cannot be appointed due to potential conflict of interest.[57]
 All new Justices of the Peace undergo comprehensive training before sitting. There is a mentoring program to help guide new appointees (mentors are magistrates with at least three years' service[58]). The training is delivered by the Judicial College and covers the necessary law and procedure required for their role. They continue to receive training throughout their judicial career, and are appraised every four years (every two years for a Presiding Justice) to check that they continue to remain competent in their role.[59] Additional training is given to justices choosing to sit in the Youth Court, or those dealing with family matters. New JPs sit with mentors on at least six occasions during their first eighteen months. Justices of the Peace are unpaid appointees, but they may receive allowances to cover travelling expenses, subsistence, and loss of earnings for those not paid by their employer while sitting as a magistrate, up to £116.78 a day. Such person may sit at any magistrates' court in England and Wales, but in practice they are appointed to their local bench (a colloquial and legal term for the local court). Justices of the Peace often sit as a panel of three; two as a minimum in most cases, save for cases under the Single Justice Procedure. Many are members of the Magistrates' Association, which provides advice, training and represents magistrates.[60] Justices of the Peace must sit for a minimum of 26 sessions (half-days) per year.[61] An employer must, by law, allow a Justice of the Peace reasonable time off work to serve as such.[61][62][63]
 The lead magistrate is known as a Presiding Justice (PJ) and should be addressed in court as ""sir"" or ""ma'am"" or ""your worship"", and the magistrates collectively as ""your worships"". In writing they are their usual name followed by ""JP"" (for Justice of the Peace).[64] Other magistrates on the bench are known as ""wingers"".[65] All three magistrates contribute equally to the decision-making and carry equal authority, but the Presiding Justice will speak on their behalf in open court.[65]
 Magistrates' courts today can deal with lesser offences such as all summary offences, and some more serious triable 'either-way' matters, but where the magistrates' deem that their sentencing powers are sufficient. However all criminal cases start in the magistrates' court. They handle over 95% of the criminal cases in England and Wales and Northern Ireland.[66] With more serious offences, magistrates are responsible for indictment and committal to the Crown Court (a task in former times dealt with by a grand jury). Magistrates also have a civil jurisdiction, such as a family jurisdiction, or appeals against matters relating to licensing. Although they had a licensing jurisdiction dealing liquor, betting and clubs licensing applications, this was transferred under the Licensing Act 2003 to local authorities. The magistrates now act in licensing matters only as an appeal court from the decisions of the local authority. Justices of the Peace are responsible for granting orders such as search warrants to the Police and other authorities. They used to have to live within 15 miles of where they sit in case needed to sign a warrant after hours. were replaced with Local Justice Areas by the Courts Act 2003, meaning this is not formally required. Section 7 of the Courts Act 2003 states that ""There shall be a commission of the peace for England and Wales—…b) addressed generally, and not by name, to all such persons as may from time to time hold office as justices of the peace for England and Wales"". Thus, every magistrate in England and Wales may act as a magistrate anywhere there.[67]
 Cardiff Magistrates' Court is the only court in the country which deals with offences under the Companies Act 2006, such as for late filing of accounts or directors' offences. Westminster Magistrates' Court has special responsibilities for dealing with terrorism and extradition offences throughout the UK.[68]
 The Courts Act 2003 provides the current framework for appointment of the justices, which is done by the Lord Chancellor in the name of sovereign. Justices can also be removed by the same mechanism.[69]
 Before 1714, magistrates were liable to be approached at any time and in any place by people legally recognised as paupers, appealing for aid if parish authorities refused to provide any. It was relatively common for these magistrates to write out, on the spot, an order requiring aid to be granted.[70] The magistracy is an ancient institution, dating in England from at least 1327. The role is underpinned by the principles of 'local justice' and 'justice by one's peers.[71]'
 As at 2021, 56% of sitting magistrates were women, 13% were black, Asian and minority ethnic, and 82% aged above 50 as at 1 April 2021.[72]
 Within the Scottish legal system Justices of the Peace are trained volunteers who currently sit in the Justice of the Peace courts. These courts were introduced in 2009 as a replacement for the district courts (established in 1975), which in turn replaced burgh police courts.[73] Justices sit alone or in threes with a qualified legal assessor as convener or clerk of court. They handle many cases of breaches of the peace – drunkenness, minor assaults, petty theft, and offences under the Civic Government (Scotland) Act 1982.[74]
 The maximum sentencing power of a justice of the peace is 60 days imprisonment, or a fine up to £2,500, or both, and the ability to disqualify drivers.[75]
 In 2006, the Scottish Government announced its intention to unify the management of the sheriff and district courts in Scotland but retain lay justices, as part of its initiative to create a unified judiciary under the Lord President. Following the passage of the Criminal Proceedings etc. (Reform) (Scotland) Act 2007 the justice of the peace courts were implemented on a sheriffdom-by-sheriffdom basis.[76]: Section 59 
 In Glasgow, the volume of business required the employment of three solicitors as ""stipendiary magistrates"" who sat in place of the lay justices. The stipendiary magistrates' court had the same sentencing power as the summary sheriff court in summary proceedings, which was the ability to sentence an offender to up to one year in prison or fine them up to £10,000. Stipendiary magistrates were replaced by summary sheriffs.[77][78]
 In Northern Ireland, the situation initially continued as it had in pre-1922 Ireland. However, justices of the peace no longer sat out of petty sessions after 1935 (Summary Jurisdiction and Criminal Justice Act (Northern Ireland) 1935).[79] Since then, magistrates' courts in Northern Ireland have consisted of legally qualified resident magistrates (now known as district judges (magistrates' courts)) sitting alone, except in cases involving children, where two lay panelists sat with the magistrate. Justices of the peace were confined to the power to conduct committal hearings, bind persons over to the peace, sign warrants, summons, and other official documents. They were appointed by the Lord Chancellor on the recommendation of a committee in each county court division.
 The Justice (Northern Ireland) Act 2002 introduced a new office of lay magistrate, to sit alongside resident magistrates at magistrates' courts in certain matters. Unlike in England and Wales, ""lay magistrate"" is the official title of the position, to distinguish from existing justices of the peace who do not sit in the magistrates' courts. The first lay magistrates were appointed in 2005. Two lay magistrates sit with the district judge (magistrates' court) in criminal proceedings involving children (replacing the former lay panelists) and Family Proceedings Court matters. The district judge (magistrates' court), who is a barrister or solicitor of at least seven years standing, presides over the bench.[80] Most criminal justice functions of JPs were transferred to lay magistrates. It is expected that there will be no further appointments of justices of the peace in Northern Ireland, although those already appointed retain the title and any functions not transferred to lay magistrate under the 2002 Act.
 In some US states, the justice of the peace is a judge of a court of limited jurisdiction, a magistrate, or a quasi-judicial official with certain statutory or common law magisterial powers.[81] Some states have special qualifications or unique features for the office.
 The justice of the peace typically presides over a court that hears misdemeanor cases, traffic violations, and other petty criminal infractions. The justice of the peace may also have authority over cases involving small debts, landlord and tenant disputes, or other small claims court proceedings. Proceedings before justices of the peace are often faster and less formal than the proceedings in other courts. In some jurisdictions a party convicted or found liable before a justice of the peace may have the right to a trial de novo before the judge of a higher court rather than an appeal strictly considered.
 A justice of the peace also performs civil marriages.
 A justice of the peace has the same jurisdiction as a municipal magistrate with respect to traffic and misdemeanor cases and restraining orders, though over cases whose affairs are not contained within the confines of a single municipality. Additionally, the Justice Court hears cases involving county ordinances (ordinances enacted by the board of supervisors that apply only to unincorporated areas), civil lawsuits up to a limit of $10,000, small claims cases up to $2,500 (up to $3,500 in Maricopa County[82]), and issues evictions, called writs of restitution (after a forcible detainer or special detainer action (eviction) being successfully completed by a landlord). Justices of the peace, also called JPs, or Judges of the Justice Court, are elected in partisan elections for four-year terms from specific districts called precincts. They have the same authority and responsibility as all other judges in the state with respect to performing marriages, administering oaths, adhering to the code of judicial conduct, and all aspects of justice administration. However, Arizona law does not require justices of the peace to be lawyers. Many justices of the peace are not legally trained, although all are required by the Arizona Supreme Court to complete a course at the Arizona Judicial College. As with JPs, municipal judges in Arizona are not required to be lawyers.
 In Arkansas, a justice of the peace is an elected official equivalent to a county commissioner or county supervisor in some other states. Arkansas JPs sit on a county quorum court, composed of 9, 11, 13 or 15 JPs. The quorum court is a part-time body, elected from single-member districts, that has overall responsibility for county affairs. Among their responsibilities are passing the budget, creating new ordinances (at the misdemeanor level), setting property tax millage levels, and working with other elected officials. The full-time elected county administrator, who presides over the quorum court, is the county judge. Neither JPs nor the county judge have any judicial authority, though they do have the power to preside over civil marriages. Justices of the peace are elected every two years to these partisan offices.
 Justices of the peace in Connecticut can preside over marriages. Unlike some states, Connecticut JPs are not penalized for refusing to perform such ceremonies. They have the same general oath-giving powers as a notary public.
 Florida had justices of the peace (with corresponding constables) from the time of its acquisition from Spain in 1821 until the Florida Constitution was amended in 1968 to abolish the post. From about 1940 to 1968, Florida counties had the ability to hold local referendums to allow county voters to abolish the post on a county-by-county basis. For example, Leon County, the location of Tallahassee, Florida's capital city, voted to abolish justices of the peace (and their associated constables) in the Fall elections of 1958. By 1958, the county commission had reduced the number of JOP districts from a turn-of-the-century peak of 13 districts to just two districts. The automobile age made the county sheriff able to patrol the entire county and made it possible for the citizenry to travel to the courthouse for legal proceedings.
 Justices of the peace in Louisiana are elected to serve six-year terms as the judicial authority of a ward or district, but not where city courts exist. They have jurisdiction in civil matters when the amount in dispute does not exceed $5,000. They do not have jurisdiction when a title to real estate is involved, when the state or any political subdivision is a defendant, or in successions or probate matters. They are authorized to perform marriage ceremonies. There are around 390 such Justices of the peace.[83][84]
 In Maine, the office of the Justice of the Peace was merged with Notary Public, a process that took place from 1981 to 1988.[85] The duties that were attached to the office of Justice of the Peace were fully transferred to the Notary Public in 1988. The office currently named as ""Justice of the Peace"" in Maine is a court officer, an attorney, involved in the process of issuing warrants and responding to complaints. The office however is not related to either the current Notary Public office or the previous Justice of the Peace office and is part of the Maine Bureau of Corporations, Elections & Commissions.[85]
 Justices of the peace in the Commonwealth of Massachusetts are commissioned by the Governor with the advice and consent of the Council for seven-year terms. They are often called on to solemnize non-religious marriages, especially same-sex marriages, which certain religious officials are not willing to oversee. They have the same general oath-giving powers as a notary public, and are also empowered to issue certain writs. Justices of peace are also empowered to keep the peace and suppress riots, and enforce all laws that keep public peace.[86] Furthermore, upon the recommendation of a town selectman, they may be authorized by the governor to take bail in criminal cases.[87]
 In Minnesota, the office of the Justice of the Peace was abolished in 1977 (Minn. Stat. 487.35). It has not existed for 40-plus years although some people who offer private wedding officiant services erroneously claim to be Justices of the Peace, this term may not properly be used inasmuch as the office has been abolished. Under Minnesota law, however, judges, retired judges, court administrators, retired court administrators, and other public officials designated in statute may officiate or solemnize marriage ceremonies in addition to licensed or ordained ministers of any religious denomination who have filed their credentials with a county registrar (Minn, Stat. 517.04).
 Justice courts are courts in New York State that handle traffic tickets, criminal and environmental conservation law matters, small claims and local code violations such as zoning. Though justice courts constitutionally are part of the New York State Unified Court System, state law generally makes justice courts independent of New York's Office of Court Administration (OCA) and instead makes justice courts the responsibility of their sponsoring localities. Town justice courts are often called town courts, and village justice courts are often called village courts. City courts in New York State handle mostly the same types of cases but are not justice courts.
 The official title for judges in justice courts is justice, the same as in New York Supreme Court. However, in common usage, most people, including lawyers, call them judge. In general, justices in justice court do not have to be lawyers, except for justice courts with jurisdiction in villages. The vast majority are not. Many of these courts are in small towns and villages where none of the residents are lawyers. In the larger towns, the justices are almost always lawyers.
 While justices and their court clerks receive training from OCA, there is tremendous variability in how cases are handled. This includes court procedures and substantive results. Some courts will dismiss a traffic ticket if the officer does not appear for a trial, while others will adjourn the matter to give the officer another chance. In some courts the police prosecute their own tickets, while in others an assistant district attorney from the county or a town or village attorney will prosecute the tickets. This may even vary by the type of officer, with state troopers and deputies prosecuting their tickets and a town attorney prosecuting tickets written by the town police.
 Larger towns can have very busy caseloads, including several sessions a week with dozens of cases at each session, and people may have to wait hours before their cases are heard. In some small towns the caseload is extremely light, and a court might meet once a month and have only a few cases.
 All criminal prosecutions that occur in towns and villages are commenced in a justice court. Misdemeanors are handled exclusively in the justice court, while felonies generally move up to county court after defendants are arraigned in a Justice Court before the case moves forward.
 Similar matters in some places outside New York are handled by a justice of the peace.
 Town and village Justices also possess limited powers of a New York notary public, ex-officio, only within the county in which the town or village for which they serve is located; they may administer oaths and affirmations and take acknowledgments and proofs of execution. Some Justices seek and obtain a formal New York notary public commission to permit free travel statewide and enjoy the additional privileges and international legal recognition of a notary public.[88]
 New Hampshire justices of the peace are commissioned magisterial officers, appointed by the Governor and Executive Council to terms of five years, with the power to administer oaths, acknowledge instruments, perform marriage ceremonies[89] and, effective 1 January 2008, solemnize civil unions for same-sex couples.[90] They may also order compulsory mental examinations for good cause,[91] act as a magisterial official regarding enforcement complaints on orders for isolation or quarantine issued by the Commissioner of Health and Human Services,[92] administer oaths of office to public officials,[93][94] take depositions[95] and issue subpoenas.[96][97] New Hampshire justices of the peace are also authorized, upon a showing of probable cause supported by affidavit, to issue arrest warrants,[98][99] search warrants,[100] administrative inspection warrants[101] and by court appointment, to fix and receive bail in criminal cases.[102][103][104]
 Justices of the peace existed during the time of the Province of North Carolina (1712–1776.) They were appointed by the colonial Governor and served in counties or districts. Justices were essential for conducting court business.  After independence, the 1776 Constitution of North Carolina and an act passed by the North Carolina General Assembly of 1777 re-defined justices of peace in the State of North Carolina. Justices of the peace were authorized to marry and had other court duties and assisted in collecting taxes. Duties of the justices were further reduced after the 1868 North Carolina Constitution and inferior courts were abolished.[105][106][107][108][109]
 See also Oregon Justice Courts
 In Oregon, as of 2023, Justices of the Peace preside over 22 Justice Courts.[110][circular reference]  Oregon Justices of the Peace are state court judges, even though their salary is paid by their respective counties.[111]  A Justice of the Peace is elected for a term of six years.[112] If a Justice of the Peace position becomes vacant during the Justice's term, the Governor appoints a Justice to fill the vacancy until the next general election.[113]
 In Oregon, Justices of the Peace have jurisdiction over civil lawsuits of less than $10,000;[114] evictions;[115] misdemeanors,[116] and violations,[117] like traffic tickets, boating violations, and wildlife violations. Further, Justice Courts have jurisdiction over violations of the county code, and some Justice Courts act as the municipal court of certain cities.[118] Justice courts do not have jurisdiction over certain types of civil cases, including disputes over title to real estate, false imprisonment, libel, slander, and malicious prosecution.[119] Justices of the Peace are authorized to perform courthouse weddings in Oregon.[120]
 In Texas, JPs are elected on a partisan ballot every four years.
 Texas does not require a JP to be an attorney in good standing. However, JPs are required to be ""well versed in the law"" and take mandatory classes to retain their office. New JPs are required to take 80 hours of legal, state-mandated classes the first year, and 20 hours each year thereafter during their tenure in office.
 Sections 18 and 19 of Article V of the Texas Constitution, as well as Chapters 27 and 28 of the Texas Government Code, outline the duties of these Courts and their officers.
 Under Section 18, the number of JPs (and associated constables) is dependent on the size of the county:
 Section 19 sets forth the minimum jurisdiction of the JP court:
 JP cases are appealed to the county court level; the appeal results in a trial de novo. In criminal cases, cases beginning in justice court cannot be appealed beyond the county level court unless the fine is more than $100 or a constitutional matter is asserted.
 In smaller counties without a coroner, the JP has the duty to perform inquests. The JP is also called out for any unattended deaths in the county.
 A JP in a large precinct in a large county will work 5 days a week, 8 or more hours daily. Their duties will include, but are not necessarily limited to the following: trials of civil matters, both to a 6-person jury and to the bench, with an amount in controversy not exceeding $20,000.00. Trials of criminal matters involving traffic violations and class C misdemeanors punishable by fine only. Pre-trial motion dockets and show-cause hearings are held, and all discovery must be approved by the Judge in advance in civil cases. All criminal matters are controlled by the rules of criminal procedure and evidence. A much more restricted and smaller set of rules apply in civil matters unless, in the Judge's discretion, it is believed to be in the best interests of justice to apply the standard rules of evidence and procedure. The court has the exclusive jurisdiction of evictions. A Texas JP Judge will also magistrate prisoners and set bail. The Judge will hear juvenile violations such as truancy, underage drinking and smoking. Warrants of Arrest, Alias, Search and Capias Profine are issued. Protective Orders can be issued and result in jail time if violated. Several administrative matters are heard including the finding of a Dangerous Dog, Occupational Drivers License and tow hearings. Many writs are issued such as writs of re-entry to apartments, possession of realty and to reinstate utilities a landlord may have turned off. A JP has contempt power of $100 and up to 3 days in jail per occurrence. A JP is also authorized to perform marriage ceremonies.[121]
 Justices in Vermont are elected county officials, serving two-year terms. They are elected from each town or city within a county, and the number of justices elected from each municipality varies based on population, from as few as 3 or 5, to as many as 12 or 15. They generally serve as election, poll, and town meeting officials, and sit on the boards of civil authority and tax abatement within their municipalities. When assembled as the board of civil authority, they have the authority to decide, in the first instance, election disputes and disagreements about whether a voter should be registered. When the assessed value of property for real estate tax purposes is appealed to the board of civil authority, at least three board members are appointed to inspect the property. They may perform civil marriages throughout the state and are eligible to serve as notaries without payment of the usual registration fee. Justices may also serve as a magistrate when commissioned by the Supreme Court.[122]
 The option to serve as a magistrate has never been invoked and likely never will be; in June 2019 Associate Justice of the Vermont Supreme Court Marilyn Skoglund described the idea of commissioning a justice of the peace as an actual magistrate as ""a truly frightening idea"" and stated that she had never heard of such a thing actually happening.[123]
 Esther Hobart Morris became the first female justice of the peace in the United States in 1870.[124][125] She began her tenure as justice in South Pass City, Wyoming, on February 14, 1870, serving a term of nearly 9 months.[124][125] The Sweetwater County Board of County Commissioners appointed Morris as justice of the peace after the previous justice, R.S. Barr, resigned in protest of Wyoming Territory's passage of the women's suffrage amendment in December 1869.[125][126]
 In many states, the office of justice of the peace has been abolished or transferred to another court, such as the magistrate court.[127] Cases in large cities may be heard in a municipal court which has jurisdiction only within that city. Most efforts to abolish the office of justice of the peace have been led by the American Bar Association, which views non-lawyer judges as no longer necessary, as there are now far more persons with formal legal education than in the past when justices of the peace were first used.
 California formerly had justice of the peace courts staffed by lay judges, but began phasing them out after a landmark 1974 decision in which the Supreme Court of California unanimously held that it was a violation of federal due process (in the state's view of the Fourteenth Amendment to the U.S. Constitution) to allow a nonlawyer to preside over a criminal trial which could result in incarceration of the defendant.[128] The court specifically recognized that in the aftermath of Gideon v. Wainwright (1963), it was unreasonable to allow a case to be tried before a layperson incapable of understanding the legal arguments of the defense attorney to whom the defendant was entitled under Gideon.[128] In 1994, the remaining justice courts were consolidated into the municipal courts by the passage of Proposition 191, and in 1998, the electorate passed Proposition 220, which authorized the merger of the remaining municipal courts (the only remaining courts of inferior jurisdiction) into the superior courts (the courts of general jurisdiction).[129] However, the judges affected by each merger in each county had to affirmatively consent and the municipal court judges then had to formally become superior court judges.  This process was completed on February 8, 2001, when California's last four municipal court judges were sworn in as superior court judges in Kern County.[130] Under current California law, all California judges must be licensed attorneys at the time they join the bench.
 However, the Supreme Court of the United States held in North v. Russell, 427 U.S. 328 (1976),[131] that the use of nonlawyer judges in Kentucky's system of police courts accords with the Fourteenth Amendment guarantees of due process and equal protection of the laws.[132]
"
Lawrence Washington (1718–1752),https://en.wikipedia.org/wiki/Lawrence_Washington_(1718%E2%80%931752),"Lawrence Washington (1718 – July 26, 1752) was an American soldier, planter, politician, and prominent landowner in colonial Virginia.  As a founding member of the Ohio Company of Virginia, and a member of the colonial legislature representing Fairfax County, Virginia, he founded the town of Alexandria, Virginia on the banks of the Potomac River in 1749.
 Washington was the older half-brother of George Washington, the future President of the United States. He was the first of the family to live in the house known as Mount Vernon, which he named after British Admiral Edward Vernon, his commanding officer in the War of Jenkins' Ear.[1][2] Washington became ill with tuberculosis. He and George travelled to Barbados, hoping that the warm climate would alleviate his ill health. This failed, and Lawrence died at Mount Vernon the following year.
 Lawrence was born into the Washington family, being believed to have been born in 1718, the second child of Augustine Washington and his wife Jane Butler Washington (whose first-born son, Butler, died in infancy in 1716.)[3] The family was then living in Westmoreland County, Virginia, along the Potomac River. In 1729, Augustine took Lawrence and younger son Augustine, Jr., to England and enrolled them in the Appleby Grammar School in Appleby-in-Westmorland, Cumbria.[4] Augustine would return to Virginia months later and discover that his wife had died, leaving daughter Jane in the care of the extended Washington family in Westmoreland County. In 1731 he married the young heiress Mary Ball, and they had a family. This second Augustine Washington family - including George and his siblings - moved into a home on the Little Hunting Creek plantation in 1735. The foundations of this home can still be seen underneath the present Mount Vernon.[5]
 Lawrence completed his education and returned to Virginia in 1738 to oversee the management of his father's 2,000-plus acre plantation on the Potomac River at Little Hunting Creek (then in Prince William County; after 1742 Fairfax County). In late 1738, Augustine moved his young family to Ferry Farm, which he had recently purchased, on the edge of Fredericksburg in King George County. Prince William County Deed books reveal that the following spring, March 1739, Lawrence Washington began to purchase tracts of land bordering the family's Little Hunting Creek estate: the purchases, in his own name, indicates Lawrence had attained his majority (age 21).
 In late 1739, the British Parliament decided to raise a ""Regiment of Foot"", an infantry unit, in the American colonies, to be used in the West Indies for the war against Spain, known as the War of Jenkins' Ear. The regiment, to be composed of four battalions, was designated Colonel William Gooch's Regiment of Foot. The company commanders were to be recruited in the colonies and Colonel William Blakeney was sent across the Atlantic with blank commissions, signed by King George II, to be distributed to the various governors.  On 10 July 1740, Lieutenant Governor of Virginia William Gooch awarded the senior (of 4) Captain's Commission in one of Virginia's companies to Lawrence Washington: his Commission survives in the archives of the Mount Vernon estate.[6]
 The four Virginia Companies mustered at Williamsburg, Virginia, in August 1740, but the transport ships did not set sail for Jamaica until early October. The main British invasion force did not arrive in Jamaica until early January 1741, and the Conjunct Expedition, under the dual command of Vice Admiral Edward Vernon and Brigadier General Thomas Wentworth, got under way in late January.  In early February, commanders decided to assault the Spanish fortress at Cartagena in present-day Colombia. At that time, some of the Americans were seconded to Admiral Vernon's warships to be used as Marines.  Lawrence was fortunate, as he would later write to his father, to be appointed ""Captain of the Soldiers acting as Marines"" on board Vernon's flagship, the 80-gun HMS Princess Caroline.[7] Because of this service, the 43rd Foot was called ""Gooch's Marines.""[8]
 Washington survived the Battle of Cartagena de Indias and expeditions against the seaport of New Granada and against Cuba and Panama, which suffered a high rate of casualties, mostly from disease. The assault against Cartagena, in March–April 1741, proved a disaster, as over half of the British force fell ill and died of tropical diseases, chiefly yellow fever.[9] The fever predominated amongst the newly arrived troop ships, while the crews on Vernon's warships, having already been in the Caribbean for one year, were largely inured against disease.[10] Washington survived the fevers which killed off nearly 90 percent of the American colonists because of his fortunate (early) transfer from a troop ship to Vernon's flagship, which, having been in the tropics for over one year, had a crew already inoculated against tropical diseases (as well as less crowded conditions aboard ship.)
 In January 1741, some 3,255 officers and men of Gooch's ""American Regiment"" were on board ship in the harbor of Kingston, Jamaica. Not quite two years later, on 24 October 1742, the American survivors could muster only 17 officers and 130 enlisted men, who returned to North America in November and December, accompanied by 268 sick soldiers.[8]  On 17 January 1743, The New York Weekly Journal contained a dispatch from Jamaica dated 24 October 1742:
 Writing a history of British Settlements in North America in 1748, Bostonian Dr. William Douglas said that of 500 men sent from Massachusetts for the expedition, not over 50 returned.[8]
 Washington also participated in the 1741 British landing at Guantanamo (Cumberland Harbor) Cuba, part of Admiral Vernon's plan—never realized—to attack Santiago de Cuba from the rear (by land) and from the front (by sea).[12]
 Upon his return to Virginia at the end of 1742, Washington discovered that the post of militia commander, Adjutant, was vacant. He applied for it and was appointed Adjutant, at the rank of Major, by Governor Gooch in Spring 1743.[4]
 Washington was married in July 1743 to Anne Fairfax (1728–1761), the eldest daughter of English-born Colonel William Fairfax of neighboring Belvoir, and his late wife Sarah (née Walker), born to a prominent family in the Bahamas, where Fairfax had been working when they married. The marriage of the 15-year-old Anne to the newly returned 25-year-old army veteran appears to have been prompted by Anne's disclosure to her parents that the family's minister, the Reverend Charles Green of Truro Parish, had taken opportunities with her.[13]
 About the time of his marriage he began the rebuilding of a house on the site of his father's earlier residence on Little Hunting Creek, naming it Mount Vernon in honor of his wartime commander.[14]
 The new county of Fairfax was created (from northern Prince William County) in 1742. Washington was elected to Virginia's House of Burgesses in 1744 as a representative for Fairfax (both the county and the family.) In 1747, he joined with his father-in-law and other prominent landowners and businessmen in the Northern Neck to create the Ohio Company of Virginia, with the intention of opening trade to the American interior linked to the Potomac River.  To do so, the Company required an ""entrepôt"", a gateway for trade.  The site of Hugh West's tobacco warehouse, on the western banks of the Potomac near the mouth of (Great) Hunting Creek, was deemed a suitable location because its deep water access allowed ships from London to sail directly to the wharf.  But, the local tobacco planters wanted to site a new town away from the river (and its ""played out"" tobacco fields) and further upstream on Hunting Creek.[15]  During the legislative session of 1748–49, Washington had the role of promoting the river site and securing the votes necessary to approve a new town on the Potomac, where it would best serve the interests of the Ohio Company.
 In May 1749, Governor William Gooch signed an Act to establish the town of Alexandria. Washington was granted permission to ""be absent from the Service of the House, for the Recovery of his Health.""[16]  Prior to the first public auction of town lots in July 1749, Washington sailed to London to conduct business on behalf of the Ohio Company, and to consult English physicians regarding his health.  His younger brother George, an aspiring land surveyor, attended the ""Public Vendue"" (auction). He copied the town map, ""A Plan of Alexandria, Now Belhaven"", and listed the selling prices of the individual lots for his brother. Although established as ""Alexandria"", the town was immediately called ""Belhaven"" – in honor of Scottish patriot John Hamilton, 2nd Lord Belhaven. In 1751, the town council held the ""Belhaven Lottery"" to raise money for a city hall, and George Washington's correspondence throughout the French and Indian War of the late 1750s referred to ""Belhaven"".
 George Washington accompanied his half-brother Lawrence to the warm springs at Bath (present-day Berkeley Springs, West Virginia). Lawrence visited these frequently to improve his health, as he had contracted tuberculosis. In the summer of 1749 Lawrence sailed for England to seek medical advice.[4]
 In 1751, they took a ship to the island of Barbados hoping that a stay in the warm tropical climate might help Lawrence, who was now very ill with tuberculosis. (This was the only ocean crossing taken by George Washington during his lifetime; his other future travels did not extend beyond the borders of the future United States of America).  In Barbados, George Washington contracted smallpox; although he suffered some scarring on his face, his survival meant he was immune to other attacks. Smallpox later caused the most deaths during the American Revolutionary War, and more people died of disease than of battle wounds.[17]
 Lawrence Washington died of tuberculosis at his Mount Vernon home in July 1752.  His widow Anne remarried into the Lee family shortly thereafter. Twenty-year-old George lived at, and managed, the Mount Vernon plantation.  Upon the death of Lawrence's widow Anne, George Washington inherited the estate at Mount Vernon.
 Lawrence and Anne had four children together, but none survived childhood; the first three died in infancy:
 The only surviving portrait of Lawrence Washington is at Mount Vernon, where George Washington is known to have kept it in his private studio/library on the ground floor: it is the only Washington family portrait honored with a place in the Study. A group of American art experts – including James Flexner – were invited to Mount Vernon around 1966 to examine the many family portraits owned by George Washington. In their brief examination of the unsigned, undated painting of Lawrence Washington, those social historians noted that his jacket lacked a collar, which was a style of dress popular in England in the 1730s. On that basis alone, they estimated the portrait was painted in England before Lawrence left Appleby School in 1738 (to return to Virginia). This attribution was widely accepted by most Washington scholars, notwithstanding the fact that a few years later (1969) experts in colonial American military history identified the portrait as the finest surviving example of an American Regiment officer's uniform circa 1740–1742.
 Although it is unsigned and undated, the portrait depicts Lawrence in his military uniform as the Adjutant of the Virginia militia. He posed wearing the scarlet ""undressed"" frock coat issued him in 1740–41 when he was commissioned as a Captain in the British Army, and the green regimental vest of the ""American Regiment"". The vest is trimmed with gold lace (as befits a commanding officer), as is the peak of his tri-corn hat. When experts on Colonial American military history visited Mt. Vernon in 1968–69, they described the portrait as ""the finest example"" of an American officer's uniform from the War of Jenkins' Ear, and published their research (Plate 325) in the journal for the study of military uniforms, Military Collector and Historian.[20][21] The officer's cockade in his hat appears to be the same one later worn by George Washington when he had his portrait painted in 1772.
 Lawrence's ""porthole"" portrait is nearly identical to that of his neighbor and friend, George Mason of Gunston Hall.[22] The Mason portrait is known to have been painted by the young Philadelphia painter John Hesselius in the spring/summer of 1750 to commemorate his 4 April wedding to Ann Elibeck. An expert on the works of Hesselius (father and son), Professor Roland Fleischer, notes that early works of John Hesselius are characterized by the canvas having been primed with a dark, reddish base paint, and that some of his works were signed, others were not. With Lawrence already in failing health and known to have been contemplating returning to the Caribbean in hopes of recovering his health, the younger painter may have depicted Lawrence in more robust health than was accurate in 1750.
 With (father) Gustavus Hesselius established in Philadelphia, young John Hesselius made his first attempt at establishing himself as a professional portrait painter by traveling to the Virginia capital of Williamsburg in spring 1750.[23] There were no professional portrait painters then working in the prosperous colony and Hesselius likely expected to set up a temporary studio in the capital, counting on business from Virginia's wealthy gentry class who attended the Virginia assembly (the House of Burgesses). Unfortunately for Hesselius, the Virginia legislature did not meet in spring 1752, because of the absence of a (resident) Governor. Hesselius then worked his way back to Philadelphia by visiting the plantation estates of prominent Virginian families. It is likely that, after painting the newlywed George and Ann Mason, Hesselius may have obtained a letter of introduction to paint Lawrence and Anne Washington at Mount Vernon in 1750.
 Hesselius would, later, play an indirect role in the portraiture of George Washington, when he took on a young apprentice saddlemaker who aspired to become a painter. The young teen who learned to paint in the Annapolis studio of John Hesselius in 1763 was Charles Willson Peale.
 Lawrence Washington appears in the 2014 Ubisoft action-adventure video game, Assassin's Creed: Rogue as a member of the Templar Order and the first target of the protagonist Shay Patrick Cormac, who kills him during a garden party at Mount Vernon.[24]
 He also appears in the AMC television series, Turn, as inspiration to General George Washington while he is hallucinating from ""melancholia"". He is pictured in a scene in Barbados while ill from tuberculosis and being visited by a young George Washington. He is again depicted later that episode in a forest scene inspiring General George Washington. The younger Washington is lamenting that he has many shortcomings militarily compared to his older brother. Lawrence counsels him that George did things he would not dare, among them, the famous raid across the Delaware River.
  Media related to Lawrence Washington (1718–1752) at Wikimedia Commons
"
Little Hunting Creek,https://en.wikipedia.org/wiki/Little_Hunting_Creek,"
.mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}38°42′47″N 77°04′27″W﻿ / ﻿38.71294°N 77.07412°W﻿ / 38.71294; -77.07412
 Little Hunting Creek is a 3.6-mile-long (5.8 km)[1] primarily tidal tributary of the Potomac River located in Fairfax County, Virginia, not to be confused with Hunting Creek farther north.  A stone-arch bridge, completed in 1931,[2]
carries traffic on the George Washington Memorial Parkway across the narrow mouth of the creek, located 96.6 miles (155.5 km) upriver from the mouth of the Potomac.[3]
The Washington family built its Mount Vernon plantation on the Potomac River along both banks of Little Hunting Creek during colonial times.[4][5]
The creek is bordered by residential communities in addition to the Mount Vernon property.  It is a popular location for recreational fishing, and much of the wildlife characteristic of the tidal Potomac wetlands can be spotted there.
 At the time of John Smith's explorations of Virginia in the early 17th century, Little Hunting Creek was the site of a settlement of Indians from the Doeg (Dogue) tribe.[6]
By the latter part of that century, the land by Little Hunting Creek had come into the hands of John Washington, great-grandfather of George Washington.[4]  What was originally known as Little Hunting Creek Plantation eventually became known as George Washington's Mount Vernon estate.[5]  The creek separated the Mansion House Farm to the west from the River Farm to the east.[5]
 In 1765, when Fairfax Parish was carved out of the existing Truro Parish, Little Hunting Creek marked the southeastern end of the dividing line.[7][8]
 In 1929, a bridge was designed as a part of the Mount Vernon Memorial Highway, and completed in 1931.  The bridge was not part of the original plans for the highway – other creeks were successfully filled in with soil to cut costs; but the Little Hunting could not be filled in, because the soil in the creek bed was very unstable.[2]
 By the 1960s residential communities began to spring up on both sides of the creek.[9][10]
The remnants of a sewage treatment plant are situated on the eastern shore of the creek downstream of the North Branch fork.
 The headwaters of Little Hunting Creek begin in Huntley Meadows Park on the west side of U.S. Route 1,[11]
about 3 miles (4.8 km) northwest of the Potomac River entrance to the creek.  The creek is fed by three major tributaries.[11]  The South Branch (known during colonial times as Carney's Gut, after the tenant there)[12] is located in the southeast portion of what is now the Stratford Landing neighborhood.  It empties into the east side of the creek a few hundred yards northeast of the Potomac River entrance, and bifurcates southeast of Fort Hunt Elementary School into a fork heading north and a fork heading east toward Fort Hunt Park.  The North Branch breaks off from the main creek in a northeasterly direction about 1.5 statute miles (2.4 km) up from the Potomac River entrance, near the northwestern end of Stratford Landing.  The Paul Spring Branch breaks off about another 1.5 statute miles (2.4 km) up the North Branch in the Hollin Hall area near Sherwood Hall Lane, and continues northward along Fort Hunt Road into the Hollin Hills community and beyond.
 Little Hunting Creek is home to wildlife.  Migratory birds such as mallards, Canada geese, great blue herons, and great egrets are common sights along the creek. Ospreys nest there, usually on top of a post in the creek. Bald eagles have been spotted in the area. Muskrats and the occasional beaver have been seen in Little Hunting Creek, and groundhogs and foxes can be found in the woods near the water. Fish such as catfish, carp, perch, and largemouth bass are found in the creek.[13]
In 2004, Little Hunting Creek became one of the first places where invasive northern snakehead fish were caught near the Potomac.[14]
 Little Hunting Creek remains navigable by canoe or kayak upstream to the North Branch fork or farther, depending on the tides.  Both sides of the navigable creek border residential communities, and many homeowners maintain private boat docks for launching canoes, sailboats and other vessels into the creek and from there to the Potomac River. Nesting ospreys can often be observed from a road in the Stratford Landing neighborhood, Thomas J. Stockton Parkway, that runs alongside the creek.[15]
The creek is an excellent place for largemouth bass fishing.[13]
 In 2004, a debate arose about whether the installation of duck blinds and duck hunting should be permitted on the creek.[16]
 Swimming is not advised, particularly in light of drownings that have occurred in the vicinity of the stone-arch bridge.[15][17]
"
Ferry Farm,https://en.wikipedia.org/wiki/Ferry_Farm,"
 Ferry Farm, also known as the George Washington Boyhood Home Site or the Ferry Farm Site, is the farm and home where George Washington spent much of his childhood. The site is located in Stafford County, Virginia, along the northern bank of the Rappahannock River, across from the city of Fredericksburg. In July 2008, archaeologists announced that they had found remains of the boyhood home, which had suffered a fire during 1740, including artifacts such as pieces of a cream-colored tea set probably belonging to George's mother, Mary Ball Washington.[5][6][7] In 2015, the George Washington Foundation began constructing a replica of Washington's boyhood home on the site of the original building. The replica house was completed in 2018 and is open to the public.[8][9]
 The farm was named after the Washington family had left the property. Its namesake was a free ferry that crossed the Rappahannock River on Washington land—the family did not own or operate it. It is unclear what the farm was called during the Washington occupancy. Sometime in the late 19th century the farm became known as Pine Grove, as well as The Ferry Farm. The farm rose to national prominence during the Washington Birth Bicentennial of 1932—during the years surrounding this celebration some authors cited both the names Ferry Farm and Pine Grove.
 Over thousands of years, American Indians periodically inhabited the lands that today make up Ferry Farm. Archaeological finds include a spear point made over 10,000 years ago by a big-game hunter, numerous tools associated with bands of hunter/gatherers, and pottery created by native farmers.
 The first European claim on the property was a land patent granted to John Catlett in 1666. By 1710, the tract had been subdivided into several small farms, with Maurice Clark in ownership of what would become Ferry Farm. In 1727, the property was sold to William Strother, a lawyer, and a Burgess for the newly formed King George County.
 In 1738, George Washington's father, Augustine, acquired the plantation from the Strother estate. Augustine Washington held political office, owned several thriving plantations, and was a managing partner of Accokeek Iron Furnace located six miles north of Ferry Farm on a tributary of the Potomac River. He moved to Ferry Farm in the fall of 1738 with his second wife, Mary Ball Washington, and their five young children.
 George Washington's biographers could claim three locations significant as his homesteads through his lifetime. Pope's Creek, Virginia from 1732 to 1735 was his birthplace. Little Hunting Creek, later to be renamed Mount Vernon by elder brother Lawrence after his 1743 inheritance of the property was briefly the family home to Augustine and Mary Washington and their brood of five including his third son George, from 1735 to 1738.[10][11] The Washington-era farm, then referred to by others as the Washington Farm and by the Washingtons as the Home Farm, had a 1½-story central-passage house, two rooms deep, perched atop a bluff on the Rappahannock River.  It was built by Augustine Washington.  Based on excavations at the site in 2008, the structure was approximately 54 feet (16 m) wide by 28 feet (8.5 m) deep.[12]  It was the second of five houses at the site.  George was six when the family moved to the farm in 1738.[12] He inherited the farm and lived in the house until his early 20s. However, George Washington was not sentimental about the land. Washington saw the land as a ""crowded, busy, trouble-filled place of limited options.""[13] Washington's father Augustine had left behind a small set of surveying instruments after he died. At the age of 16 Washington used his father's surveying tools to survey for prominent Virginia grandees and instantly became hooked; George Washington had found his first true calling. Surveying linked George to his brother Lawrence and the Fairfaxes.[14] At age 18 Washington was granted a 453-acre tract in western Frederick County by Lord Fairfax.  Washington surveyed the 453 acres of land and also purchased an adjoining tract. Washington soon had acquired close to two thousand acres in western lands. Washington had acquired more land through his own hard work than Ferry Farm would be worth in the three years Washington had to wait to legally claim Ferry Farm.[15] He often stayed with his half-brother Lawrence at Mount Vernon. Washington's mother lived in the house until 1772, when she moved to Fredericksburg, and the farm was sold to a friend Hugh Mercer.  The Washington-era house was in ruinous condition by 1833.
 A new farmhouse was built at the site in 1850 for the use of an overseer.  It was the site of skirmishing during the American Civil War in 1862.[2]  Throughout the Civil War, the area surrounding Ferry Farm was prone to encounters between Confederate and Union armies due to a concentration of ferry and train traffic.[13] Because of these encounters, many significant battles occurred in the city surrounding Ferry Farm, although no battles occurred on the actual site.
 Ferry Farm was periodically occupied by Union Soldiers as a war campground, which military personnel used to prepare for battle.[18]
In 1862, amid the Civil War, President Abraham Lincoln, familiar with Weems' Washington myths, toured Ferry Farm. While occupying the Farm, many soldiers familiar with the Cherry Tree Myth, carved trinkets, such as rings, from a tree they believed to be the myth.[13]  Much of the farm was destroyed during this time.  Another new farmhouse was built during the 1870s, along with many outbuildings.  A surviving ""surveyor's shed"" is a remnant of this period.[19]
 Since the 1920s, the property has been the subject of many preservation and money-making schemes. James Beverly Colbert sought to profit from the land's associations with Washington. The writer George Allan England agreed to write real-estate ads promoting the historical value of the land for a $5,000 fee upon the land's sale. He made full use of the many Weem's tales. Colbert also helped promote the thought that a non-important structure called ""The Surveyor Shed,"" was a genuine relic, though it was not. Since the bicentennial celebration of George Washington's Birthday was approaching, Colbert and England thought the land would sell quickly, but there were other historical sites vying for preservation and the Great Depression finally killed their dream.[13]
 In each subsequent decade, different groups of preservationists have tried to make a ""national shrine"" out of Washington's boyhood home. The 1960s saw the creation of a home for troubled boys on the site. This project left on the landscape the site's most visible feature—a large pseudo-Georgian building which now houses a museum, offices, and archaeological lab, which, since 2006, is viewable for visitors.
 In the 1990s Stafford County's Board of Supervisors set out to both preserve the site and bring business to the county. Their attempts ultimately led to rezoning and a bid by Wal-Mart to buy the property and construct a large store adjacent to the boyhood site. This was opposed by many in Fredericksburg, which would have been able to see the back of the store from town. There was a widespread feeling that such a change in this historical town's viewshed would have had adverse effects on the town's crucial tourist trade as well as harming the town's charm and quality of life. The result was a deal whereby Historic Kenmore (the circa 1770s Fredericksburg home of Washington's sister Elizabeth Washington Lewis and her husband Fielding Lewis), in conjunction with the National Park Service and commonwealth funds, purchased the site. Historic Kenmore became George Washington's Fredericksburg Foundation and in 2008 The George Washington Foundation.  This foundation oversees both sites as well as Augustine Washington's ironworks at Accokeek (Potomac) Furnace (1726~ 56) in Stafford County, part of the Principio Company.[20]
 It was declared a National Historic Landmark in 2000.[2][4][21]
 Extensive archaeological investigations began in 2002 under the direction of David Muraca (formerly of Colonial Williamsburg) and Philip Levy of the University of South Florida. The goal of the excavations is to locate and understand the original Washington farm complex. In 2008, Levy and Muraca announced that one of three sites excavated yielded the original home site, including foundations of a 53 by 37 feet (11 m) home.  The home had suffered a small fire during George Washington's lifetime.[5] In 2018 a replica of the original home was completed above these foundations and guided tours are offered on a daily basis.[22] Ferry Farm also runs children's programs and other public events.
 It is located at 237 King's Highway (Virginia Route 3), near Fredericksburg.[2]  A building associated with George Washington's surveying work is listed at 712 King's Highway.[23]
 Samuel Warren and Irma Warren bought the land in 1969. The Warrens were interested in historical preservation and kept the property in the family for nearly 30 years but they eventually sought to rezone the land as a B-1 commercial lot.[13] They then reached an agreement with Stafford County, with the county agreeing to rezoning a parcel of the land, and the Warrens agreeing to donate 24 acres surrounding the Colbert house.[13] The sale, however, marked the start of a long controversy between commercial interests and proponents of national heritage. Robert Siegrist entered the controversy and expressed an interest in maintaining the land that was recently donated to Stafford County. Siegrist maintained the property for 4 years until early on the morning of September 26, 1994, when a fire broke out at the Colbert house.[13] Stafford County quickly ended its relationship with Siegrist and looked for alternate applications of the property.
In 1995, the Walmart corporation sought the land surrounding the Washington Site, and worked to purchase it. Included in the purchase was a promise that the site heritage would be preserved through a columned plaza with ""special tributes to George and the Cherry Tree."" However, there was a large opposition including Fredericksburg small business owners, local historical preservationists, and many national organizations such as Daughters of the American Revolution.[13] The conflict came to a head on April 1, 1995, after the review board assigned to the task rejected Walmart's offer.[13]
 The George Washington Foundation purchased the land in 1996 and since 2003 has supported an archeological field school at the site. The summer excavations are led by Philip Levy (associate professor of history at the University of South Florida) and David Muraca (who previously worked at Colonial Williamsburg). Digs are organized through university groups and consist of undergraduate and graduate volunteers. Archeologists and volunteers have found artifacts dating from the prehistoric period through the Civil War and beyond.
 In 2006–2008 the archeology team headed by Muraca and Levy, funded by the George Washington Foundation, found the cellar to, and thus the location of George Washington's boyhood home. Using the remains of the building and the artifacts within, they have dated the feature to the 1740s, the time when the Washingtons lived on the land.[24] In 2015, groundbreaking began on a replica house built above the original foundations using information gleaned from the archaeological remains, contemporary descriptions of the original house, and knowledge of similar houses in Colonial Virginia.[25] The house was built using eighteenth-century building techniques by experts in colonial craftsmanship, and completed and opened to the public in 2018. It is stocked with reproductions of the furniture and objects listed in Augustine Washington's probate inventory as having been in the house when he died in 1743. Since they are reproductions, tour guides encourage guests to interact with all of the objects in the house, allowing them to sit on the furniture, open cabinets, and handle objects.[9]
 The George Washington Foundation now owns the land as well as Kenmore, the Fredericksburg home of Washington's sister Elizabeth (known as ""Betty"") and her husband Fielding Lewis. The GWF is funded by donors and grants and has been managing the land since the Walmart purchase. In addition to funding, staffing, cataloging, and overseeing the site's excavations there are now plans to recreate the historical landscape and build a new visitor's center on the site.[24]
 Artifacts found at the Ferry Farm site have dated back to 10,000 B.C. This period of prehistoric artifacts continues on until 1,500 B.C.[26] Artifacts from this period include hunting and gather tools, which prehistoric peoples used to hunt wildlife around and in the Rappahannock River. The land around Ferry Farm was rich in nutrients and populated with multiple animals.
""Such diversity in food sources required these groups to expand the types of tools they used.""[27] 
Examples of artifacts that were found include spear points, stone axes, and quartz scrapers, which were used as tools for hunting.
 While evidence is scarce, artifacts have been dated back to 1,500 B.C. until 1,500 A.D. when Woodland Indians inhabited the land around current day Ferry Farm.[28]
 Popular artifacts found from the Colonial Period include the ends of 18th century wig hair curlers.
""[...] These were associated with maintaining wigs for George Washington's younger brothers. The fashionable gentlemen of the late 18th century wore a wig- the single most expensive part of the gentry-class man's wardrobe.""[29]
 In 1862 the Civil War arrived at Ferry Farm leaving behind a variety of artifacts throughout the duration of the war. Such as objects from battles including bullets, ink bottles, buttons and medallions from uniforms to name a few.[30]
 Ferry Farm is the setting for some of the best known stories about George Washington, most particularly those brought to the American public by Mason Locke Weems, best known as Parson Weems, in the early 19th century. These include the anecdote, appearing first in the 1806 edition of Weems's Life of Washington, in which a 6-year-old George barked one of his father Augustine's favorite English cherry trees with a new hatchet. Upon being confronted by his father, the boy confessed, saying: ""I cannot tell a lie; I did it with my little hatchet.[31]
 George Washington's step-grandson, George Washington Parke Custis, published a newer version of the story in which George Washington tried to break his mothers new colt (horse). In this story, Washington rode the horse ""so hard"" that one of the horse's blood vessel's burst, killing the horse. Like the cherry tree story, when confronted George Washington admitted to killing the horse.[32]
 Both stories celebrate George Washington's honesty and share a theme of loss, which Washington was painfully familiar with after his father's death and losing his opportunity to study abroad.[33]
 It has also been claimed to be the site where Washington ""threw a silver dollar across the Rappahannock River.""  It is possible to ""skip"" a coin or flat rock across that area. Regardless, the river was considerably wider during this period than it is today, making the feat that much more difficult.  Each year during the celebration of Washington's birthday, townspeople are invited to attempt to recreate this event. In the summer of 2006, archaeology intern Jim Trueman duplicated Washington's alleged throw and did it again the following summer to prove the first throw was not a fluke.[34]
"
"Fredericksburg, Virginia","https://en.wikipedia.org/wiki/Fredericksburg,_Virginia","

 Fredericksburg is an independent city in Virginia, United States. As of the 2020 census, the population was 27,982.[4] It is 48 miles (77 km) south of Washington, D.C., and 53 miles (85 km) north of Richmond.[5][6] The Bureau of Economic Analysis of the United States Department of Commerce combines the city of Fredericksburg with neighboring Spotsylvania County for statistical purposes.
 Located near where the Rappahannock River crosses the Atlantic Seaboard fall line, Fredericksburg was a prominent port in Virginia during the colonial era. During the Civil War, Fredericksburg, located halfway between the capitals of the opposing forces, was the site of the Battle of Fredericksburg and Second Battle of Fredericksburg. These battles are preserved, in part, as the Fredericksburg and Spotsylvania National Military Park. More than 10,000 African-Americans in the region left slavery for freedom in 1862 alone, getting behind Union lines. Tourism is a major part of the economy. Approximately 1.5 million people visit the Fredericksburg area annually, including the battlefield park, the downtown visitor center, events, museums, art shops, galleries, and many historical sites.[7]
 Fredericksburg is home to Central Park (as of 2004, the second-largest mall on the East Coast). The Spotsylvania Towne Centre is located in Spotsylvania County, adjacent to the city. Major employers include the University of Mary Washington (named for the mother of George Washington, who lived here), Mary Washington Healthcare, and GEICO. Many Fredericksburg area residents commute to work by car, bus, and rail to Washington, D.C., and Richmond, as well as Fairfax, Prince William, and Arlington counties.[8][9][10][11]
 At the time of European encounter, the indigenous inhabitants of the area that became Fredericksburg were a Siouan-speaking tribe called the Manahoac. English colonists recorded the name of the Manahoac village there as Mahaskahod.[12] Siouan tribes occupied much of the area of the Piedmont. The Tidewater areas of the coastal plain had primarily Algonquian-speaking tribes making up the Powhatan Confederacy.
 Located on the Rappahannock River near the head of navigation at the fall line, Fredericksburg developed as the frontier of colonial Virginia shifted west from the coastal plain into the Piedmont. The land on which the city was founded was part of a tract patented in 1671. The Virginia General Assembly established a fort on the Rappahannock in 1676, just downriver of the present-day city. In 1714, Lieutenant Governor Alexander Spotswood sponsored a German settlement called Germanna on the Rapidan River, a tributary of the Rappahannock upstream from the future site of the city. In 1716, he led an exploratory expedition westward over the Blue Ridge Mountains.
 As interest in the frontier grew, the colonial assembly formed Spotsylvania County in 1720, named after Royal Lieutenant Governor Alexander Spotswood. In 1728, Fredericksburg was declared a port for the county, of which it was then a part. Named for Frederick, Prince of Wales,[13] son of King George II, the colonial town named its streets after the members of the royal family. The county court was moved to Fredericksburg in 1732. Hence, the community served as county seat until 1780. The court was then moved to Spotsylvania Courthouse, Virginia – closer to the geographical center of Spotsylvania County. In 1781, Fredericksburg was incorporated as a town with its own court, council, and mayor. It received its charter as an independent city in 1879 and under Virginia law, was separated from Spotsylvania County. The city adopted its present city manager/council form of government in 1911.
 The city has close associations with George Washington, whose family in 1738 moved to Ferry Farm in Stafford County near the Rappahannock River opposite Fredericksburg. Washington's mother, Mary, later moved to the city, and his sister Betty lived at Kenmore, a plantation house then outside the city. Several citizens played active roles during the American Revolution (1763–1781). For example, a number of locals signed the Leedstown Resolves, which formed an association to protest the Stamp Act in the 1760s.[14] In the 1770s, Fielding Lewis, owner of Kenmore Plantation and brother-in-law to George Washington, also operated an arms factory for the Continental Army. Other significant early residents include the Revolutionary War generals Hugh Mercer and George Weedon, naval war hero John Paul Jones, and future U.S. president James Monroe. Thomas Jefferson wrote the Virginia Statute for Religious Freedom in Fredericksburg.[15]
 During the 19th century, mills continued to be developed along the Rappahannock River, which provided water power. There were mills for grinding flour, processing and weaving cotton, and other manufacturing. Fredericksburg sought to maintain its sphere of trade, but with limited success. It promoted the development of a canal on the Rappahannock and construction of a turnpike and plank road to bind the interior country to the market town. By 1837, a north–south railroad, which became the Richmond, Fredericksburg and Potomac Railroad, linked the town to Richmond, the state capital. A much-needed railroad joining the town to the West's arming region was not finished until after the Civil War.
 During the Civil War, Fredericksburg was strategically important because of its port location midway between Washington and Richmond, the opposing capitals of the Union and the Confederacy. During the Battle of Fredericksburg from December 11–15, 1862, the town sustained significant damage from bombardment and looting by the Union forces.
 During that engagement, nearly 10,000 enslaved people left area plantations and city households to gain freedom by crossing the Rappahannock River to Stafford County and join the Union lines, part of a movement by enslaved people throughout the South in wartime.[16] John Washington, a literate enslaved person who shortly crossed to freedom, wrote later about people watching the approach of Union troops across the river from Fredericksburg: ""No one could be seen on the street but the colored people. and every one of them seemed to be in the best of humors.""[17]
 The Second Battle of Fredericksburg was fought in and around the town on May 3, 1863, in connection with the Chancellorsville campaign (April 27, 1863 – May 6, 1863). The battles of the Wilderness and Spotsylvania Court House were fought nearby in May 1864. The Washington Woolen Mill, a large three-story building, was converted to use as a hospital during the war.
 After the war, Fredericksburg recovered its former position as a center of local trade and slowly grew beyond its prewar boundaries. Neither the city of Fredericksburg nor the surrounding counties reached the 1860 level of population again until well into the 20th century. After the war, many freedmen moved to Richmond and Petersburg, where there had been established free black communities before the war, and there was more work.
 In the early 20th century, as the Jim Crow era continued in the South, there was widespread population movement. Many African-Americans left rural areas of the South for work and other opportunities in industrial cities of the North and Midwest in the Great Migration. Some settled in Washington, D.C., where there were more opportunities, or further north.
 War-related buildup at defense facilities for World War II added to the area's population in the 1940s. The 1960s brought renewed growth and development, fueled by the construction of Interstate 95, which eased commuting and trade. By the 1970s, the city and the area had become a bedroom community for jobs in Northern Virginia and Washington, D.C. Headquarters agencies, lobbyists, consultants, defense and government contractors, and a range of other businesses were part of the regional economy influenced by the U.S. government. The city also benefited from its relative proximity to four military installations: the United States Marine Corps' Quantico Base, the U.S. Army's Fort Belvoir, the U.S. Navy's Dahlgren Surface Weapons Base, and the Virginia National Guard's Fort A.P. Hill.
 The University of Mary Washington was founded in Fredericksburg in 1908 as the State Normal and Industrial School for Women, to train white women for teaching K-12 and industrial skills. Adopting the name of Mary Washington College in 1938, the college was for many years associated with the University of Virginia (then limited to white men) as a women's liberal arts college. The college officially desegregated in 1964. The college became independent of the University of Virginia and began to accept men in 1970. In 2004, the college changed its name from Mary Washington College to the University of Mary Washington. Two additional campuses for graduate and professional studies and education and research are located in Stafford County and in King George County, respectively.
 Musician Link Wray invented the power chord of modern rock guitar in Fredericksburg in 1958 during an improvisation of the instrumental piece ""Rumble"", a single subsequently released by Wray & His Ray Men.[18] This innovation became widely used by rock guitarists. In the early 21st century, the local music scene includes a wide variety of genres.
 A commuter rail line – the Virginia Railway Express – was established in the 1980s, providing passage to Washington, D.C. and other cities north of Fredericksburg.
 The city has become the regional healthcare center for the area. Retail, real estate, and other commercial growth exploded in the early 21st century, eventually slowing during the Great Recession beginning in 2007. Hispanic growth skyrocketed from 2011 to 2020, with Chancellor Green in nearby Spotsylvania County becoming a local enclave.[citation needed]
 According to the U.S. Census Bureau, the city has a total area of 10.5 square miles (27.2 km2), 10.4 square miles (27.0 km2) of which is land and 0.1 square miles (0.2 km2), or 0.67%, of which is water.[19] The city is part of the boundary between the Piedmont and Tidewater regions, and as such is located on the fall line, as evident on the Rappahannock River. US 1, US 17, and I-95 all pass through the city, which is located 53 miles (85 km) south of downtown Washington, D.C.
 The city is bounded on the north and east by the Rappahannock River; across the river is Stafford County. The city is bounded on the south and west by Spotsylvania County.
 Fredericksburg has a four-season humid subtropical climate (Köppen Cfa), with cool winters and hot, humid summers. Daytime temperatures for much of the year average slightly higher than in Washington, D.C. due to the southerly aspect, although the inland location and distance from the urban heat island present in the nation's capital make for significantly cooler low temperatures.
 Note: the US Census treats Hispanic/Latino as an ethnic category. This table excludes Latinos from the racial categories and assigns them to a separate category. Hispanics/Latinos can be of any race.
 As of the census[29] of 2020, there were about 29,000 people, 8,102 households, and 3,925 families residing in the city. The population density was 1,833 inhabitants per square mile (708/km2). There were 8,888 housing units at an average density of 845 per square mile (326/km2). The racial makeup of the city was 54% White, 21% Black or African American, 0.31% Native American, 4.74 Asian, 0.067 Pacific Islander, 2.56% from other races, and 1.95% from two or more races.12% of the population were Hispanic or Latino of any race.
 There were 8,102 households, out of which 21.6% had children under the age of 18 living with them, 31.8% were married couples living together, 13.1% had a female householder with no husband present, and 51.6% were non-families. 39.2% of all households were made up of individuals, and 12.8% had someone living alone who was 65 years of age or older. The average household size was 2.09 and the average family size was 2.81.
 In the city, the population was spread out, with 17.8% under the age of 18, 23.8% from 18 to 24, 27.2% from 25 to 44, 18.4% from 45 to 64, and 12.8% who were 65 years of age or older. The median age was 30 years. For every 100 females, there were 81.8 males. For every 100 females age 18 and over, there were 78.4 males.
 According to data from the US Census, the median household income in the city is $83,445 and the median income for a family is $121,781.[30] The per capita income for the city is $43,063. 18% of the population is below the poverty line.[31]
 The Fredericksburg Police Department[32] tracks crime information under the state-level system of the Uniform Crime Reporting program.[note 1] Per state code, the central repository for crime statistics rests with the Department of State Police, which compiles data from all of the participating agencies into an annual publication Archived November 25, 2011, at the Wayback Machine.[33]
 By long-standing tradition (dating back to the Federal Hatch Act of 1939, which prohibited government employees from participating in partisan politics), local elections in Fredericksburg are officially non-partisan. Neither the mayoral and council elections nor local constitutional positions (e.g. sheriff, Commissioner of Revenue, Commonwealth Attorney) list candidates with a party label.
 Like the rest of Northern Virginia, Fredericksburg has trended strongly Democratic in the early 21st century. In the 2008 presidential election, voters in Fredericksburg gave Barack Obama a total of 63.6% of the vote.[35] Only Arlington County, Alexandria, and Falls Church in Northern Virginia had a higher percentage of votes for Obama.[36] No Republican presidential candidate has carried Fredericksburg since George H. W. Bush did so in 1988. In the 2016 presidential election, then-candidate Donald Trump garnered the lowest percentage of the city's vote for any Republican candidate since 1936; about two percent fewer votes were garnered in 2020.
 Fredericksburg operates with a council-manager government, with Kerry Devine as the current mayor, first elected in 2024.
 The following is the current makeup of City Council.[37]
 Despite recent decades of suburban growth, reminders of the area's past abound. The 40-block Fredericksburg Historic District, listed on the National Register of Historic Places, embraces the city's downtown area and contains more than 350 buildings and locations dating to the 18th and 19th centuries, including the Fredericksburg Town Hall and Market Square, Lewis Store, and former site of the Slave Auction Block.
 Within the historic district, four 18th-century historic sites have been managed by the ""Washington Heritage Museums"": the Mary Washington House, where George Washington's mother lived in her final years; the late 18th-century Rising Sun Tavern, and the Hugh Mercer Apothecary Shop (the fourth, the St. James House (built 1768), is open to the public only during Historic Garden Week). Important public buildings include the 1852 courthouse designed by James Renwick, whose works include the Smithsonian Institution's castle building in Washington and St. Patrick's Cathedral in New York City, and the 1816 town hall and market house, now operated as the Fredericksburg Area Museum and Cultural Center. Another site of interest is St. George's Church.  The James Monroe Museum and Memorial Library is located on the site where Monroe practiced law from 1786 to 1788. The museum is housed in a building made up of three individual structures, constructed at different times, beginning in 1816.
 Near the historic district is the Lewis Plantation, later named Kenmore, the plantation home of George Washington's sister Betty and her husband, Fielding Lewis.
 Civil War battles are commemorated in Fredericksburg and Spotsylvania National Military Park. Formed by an act of Congress in 1927, the national military park preserves portions of the battlefields of Fredericksburg, Chancellorsville, the Wilderness, and Spotsylvania Court House. The Fredericksburg National Cemetery, also part of the park, was developed by the federal government after the war on Marye's Heights on the Fredericksburg battlefield. It contains more than 15,000 Union burials from the area's battlefields. Many unidentified soldiers were buried in mass graves.
 Among the 10,000 slaves crossing the Rappahannock for freedom with the Union in 1862 was John Washington. A literate slave from Fredericksburg, he settled in New York and wrote an account of the wartime events several years later. His manuscript was discovered in the 1990s. It was published as the basis of two books, David W. Blight's A Slave No More (2007), and John Washington's Civil War: A Slave Narrative (2008), edited by Crandall Shifflett.[40] In 2010, the National Park Service, which manages the battlefield, Stafford County, and the City of Fredericksburg worked collaboratively to post new historical markers on either side of the Rappahannock River as part of a ""Freedom Trail"" to mark this exodus.[16]
 Notable 20th-century sites and structures include the campus of the University of Mary Washington (begun in 1908), and Carl's Ice Cream, an Art Moderne roadside ice cream stand, listed on the National Register of Historic Places.
 Nearby points of interest include Ferry Farm historic site across the Rapahannock in Stafford County where Washington spent his boyhood, and the George Washington Birthplace National Monument, located 38 miles (61 km) to the east in Westmoreland County on the Northern Neck. The historic community of Falmouth lies across the Rappahannock to the north and includes the historic house Belmont, home of American Impressionist artist Gari Melchers.
 Public parks run by the city include:[41]
 Central Rappahannock Regional Library
 The Fredericksburg City Public Schools are run independently of the surrounding counties. The public primary and secondary schools include:[43]
 Private schools include:
 The University of Mary Washington, established in 1908 and opened in 1911, is a four-year public university within the city.
 Germanna Community College, established in 1970, is a public two-year program with a campus in Fredericksburg.
 Fredericksburg's daily newspaper is The Free Lance–Star. The Free Lance was first published in 1885, and competed with two twice-weekly papers in the city during the late 19th century, the Fredericksburg News and The Virginia Star. While the News folded in 1884, the Star moved to daily publication in 1893. In 1900, the two companies merged, with both newspapers continuing publication until 1926, when they merged as a single daily newspaper under the current title. Until June 19, 2014, the Free Lance–Star was owned and operated by members of the Rowe family of Fredericksburg. At that time, Sandton Capital Partners purchased the paper. On December 31, 2015, the newspaper and associated website were purchased by Berkshire Hathaway's BH Media Group.[45] Fredericksburg Today, an online hyperlocal news site began operation following the 2014 bankruptcy of The Free Lance–Star.[46] In 2024, Fredericksburg Today was replaced by The Fredericksburg Free Press, a 501(c)(3) nonprofit dedicated to providing impartial and nonpartisan digital news to the Fredericksburg region.[47][48]
 Fredericksburg and the nearby region have several radio stations, including (on the FM dial) WQIQ (88.3, ""Radio IQ"", public radio, licensed to nearby Spotsylvania), WLJV (89.5, contemporary Christian), WPER (90.5, Christian), WFLS (93.3, country), WGRQ (95.9, ""SuperHits"", classic hits, licensed to nearby Fairview Beach), WWUZ (96.9, classic rock, licensed to nearby Bowling Green), WVBX (99.3, contemporary hit radio, licensed to nearby Spotsylvania), WBQB (""B-101.5"", adult contemporary) and WGRX (""Thunder 104.5"", country, licensed to nearby Falmouth). Fredericksburg AM stations include WFVA (1230, news and talk) and WNTX (1350, talk, news, and sports). WGRQ and WGRX are owned locally by Telemedia Broadcasting. WFLS, WWUZ, WVBX, and WNTX are owned by Alpha Media.[citation needed]
 In 2001, the Arbitron media service began listing the Fredericksburg area as a nationally rated radio market.[citation needed] As of the fall of 2014, the area ranked 146th out of 272 markets surveyed, with a total market population of more than 325,000. Large broadcast companies like Clear Channel Communications and Cumulus Broadcasting are not active in the local market; almost all of its stations remain locally or regionally owned.[citation needed]
 In television, Fredericksburg is part of the Washington market. One local television station, NBC affiliate WHFV, was briefly on the air in the 1970s.[citation needed]
 Studio Ironcat, A small publishing company based in Fredericksburg, Virginia, dedicated to publication of manga and later, Amerimanga.[49][50]
 The Fredericksburg Nationals minor league baseball team began play at Virginia Credit Union Stadium in 2021.[51]
 Sports at the secondary education level are run through the Virginia High School League. On the collegiate level are the University of Mary Washington Eagles. Other amateur athletics include Fredericksburg FC of the National Premier Soccer League (NPSL); and the Rappahannock Rugby Football Club, a senior men's and women's rugby club competing in Division II (men) and Division III (women) of the Capital Rugby Union.[citation needed]
 Fredericksburg is traversed by a series of rural and suburban four-lane highways and a multitude of small, two-lane roads. The primary highway serving Fredericksburg is Interstate 95, which connects northward to Washington, D.C. and southward to Richmond, Virginia. Among the major arterial roads is U.S. Route 17, providing northwest–southeast transportation across the region. Through Fredericksburg, I-95 and US 17 are concurrent, though a local business route on the latter provides local access to downtown. Route 3 (Plank Road) is a major east–west route that connects downtown Fredericksburg (via the Blue and Gray Parkway bypass), southern Stafford and King George counties, and Route 301 to the east with the large shopping centers, Spotsylvania Town Center and Central Park. To the west, Route 3 reaches Culpeper, where it meets Route 29 and Route 15.[citation needed]
 Most of Fredericksburg's traffic flow is to or from the north (Washington, D.C. metropolitan area) during peak commuting hours, primarily via I-95 and U.S. Route 1. The Route 1 bridge over the Rappahannock River is often a traffic bottleneck, and Route 3 has become increasingly congested as residential development grows and as the location of major regional shopping centers.[citation needed]
 As an alternative to I-95, some commuters use the Virginia Railway Express rail service to Washington. Long-distance rail service to the north is available on Amtrak's Northeast Regional trains.[52] Long-distance rail service to the south is provided by Amtrak's Silver Meteor, Carolinian, Palmetto and Piedmont trains[53]
 Fredericksburg Regional Transit (FRED) is a bus service that started in 1996 in Fredericksburg and serves most area communities, retail shopping centers, two VRE stations, and downtown Fredericksburg.[54]
 Four major airports serve Fredericksburg and the surrounding area.  Reagan National and Dulles International Airports are to the north within Virginia. Beyond them to the northeast is Baltimore/Washington International Airport in Maryland, and Richmond International Airport is south of Fredericksburg.
 38°18′07″N 77°28′15″W﻿ / ﻿38.301829°N 77.470778°W﻿ / 38.301829; -77.470778
"
Mount Vernon,https://en.wikipedia.org/wiki/Mount_Vernon,"


 Mount Vernon is the former residence and plantation of George Washington, a Founding Father, commander of the Continental Army in the Revolutionary War, and the first president of the United States, and his wife, Martha. An American landmark, the estate lies on the banks of the Potomac River in Fairfax County, Virginia, approximately 15 miles south of Washington, D.C..
 The Washington family acquired land in the area in 1674. Around 1734, the family embarked on an expansion of its estate that continued under George Washington, who began leasing the estate in 1754 before becoming its sole owner in 1761.[4]
 The mansion was built of wood in a loose Palladian style; the original house was built in about 1734 by George Washington's father Augustine Washington.[4] George Washington expanded the house twice, once in the late 1750s and again in the 1770s.[4] It remained Washington's home for the rest of his life. Following his death in 1799, the estate progressively declined under the ownership of several successive generations of the family as revenues were insufficient to maintain it adequately.
 In 1858, the house's historical importance was recognized and was taken over by the Mount Vernon Ladies' Association, along with part of the Washington property estate. The mansion and its surrounding buildings escaped damage from the American Civil War, which damaged many properties in the Confederate States of America during the Civil War.
 Mount Vernon was designated a National Historic Landmark in 1960 and is listed on the National Register of Historic Places. It is still owned and maintained in trust by the Mount Vernon Ladies' Association, being open to the public daily[5] in recognition of George Washington's 1794 acknowledgement of public interest in his estate: ""I have no objection to any sober or orderly person's gratifying their curiosity in viewing the buildings, Gardens, &ca. about Mount Vernon.""[6]
 When George Washington's ancestors acquired the estate, it was known as Little Hunting Creek Plantation, named after the nearby Little Hunting Creek.[7] When Washington's older half-brother, Lawrence Washington, inherited it, he renamed it after Edward Vernon,[8] a vice admiral and his commanding officer during the War of Jenkins' Ear who captured Portobelo from the Spanish.[9] When George Washington inherited the property, he retained the name.[7]
 The estate contained 8,000 acres (3,200 ha) when George Washington lived there.[10] As of 2011, the property consists of 500 acres (200 ha),[11] including the mansion and over 30 other buildings near the riverfront.[12]
 Construction on the present mansion at Mount Vernon began in approximately 1734 and was built in incremental stages by an unknown architect under the supervision of Augustine Washington.[4] This staggered and unplanned evolution is indicated by the off-center main door. As completed and seen today, the house is in a loose Palladian style. The principal block, dating from about 1734, was a one-story house with a garret.[4] In the 1750s, the roof was raised to a full second story and a third floor garret. There were also one-story extensions added to the north and south ends of the house; these were torn down during the next building phase.[13] The present day mansion is 11,028 sq ft (1,025 m2).[14]
 In 1774, the second expansion began. A two-story wing was added to the south side. Two years later a large two-story room was added to the north side.[13] Two single-story secondary wings were built in 1775. These secondary wings, which house the servants hall on the northern side and the kitchen on the southern side, are connected to the corps de logis by symmetrical, quadrant colonnades, built in 1778. The completion of the colonnades cemented the classical Palladian arrangement of the complex and formed a distinct cour d'honneur, known at Mount Vernon as Mansion Circle, giving the house its imposing perspective.
 The corps de logis has a hipped roof with dormers and the secondary wings have gable roofs with dormers. In addition to its second story, the importance of the corps de logis is further emphasized by two large chimneys piercing the roof and by a cupola surmounting the center of the house; this octagonal focal point has a short spire topped by a gilded dove of peace.[15] This placement of the cupola is more in the earlier Carolean style than Palladian and was probably incorporated to improve ventilation of the enlarged attic and enhance the overall symmetry of the structure and the two wings; a similar cupola crowns the Governor's House at Williamsburg, of which Washington would have been aware.
 Though no architect is known to have designed Mount Vernon, some attribute the design to John Ariss, a prominent Virginia architect who designed Paynes Church in Fairfax County (now destroyed) and likely Mount Airy in Richmond County.[16] Other sources credit Colonel Richard Blackburn, who also designed Rippon Lodge in Prince William County and the first Falls Church.[17][18] Blackburn's granddaughter Anne married Bushrod Washington, George's nephew, and is interred at the Washingtons' tomb on the grounds. Most architectural historians believe that the design of Mount Vernon is solely attributable to Washington alone and that the involvement of any other architects is based on conjecture.[19]
 The rooms at Mount Vernon have mostly been restored to their appearance at the time of George and Martha Washington's occupancy. Rooms include Washington's study, two dining rooms, the larger of which is known as the New Room, the West Parlour, the Front Parlour, the kitchen and some bedrooms.[20]
 The interior design follows the classical concept of the exterior, but owing to the mansion's piecemeal evolution, the internal architectural features – the doorcases, mouldings and plasterwork – are not consistently faithful to one specific period of the 18th-century revival of classical architecture. Instead they range from Palladianism to a finer and later neoclassicism in the style of Robert Adam.[20] This varying of the classical style is best exemplified in the doorcases and surrounds of the principal rooms. In the West Parlour and Small Dining rooms there are doorcases complete with ionic columns and full pediments, whereas in the hall and passageways the doors are given broken pediments supported by an architrave.[20] Many of the rooms are lined with painted panelling and have ceilings ornamented by plasterwork in a Neoclassical style; much of this plasterwork can be attributed to an English craftsman, John Rawlins, who arrived from London in 1771 bringing with him the interior design motifs then fashionable in the British capital.[20]
 Visitors to Mount Vernon now see Washington's study, a room to which in the 18th century only a privileged few were granted entry. This simply furnished room has a combined bathroom, dressing room and office; the room was so private that few contemporary descriptions exist. Its walls are lined with naturally grained paneling and matching bookcases.[21] In contrast to the privacy of the study, since Washington's time, the grandest, most public and principal reception room has been the so-called New Room or Large Dining Room – a two-storied salon notable for its large Palladian window, occupying the whole of the mansion's northern elevation, and its fine Neoclassical marble chimneypiece.[22] The history of this chimneypiece to some degree explains the overall restrained style of the house. When it was donated to Washington by English merchant Samuel Vaughan, Washington was initially reluctant to accept the gift, stating that it was ""too elegant & costly I fear for my own room, & republican stile of living.""[23]
 Efforts have been made to restore the rooms and maintain the atmosphere of the 18th century; this has been achieved by using original color schemes and by displaying furniture, carpets and decorative objects which are contemporary to the house. The rooms contain portraits and former possessions of George Washington and his family.[20]
 The gardens and grounds contain English boxwoods, taken from cuttings sent by Major General Henry Lee III a Governor of Virginia and the father of Robert E. Lee, which were planted in 1786 by George Washington and now crowd the entry path. A carriage road skirts a grassy bowling green to approach the mansion entrance. To each side of the green is a garden contained by red brick walls. These Colonial Revival gardens[24] grew the household's vegetables, fruit and other perishable items for consumption. The upper garden, located to the north, is bordered by the greenhouse.[25] Ha-ha walls are used to separate the working farm from the pleasure grounds that Washington created for his family and guests.[26] The overseer's quarter, spinning room, salt house, and gardener's house are between the upper garden and the mansion.
 The lower garden, or southern garden, is bordered on the east by the storehouse and clerk's quarters, smokehouse, wash house, laundry yard, and coach house. A paddock and stable are on the southern border of the garden; east of them, a little down the hillside, is the icehouse. The original tomb is located along the river. The newer tomb in which the bodies of George and Martha Washington have rested since 1831 is south of the fruit garden; the slave burial ground is nearby, a little farther down the hillside. A ""Forest Trail"" runs through woods down to a recreated pioneer farm site on low ground near the river; the 4-acre (16,000 m2) working farm includes a re-creation of Washington's 16-sided treading barn.[27]
 A museum and education center are on the grounds and exhibit examples of Washington's survey equipment, weapons, and clothing, and the  dentures worn by Washington as the first U.S. president. In 2013, the Fred W. Smith National Library for the Study of George Washington opened on Mount Vernon;[28] the library, which is open for scholarship by appointment only, fosters new scholarship about George Washington and safeguards original Washington books and manuscripts.
 In 1674, John Washington, the great-grandfather of George Washington, and Nicholas Spencer came into possession of the land from which Mount Vernon plantation would be carved, originally known by its Piscataway name of Epsewasson.[29][a] The successful patent on the acreage was largely executed by Spencer, who acted as agent for his cousin Thomas Colepeper, 2nd Baron Colepeper,[29] the English landowner who controlled the Northern Neck of Virginia, in which the tract lay.[30]
 When John Washington died in 1677, his son Lawrence, George Washington's grandfather, inherited his father's stake in the property. In 1690, he agreed to formally divide the estimated 5,000 acre (20 km2) estate with the heirs of Nicholas Spencer, who had died the previous year. The Spencers took the larger southern half bordering Dogue Creek in the September 1674 land grant from Lord Culpeper, leaving the Washingtons the portion along Little Hunting Creek. The Spencer heirs paid Lawrence Washington 2,500 lb (1,100 kg) of tobacco as compensation for their choice.[29]
 Lawrence Washington died in 1698, bequeathing the property to his daughter Mildred. On 16 April 1726, she agreed to a one-year lease on the estate to her brother Augustine Washington, George Washington's father, for a peppercorn rent; a month later the lease was superseded by Augustine's purchase of the property for £180.[31] He built the original house on the site around 1734, when he and his family moved from Pope's Creek to Eppsewasson,[32] which he renamed Little Hunting Creek.[33] The original stone foundations of what appears to have been a two-roomed house with a further two rooms in a half-story above are still partially visible in the present house's cellar.[32]
 Augustine Washington recalled his eldest son, Lawrence, George's half-brother, home from school in England in 1738, and set him up on the family's Little Hunting Creek tobacco plantation, thereby allowing Augustine to move his family back to Fredericksburg at the end of 1739.[7] In 1739, Lawrence, having reached 21 years of age, began buying up parcels of land from the adjoining Spencer tract, starting with a plot around the grist mill on Dogue Creek. In mid-1740, Lawrence received a coveted officer's commission in the British Army and made preparations to go off to war in the Caribbean with the newly formed American Regiment to fight in the War of Jenkins' Ear.[34] He served under Admiral Edward Vernon; returning home, he named his estate after his commander.
 Lawrence died in 1752, and his will stipulated that his widow should own a life estate in Mount Vernon, the remainder interest falling to his half-brother George; George Washington was already living at Mount Vernon and probably managing the plantation. Lawrence's widow, Anne Fairfax, remarried into the Lee family and moved out.[35] Following the death of Anne and Lawrence's only surviving child in 1754, George, as executor of his brother's estate leased his sister-in-law's estate. Upon the death of Anne Fairfax in 1761, he succeeded to the remainder interest and became sole owner of the property.[36]
 In 1758, Washington began the first of two major additions and improvements by raising the house to two-and-a-half stories.[36] The second expansion was begun during the 1770s, shortly before the outbreak of the Revolutionary War. Washington had rooms added to the north and south ends, unifying the whole with the addition of the cupola and two-story piazza overlooking the Potomac River. The final expansion increased the mansion to 21 rooms and an area of 11,028 square feet.[26] The great majority of the work was performed by enslaved African Americans and artisans.[37]
 George Washington expanded the estate by purchasing surrounding parcels of land beginning in the late 1750s and was still adding to the estate into the 1780s, including the River Farm estate.[38] From 1759 until the Revolutionary War, Washington, who at the time aspired to become a prominent agriculturist, had five separate farms as part of his estate. He took a scientific approach to farming and kept extensive and meticulous records of both labor and results.
 In a letter dated 20 September 1765, Washington writes about receiving poor returns for his tobacco production:
 In the same letter he asks about the prices of flax and hemp, with a view to their production:
 The tobacco market declined, and many planters in Northern Virginia converted to mixed crops. By 1766, Washington ceased growing tobacco at Mount Vernon and replaced the crop with wheat, corn, and other grains. Besides hemp and flax, he experimented with 60 other crops including cotton and silk. He also derived income from a gristmill which produced cornmeal and flour for export and also ground neighbors' grain for fees. Washington similarly sold the services of the estate's looms and blacksmith.
 Washington built and operated a small fishing fleet, permitting Mount Vernon to export fish. Washington practiced the selective breeding of sheep in an effort to produce better quality wool. He was not as invested in animal husbandry as he was in cropping experiments, which were elaborate and included complex field rotations, nitrogen fixing crops and a range of soil amendments.[40] The Washington household consumed a wider range of protein sources than was typical for the Chesapeake population of his day, which consumed a great deal of beef.[41]
 The new crops were less labor-intensive than tobacco; hence, the estate had a surplus of slaves. But Washington refused to break up families for sale. Washington began to hire skilled indentured servants from Europe to train the redundant slaves for service on and off the estate.[42]
Following his service in the war, Washington returned to Mount Vernon and in 1785–1786 spent a great deal of effort improving the landscaping of the estate. It is estimated that during his two terms as President of the United States (1789–1797), Washington spent a total of 434 days in residence at Mount Vernon. After his presidency, Washington tended to repairs to the buildings, socializing, and further gardening. In 1797, farm manager James Anderson, a recent Scottish immigrant, suggested the establishment of a whisky distillery,[43] which proved to be the estate's most profitable business venture over the decade of its operation.[44]
 In his will, written several months before his death in December 1799, George Washington left directions for the emancipation of all the slaves who belonged to him. Of the 317 slaves at Mount Vernon in 1799, a little less than half, 123 individuals, belonged to George Washington. Under the terms of his will, these slaves were to be set free upon Martha Washington's death.[45]
 In accordance with state law, George Washington stipulated in his will that elderly slaves or those who were too sick to work were to be supported throughout their lives by his estate. Children without parents, or those whose families were too poor or indifferent to see to their education, were to be bound out (or apprenticed) to masters and mistresses who would teach them reading, writing, and a useful trade, until they were ultimately freed at the age of twenty-five.[45]
 When Martha Washington's first husband, Daniel Parke Custis, died without a will, she received a life interest in one-third of his estate, including the slaves. Neither George nor Martha Washington could free these slaves by law. Upon Martha's death, these slaves reverted to the Custis estate and were divided among her grandchildren. By 1799, 153 slaves at Mount Vernon were part of this dower property.[45]
 Martha signed a deed of manumission in December 1800.[46] Abstracts of court records in Fairfax County, Virginia record this transaction. The slaves received their freedom on January 1, 1801.[45]
 On December 12, 1799, Washington spent several hours riding over the plantation, in snow, sleet, and freezing rain. He ate his supper later that evening without changing from his wet clothes. The following day, he awoke with a severe sore throat (either quinsy or acute epiglottitis) and became increasingly hoarse as the day progressed. All the available medical treatments failed to improve his condition, and he died at Mount Vernon at around 10 pm on December 14, 1799, aged 67.
 On December 18, a funeral was held at Mount Vernon, where his body was interred.[47] Congress passed a joint resolution to construct a marble monument in the United States Capitol for his body, an initiative supported by Martha. In December 1800, the United States House passed an appropriations bill for $200,000 (~$4.55 million in 2023) to build the mausoleum, which was to be a pyramid with a base 100 feet (30 m) square. Southerners who wanted his body to remain at Mount Vernon defeated the measure.[48]
 In accordance with his will, Washington was entombed in a family crypt he had built upon first inheriting the estate. It was in disrepair by 1799, so Washington's will also requested that a new, larger tomb be built. This was not executed until 1831, nearly the centennial of his birth. The need for a new tomb was confirmed when an unsuccessful attempt was made to steal his skull.[49] A joint Congressional committee in early 1832 debated the removal of Washington's body from Mount Vernon to a crypt in the Capitol, built by Charles Bulfinch in the 1820s. Southern opposition was intense, exacerbated by an ever-growing rift between North and South. Congressman Wiley Thompson of Georgia expressed the Southerners' fears when he said:
 In 1831, the bodies of George and Martha Washington, along with other members of the family, were moved from the old crypt to the new family tomb.[50] On October 7, 1837, Washington's remains, encased in a lead inner casket, were transferred from the closed tomb to a sarcophagus presented by John Struthers of Philadelphia. It was placed on the right side of the gateway to the tomb. A similar structure was provided for Martha's remains, which was placed on the left.[51] Other members of the Washington family are interred in an inner vault, behind the vestibule containing the sarcophagi.
 Following Martha Washington's death in 1802, George Washington's will was carried out in accordance with the terms of his bequests. The largest part of his estate, which included both his papers and Mount Vernon, passed to his nephew, Bushrod Washington, an Associate Justice of the Supreme Court of the United States.[52] The younger Washington and his wife then moved to Mount Vernon.[53]
 Bushrod Washington did not inherit much cash and was unable to support the upkeep of the estate's mansion on the proceeds from the property and his Supreme Court salary. He sold some of his own slaves to gain working capital.[54] However, the farms' low revenues left him short, and he was unable to adequately maintain the mansion.
 Following Bushrod Washington's death in 1829, ownership of the plantation passed to George Washington's grandnephew, John Augustine Washington II. After he died in 1832, his wife, Jane Charlotte inherited the estate, and her son began managing it. Upon her death in 1855, John Augustine Washington III inherited the property. As his funds dwindled and the wear and tear of hundreds of visitors began to take its toll, Washington could do little to maintain the mansion and its surroundings.[55]
 Washington suggested to the United States Congress that the federal government purchase the mansion.[55] However, Congress paid little interest to Washington's offer, as the legislature was focusing on the conditions that shortly led to the American Civil War.[55] Washington then traveled to Richmond, where he was equally unsuccessful in appealing to the Virginia General Assembly for the state to purchase the mansion.[55] The mansion's decline continued.[55]
 In 1858, Washington sold the mansion and a portion of the estate's land to the Mount Vernon Ladies' Association, which was under the leadership of Ann Pamela Cunningham.[55] The association paid the final installment of the purchase price of $200,000 ($6.3 million in 2020 dollars) in December 1859, taking possession in February 1860.[55] The estate first opened to the public during that year.[10]
 The estate served as a neutral ground for both sides during the Civil War, although fighting raged across the nearby countryside. Troops from both the Union and the Confederacy toured the building. The two women caretakers asked that the soldiers leave their arms behind and either change to civilian clothes or at least cover their uniforms. They usually did as asked.[56]
 Harrison Howell Dodge became the third resident superintendent in 1885. During his 52 years' overseeing the estate, he doubled the facility's acreage, improved the grounds, and added many historic artifacts to the collections. Dodge reviewed George Washington's writings about the estate, visited other Colonial-era gardens, and traveled to England to see gardens dating from the Georgian period. Using that knowledge, Dodge oversaw the restoration of the site completed by Charles Wilson Killam,[57] and put in place a number of improvements that Washington had planned but had never implemented.[58]
 Charles Wall was assistant superintendent from 1929 to 1937, then resident superintendent for 39 years. He oversaw restoration of the house by Killam and planted greenery consistent with what was used in the 18th century. In 1974, a campaign he organized was successful in preserving as parkland areas in Maryland across the Potomac River from Mount Vernon, as part of an effort to retain the bucolic vista from the house.[59] His office was the same one used in the 18th century by Washington.[60]
 Steamboats began to carry tourists to the Mount Vernon estate in 1878.[61] In 1892, the Washington, Alexandria and Mount Vernon Electric Railway opened, providing electric trolley service between Alexandria and the estate.[62][63][64] The electric railway and its successors carried tourists and others between Washington, D.C., and Mount Vernon from 1896 to 1932, when the federal government acquired part of its route on which to construct the George Washington Memorial Parkway.[63]
 [65] The parkway, originally named the Mount Vernon Memorial Parkway, opened in 1932.[62]
 In 2007, the estate opened a reconstruction of George Washington's distillery on the site of Washington's original distillery, a short distance from his mansion on the Potomac River. Construction of the distillery cost $2.1 million. The fully functional replica received special legislation from the Virginia General Assembly to produce up to 5,000 US gal (19,000 L) of whiskey annually, for sale only at the Mount Vernon gift shop.
 Frank Coleman, spokesman for the Distilled Spirits Council that funded the reconstruction, said the distillery ""will become the equivalent of a national distillery museum"" and serve as a gateway to the American Whiskey Trail.[66] In 2019, Mount Vernon began an annual whiskey festival.[67]
 As of 2020, the estate had received more than 85 million visitors.[10] In addition to the mansion, visitors can see original and reconstructed outbuildings and barns (including slaves' quarters), an operational blacksmith shop, and the Pioneer Farm. Each year on Christmas Day, Aladdin the Christmas Camel recreates Washington's 1787 hiring of a camel for 18 shillings to entertain his guests with an example of the animal that brought the Three Wise Men to Bethlehem to visit the newborn Jesus.[68]
 Mount Vernon remains a privately owned property. The non-profit Mount Vernon Ladies' Association has not received any funds from the federal government to support the restoration and maintenance of the mansion and the estate's 500-acre (2.0 km2) grounds or its educational programs and activities.[69]
 The association derives its income from charitable donations and the sales of tickets, produce and goods to visitors. These enable the Association to continue its mission ""to preserve, restore, and manage the estate of George Washington to the highest standards and to educate visitors and people throughout the world about the life and legacies of George Washington, so that his example of character and leadership will continue to inform and inspire future generations.""[70] Admission to Mount Vernon is free on Presidents' Day (the third Monday of February) and on George Washington's birthday (February 22).[71]
 Mount Vernon was featured in a 1-cent United States postage stamp in 1936 within the Army and Navy Commemorative Series. The green stamp, which was the first in the series, also contained portraits of George Washington and Nathanael Greene, a Major General of the Continental Army during the Revolutionary War.[72] 
 In 1956, a 1.5-cent stamp within the Liberty Issue of U.S. postage stamps memorialized Mount Vernon as a national shrine. The Liberty Issue was originally planned to honor six presidents, six famous Americans, and six historic national shrines. The Mount Vernon stamp, which featured a view of Washington's home facing the Potomac River, was the issue's first that commemorated a shrine.[73]
 Mount Vernon was designated a National Historic Landmark on December 19, 1960, and listed on the National Register of Historic Places on October 15, 1966.[1][2] Development and improvement of the estate is an ongoing concern. Following a $110 million fundraising campaign, two new buildings that GWWO, Inc./Architects had designed opened in 2006 as venues for additional background on George Washington and the American Revolution. The Ford Orientation Center introduces visitors to George Washington and Mount Vernon with displays and a film. The Donald W. Reynolds Museum and Education Center houses many artefacts related to Washington along with multimedia displays and further films using modern entertainment technology.
 Mount Vernon was put on the tentative list for World Heritage Site status in the early 2000s. It was submitted but failed to get approved. In 2014, Mount Vernon awarded its first Cyrus A. Ansary Prize for Courage and Character to former President George H. W. Bush.[74][75]
 The airspace surrounding Mount Vernon is restricted to prevent damage from aircraft vibrations.[76][77] As a consequence, overhead/aerial photography has been limited and requires unique approaches.[78]
 In 1955, a 485-acre farm across from Mount Vernon went up for sale. There were rumors that an oil company was to buy it. Charles Wagner, a resident of the Moyaone Association, a community next to the proposed site, reached out to Charles Wall, the Resident Director of Mount Vernon.[79] The Mount Vernon Ladies' Association and its then leader, Ohio Member of Congress Frances P. Bolton, had expressed a desire to protect the view from Mount Vernon. At this point Bolton, Wagner, Wall, and Moyaone resident Robert W. Straus developed a decades-long plan to protect the Mount Vernon viewshed, which came to be known as Operation Overview.[80][81]
 The first step was taken in 1957 when Bolton founded the Accokeek Foundation, one of the nation's first land trusts.[81] The Foundation was used to purchase 200 acres (81 ha) of land across from Mount Vernon to help preserve the area,[82]
 In 1961 and at Bolton's instigation, a joint resolution to preserve the viewshed was introduced in the United States Senate by Senator Clinton Anderson with identical text in the United States House of Representatives by Representative John P. Saylor. The resolution was quickly passed and signed by President John F. Kennedy. Its purpose was to ""preserve lands which provide the principal overview from the Mount Vernon Estate and Fort Washington"" in order to designate 133 acres (54 ha) around Mockley Point, which was to be the site of water treatment plant, as a national landmark. The resolution also authorized the National Park Service to receive donations and scenic easements from adjacent communities.[83] At this point Bolton and the Accokeek Foundation transferred their land to the National Park Service to form Piscataway Park.[79] In addition, Moyaone Association residents transferred conservation easements to the Park Service to further protect the viewshed. In 2020, the Moyaone Reserve was given National Register of Historic Places status.[84]
 The Fairfax Connector Routes 101, 151 and 152 buses travel daily between the Mount Vernon estate and the Huntington station on Washington Metro's Yellow Line.[85] The Route 11C Metrobus travels between the estate and the Braddock Road station on Metro's Blue and Yellow Lines during weekday peak hours.[86]
 The 17-mile (27 km)-long Mount Vernon Trail travels along the George Washington Memorial Parkway and the Potomac River between the Mount Vernon estate and Rosslyn in Arlington County, Virginia, where it connects to the Custis Trail.[87][88] The shared-use path is a part of the Potomac Heritage Trail, the East Coast Greenway and U.S. Bicycle Route 1.
 The Mount Vernon Trail connects to shared-use paths that travel on the Francis Scott Key Bridge, the Theodore Roosevelt Bridge, the Arlington Memorial Bridge and the George Mason Memorial Bridge (one of the 14th Street bridges).[88][89] The bridges cross the river into Washington, D.C., where their shared-use paths connect to the Rock Creek and Potomac Parkway Trail, the Chesapeake and Ohio Canal towpath and the Capital Crescent Trail.[88][89]
  Media related to Mount Vernon at Wikimedia Commons
"
Appleby Grammar School,https://en.wikipedia.org/wiki/Appleby_Grammar_School,"

 Appleby Grammar School is a co-educational comprehensive secondary school and sixth form in Appleby-in-Westmorland for students aged 11 to 18. Since 2011, it has been an Academy. Until 2013, the school was a registered charity.[1]
 The origins of Appleby Grammar lie in the three chantries established in the town's two medieval churches; those of the Blessed Virgin Mary (founded c. 1260 by William de Goldyngton, Mayor of Appleby) and of St Nicholas (founded in 1334 by Robert de Threlkeld), both in the Church of St Lawrence,[2] and that of the Virgin Mary, founded by William L'English before 1344 in the Church of St Michael, Bongate.[3] These chantries, constituted to celebrate masses for the souls of their founders, were also endowed (as deeds of 1478 and 1518 [WSMB/A] and 1533 show)[4] with monies to enjoin the chaplain to teach a free grammar school in the borough, initially in the church itself, as a part of his duty.[citation needed]
 The first mention of a school in Appleby is shown by the sale in 1452 of a burgage house made by John Marshall, Vicar of St Michael's, to Thomas Lord Clifford (also responsible for erecting the greater part of the present Appleby Castle during the reign of Henry VI). The property was described in the sale as ""on the west side of Kirkgate extending in length to a certain narrow lane called School-house Gate"".[citation needed]
 In consideration of the loss sustained by the dissolution of the chantries in the time of Edward VI, Queen Mary granted to the school at Appleby a yearly rent charge of £5 10s. 8d., its revenues being replaced by a grant payable from the income of the Rectory of Crosby Ravensworth. Further bequests were made from the wills of Robert Langton (Archdeacon of Dorset 1486–1514, educated in Appleby) and Dr. Miles Spencer (d. 1569). These legacies enabled the Borough to purchase Royal Letters Patent, endowed by Queen Elizabeth I in 1574, and so provide a firm basis for the continued establishment and survival of the grammar school, ""with ten governors, who are to appoint successors, nominate the master and usher, make statutes for the regulation of the school, and receive lands and possessions, so as they exceed not the clear yearly value of £40"" (though this limitation has been greatly exceeded).[citation needed]
 The incumbent headmaster in 1574, John Boste, later a Catholic convert and martyr (canonised by Pope Paul VI in 1970 as one of the Forty Martyrs of England and Wales) was followed in 1580 by Reginald Bainbrigg, a considerable scholar, who made tours of Hadrian's Wall in 1599 and 1601 and corresponded with William Camden and Sir Robert Cotton on antiquarian matters. On his death (c.1613) he bequeathed some 295 volumes to the school library, which grew considerably in size as witnessed by the catalogues of 1656, 1782 and 1847. Its funds were augmented each year by contributions from departing pupils. The library is now in the care of the University Library of Newcastle-upon-Tyne.[citation needed]
 Official criticism of the school in 1869 by the Schools Inquiry Commission (1864–1868), which examined endowed grammar schools under the chairmanship of Lord Taunton, revealed an uncertain future as a high grade classical school. In 1868, there were only 16 pupils attending but by 1880 there were 80 boarders alone.[citation needed]
 Fruitless proposals were made by the governors to rebuild and amend the existing buildings, and in 1887 construction of a new school was completed at Battlebarrow,[5] on the outskirts of the town, on a site provided by land purchased from St Anne's Hospital[6] and Lord Hothfield. A new scheme for the administration of the school along more modern lines was implemented in 1891. Thereafter, there followed a steady growth in pupil numbers, from 45 in 1887, 68 in 1914, 135 in 1940 to 170 in 1955, when girls were first admitted.[7]
 In the early 1950s, because of the large size of the catchment area and problems students would face under adverse weather conditions, there were Government proposals for comprehensive education to be provided on larger sites, for pupils of all academic abilities, offering modern and technical courses. Westmorland County Council (1889–1974) suggested a development plan for North Westmorland, which was considered and agreed upon by the governors of both Appleby and Kirkby Stephen Grammar Schools, for defined catchment areas to be set in place. Appleby would take pupils from an area including Appleby, Asby, the fellside villages and villages west of the A66. The catchment area would eventually extend to Cliburn, Morland, Newby, Reagill and Sleagill.[citation needed]
 In 1955, an extension at Appleby was completed to accommodate domestic science, woodwork, science and art rooms, and a girls' cloakroom was added to the ground floor. A new school was also completed at Kirkby Stephen, both schools becoming co-educational for the 1955–56 academic year and Appleby ceasing to take boarders.[citation needed]
 In 1959, while retaining the title of grammar school, Appleby and Kirkby Stephen schools became comprehensive and expanded rapidly so that, by 1974, 400 years after the establishment of the Elizabethan post-chantry grammar school, there were over 560 pupils on the school roll.[8]
 The father, probably, and two step-brothers, definitely, of the founding President of the United States, George Washington, attended the school.[9]
 On his death, the widow of Washington's paternal grandfather Lawrence Washington of Virginia, Mildred (née Warner), married George Gale. The Gale family were the chief tobacco merchants of Whitehaven, Cumberland. In 1700, pregnant, Mildred Gale moved with her new husband and three children, John (6), Augustine (3), and Mildred (an infant), to Whitehaven.[10] In 1701, Mildred died in childbirth; she was buried in St Nicholas Churchyard in Whitehaven. Mildred's sister, Mary, is an ancestor of Queen Elizabeth II. George Gale sent the boys to board at Appleby Grammar until custody of the children was successfully challenged by the Washington family, and the boys returned to Virginia, to live near Chotank Creek.[citation needed]
 Washington's father, Augustine, chose to enrol his two sons from his first marriage to Jane Butler, Lawrence and Augustine, at Appleby Grammar. George was the first son of his second marriage to Mary Ball. Were it not for the sudden death of his father in 1743, on reaching the age at which the two older boys had made the long voyage from Virginia, George would have most certainly followed in their footsteps.[11]
 In 2008, Appleby Grammar School was one of five Cumbrian schools presented with the DCSF International School Award in recognition of links with schools abroad.[12] It was rated 'Good' in its Ofsted inspection in the same year.[13] In the 2011 Ofsted inspection, the school was rated as 'Satisfactory'.[14] In 2013 and again in 2016, Ofsted report's determined that the school required improvement.[15][16]
 In April 2022, the school was graded as 'Good' in all areas.
"
Lower Church,https://en.wikipedia.org/wiki/Lower_Church,"
 Lower Church is a historic Methodist church, formerly an Episcopal church, located near Hartfield, Middlesex County, Virginia.  It was constructed in 1717, and is a one-story, rectangular brick building with a clipped gable roof.  It measures 56 by 34 feet (17 m × 10 m).[3]
 It was listed on the National Register of Historic Places in 1973.[1]
 Official website
"
"Hartfield, Virginia","https://en.wikipedia.org/wiki/Hartfield,_Virginia","
 Hartfield is an unincorporated community in Middlesex County, Virginia, United States. Hartfield is located at the southern junction of Virginia State Route 3 and Virginia State Route 33, 9 miles (14 km) east-southeast of Saluda. Hartfield has a post office with ZIP code 23071, which opened on September 5, 1889.[2][3]
 William Churchill, patriarch of one of the first Virginia's colonial families, built Wilton House in Hartfield in 1763. Wilton House is a T-shaped Georgian plantation house near the Piankatank River in the Tidewater region on Virginia's Middle Peninsula. It's now open as a guest house.
 
"
Surveying,https://en.wikipedia.org/wiki/Surveying,"

 Surveying or land surveying is the technique, profession, art, and science of determining the terrestrial two-dimensional or three-dimensional positions of points and the distances and angles between them. These points are usually on the surface of the Earth, and they are often used to establish maps and boundaries for ownership, locations, such as the designated positions of structural components for construction or the surface location of subsurface features, or other purposes required by government or civil law, such as property sales.[1]
 A professional in land surveying is called a land surveyor.
Surveyors work with elements of geodesy, geometry, trigonometry, regression analysis, physics, engineering, metrology, programming languages, and the law. They use equipment, such as total stations, robotic total stations, theodolites, GNSS receivers, retroreflectors, 3D scanners, lidar sensors, radios, inclinometer, handheld tablets, optical and digital levels, subsurface locators, drones, GIS, and surveying software.
 Surveying has been an element in the development of the human environment since the beginning of recorded history. It is used in the planning and execution of most forms of construction. It is also used in transportation, communications, mapping, and the definition of legal boundaries for land ownership. It is an important tool for research in many other scientific disciplines.
 The International Federation of Surveyors defines the function of surveying as follows:[2]
 Surveying has occurred since humans built the first large structures. In ancient Egypt, a rope stretcher would use simple geometry to re-establish boundaries after the annual floods of the Nile River. The almost perfect squareness and north–south orientation of the Great Pyramid of Giza, built c. 2700 BC, affirm the Egyptians' command of surveying. The groma instrument may have originated in Mesopotamia (early 1st millennium BC).[3] The prehistoric monument at Stonehenge (c. 2500 BC) was set out by prehistoric surveyors using peg and rope geometry.[4]
 The mathematician Liu Hui described ways of measuring distant objects in his work Haidao Suanjing or The Sea Island Mathematical Manual, published in 263 AD.
 The Romans recognized land surveying as a profession. They established the basic measurements under which the Roman Empire was divided, such as a tax register of conquered lands (300 AD).[5] Roman surveyors were known as Gromatici.
 In medieval Europe, beating the bounds maintained the boundaries of a village or parish. This was the practice of gathering a group of residents and walking around the parish or village to establish a communal memory of the boundaries. Young boys were included to ensure the memory lasted as long as possible.
 In England, William the Conqueror commissioned the Domesday Book in 1086. It recorded the names of all the land owners, the area of land they owned, the quality of the land, and specific information of the area's content and inhabitants. It did not include maps showing exact locations.
 Abel Foullon described a plane table in 1551, but it is thought that the instrument was in use earlier as his description is of a developed instrument.
 Gunter's chain was introduced in 1620 by English mathematician Edmund Gunter. It enabled plots of land to be accurately surveyed and plotted for legal and commercial purposes.
 Leonard Digges described a theodolite that measured horizontal angles in his book A geometric practice named Pantometria (1571). Joshua Habermel (Erasmus Habermehl) created a theodolite with a compass and tripod in 1576. Johnathon Sission was the first to incorporate a telescope on a theodolite in 1725.[6]
 In the 18th century, modern techniques and instruments for surveying began to be used. Jesse Ramsden introduced the first precision theodolite in 1787. It was an instrument for measuring angles in the horizontal and vertical planes. He created his great theodolite using an accurate dividing engine of his own design. Ramsden's theodolite represented a great step forward in the instrument's accuracy. William Gascoigne invented an instrument that used a telescope with an installed crosshair as a target device, in 1640. James Watt developed an optical meter for the measuring of distance in 1771; it measured the parallactic angle from which the distance to a point could be deduced.
 Dutch mathematician Willebrord Snellius (a.k.a. Snel van Royen) introduced the modern systematic use of triangulation. In 1615 he surveyed the distance from Alkmaar to Breda, approximately 72 miles (116 km). He underestimated this distance by 3.5%. The survey was a chain of quadrangles containing 33 triangles in all. Snell showed how planar formulae could be corrected to allow for the curvature of the Earth. He also showed how to resect, or calculate, the position of a point inside a triangle using the angles cast between the vertices at the unknown point. These could be measured more accurately than bearings of the vertices, which depended on a compass. His work established the idea of surveying a primary network of control points, and locating subsidiary points inside the primary network later. Between 1733 and 1740, Jacques Cassini and his son César undertook the first triangulation of France. They included a re-surveying of the meridian arc, leading to the publication in 1745 of the first map of France constructed on rigorous principles. By this time triangulation methods were well established for local map-making.
 It was only towards the end of the 18th century that detailed triangulation network surveys mapped whole countries. In 1784, a team from General William Roy's Ordnance Survey of Great Britain began the Principal Triangulation of Britain. The first Ramsden theodolite was built for this survey. The survey was finally completed in 1853. The Great Trigonometric Survey of India began in 1801. The Indian survey had an enormous scientific impact. It was responsible for one of the first accurate measurements of a section of an arc of longitude, and for measurements of the geodesic anomaly. It named and mapped Mount Everest and the other Himalayan peaks. Surveying became a professional occupation in high demand at the turn of the 19th century with the onset of the Industrial Revolution. The profession developed more accurate instruments to aid its work. Industrial infrastructure projects used surveyors to lay out canals, roads and rail.
 In the US, the Land Ordinance of 1785 created the Public Land Survey System. It formed the basis for dividing the western territories into sections to allow the sale of land. The PLSS divided states into township grids which were further divided into sections and fractions of sections.[1]
 Napoleon Bonaparte founded continental Europe's first cadastre in 1808. This gathered data on the number of parcels of land, their value, land usage, and names. This system soon spread around Europe.
 Robert Torrens introduced the Torrens system in South Australia in 1858. Torrens intended to simplify land transactions and provide reliable titles via a centralized register of land. The Torrens system was adopted in several other nations of the English-speaking world.
Surveying became increasingly important with the arrival of railroads in the 1800s. Surveying was necessary so that railroads could plan technologically and financially viable routes.
 At the beginning of the century, surveyors had improved the older chains and ropes, but they still faced the problem of accurate measurement of long distances. Trevor Lloyd Wadley developed the Tellurometer during the 1950s. It measures long distances using two microwave transmitter/receivers.[7]
During the late 1950s Geodimeter introduced electronic distance measurement (EDM) equipment.[8] EDM units use a multi frequency phase shift of light waves to find a distance.[9] These instruments eliminated the need for days or weeks of chain measurement by measuring between points kilometers apart in one go.
 Advances in electronics allowed miniaturization of EDM. In the 1970s the first instruments combining angle and distance measurement appeared, becoming known as total stations. Manufacturers added more equipment by degrees, bringing improvements in accuracy and speed of measurement. Major advances include tilt compensators, data recorders and on-board calculation programs.
 The first satellite positioning system was the US Navy TRANSIT system. The first successful launch took place in 1960. The system's main purpose was to provide position information to Polaris missile submarines. Surveyors found they could use field receivers to determine the location of a point. Sparse satellite cover and large equipment made observations laborious and inaccurate. The main use was establishing benchmarks in remote locations.
 The US Air Force launched the first prototype satellites of the Global Positioning System (GPS) in 1978. GPS used a larger constellation of satellites and improved signal transmission, thus improving accuracy. Early GPS observations required several hours of observations by a static receiver to reach survey accuracy requirements. Later improvements to both satellites and receivers allowed for Real Time Kinematic (RTK) surveying. RTK surveys provide high-accuracy measurements by using a fixed base station and a second roving antenna. The position of the roving antenna can be tracked.
 The theodolite, total station and RTK GPS survey remain the primary methods in use.
 Remote sensing and satellite imagery continue to improve and become cheaper, allowing more commonplace use. Prominent new technologies include three-dimensional (3D) scanning and lidar-based topographical surveys. UAV technology along with photogrammetric image processing is also appearing.
 The main surveying instruments in use around the world are the theodolite, measuring tape, total station, 3D scanners, GPS/GNSS, level and rod. Most instruments screw onto a tripod when in use. Tape measures are often used for measurement of smaller distances. 3D scanners and various forms of aerial imagery are also used.
 The theodolite is an instrument for the measurement of angles. It uses two separate circles, protractors or alidades to measure angles in the horizontal and the vertical plane. A telescope mounted on trunnions is aligned vertically with the target object. The whole upper section rotates for horizontal alignment. The vertical circle measures the angle that the telescope makes against the vertical, known as the zenith angle. The horizontal circle uses an upper and lower plate. When beginning the survey, the surveyor points the instrument in a known direction (bearing), and clamps the lower plate in place. The instrument can then rotate to measure the bearing to other objects. If no bearing is known or direct angle measurement is wanted, the instrument can be set to zero during the initial sight. It will then read the angle between the initial object, the theodolite itself, and the item that the telescope aligns with.
 The gyrotheodolite is a form of theodolite that uses a gyroscope to orient itself in the absence of reference marks. It is used in underground applications.
 The total station is a development of the theodolite with an electronic distance measurement device (EDM). A total station can be used for leveling when set to the horizontal plane. Since their introduction, total stations have shifted from optical-mechanical to fully electronic devices.[10]
 Modern top-of-the-line total stations no longer need a reflector or prism to return the light pulses used for distance measurements. They are fully robotic, and can even e-mail point data to a remote computer and connect to satellite positioning systems, such as Global Positioning System. Real Time Kinematic GPS systems have significantly increased the speed of surveying, and they are now horizontally accurate to within 1 cm ± 1 ppm in real-time, while vertically it is currently about half of that to within 2 cm ± 2 ppm.[11]
 GPS surveying differs from other GPS uses in the equipment and methods used. Static GPS uses two receivers placed in position for a considerable length of time. The long span of time lets the receiver compare measurements as the satellites orbit. The changes as the satellites orbit also provide the measurement network with well conditioned geometry. This produces an accurate baseline that can be over 20 km long. RTK surveying uses one static antenna and one roving antenna. The static antenna tracks changes in the satellite positions and atmospheric conditions. The surveyor uses the roving antenna to measure the points needed for the survey. The two antennas use a radio link that allows the static antenna to send corrections to the roving antenna. The roving antenna then applies those corrections to the GPS signals it is receiving to calculate its own position. RTK surveying covers smaller distances than static methods. This is because divergent conditions further away from the base reduce accuracy.
 Surveying instruments have characteristics that make them suitable for certain uses. Theodolites and levels are often used by constructors rather than surveyors in first world countries. The constructor can perform simple survey tasks using a relatively cheap instrument. Total stations are workhorses for many professional surveyors because they are versatile and reliable in all conditions. The productivity improvements from a GPS on large scale surveys make them popular for major infrastructure or data gathering projects. One-person robotic-guided total stations allow surveyors to measure without extra workers to aim the telescope or record data. A fast but expensive way to measure large areas is with a helicopter, using a GPS to record the location of the helicopter and a laser scanner to measure the ground. To increase precision, surveyors place beacons on the ground (about 20 km (12 mi) apart). This method reaches precisions between 5–40 cm (depending on flight height).[12]
 Surveyors use ancillary equipment such as tripods and instrument stands; staves and beacons used for sighting purposes; PPE; vegetation clearing equipment; digging implements for finding survey markers buried over time; hammers for placements of markers in various surfaces and structures; and portable radios for communication over long lines of sight.
 Land surveyors, construction professionals, geomatics engineers and civil engineers using total station, GPS, 3D scanners, and other collector data use land surveying software to increase efficiency, accuracy, and productivity. Land Surveying Software is a staple of contemporary land surveying.[13]
 Typically, much if not all of the drafting and some of the designing for plans and plats of the surveyed property is done by the surveyor, and nearly everyone working in the area of drafting today (2021) utilizes CAD software and hardware both on PC, and more and more in newer generation data collectors in the field as well.[14] Other computer platforms and tools commonly used today by surveyors are offered online by the U.S. Federal Government and other governments' survey agencies, such as the National Geodetic Survey and the CORS network, to get automated corrections and conversions for collected GPS data, and the data coordinate systems themselves.
 Surveyors determine the position of objects by measuring angles and distances. The factors that can affect the accuracy of their observations are also measured. They then use this data to create vectors, bearings, coordinates, elevations, areas, volumes, plans and maps. Measurements are often split into horizontal and vertical components to simplify calculation.
GPS and astronomic measurements also need measurement of a time component.
 Before EDM (Electronic Distance Measurement) laser devices, distances were measured using a variety of means. In pre-colonial America Natives would use the ""bow shot"" as a distance reference (""as far as an arrow can slung out of a bow"", or ""flights of a Cherokee long bow"").[15] Europeans used chains with links of a known length such as a Gunter's chain, or measuring tapes made of steel or invar. To measure horizontal distances, these chains or tapes were pulled taut to reduce sagging and slack. The distance had to be adjusted for heat expansion. Attempts to hold the measuring instrument level would also be made. When measuring up a slope, the surveyor might have to ""break"" (break chain) the measurement- use an increment less than the total length of the chain. Perambulators, or measuring wheels, were used to measure longer distances but not to a high level of accuracy. Tacheometry is the science of measuring distances by measuring the angle between two ends of an object with a known size. It was sometimes used before to the invention of EDM where rough ground made chain measurement impractical.
 Historically, horizontal angles were measured by using a compass to provide a magnetic bearing or azimuth. Later, more precise scribed discs improved angular resolution. Mounting telescopes with reticles atop the disc allowed more precise sighting (see theodolite). Levels and calibrated circles allowed the measurement of vertical angles. Verniers allowed measurement to a fraction of a degree, such as with a turn-of-the-century transit.
 The plane table provided a graphical method of recording and measuring angles, which reduced the amount of mathematics required. In 1829 Francis Ronalds invented a reflecting instrument for recording angles graphically by modifying the octant.[16]
 By observing the bearing from every vertex in a figure, a surveyor can measure around the figure. The final observation will be between the two points first observed, except with a 180° difference. This is called a close. If the first and last bearings are different, this shows the error in the survey, called the angular misclose. The surveyor can use this information to prove that the work meets the expected standards.
 The simplest method for measuring height is with an altimeter using air pressure to find the height. When more precise measurements are needed, means like precise levels (also known as differential leveling) are used. When precise leveling, a series of measurements between two points are taken using an instrument and a measuring rod. Differences in height between the measurements are added and subtracted in a series to get the net difference in elevation between the two endpoints. With the Global Positioning System (GPS), elevation can be measured with satellite receivers. Usually, GPS is somewhat less accurate than traditional precise leveling, but may be similar over long distances.
 When using an optical level, the endpoint may be out of the effective range of the instrument. There may be obstructions or large changes of elevation between the endpoints. In these situations, extra setups are needed. Turning is a term used when referring to moving the level to take an elevation shot from a different location. To ""turn"" the level, one must first take a reading and record the elevation of the point the rod is located on. While the rod is being kept in exactly the same location, the level is moved to a new location where the rod is still visible. A reading is taken from the new location of the level and the height difference is used to find the new elevation of the level gun, which is why this method is referred to as differential levelling. This is repeated until the series of measurements is completed. The level must be horizontal to get a valid measurement. Because of this, if the horizontal crosshair of the instrument is lower than the base of the rod, the surveyor will not be able to sight the rod and get a reading. The rod can usually be raised up to 25 feet (7.6 m) high, allowing the level to be set much higher than the base of the rod.
 The primary way of determining one's position on the Earth's surface when no known positions are nearby is by astronomic observations. Observations to the Sun, Moon and stars could all be made using navigational techniques. Once the instrument's position and bearing to a star is determined, the bearing can be transferred to a reference point on Earth. The point can then be used as a base for further observations. Survey-accurate astronomic positions were difficult to observe and calculate and so tended to be a base off which many other measurements were made. Since the advent of the GPS system, astronomic observations are rare as GPS allows adequate positions to be determined over most of the surface of the Earth.
 Few survey positions are derived from the first principles. Instead, most surveys points are measured relative to previously measured points. This forms a reference or control network where each point can be used by a surveyor to determine their own position when beginning a new survey.
 Survey points are usually marked on the earth's surface by objects ranging from small nails driven into the ground to large beacons that can be seen from long distances. The surveyors can set up their instruments in this position and measure to nearby objects. Sometimes a tall, distinctive feature such as a steeple or radio aerial has its position calculated as a reference point that angles can be measured against.
 Triangulation is a method of horizontal location favoured in the days before EDM and GPS measurement. It can determine distances, elevations and directions between distant objects. Since the early days of surveying, this was the primary method of determining accurate positions of objects for topographic maps of large areas. A surveyor first needs to know the horizontal distance between two of the objects, known as the baseline. Then the heights, distances and angular position of other objects can be derived, as long as they are visible from one of the original objects. High-accuracy transits or theodolites were used, and angle measurements were repeated for increased accuracy. See also Triangulation in three dimensions.
 Offsetting is an alternate method of determining the position of objects, and was often used to measure imprecise features such as riverbanks. The surveyor would mark and measure two known positions on the ground roughly parallel to the feature, and mark out a baseline between them. At regular intervals, a distance was measured at right angles from the first line to the feature. The measurements could then be plotted on a plan or map, and the points at the ends of the offset lines could be joined to show the feature.
 Traversing is a common method of surveying smaller areas. The surveyors start from an old reference mark or known position and place a network of reference marks covering the survey area. They then measure bearings and distances between the reference marks, and to the target features. Most traverses form a loop pattern or link between two prior reference marks so the surveyor can check their measurements.
 Many surveys do not calculate positions on the surface of the Earth, but instead, measure the relative positions of objects. However, often the surveyed items need to be compared to outside data, such as boundary lines or previous survey's objects. The oldest way of describing a position is via latitude and longitude, and often a height above sea level. As the surveying profession grew it created Cartesian coordinate systems to simplify the mathematics for surveys over small parts of the Earth. The simplest coordinate systems assume that the Earth is flat and measure from an arbitrary point, known as a 'datum' (singular form of data). The coordinate system allows easy calculation of the distances and direction between objects over small areas. Large areas distort due to the Earth's curvature. North is often defined as true north at the datum.
 For larger regions, it is necessary to model the shape of the Earth using an ellipsoid or a geoid. Many countries have created coordinate-grids customized to lessen error in their area of the Earth.
 A basic tenet of surveying is that no measurement is perfect, and that there will always be a small amount of error.[17] There are three classes of survey errors:
 Surveyors avoid these errors by calibrating their equipment, using consistent methods, and by good design of their reference network. Repeated measurements can be averaged and any outlier measurements discarded. Independent checks like measuring a point from two or more locations or using two different methods are used, and errors can be detected by comparing the results of two or more measurements, thus utilizing redundancy.
 Once the surveyor has calculated the level of the errors in his or her work, it is adjusted. This is the process of distributing the error between all measurements. Each observation is weighted according to how much of the total error it is likely to have caused and part of that error is allocated to it in a proportional way. The most common methods of adjustment are the Bowditch method, also known as the compass rule, and the principle of least squares method.
 The surveyor must be able to distinguish between accuracy and precision. In the United States, surveyors and civil engineers use units of feet wherein a survey foot breaks down into 10ths and 100ths. Many deed descriptions containing distances are often expressed using these units (125.25 ft). On the subject of accuracy, surveyors are often held to a standard of one one-hundredth of a foot; about 1/8 inch. Calculation and mapping tolerances are much smaller wherein achieving near-perfect closures are desired. Though tolerances will vary from project to project, in the field and day to day usage beyond a 100th of a foot is often impractical.
 Local organisations or regulatory bodies class specializations of surveying in different ways. Broad groups are:
 Based on the considerations and true shape of the Earth, surveying is broadly classified into two types.
 Plane surveying assumes the Earth is flat. Curvature and spheroidal shape of the Earth is neglected. In this type of surveying all triangles formed by joining survey lines are considered as plane triangles. It is employed for small survey works where errors due to the Earth's shape are too small to matter.[18]
 In geodetic surveying the curvature of the Earth is taken into account while calculating reduced levels, angles, bearings and distances. This type of surveying is usually employed for large survey works. Survey works up to 100 square miles (260 square kilometers ) are treated as plane and beyond that are treated as geodetic.[19] In geodetic surveying necessary corrections are applied to reduced levels, bearings and other observations.[18]
 The basic principles of surveying have changed little over the ages, but the tools used by surveyors have evolved. Engineering, especially civil engineering, often needs surveyors.
 Surveyors help determine the placement of roads, railways, reservoirs, dams, pipelines, retaining walls, bridges, and buildings. They establish the boundaries of legal descriptions and political divisions. They also provide advice and data for geographical information systems (GIS) that record land features and boundaries.
 Surveyors must have a thorough knowledge of algebra, basic calculus, geometry, and trigonometry. They must also know the laws that deal with surveys, real property, and contracts.
 Most jurisdictions recognize three different levels of qualification:
 Related professions include cartographers, hydrographers, geodesists, photogrammetrists, and topographers, as well as civil engineers and geomatics engineers.
 Licensing requirements vary with jurisdiction, and are commonly consistent within national borders. Prospective surveyors usually have to receive a degree in surveying, followed by a detailed examination of their knowledge of surveying law and principles specific to the region they wish to practice in, and undergo a period of on-the-job training or portfolio building before they are awarded a license to practise. Licensed surveyors usually receive a post nominal, which varies depending on where they qualified. The system has replaced older apprenticeship systems.
 A licensed land surveyor is generally required to sign and seal all plans. The state dictates the format, showing their name and registration number.
 In many jurisdictions, surveyors must mark their registration number on survey monuments when setting boundary corners. Monuments take the form of capped iron rods, concrete monuments, or nails with washers.
 Most countries' governments regulate at least some forms of surveying. Their survey agencies establish regulations and standards. Standards control accuracy, surveying credentials, monumentation of boundaries and maintenance of geodetic networks. Many nations devolve this authority to regional entities or states/provinces. Cadastral surveys tend to be the most regulated because of the permanence of the work. Lot boundaries established by cadastral surveys may stand for hundreds of years without modification.
 Most jurisdictions also have a form of professional institution representing local surveyors. These institutes often endorse or license potential surveyors, as well as set and enforce ethical standards. The largest institution is the International Federation of Surveyors (Abbreviated FIG, for French: Fédération Internationale des Géomètres). They represent the survey industry worldwide.
 Most English-speaking countries consider building surveying a distinct profession. They have their own professional associations and licensing requirements. A building surveyor can provide technical building advice on existing buildings, new buildings, design, compliance with regulations such as planning and building control. A building surveyor normally acts on behalf of his or her client ensuring that their vested interests remain protected. The Royal Institution of Chartered Surveyors (RICS) is a world-recognised governing body for those working within the built environment.[20]
 One of the primary roles of the land surveyor is to determine the boundary of real property on the ground. The surveyor must determine where the adjoining landowners wish to put the boundary. The boundary is established in legal documents and plans prepared by attorneys, engineers, and land surveyors. The surveyor then puts monuments on the corners of the new boundary. They might also find or resurvey the corners of the property monumented by prior surveys.
 Cadastral land surveyors are licensed by governments.
The cadastral survey branch of the Bureau of Land Management (BLM) conducts most cadastral surveys in the United States.[21] They consult with Forest Service, National Park Service, Army Corps of Engineers, Bureau of Indian Affairs, Fish and Wildlife Service, Bureau of Reclamation, and others. The BLM used to be known as the United States General Land Office (GLO).
 In states organized per the Public Land Survey System (PLSS), surveyors must carry out BLM cadastral surveys under that system.
 Cadastral surveyors often have to work around changes to the earth that obliterate or damage boundary monuments. When this happens, they must consider evidence that is not recorded on the title deed. This is known as extrinsic evidence.[22]
 Quantity surveying is a profession that deals with the costs and contracts of construction projects. A quantity surveyor is an expert in estimating the costs of materials, labor, and time needed for a project, as well as managing the financial and legal aspects of the project. A quantity surveyor can work for either the client or the contractor, and can be involved in different stages of the project, from planning to completion. Quantity surveyors are also known as Chartered Surveyors in the UK.
 Some U.S. Presidents were land surveyors. George Washington and Abraham Lincoln surveyed colonial or frontier territories early in their career, prior to serving in office.
 Ferdinand Rudolph Hassler is considered the ""father"" of geodetic surveying in the U.S.[23]
 David T. Abercrombie practiced land surveying before starting an outfitter store of excursion goods. The business would later turn into Abercrombie & Fitch lifestyle clothing store.
 Percy Harrison Fawcett was a British surveyor that explored the jungles of South America attempting to find the Lost City of Z. His biography and expeditions were recounted in the book The Lost City of Z and were later adapted on film screen.
 Inō Tadataka produced the first map of Japan using modern surveying techniques starting in 1800, at the age of 55.
"
Draftsman,https://en.wikipedia.org/wiki/Draftsman,"A drafter (also draughtsman / draughtswoman in British and Commonwealth English, draftsman / draftswoman, drafting technician, or CAD technician in American and Canadian English) is an engineering technician who makes detailed technical drawings or CAD designs for machinery, buildings, electronics, infrastructure, sections, etc. Drafters use computer software and manual sketches to convert the designs, plans, and layouts of engineers and architects into a set of technical drawings. Drafters operate as the supporting developers and sketch engineering designs and drawings from preliminary design concepts.
 In the past, drafters sat or stood at drawing boards and used pencils, pens, compasses, rulers, protractors, triangles, and other drafting devices to prepare a drawing by hand. From the 1980s through 1990s, board drawings were going out of style as the newly developed computer-aided design (CAD) system was released and was able to produce technical drawings at a faster pace.
 Many modern drafters now use computer software such as AutoCAD, Revit, and SolidWorks to flesh out the designs of engineers or architects into technical drawings and blueprints but board drafting still remains the base of the CAD system. Many of these drawings are utilized to create structures, tools or machines. In addition, the drawings also include design specifications like dimensions, materials and procedures.[1] Consequently, drafters may also be casually referred to as CAD operators, engineering draftspersons, or engineering technicians.[2]
 With CAD systems, drafters can create and store drawings electronically so that they can be viewed, printed, or programmed directly into automated manufacturing systems. CAD systems also permit drafters to quickly prepare variations of a design. Although drafters use CAD extensively, it is only a tool. Drafters still need knowledge of traditional drafting techniques, in addition to CAD skills. Despite the near global use of CAD systems, manual drafting and sketching are used in certain applications.[2]
 Drafters' drawings provide visual guidelines and show how to construct a product or structure. Drawings include technical details and specify dimensions, materials, and procedures. Drafters fill in technical details using drawings, rough sketches, specifications, and calculations made by engineers, surveyors, architects, or scientists. For example, drafters use their knowledge of standardized building techniques to draw in the details of a structure. Some use their understanding of engineering and manufacturing theory and standards to draw the parts of a machine; they determine design elements, such as the numbers and kinds of fasteners needed to assemble the machine. Drafters use technical handbooks, tables, calculators, and computers to complete their work.[2]
 Drafting work has many specialties such as:[2][1]
 Drafters work in architectural offices, manufacturing companies, engineering firms, CAD-specific work-groups, construction companies, engineering consultancy firms, the government, natural resource companies or are independently self-employed. Drafting technologists and technicians often work as part of a broader multidisciplinary engineering team in support of engineers, architects or industrial designers or they may work on their own. The position of a drafter is one of a skilled assistant to architects and engineers. Drafters usually work in offices, seated at adjustable drawing boards or drafting tables when doing manual drawings, although modern drafters work at computer terminals much of the time. They usually work in an office environment, but some may have to travel and spend time on manufacturing plants or construction sites. As drafters spend long periods in front of computers doing detailed technical work, they may be susceptible to eyestrain, back discomfort, and hand and wrist problems. Most drafters work standard 40-hour weeks; only a small number work part-time.[2]
 High school courses in English, mathematics, science, electronics, computer technology, drafting and design, visual arts, and computer graphics are useful for people considering a drafting career. Attributes required by drafters include technical writing skills, problem-solving skills, the ability to visualize three-dimensional objects from two-dimensional drawings as well as drawing the relationships between parts in machinery and various pieces of infrastructure. Other skills include an in-depth knowledge of the qualities of metals, plastics, wood, bricks and stone and other materials used in the overall manufacturing processes and of construction methods and standards. Technical expertise, a strong understanding of construction and the manufacturing process, and a solid knowledge of drafting and design principles are also important assets in becoming a drafter.[3] In the modern job marketplace, in addition to technical skills enabling CAD drafters to draw up plans, soft skills are also crucial as CADD drafters have to communicate with clients and articulate their drawing plans in an effective way with fellow team members in a real-world setting.[3]
 Employers prefer applicants who have also completed training after high school at a trade or technical school. Prospective drafters will also need to have a strong background knowledge and experience with CADD software. Though licenses are not a prerequisite for becoming drafters, the American Design Drafting Association (ADDA) does offer certification and licensing to make a prospective drafting applicant more competitive in the labour market. Licensing and certification highlights one's core competence and knowledge of a specific drafting specialty. Drafting and design certificates and diplomas are generally offered by vocational institutes such as career training schools, trade and technical schools, and non-university higher educational institutions such as community colleges or industrial training institutes.[1]
 Apprenticeships combine paid on-the-job training and practical work experience with theoretical in-class instruction. Those interested in becoming drafters can earn qualifications as either drafting technologists or drafting technicians. Drafting technologists usually have a 2 to 3-year diploma in engineering design or drafting technology from a community college or technical school.[2] Drafters starting out tend to move from company to company to gain experience and rise up in the professional ranks or they can start their own business and become self-employed to fully establish themselves within the professional pecking order. Compared to an entry-level drafter who is starting out and often lacks job experience, a more seasoned drafter often rises up within the professional ranks into a management position where they are assigned and tasked with supervising entire projects in addition to overseeing and delegating junior and entry-level drafters. If drafters with well-established careers wish to further their education and broaden their employment prospects, it is also possible for experienced drafters to enter related fields such as engineering, architecture, industrial design, interior design, exhibit design, landscape design, set design, and animation.[4][5]
  This article incorporates public domain material from Occupational Outlook Handbook, 2014–15 Edition, Drafters (visited January 26, 2015). United States Department of Labor (US DOL), Bureau of Labor Statistics (BLS).
"
Cartography,https://en.wikipedia.org/wiki/Cartography,"Cartography (/kɑːrˈtɒɡrəfi/; from Ancient Greek: χάρτης chartēs, 'papyrus, sheet of paper, map'; and γράφειν graphein, 'write') is the study and practice of making and using maps. Combining science, aesthetics and technique, cartography builds on the premise that reality (or an imagined reality) can be modeled in ways that communicate spatial information effectively.
 The fundamental objectives of traditional cartography are to:
 Modern cartography constitutes many theoretical and practical foundations of geographic information systems (GIS) and geographic information science (GISc).
 What is the earliest known map is a matter of some debate, both because the term ""map"" is not well-defined and because some artifacts that might be maps might actually be something else. A wall painting that might depict the ancient Anatolian city of Çatalhöyük (previously known as Catal Huyuk or Çatal Hüyük) has been dated to the late 7th millennium BCE.[1][2] Among the prehistoric alpine rock carvings of Mount Bego (France) and Valcamonica (Italy), dated to the 4th millennium BCE, geometric patterns consisting of dotted rectangles and lines are widely interpreted[3][4] in archaeological literature as depicting cultivated plots.[5] Other known maps of the ancient world include the Minoan ""House of the Admiral"" wall painting from c. 1600 BCE, showing a seaside community in an oblique perspective, and an engraved map of the holy Babylonian city of Nippur, from the Kassite period (14th – 12th centuries BCE).[6] The oldest surviving world maps are from 9th century BCE Babylonia.[7] One shows Babylon on the Euphrates, surrounded by Assyria, Urartu[8] and several cities, all, in turn, surrounded by a ""bitter river"" (Oceanus).[9] Another depicts Babylon as being north of the center of the world.[7]
 The ancient Greeks and Romans created maps from the time of Anaximander in the 6th century BCE.[10] In the 2nd century CE, Ptolemy wrote his treatise on cartography, Geographia.[11] This contained Ptolemy's world map – the world then known to Western society (Ecumene). As early as the 8th century, Arab scholars were translating the works of the Greek geographers into Arabic.[12] Roads were essential in the Roman world, motivating the creation of maps, called itinerarium, that portrayed the world as experienced via the roads. The Tabula Peutingeriana is the only surviving example.
 In ancient China, geographical literature dates to the 5th century BCE. The oldest extant Chinese maps come from the State of Qin, dated back to the 4th century BCE, during the Warring States period. In the book Xin Yi Xiang Fa Yao, published in 1092 by the Chinese scientist Su Song, a star map on the equidistant cylindrical projection.[13][14] Although this method of charting seems to have existed in China even before this publication and scientist, the greatest significance of the star maps by Su Song is that they represent the oldest existent star maps in printed form.
 Early forms of cartography of India included depictions of the pole star and surrounding constellations.[15] These charts may have been used for navigation.[15]
 Mappae mundi ('maps of the world') are the medieval European maps of the world. About 1,100 of these are known to have survived: of these, some 900 are found illustrating manuscripts, and the remainder exist as stand-alone documents.[16]
 The Arab geographer Muhammad al-Idrisi produced his medieval atlas Tabula Rogeriana (Book of Roger) in 1154. By combining the knowledge of Africa, the Indian Ocean, Europe, and the Far East (which he learned through contemporary accounts from Arab merchants and explorers) with the information he inherited from the classical geographers, he was able to write detailed descriptions of a multitude of countries. Along with the substantial text he had written, he created a world map influenced mostly by the Ptolemaic conception of the world, but with significant influence from multiple Arab geographers. It remained the most accurate world map for the next three centuries.[17][18] The map was divided into seven climatic zones, with detailed descriptions of each zone. As part of this work, a smaller, circular map depicting the south on top and Arabia in the center was made. Al-Idrisi also made an estimate of the circumference of the world, accurate to within 10%.[19]
 In the Age of Discovery, from the 15th century to the 17th century, European cartographers both copied earlier maps (some of which had been passed down for centuries) and drew their own based on explorers' observations and new surveying techniques. The invention of the magnetic compass, telescope and sextant enabled increasing accuracy. In 1492, Martin Behaim, a German cartographer and advisor to the king John II of Portugal, made the oldest extant globe of the Earth.[20]
 In 1507, Martin Waldseemüller produced a globular world map and a large 12-panel world wall map (Universalis Cosmographia) bearing the first use of the name ""America."" Portuguese cartographer Diogo Ribero was the author of the first known planisphere with a graduated Equator (1527). Italian cartographer Battista Agnese produced at least 71 manuscript atlases of sea charts. Johannes Werner refined and promoted the Werner projection. This was an equal-area, heart-shaped world map projection (generally called a cordiform projection) that was used in the 16th and 17th centuries. Over time, other iterations of this map type arose; most notable are the sinusoidal projection and the Bonne projection. The Werner projection places its standard parallel at the North Pole; a sinusoidal projection places its standard parallel at the equator; and the Bonne projection is intermediate between the two.[21][22]
 In 1569, mapmaker Gerardus Mercator first published a map based on his Mercator projection, which uses equally-spaced parallel vertical lines of longitude and parallel latitude lines spaced farther apart as they get farther away from the equator. By this construction, courses of constant bearing are conveniently represented as straight lines for navigation. The same property limits its value as a general-purpose world map because regions are shown as increasingly larger than they actually are the further from the equator they are. Mercator is also credited as the first to use the word ""atlas"" to describe a collection of maps. In the later years of his life, Mercator resolved to create his Atlas, a book filled with many maps of different regions of the world, as well as a chronological history of the world from the Earth's creation by God until 1568. He was unable to complete it to his satisfaction before he died. Still, some additions were made to the Atlas after his death, and new editions were published after his death.[23][24]
 In 1570, the Brabantian cartographer Abraham Ortelius, strongly encouraged by Gillis Hooftman, created the first true modern atlas, Theatrum Orbis Terrarum.[25] In a rare move, Ortelius credited mapmakers who contributed to the atlas, the list of which grew to 183 individuals by 1603.[26]
 In the Renaissance, maps were used to impress viewers and establish the owner's reputation as sophisticated, educated, and worldly. Because of this, towards the end of the Renaissance, maps were displayed with equal importance of painting, sculptures, and other pieces of art.[27] In the sixteenth century, maps were becoming increasingly available to consumers through the introduction of printmaking, with about 10% of Venetian homes having some sort of map by the late 1500s.
 There were three main functions of maps in the Renaissance:[28]
 In medieval times, written directions of how to get somewhere were more common than the use of maps. With the Renaissance, cartography began to be seen as a metaphor for power.[28] Political leaders could lay claim to territories through the use of maps, and this was greatly aided by the religious and colonial expansion of Europe. The Holy Land and other religious places were the most commonly mapped during the Renaissance.
 In the late 1400s to the late 1500s, Rome, Florence, and Venice dominated map-making and trade. It started in Florence in the mid-to late 1400s. Map trade quickly shifted to Rome and Venice but then was overtaken by atlas makers in the late 16th century.[29] Map publishing in Venice was completed with humanities and book publishing in mind, rather than just informational use.
 There were two main printmaking technologies in the Renaissance: woodcut and copper-plate intaglio, referring to the medium used to transfer the image onto paper.
 In woodcut, the map image is created as a relief chiseled from medium-grain hardwood. The areas intended to be printed are inked and pressed against the sheet. Being raised from the rest of the block, the map lines cause indentations in the paper that can often be felt on the back of the map. There are advantages to using relief to make maps. For one, a printmaker doesn't need a press because the maps could be developed as rubbings. Woodblock is durable enough to be used many times before defects appear. Existing printing presses can be used to create the prints rather than having to create a new one. On the other hand, it is hard to achieve fine detail with the relief technique. Inconsistencies in linework are more apparent in woodcut than in intaglio. To improve quality in the late fifteenth century, a style of relief craftsmanship developed using fine chisels to carve the wood, rather than the more commonly used knife.
 In intaglio, lines are engraved into workable metals, typically copper but sometimes brass. The engraver spreads a thin sheet of wax over the metal plate and uses ink to draw the details. Then, the engraver traces the lines with a stylus to etch them into the plate beneath.[30] The engraver can also use styli to prick holes along the drawn lines, trace along them with colored chalk, and then engrave the map. Lines going in the same direction are carved at the same time, and then the plate is turned to carve lines going in a different direction. To print from the finished plate, ink is spread over the metal surface and scraped off such that it remains only in the etched channels. Then the plate is pressed forcibly against the paper so that the ink in the channels is transferred to the paper. The pressing is so forceful that it leaves a ""plate mark"" around the border of the map at the edge of the plate, within which the paper is depressed compared to the margins.[31] Copper and other metals were expensive at the time, so the plate was often reused for new maps or melted down for other purposes.[31]
 Whether woodcut or intaglio, the printed map is hung out to dry. Once dry, it is usually placed in another press to flatten the paper. Any type of paper that was available at the time could be used to print the map, but thicker paper was more durable.
 Both relief and intaglio were used about equally by the end of the fifteenth century.
 Lettering in mapmaking is important for denoting information. Fine lettering is difficult in woodcut, where it often turned out square and blocky, contrary to the stylized, rounded writing style popular in Italy at the time.[31] To improve quality, mapmakers developed fine chisels to carve the relief. Intaglio lettering did not suffer the troubles of a coarse medium and so was able to express the looping cursive that came to be known as cancellaresca.[31] There were custom-made reverse punches that were also used in metal engraving alongside freehand lettering.[30]
 The first use of color in map-making cannot be narrowed down to one reason. There are arguments that color started as a way to indicate information on the map, with aesthetics coming second. There are also arguments that color was first used on maps for aesthetics but then evolved into conveying information.[31] Either way, many maps of the Renaissance left the publisher without being colored, a practice that continued all the way into the 1800s. However, most publishers accepted orders from their patrons to have their maps or atlases colored if they wished. Because all coloring was done by hand, the patron could request simple, cheap color, or more expensive, elaborate color, even going so far as silver or gold gilding. The simplest coloring was merely outlines, such as of borders and along rivers. Wash color meant painting regions with inks or watercolors. Limning meant adding silver and gold leaf to the map to illuminate lettering, heraldic arms, or other decorative elements.
 The early modern period saw the convergence of cartographical techniques across Eurasia and the exchange of mercantile mapping techniques via the Indian Ocean.[32]
 In the early seventeenth century, the Selden map was created by a Chinese cartographer. Historians have put its date of creation around 1620, but there is debate in this regard. This map's significance draws from historical misconceptions of East Asian cartography, the main one being that East Asians did not do cartography until Europeans arrived. The map's depiction of trading routes, a compass rose, and scale bar points to the culmination of many map-making techniques incorporated into Chinese mercantile cartography.[33]
 In 1689, representatives of the Russian tsar and Qing Dynasty met near the border town of Nerchinsk, which was near the disputed border of the two powers, in eastern Siberia.[34] The two parties, with the Qing negotiation party bringing Jesuits as intermediaries, managed to work a treaty which placed the Amur River as the border between the Eurasian powers, and opened up trading relations between the two. This treaty's significance draws from the interaction between the two sides, and the intermediaries who were drawn from a wide variety of nationalities.
 Maps of the Enlightenment period practically universally used copper plate intaglio, having abandoned the fragile, coarse woodcut technology. Use of map projections evolved, with the double hemisphere being very common and Mercator's prestigious navigational projection gradually making more appearances.
 Due to the paucity of information and the immense difficulty of surveying during the period, mapmakers frequently plagiarized material without giving credit to the original cartographer. For example, a famous map of North America known as the ""Beaver Map"" was published in 1715 by Herman Moll. This map is a close reproduction of a 1698 work by Nicolas de Fer. De Fer, in turn, had copied images that were first printed in books by Louis Hennepin, published in 1697, and François Du Creux, in 1664. By the late 18th century, mapmakers often credited the original publisher with something along the lines of, ""After [the original cartographer]"" in the map's title or cartouche.[35]
 In cartography, technology has continually changed in order to meet the demands of new generations of mapmakers and map users. The first maps were produced manually, with brushes and parchment; so they varied in quality and were limited in distribution. The advent of magnetic devices, such as the compass and much later, magnetic storage devices, allowed for the creation of far more accurate maps and the ability to store and manipulate them digitally.
 Advances in mechanical devices such as the printing press, quadrant, and vernier allowed the mass production of maps and the creation of accurate reproductions from more accurate data. Hartmann Schedel was one of the first cartographers to use the printing press to make maps more widely available. Optical technology, such as the telescope, sextant, and other devices that use telescopes, allowed accurate land surveys and allowed mapmakers and navigators to find their latitude by measuring angles to the North Star at night or the Sun at noon.
 Advances in photochemical technology, such as the lithographic and photochemical processes, make possible maps with fine details, which do not distort in shape and which resist moisture and wear. This also eliminated the need for engraving, which further speeded up map production.[37]
 In the 20th century, aerial photography, satellite imagery, and remote sensing provided efficient, precise methods for mapping physical features, such as coastlines, roads, buildings, watersheds, and topography. The United States Geological Survey has devised multiple new map projections, notably the Space Oblique Mercator for interpreting satellite ground tracks for mapping the surface. The use of satellites and space telescopes now allows researchers to map other planets and moons in outer space.[38] Advances in electronic technology ushered in another revolution in cartography: ready availability of computers and peripherals such as monitors, plotters, printers, scanners (remote and document) and analytic stereo plotters, along with computer programs for visualization, image processing, spatial analysis, and database management, have democratized and greatly expanded the making of maps. The ability to superimpose spatially located variables onto existing maps has created new uses for maps and new industries to explore and exploit these potentials. See also digital raster graphic.
 In the early years of the new millennium, three key technological advances transformed cartography:[39] the removal of Selective Availability in the Global Positioning System (GPS) in May 2000, which improved locational accuracy for consumer-grade GPS receivers to within a few metres; the invention of OpenStreetMap in 2004, a global digital counter-map that allowed anyone to contribute and use new spatial data without complex licensing agreements; and the launch of Google Earth in 2005 as a development of the virtual globe EarthViewer 3D (2004), which revolutionised accessibility of accurate world maps, as well as access to satellite and aerial imagery. These advances brought more accuracy to geographical and location-based data and widened the range of applications for cartography, for example in the development of satnav devices.
 Today most commercial-quality maps are made using software of three main types: CAD, GIS and specialized illustration software. Spatial information can be stored in a database, from which it can be extracted on demand. These tools lead to increasingly dynamic, interactive maps that can be manipulated digitally.
 On the other hand, we can observe a reverse trend. In contemporary times, there is a resurgence of interest in the most beautiful periods of cartography, with various maps being created using, for example, Renaissance-style aesthetics. We encounter imitators or continuators of Renaissance traditions that merge the realms of science and art. Among them are figures such as Luther Phillips (1891–1960) and Ruth Rhoads Lepper Gardner (1905–2011),[40] who still operated using traditional cartographic methods, as well as creators utilizing modern developments based on GIS solutions[41][42] and those employing techniques that combine advanced GIS/CAD methods with traditional artistic forms.[43]
 Field-rugged computers, GPS, and laser rangefinders make it possible to create maps directly from measurements made on site.
 There are technical and cultural aspects to producing maps. In this sense, maps can sometimes be said to be biased. The study of bias, influence, and agenda in making a map is what comprise a map's deconstruction. A central tenet of deconstructionism is that maps have power. Other assertions are that maps are inherently biased and that we search for metaphor and rhetoric in maps.[44]
 It is claimed that the Europeans promoted an ""epistemological"" understanding of the map as early as the 17th century.[44] An example of this understanding is that ""[European reproduction of terrain on maps] reality can be expressed in mathematical terms; that systematic observation and measurement offer the only route to cartographic truth…"".[44]
 A common belief is that science heads in a direction of progress, and thus leads to more accurate representations of maps. In this belief, European maps must be superior to others, which necessarily employed different map-making skills. ""There was a 'not cartography' land where lurked an army of inaccurate, heretical, subjective, valuative, and ideologically distorted images. Cartographers developed a 'sense of the other' in relation to nonconforming maps.""[44]
 Depictions of Africa are a common target of deconstructionism.[45] According to deconstructionist models, cartography was used for strategic purposes associated with imperialism and as instruments and representations of power[46] during the conquest of Africa. The depiction of Africa and the low latitudes in general on the Mercator projection has been interpreted as imperialistic and as symbolic of subjugation due to the diminished proportions of those regions compared to higher latitudes where the European powers were concentrated.[47]
 Maps furthered imperialism and colonization of Africa in practical ways by showing basic information like roads, terrain, natural resources, settlements, and communities. Through this, maps made European commerce in Africa possible by showing potential commercial routes and made natural resource extraction possible by depicting locations of resources. Such maps also enabled military conquests and made them more efficient, and imperial nations further used them to put their conquests on display. These same maps were then used to cement territorial claims, such as at the Berlin Conference of 1884–1885.[46]
 Before 1749, maps of the African continent had African kingdoms drawn with assumed or contrived boundaries, with unknown or unexplored areas having drawings of animals, imaginary physical geographic features, and descriptive texts. In 1748, Jean B. B. d'Anville created the first map of the African continent that had blank spaces to represent the unknown territory.[46]
 In understanding basic maps, the field of cartography can be divided into two general categories: general cartography and thematic cartography. General cartography involves those maps that are constructed for a general audience and thus contain a variety of features. General maps exhibit many reference and location systems and often are produced in a series. For example, the 1:24,000 scale topographic maps of the United States Geological Survey (USGS) are a standard as compared to the 1:50,000 scale Canadian maps. The government of the UK produces the classic 1:50,000 (replacing the older 1 inch to 1 mile) ""Ordnance Survey"" maps of the entire UK and with a range of correlated larger- and smaller-scale maps of great detail. Many private mapping companies have also produced thematic map series.
 Thematic cartography involves maps of specific geographic themes, oriented toward specific audiences. A couple of examples might be a dot map showing corn production in Indiana or a shaded area map of Ohio counties, divided into numerical choropleth classes. As the volume of geographic data has exploded over the last century, thematic cartography has become increasingly useful and necessary to interpret spatial, cultural and social data.
 A third type of map is known as an ""orienteering,"" or special purpose map. This type of map falls somewhere between thematic and general maps. They combine general map elements with thematic attributes in order to design a map with a specific audience in mind. Oftentimes, the type of audience an orienteering map is made for is in a particular industry or occupation. An example of this kind of map would be a municipal utility map.[48]
 A topographic map is primarily concerned with the topographic description of a place, including (especially in the 20th and 21st centuries) the use of contour lines showing elevation. Terrain or relief can be shown in a variety of ways (see Cartographic relief depiction). In the present era, one of the most widespread and advanced methods used to form topographic maps is to use computer software to generate digital elevation models which show shaded relief. Before such software existed, cartographers had to draw shaded relief by hand. One cartographer who is respected as a master of hand-drawn shaded relief is the Swiss professor Eduard Imhof whose efforts in hill shading were so influential that his method became used around the world despite it being so labor-intensive.[49][50]
 A topological map is a very general type of map, the kind one might sketch on a napkin. It often disregards scale and detail in the interest of clarity of communicating specific route or relational information. Beck's London Underground map is an iconic example. Although the most widely used map of ""The Tube,"" it preserves little of reality: it varies scale constantly and abruptly, it straightens curved tracks, and it contorts directions. The only topography on it is the River Thames, letting the reader know whether a station is north or south of the river. That and the topology of station order and interchanges between train lines are all that is left of the geographic space.[51] Yet those are all a typical passenger wishes to know, so the map fulfills its purpose.[52]
 Modern technology, including advances in printing, the advent of geographic information systems and graphics software, and the Internet, has vastly simplified the process of map creation and increased the palette of design options available to cartographers. This has led to a decreased focus on production skill, and an increased focus on quality design, the attempt to craft maps that are both aesthetically pleasing and practically useful for their intended purposes.
 A map has a purpose and an audience. Its purpose may be as broad as teaching the major physical and political features of the entire world, or as narrow as convincing a neighbor to move a fence. The audience may be as broad as the general public or as narrow as a single person. Mapmakers use design principles to guide them in constructing a map that is effective for its purpose and audience.
 The cartographic process spans many stages, starting from conceiving the need for a map and extending all the way through its consumption by an audience. Conception begins with a real or imagined environment. As the cartographer gathers information about the subject, they consider how that information is structured and how that structure should inform the map's design. Next, the cartographers experiment with generalization, symbolization, typography, and other map elements to find ways to portray the information so that the map reader can interpret the map as intended. Guided by these experiments, the cartographer settles on a design and creates the map, whether in physical or electronic form. Once finished, the map is delivered to its audience. The map reader interprets the symbols and patterns on the map to draw conclusions and perhaps to take action. By the spatial perspectives they provide, maps help shape how we view the world.[53]
 Designing a map involves bringing together a number of elements and making a large number of decisions. The elements of design fall into several broad topics, each of which has its own theory, its own research agenda, and its own best practices. That said, there are synergistic effects between these elements, meaning that the overall design process is not just working on each element one at a time, but an iterative feedback process of adjusting each to achieve the desired gestalt.
 Some maps contain deliberate errors or distortions, either as propaganda or as a ""watermark"" to help the copyright owner identify infringement if the error appears in competitors' maps. The latter often come in the form of nonexistent, misnamed, or misspelled ""trap streets"".[56] Other names and forms for this are paper towns, fictitious entries, and copyright easter eggs.[57]
 Another motive for deliberate errors is cartographic ""vandalism"": a mapmaker wishing to leave their mark on the work. Mount Richard, for example, was a fictitious peak on the Rocky Mountains' continental divide that appeared on a Boulder County, Colorado map in the early 1970s. It is believed to be the work of draftsman Richard Ciacci. The fiction was not discovered until two years later.
 Sandy Island in New Caledonia is an example of a fictitious location that stubbornly survives, reappearing on new maps copied from older maps while being deleted from other new editions.
 With the emergence of the internet and Web mapping, technologies allow for the creation and distribution of maps by people without proper cartographic training are readily available. This has led to maps that ignore cartographic conventions and are potentially misleading.[58]
 Professional and learned societies include:
 Journals related to cartography, as well as GIS, GISc, include:
 Mapmaking
 History
 Meanings
"
Ron Chernow,https://en.wikipedia.org/wiki/Ron_Chernow,"
 Ronald Chernow (/ˈtʃɜːrnaʊ/;[1][2] born March 3, 1949) is an American writer, journalist, and biographer. He has written bestselling historical non-fiction biographies.
 Chernow won the 2011 Pulitzer Prize for Biography and the 2011 American History Book Prize for his 2010 book Washington: A Life. He is also the recipient of the National Book Award for Nonfiction for his 1990 book The House of Morgan: An American Banking Dynasty and the Rise of Modern Finance.[3] His biographies of Alexander Hamilton (2004) and John D. Rockefeller (1998) were both nominated for National Book Critics Circle Awards. His biography of Hamilton inspired the popular Hamilton musical, which Chernow worked on as a historical consultant. For another book, The Warburgs: The Twentieth-Century Odyssey of a Remarkable Jewish Family, he was awarded the 1993 George S. Eccles Prize for Excellence in Economic Writing. As a freelance journalist, Chernow has written over sixty articles for various national publications.
 Chernow was born on March 3, 1949, in Brooklyn, New York. His father, Israel, was the owner of a discount store and creator of a stock brokerage firm; his mother, Ruth, was a bookkeeper. He is brother to Bart Chernow and uncle to Shandee Chernow.[4] He is of Jewish descent.[5] Chernow was voted ""Most Likely to Succeed"", and was class president and valedictorian when he graduated in 1966 from Forest Hills High School in Queens in New York City.[6] Chernow graduated summa cum laude from Yale University in 1970 and Pembroke College at Cambridge University with degrees in English literature. He began but did not finish a PhD program. He says that in politics he is a ""disgruntled Democrat"" and gives his religion as ""Jewish, though more in the breach than the observance.""[7]
 Chernow married Valerie Stearn in 1979; she died in January 2006. Valerie S. Chernow was an assistant professor of languages and social sciences at the New York City College of Technology.[8]
 Chernow began his career as a freelance journalist. He wrote more than 60 articles for various national newspapers and magazines from 1973 to 1982. In the mid-1980s, he put his writing pursuits aside when he began serving as the director of financial policy studies with the Twentieth Century Fund in New York City. In 1986, he left the organization and refocused his efforts on writing. In addition to writing nonfiction books and biographies, Chernow contributes articles to The New York Times[9] and The Wall Street Journal. He has also commented on business, politics, and finance on national radio and television shows, and appeared as an expert in documentary films.
 In 1990, Chernow published his first book, The House of Morgan: An American Banking Dynasty and the Rise of Modern Finance, which traces four generations of the J.P. Morgan financial empire.[10] The reviewer for The New York Times Book Review said, ""As a portrait of finance, politics and the world of avarice and ambition on Wall Street, the book has the movement and tension of an epic novel. It is, quite simply, a tour de force.""[11] The House of Morgan was honored with the National Book Award for Nonfiction.[3]
 In 1993, Chernow published The Warburgs: The Twentieth-Century Odyssey of a Remarkable Jewish Family, an account of the Warburg family, who immigrated to the U.S. from Germany in 1938. The Warburg family was a prominent financial dynasty of German Jewish descent, known for their accomplishments in physics, classical music, art history, pharmacology, physiology, finance, private equity, and philanthropy. The book was awarded the Columbia Business School's George S. Eccles Prize for Excellence in Economic Writing. It was named as one of the year's ten best works by the American Library Association[12] and a Notable Book by The New York Times.
 In 1997, Chernow published a collection of essays entitled The Death of the Banker: The Decline and Fall of the Great Financial Dynasties and the Triumph of the Small Investor.
 In 1998, Chernow published the 774-page Titan: The Life of John D. Rockefeller, Sr., about the industrialist, philanthropist, and founder of the Standard Oil Company. The book reflected Chernow's continued interest in financial history, especially when shaped by compelling and influential individuals. The book remained on The New York Times Best Seller list for 16 weeks, while Time called it ""one of the great American biographies"".[13] Both publications listed it among the year's ten best books. 
 In 2004, Chernow published Alexander Hamilton. The biography was nominated for a National Book Critics Circle Award[14] and was named as the winner of the inaugural George Washington Book Prize for early American history.[15] It remained on The New York Times Best Seller list for three months. In his review for the Journal of American History, Stephen B. Presser, who is professor of business law emeritus at Northwestern University,[16] wrote:
 The biography was adapted into a Tony award-winning musical, Hamilton, by Lin-Manuel Miranda, which opened on Broadway in August 2015. Chernow served as historical consultant to the production.[18]
 
Chernow's 904-page Washington: A Life was released on October 5, 2010. It won the Pulitzer Prize for Biography[19][20] and the American History Book Prize. Professor Gordon S. Wood, renowned scholar of the Founding era, wrote:[21]  In 2011, Chernow signed a deal to write a comprehensive biography on Ulysses S. Grant.[23] Chernow explained his transition from writing about George Washington to Grant: ""Makes some sense as progression. Towering general of Revolution to towering general of Civil War. Both two-term presidents, though with very different results.""[24] Grant was released on October 10, 2017, and the biography strongly argues against the conventional wisdom that Grant was an ""adequate president, a dull companion and a roaring drunk.""[25] The book received overwhelmingly positive reviews and was named by The New York Times as one of the 10 Best Books of 2017.[26]
 In 1990, Chernow became a member of the PEN American Center. In 2006, he was named as the President of the Board of Trustees, succeeding novelist Salman Rushdie.[27]
 Ron Chernow has received honorary degrees from Long Island University, Marymount Manhattan College, Hamilton College, Washington College, and Skidmore College.[7]
"
Belvoir (plantation),https://en.wikipedia.org/wiki/Belvoir_(plantation),"Belvoir was the plantation and estate of colonial Virginia's prominent William Fairfax family. Operated with the forced labor of enslaved people,[3][4] it was located on the west bank of the Potomac River on the present site of Fort Belvoir in Fairfax County, Virginia. 
 The main house, called Belvoir Manor or Belvoir Mansion, burned in 1783 and was destroyed during the War of 1812. The site has been listed on the National Register of Historic Places since 1973 [1] as ""Belvoir Mansion Ruins and the Fairfax Grave.""
 William Green's 1669 patent for 1,150 acres (4.7 km2) encompassed most of the peninsula between Dogue Creek and Accotink Creek, along the Potomac River.  Although this property was sub-divided and sold in the early 18th century, it was reassembled during the 1730s to create the central portion of Col. William Fairfax's 2,200-acre (8.9 km2) plantation of Belvoir Manor.
 Fairfax's elegant new home was completed in 1741. The mansion was described in a 1774 rental notice as spacious and well-appointed. Its furnishings consisted of ""tables, chairs, and every other necessary article...very elegant."" The family imported ceramics from Europe and the Orient to grace its tables.
 Planters such as William Fairfax comprised a small elite of Fairfax County's population; most of their neighbors were smaller farmers who sometimes barely managed to make a living[citation needed].  After William Fairfax's death in 1757, the plantation and his slaves[3] passed to his eldest son George William Fairfax (1729–87).
 Thomas Fairfax, 6th Lord Fairfax of Cameron, moved to Virginia between 1735 and 1737 to inspect and protect his lands. Lord Fairfax came to Belvoir, to help oversee his family estates in Virginia's Northern Neck Proprietary between the Rappahannock and Potomac rivers, inherited from his mother, Catharine, daughter of Thomas Culpeper, 2nd Baron Culpeper of Thoresway; and a great portion of the Shenandoah and South Branch Potomac valleys.  The northwestern boundary of his Northern Neck Proprietary was marked by the Fairfax Stone at the headwaters of the North Branch Potomac River.  
 Fairfax's sister Anne married Lawrence Washington in 1743. A young George Washington, Lawrence's half-brother, began to visit Belvoir frequently. Wishing to advance his brother's fortunes, Lawrence introduced George to George William. A friendship grew between the two men, despite the fact that George William was seven years older. A relationship blossomed between Sally Fairfax and Washington.  Washington and Bryan Fairfax, George William's younger half-brother, also became friends.
 In 1752, Lord Fairfax moved to Greenway Court in the valley of Virginia closer to his undeveloped land. He hired George Washington to survey land at Belvoir.
 When Fairfax left Belvoir for England in 1773, he had the estate rented to Rev. Andrew Morton for seven years. Its furnishings were sold at auction in 1774.[5] It was confiscated during the American Revolutionary War by the Virginia Act of 1779.  In 1783, the mansion and several of its outbuildings were destroyed by fire, and the plantation complex gradually deteriorated into ruins. Ferdinando Fairfax, who inherited the property, apparently did not live there. The bluffs below the former mansion site were quarried for building stone, but the house site was not developed.
 Belvoir Plantation suffered further damage during the War of 1812. In August 1814, as British land forces attacked and burned the city of Washington, a British naval squadron sailed up the Potomac River and forced the surrender of Alexandria. The fleet began the 180-mile (290 km) return trip down river. On September 1, the British attempted to run the deep-water channel below the Belvoir house site, a position that previously had been identified as a strategic defensive location on the river. A hastily assembled American force, composed of Virginia and Alexandria militia under the command of U.S. Navy Captain David Porter, hurriedly began to mount a battery on the bluff above the river. For four days, British and American forces exchanged cannon and musket fire. The British fleet eventually passed the American positions, and British shells demolished what little was left of the old Belvoir Manor.
 The association of Belvoir Plantation with the Fairfax family ended with the death of Ferdinando Fairfax in 1820. During the next decade, William Herbert of Alexandria acquired the property, which he quickly used as collateral for a loan. During the 1830s, Thomas Irwin, Herbert's creditor, operated the shad fisheries at White House Point. Herbert's continued inability to pay his debts eventually led to the sale of Belvoir at public auction in 1838.
 All of the great 18th-century plantations in the Belvoir area changed considerably in the years before the Civil War. Soil exhaustion and inheritance prompted the sale and sub-division of these formerly expansive tracts of land. As a new generation of landowners took up residence in southeastern Fairfax County, patterns of land use and ownership were altered.
 In 1917, the Belvoir property was consolidated and ceded to the U.S. Army by Virginia, eventually lending its name to the modern military installation of Fort Belvoir.
 The Belvoir ruins are on the National Register of Historic Places (1973), but access is restricted since they are on the military post.[6]
"
William Fairfax,https://en.wikipedia.org/wiki/William_Fairfax,"
 William Fairfax (1691–1757) was a political appointee of the British Crown in several colonies as well as a planter and politician in the Colony of Virginia. Fairfax served as Collector of Customs in Barbados, Chief Justice and governor of the Bahamas; and Customs agent in Marblehead, Massachusetts, before being reassigned to the Colony of Virginia.[1]
 In the Virginia Colony, Fairfax acted as a land agent for his cousin's vast holdings in the colony's northeast corner, known as the Northern Neck Proprietary. Also a tobacco planter himself, Fairfax was elected to the House of Burgesses representing King William County within the proprietary, which he helped split so that Fairfax County was created. Appointed to the Governor's Council, he rose to become its president (effectively the colony's lieutenant governor). Fairfax also commissioned the construction of his plantation called Belvoir in what became Fairfax County to honor his family.
 Fairfax was born in London and baptized in Yorkshire in 1691, the second son of Henry Fairfax (d. 1708) and grandson of Henry Fairfax, 4th Lord Fairfax of Cameron.[2] His elder brother was in line to inherit the title. The family also included a daughter, Mary Fairfax, who married this brother's friend William Philip Warder in 1730.
 As a young man, he served in the Royal Navy under his kinsman, Capt. Fairfax, as well as in the army in Spain.[3] Sailing to the English colonies in the Caribbean, Fairfax served as the Customs agent in Barbados and as Chief Justice of the Bahamas under Woodes Rogers. He served as governor of the Bahamas after Rogers' departure. However, the despite marrying, climate did not agree with him, so in 1725 he secured an appointment as customs collector at Marblehead and Salem, Massachusetts.[4]
 Meanwhile, his titled cousin, Thomas Fairfax, 6th Lord Fairfax of Cameron inherited an extensive grant of land on the Northern Neck of Virginia. Residing in England in Leeds Castle, Lord Fairfax used a succession of land agents to manage his vast Virginia property. As discussed below, upon reading the 1732 obituary of his last resident agent, Robert ""King"" Carter, and learning of the vast personal wealth Carter had amassed, Lord Fairfax decided to place a trusted member of the family in charge of his 5-million-acre (20,000 km2) Northern Neck proprietary. He arranged for William Fairfax to be transferred from Massachusetts to Virginia, to be assigned as that colony's customs collector for the Potomac River and to act as his land agent.
 In the Bahamas, Fairfax married Sarah Walker (c. 1700 – January 21, 1731), the daughter of a former justice of the Vice admiralty court and acting deputy governor of the Bahamas. They had a son, George William Fairfax, followed by a daughter Anne (discussed below) and another daughter Sarah before Mrs. Sarah Fairfax died on January 21, 1731, in Marblehead, Massachusetts.
 The widower Fairfax then married Deborah Clarke, of Marblehead. Together they had three sons: Thomas, William Henry (""Billy"") and Bryan, and a daughter Hannah.
In June 1743, the eldest Fairfax daughter, Anne (then aged 15) was hastily married to Lawrence Washington.[5] At age 25 in 1742, Washington had recently returned to Virginia from two years at war in the Caribbean. Washington was appointed Adjutant (commander) of the Virginia militia, at the colonial rank of major. In the spring of 1743, the young Anne disclosed to her parents that she had been sexually molested by Charles Green, the Anglican priest of Truro Parish.[5] Surviving court documents suggest Lawrence Washington may have been staying with the Fairfax family at Belvoir before the marriage, awaiting the completion of his new home at nearby Little Hunting Creek. Washington named his home Mount Vernon, to honor Admiral Edward Vernon, under whom he has served for two years as ""Captain of the Soldiers acting as Marines"" of the American Regiment, aboard the admiral's flagship HMS Princess Caroline (80 guns).[5]  In 1745 Washington took Green to court over his actions with Anne Fairfax; he and the senior Fairfax tried to have the priest deposed for the scandal, but were unsuccessful. Green rallied support in the county, and the trial was aborted.[5] Lawrence and Anne Washington had four children together. Only one, a daughter Sarah, survived to inherit that estate upon Lawrence's death in 1752. The widow remarried, to George Lee.[6] Her sister Sarah married Alexandria merchant John Carlyle. Meanwhile, William Fairfax's eldest son George William Fairfax married Sally Cary; they had no children, but would inherit the main estate, Belvoir, discussed below.
 William Fairfax's first two sons by his second wife both died in combat while serving the Crown: Thomas (1726–1746) was killed in action on 25 June 1746 (Old Style) against the French Navy off the coast of India, aged about 20, while serving as a newly enrolled midshipman in the Royal Navy aboard HMS Harwich (50 guns). Lieutenant William Henry ""Billy"" Fairfax died of wounds received during the British Army's capture of Quebec in fall 1759 during the Seven Years' War. The youngest son, Bryan Fairfax became an Anglican priest and would return to England to claim his inheritance, the title of Lord Fairfax of Cameron from his cousin Robert Fairfax, 7th Lord Fairfax of Cameron.
 Fairfax initially lived in Westmoreland County. The governor's Council appointed him to the Westmoreland County Court in 1734, to the King George County Court in 1737, and to the Prince William County Court in 1741.
 Fairfax lacked authority to issue land grants for the proprietary because Thomas Lord Fairfax had learned of King Carter's using that agency power to enrich himself and his family. For example, in 1727 King Carter himself patented 11,000 acres in 1727, another 19,000 in 1729, and his sons George took grants for 6,243 acres in and Landon Carter took grants for 14,419 acres in June 1731.[7] Moreover, Thomas Lord Fairfax visited the Virginia colony in 1737 to secure a survey to determine the proprietary's boundaries, then returned to England to argue his case before the Privy Council, despite conflicting land grants and over the opposition of several of the colony's leaders. In 1739, Lord Fairfax granted William Fairfax the power to make land grants, after reserving for his own use 12,588 acres on the Potomac River near the Great Falls (which contained mineral deposits and after William Fairfax had confirmed that claim by obtaining a survey). William Fairfax then started to build his legacy, albeit not as boldly as had King Carter. He granted an adjacent 5,560 acres to John Colville, who in turn granted that land to William Fairfax. Catesby Cocke, who had begun as clerk of Stafford County in 1728 and who became the Prince William County clerk in 1738 (and would become the first Fairfax County Clerk in 1742) in May 1739 acquired 13,089 acres in 18 separate grants from William Fairfax.[8]
From 1738 to 1741, William Fairfax and his second wife Deborah Clark lived along the lower Potomac River. He picked out a site for a home overlooking the river adjacent to the Washington family's estate, which was later known as Mount Vernon. Fairfax commissioned a two-story brick home, which was completed in 1741 and named Belvoir Manor. He and his descendants lived there for the next 32 years. He commissioned master carpenter Richard Blackburn to construct parts of Belvoir and other projects, and his friend may have also served in the long session of the House of Burgesses discussed below.
 Historic documents and archeological artifacts found at Belvoir Manor attest to the elegant lifestyle enjoyed by the Fairfax family. The mansion, described in a 1774 rental notice, was spacious and well-appointed. Its furnishings consisted of ""tables, chairs, and every other necessary article ... very elegant.""[9]  The Fairfaxes had imported ceramics from Europe and China to grace their tables.
 Prominent citizens of the colony, including Washington, visited frequently. Thomas Fairfax, 6th Lord Fairfax of Cameron, the first member of the British nobility to reside in the colonies, lived at Belvoir briefly, in 1747. He then moved to the Shenandoah Valley and established an estate at Greenway Court. Despite the grandeur of their surroundings and the refinement of their furnishings, planters such as the Fairfaxes, Masons, McCartys, and Washingtons did not lead indolent lives. Conscious of their civic duty and of the elite class, they were the political, social, economic, and religious leaders of their immediate neighborhood and of the colony at large.
 In 1741, William Fairfax was elected a member of the House of Burgesses representing then-vast Prince William County, and traveled to Williamsburg early in the new year.[10] He introduced the bill that created Fairfax County as a separate political jurisdiction in 1742 (carved out of the northern portion of Prince William County, and probably named for Thomas, Lord Fairfax). John Colville would succeed William Fairfax as one of the Prince William representatives for the remainder of the long assembly session (that lasted through 1747) and Lawrence Washington would become one of Fairfax County's new burgesses. William Fairfax subsequently served as presiding Justice of the County Court, and as County Lieutenant, the county's chief law-enforcement officer.
 At the same time, he managed his own large properties throughout Fairfax County and served as the land agent for his cousin, Lord Fairfax. Thus, William Fairfax managed the Northern Neck estate until his death in 1757.[11] In 1747, Thomas, Lord Fairfax was one of the area's largest slaveholders, with 30 slaves, or about the same number as Daniel French Jr., Catesby Cocke, Gedney Clark (a clerk in Barbadoes who was this man's brother-in-law), or Henry Fitzhug Jr. The largest slaveowner by far was Thomas Lee Jr. (with 122 slaves), and the second largest was William Fitzhugh Jr., with John Colvil and Lawrence Washington also owning about twice as many slaves as Lord Fairfax, and this man not even making the list (of men owning 20 slaves or more).[12]
 Fairfax became George Washington's patron and surrogate father, and Washington spent a month in 1748 with a team surveying Fairfax's Shenandoah Valley property.[13]
 Fairfax resigned his position as Burgess when he was appointed to the Governor's Council, the upper chamber of the Virginia General Assembly.[14] He rose to become President of the Governor's Council in Williamsburg, a position equivalent to today's Lieutenant Governor.  In this position, he represented the colony at an important conference with the Iroquois Confederacy in Albany, New York, in 1753. New York and Virginia officials worked to gain agreement with the Iroquois to allow passage and settlement of colonists in the Shenandoah Valley, which had been an area of their warring with southern Indians.
 As the senior colonial official in Fairfax County, William Fairfax was nominally in command of the county's militia. As such, he was entitled to be called a ""Virginia colonel."" This county rank was largely honorary and carried no pay or benefits, and did not extend to a higher echelon. Formally, the entire Virginia colonial militia fell under command of the resident governor, as colonel. Day-to-day command of the militia was exercised by the Adjutant (at the rank of major). But, at the county-level, all the local militia officers adopted a separate ""colonel-major-captain-lieutenant"" rank structure for use at the local level.
 In his will of 1757, Fairfax left Belvoir and his plantation of Springfield, containing 1,400 acres (5.7 km2), to his eldest son George William Fairfax.  He left his plantation Towlston Grange, with 5,500 acres (22 km2), to his youngest son Bryan Fairfax; he left land in Culpeper County of 3,250 acres (13.2 km2) and 1,100 acres (4.5 km2) to his daughter Hannah.[15] George William Fairfax and his wife Sally Cary, both members of became known as the First Families of Virginia, lived at Belvoir. They had no children.
 In 1773, they sailed to England on business and never returned after the American Revolutionary War disrupted society. George William Fairfax wrote his good friend and neighbor George Washington to look after the estate and put it up for rent. However, in 1783, the unoccupied mansion was destroyed by fire. Rev. Bryan Fairfax also eventually sailed to England to secure his inheritance.
 British cannonballs fired from ships in the Potomac River completed Belvoir's demolition during the War of 1812. However, the ruins were placed on the National Register for Historic Places in 1972, and were subsequently excavated by high school students and professional archeologists engaged by the United States Army Corps of Engineers, whose national headquarters is now on the estate once administered from Belvoir.[16]
"
Shenandoah Valley,https://en.wikipedia.org/wiki/Shenandoah_Valley,"

 The Shenandoah Valley (/ˌʃɛnənˈdoʊə/) is a geographic valley and cultural region of western Virginia and the eastern panhandle of West Virginia in the United States. The Valley is bounded to the east by the Blue Ridge Mountains, to the west by the eastern front of the Ridge-and-Valley Appalachians (excluding Massanutten Mountain), to the north by the Potomac River, to the south by the James River, and to the Southwest by the New River Valley. The cultural region covers a larger area that includes all of the Valley plus the Virginia Highlands to the west and the Roanoke Valley to the south. It is physiographically located within the Ridge and Valley Province and is a portion of the Great Appalachian Valley.
 Named for the river that stretches much of its length, the Shenandoah Valley encompasses eight counties in Virginia and two counties in West Virginia:
 The antebellum composition included four additional counties that are now in West Virginia as well as four additional Virginia counties:[1]
 The cultural region includes five more counties in Virginia:[citation needed]
 Between the Roanoke Valley in the south and Harpers Ferry in the north, where the Shenandoah River joins the Potomac, the Valley cultural region contains 10 independent cities:
 The central section of the Shenandoah Valley is split in half by the Massanutten Mountain range, with the smaller associated Page Valley lying to its east and Fort Valley within the mountain range.
 The Shenandoah Valley contains a number of geologically and historically significant limestone caves:
 The word Shenandoah is of unknown Native American origin. It has been described as being derived from the Anglicization of Native American terms, resulting in words such as Gerando, Gerundo, Genantua, Shendo and Sherando. The meaning of these words is of some question. Schin-han-dowi, the ""River Through the Spruces""; On-an-da-goa, the ""River of High Mountains"" or ""Silver-Water""; and an Iroquois word for ""Big Meadow"", have all been proposed by Native American etymologists. The most popular, romanticized belief is that the name comes from a Native American expression for ""Beautiful Daughter of the Stars"".[2]
 Another legend relates that the name is derived from the name of the Iroquoian chief Sherando (Sherando was also the name of his people), who fought against the Algonquian Chief Opechancanough, ruler of the Powhatan Confederacy (1618–1644). Opechancanough liked the interior country so much that he sent his son Sheewa-a-nee from the Tidewater with a large party to colonize the valley. Sheewa-a-nee drove Sherando back to his former territory near the Great Lakes. According to this account, descendants of Sheewanee's party became the Shawnee. According to tradition, another branch of Iroquoians, the Senedo, lived in present-day Shenandoah County. They were exterminated by ""Southern Indians"" (Catawba or Cherokee) before the arrival of white settlers.[3][4]
 Another story dates to the American Revolutionary War. Throughout the war, Chief Skenandoa of the Oneida, an Iroquois nation based in New York, persuaded many of the tribe to side with the colonials against the British. Four Iroquois nations became British allies and caused many fatalities and damage in the frontier settlements west of Albany. Skenandoa led 250 warriors against the British and Iroquois allies. According to Oneida oral tradition, during the harsh winter of 1777–1778 at Valley Forge, where the colonials suffered, Chief Skenando provided aid to the soldiers. The Oneida delivered bushels of dry corn to the troops to help them survive. Polly Cooper, an Oneida woman, stayed some time with the troops to teach them how to cook the corn properly and care for the sick. General Washington gave her a shawl in thanks, which is displayed at Shako:wi, the museum of the Oneida Nation near Syracuse, New York. Many Oneida believe that after the war, George Washington named the Shenandoah River and valley after his ally.[5][6]
 Despite the valley's potential for productive farmland, colonial settlement from the east was long delayed by the barrier of the Blue Ridge Mountains. These were crossed by explorers John Lederer at Manassas Gap in 1671, Batts and Fallam the same year, and Cadwallader Jones in 1682. The Swiss Franz Ludwig Michel and Christoph von Graffenried explored and mapped the Valley in 1706 and 1712, respectively. Von Graffenried reported that the Indians of Senantona (Shenandoah) had been alarmed by news of the recent Tuscarora War in North Carolina.
 Governor Alexander Spotswood's legendary Knights of the Golden Horseshoe Expedition of 1716 crossed the Blue Ridge at Swift Run Gap and reached the river at Elkton, Virginia. Settlers did not immediately follow, but someone who heard the reports and later became the first permanent settler in the Valley was Adam Miller (Mueller), who in 1727 staked out claims on the south fork of the Shenandoah River, near the line that now divides Rockingham County from Page County.[7]
 The Great Wagon Road (later called the Valley Pike or Valley Turnpike) began as the Great Warriors Trail or Indian Road, a Native road through common hunting grounds shared by several tribes settled around the periphery, which included Iroquoian, Siouan and Algonquian-language family tribes. Known native settlements within the Valley were few but included the Shawnee occupying the region around Winchester, and Tuscarora around what is now Martinsburg, West Virginia. In the late 1720s and 1730s, Quakers and Mennonites began to move in from Pennsylvania. They were tolerated by the natives, while ""Long Knives"" (English settlers from coastal Virginia colony) were less welcomed. During these same decades, the valley route continued to be used by war parties of Seneca (Iroquois) and Lenape en route from New York, Pennsylvania and New Jersey to attack the distant Catawba in the Carolinas, with whom they were at war. The Catawba in turn pursued the war parties northward, often overtaking them by the time they reached the Potomac. Several fierce battles were fought among the warring nations in the Valley region, as attested by the earliest European-American settlers.[8]
 Later colonists called this route the Great Wagon Road; it became the major thoroughfare for immigrants' moving by wagons from Pennsylvania and northern Virginia into the backcountry of the South. The Valley Turnpike Company improved the road by paving it with macadam prior to the Civil War and set up toll gates to collect fees to pay for the improvements. After the advent of motor vehicles, the road was refined and paved appropriately for their use. In the 20th century, the road was acquired by the Commonwealth of Virginia, which incorporated it into the state highway system as U.S. Route 11. For much of its length, the newer Interstate 81, constructed in the 1960s, parallels the old Valley Pike.
 Along with the first German settlers, known as ""Shenandoah Deitsch"", many Scotch-Irish immigrants came south in the 1730s from Pennsylvania into the valley, via the Potomac River. The Scotch-Irish comprised the largest group of non-English immigrants from the British Isles before the Revolutionary War, and most migrated into the backcountry of the South.[9] This was in contrast to the chiefly English immigrants who had settled the Virginia Tidewater and Carolina Piedmont regions.
 Along with the Ulster Scots many Irishmen arrived in the Shenandoah valley, usually after their indentured service was up. These Irishmen usually had converted to Protestantism or kept their faith secret. In the 18th-century Thirteen Colonies and the independent United States, while interethnic marriage among Catholics remained a dominant pattern, Catholic-Protestant intermarriage became more common (notably in the Shenandoah Valley where intermarriage among Ulster Protestants and the significant minority of Irish Catholics in particular was not uncommon or stigmatized),[10] and while fewer Catholic parents required that their children be disinherited in their wills if they renounced Catholicism, it remained more common among Catholic parents to do so if their children renounced their parents' faith than it was in the rest of the U.S. population.[11]
 Governor Spotswood had arranged the Treaty of Albany with the Iroquois (Six Nations) in 1721, whereby they had agreed not to come east of the Blue Ridge in their raiding parties on tribes farther to the South. In 1736, the Iroquois began to object, claiming that they still legally owned the land to the west of the Blue Ridge; this led to a skirmish with Valley settlers in 1743. The Iroquois were on the verge of declaring war on the Virginia Colony as a result, when Governor Gooch paid them the sum of 100 pounds sterling for any settled land in the Valley that was claimed by them. The following year at the Treaty of Lancaster, the Iroquois sold all their remaining claim to the Valley for 200 pounds in gold.[12]
 The few Shawnees who still resided in the Valley abruptly headed westward in 1754, having been approached the year before by emissaries from their kindred beyond the Alleghenies.[13]
 The Shenandoah Valley was known as the breadbasket of the Confederacy during the Civil War and was seen as a backdoor for Confederate raids on Maryland, Washington, and Pennsylvania. Because of its strategic importance it was the scene of three major campaigns. The first was the Valley Campaign of 1862, in which Confederate General Stonewall Jackson defended the Valley against three numerically superior Union armies. The final two were the Valley Campaigns of 1864. First, in the summer of 1864, Confederate General Jubal Early cleared the Valley of its Union occupiers and then proceeded to raid Maryland, Pennsylvania, and D.C. Then during the autumn, Union General Philip Sheridan was sent to drive Early from the Valley and once-and-for-all deny its use to the Confederates through the use of scorched-earth tactics. The Valley, especially in the lower northern section, was also the scene of bitter partisan fighting as the region's inhabitants were deeply divided over loyalties, and Confederate partisan John Mosby and his Rangers frequently operated in the area.
 A series of newspaper mergers, ending in 1914, established the Daily News-Record of Harrisonburg as the daily newspaper of the Shenandoah Valley.
 In the late 20th century, the Valley's vineyards began to reach maturity. They constituted the new industry of the Shenandoah Valley American Viticultural Area.
 In 2018, a series of strikes and protests were held in Dayton's Cargill plant.[14][15]
 Transportation in the Shenandoah Valley consists mainly of road and rail and contains several metropolitan area transit authorities. The main north-south road transportation is Interstate 81, which parallels the old Valley Turnpike (U.S. Route 11) and the ancient Great Path of the Native Americans through its course in the valley. In the lower (northern) valley, on the eastern side, U.S. Route 340 also runs north-south, starting from Waynesboro in the south, running through the Page Valley to Front Royal, and on to Harpers Ferry, West Virginia, where it exits the valley into Maryland. Major east-west roads cross the valley as well, providing access to the Piedmont and the Allegheny Mountains. Starting from the north, these routes include U.S. Route 50, U.S. Route 522, Interstate 66, U.S. Route 33, U.S. Route 250, Interstate 64, and U.S. Route 60.
 CSX Transportation operates several rail lines through the valley, including the old Baltimore and Ohio Railroad, Virginia Central Railroad. Norfolk Southern operates the old Manassas Gap Railroad and the Norfolk and Western and Chesapeake Western. There are also more modern lines that run the length of the valley parallel to the Valley Pike and U.S. 340. The rail lines are primarily used for freight transportation, though Maryland Area Rail Commuter (MARC) trains utilize the old B&O line from stations in Martinsburg, Duffields, and Harper's Ferry to Washington Union Station in Washington, D.C., and vice versa. Amtrak utilizes Harpers Ferry and Martinsburg as well along the Capitol Limited route. Amtrak also runs the Cardinal through the valley along the old Virginia Central. 
 Several localities in the valley operate public transportation systems, including Front Royal Area Transit (FRAT), which provides weekday transit for the town of Front Royal; Page County Transit, providing weekday transit for the town of Luray and weekday service between Luray and Front Royal; and Winchester Transit, which provides weekday transit for the city of Winchester. In addition, Shenandoah Valley Commuter Bus Service Archived May 11, 2008, at the Wayback Machine offers weekday commuter bus service from the northern Shenandoah Valley, including Shenandoah County and Warren County, to Northern Virginia (Arlington County and Fairfax County) and Washington. Origination points in Shenandoah County include Woodstock. Origination points in Warren County include Front Royal and Linden.
 The Shenandoah Valley serves as the setting for the 1965 film Shenandoah and its 1974 musical adaptation. Both stories follow the Anderson family during the Civil War. An associated song by James Stewart titled ""The Legend of Shenandoah"" was a very minor hit in 1965, reaching #133 on the Billboard Bubbling Under the Hot 100 chart. One of the most famous cultural references to the area does not mention the valley itself: West Virginia's state song, ""Take Me Home, Country Roads"" by John Denver, contains the words ""Blue Ridge Mountains, Shenandoah River"" in the first verse.
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}38°30′N 78°51′W﻿ / ﻿38.500°N 78.850°W﻿ / 38.500; -78.850
"
College of William & Mary,https://en.wikipedia.org/wiki/College_of_William_%26_Mary,"
 
 The College of William & Mary[b] (abbreviated as W&M[8]) is a public research university in Williamsburg, Virginia, United States. Founded in 1693 under a royal charter issued by King William III and Queen Mary II, it is the second-oldest institution of higher education in the United States and the ninth-oldest in the English-speaking world.[9] It is classified among ""R2: Doctoral Universities – High Research Activity"".[10] In his 1985 book Public Ivies: A Guide to America's Best Public Undergraduate Colleges and Universities, Richard Moll included William & Mary as one of the original eight ""Public Ivies"". The university is among the original nine colonial colleges.
 The college educated American Presidents Thomas Jefferson, James Monroe, and John Tyler. It also educated other key figures pivotal to the development of the United States, including the first President of the Continental Congress Peyton Randolph, the first U.S. Attorney General Edmund Randolph, the fourth U.S. Supreme Court Chief Justice John Marshall, Speaker of the House of Representatives Henry Clay, Commanding General of the U.S. Army Winfield Scott, sixteen members of the Continental Congress, and four signers of the Declaration of Independence. Its connections with many Founding Fathers of the United States earned it the nickname ""the Alma Mater of the Nation"".[11] George Washington received his surveyor's license from the college in 1749, and later became the college's first American chancellor in 1788. That position was long held by the bishops of London and archbishops of Canterbury, though in modern times has been held by U.S. Supreme Court justices, Cabinet secretaries, and British Prime Minister Margaret Thatcher. Benjamin Franklin received William & Mary's first honorary degree in 1756.[12]
 William & Mary is notable for its many firsts in American higher education. In 1736, W&M became the first school of higher education in the future United States to install a student honor code of conduct.[13] The F.H.C. Society, founded in 1750, was the first collegiate fraternity in the United States, and W&M students founded the Phi Beta Kappa academic honor society in 1776, the first Greek-letter fraternity. It is the only American university issued a coat of arms by the College of Arms in London.[14] The establishment of graduate programs in law and medicine in 1779 makes it one of the first universities in the United States. The William & Mary Law School is the oldest law school in the United States, and the Wren Building, attributed to and named for the famed English architect Sir Christopher Wren, is the oldest academic building still standing in the United States.[15]
 A school of higher education for both Native American young men and the sons of the colonists was one of the earliest goals of the leaders of the Colony of Virginia. The college was founded on February 8, 1693, under a royal charter to ""make, found and establish a certain Place of Universal Study, a perpetual College of Divinity, Philosophy, Languages, and other good arts and sciences ... to be supported and maintained, in all time coming.""[16] Named in honor of the reigning monarchs King William III and Queen Mary II, the college is the second-oldest in the United States. The original plans for the college date back to 1618 at Henrico but were thwarted by the Indian massacre of 1622, a change in government (in 1624, the Virginia Company's charter was revoked by King James I and the Virginia Colony was transferred to royal authority as a crown colony), events related to the English Civil War, and Bacon's Rebellion. In 1695, before the town of Williamsburg existed, construction began on the College Building, now known as the Sir Christopher Wren Building, in what was then called Middle Plantation. It is the oldest college building in America. The college is one of the country's nine Colonial Colleges founded before the American Revolution. The charter named James Blair as the college's first president (a lifetime appointment which he held until he died in 1743). William & Mary was founded as an Anglican institution; students were required to be members of the Church of England, and professors were required to declare adherence to the Thirty-Nine Articles.[17]
 In 1693, the college was given a seat in the House of Burgesses, and it was determined tobacco taxes and export duties on furs and animal skins would support the college. The college acquired a 330 acres (1.3 km2) parcel for the new school,[18] 8 miles (13 km) from Jamestown. In 1694, the new school opened in temporary buildings.
 Williamsburg was granted a royal charter as a city in 1722 by the Crown and served as the capital of Colonial Virginia from 1699 to 1780. During this time, the college served as a law center, and lawmakers frequently used its buildings. It educated future U.S. Presidents Thomas Jefferson, James Monroe, and John Tyler. The college has been called ""the Alma Mater of a Nation"" because of its close ties to America's founding fathers and figures pivotal to the development and expansion of the United States. George Washington, who received his surveyor's license through the college despite never attending, was the college's first American chancellor. William & Mary is famous for its firsts: the first U.S. institution with a royal charter, the first Greek-letter society (Phi Beta Kappa, founded in 1776), the first collegiate society in the country (F.H.C. Society, founded in 1750), the first student honor code and the first collegiate law school in America.[c][19]
 During the American Revolution, Virginia established a freedom of religion, notably with the 1786 passage of the Virginia Statute for Religious Freedom. Future U.S. President James Madison was a key figure in the transition to religious freedom in Virginia, and Right Reverend James Madison, his cousin and Thomas Jefferson, who was on the Board of Visitors, helped the College of William & Mary to make the transition as well. In 1779, the college established graduate schools in law and medicine, making it one of the institutions that claimed to be the first university in the United States. As its president, Reverend Madison worked with the new leaders of Virginia, most notably Jefferson, on a reorganization and changes for the college which included the abolition of the Divinity School and the Indian School and the establishment of the first elective system of study and honor system.[20]
 The College of William & Mary is home to the nation's first collegiate secret society, the F.H.C. Society, popularly known as the Flat Hat Club, founded on November 11, 1750. On December 5, 1776, students John Heath and William Short (class of 1779) founded Phi Beta Kappa as a secret literary and philosophical society. Other secret societies known to exist at the college currently include: The 7 Society, 13 Club, Alpha Club, Bishop James Madison Society, The Society, The Spades, W Society, and Wren Society.[21][22]
 Thomas R. Dew, professor of history, metaphysics, and political economy, and then president of William & Mary from 1836 until he died in 1846, was an influential academic defender of slavery.[23]: 21–47 
 In 1842, alumni of the college formed the Society of the Alumni[24] which is now the sixth oldest alum organization in the United States. In 1859, a great fire destroyed the College Building. The Alumni House is one of the few original antebellum structures remaining on campus; notable others include the Wren Building, the President's House, the Brafferton, and Prince George House.
 At the outset of the American Civil War (1861–1865), enlistments in the Confederate States Army depleted the student body. On May 10, 1861, the faculty voted to close the college for the duration of the conflict. General Charles A. Whittier reported that ""thirty-two out of thirty-five professors and instructors abandoned the college work and joined the army in the field"".[25] The College Building was used as a Confederate barracks and later as a hospital, first by Confederate, and later Union forces. The Battle of Williamsburg was fought nearby during the Peninsula Campaign on May 5, 1862, and the city was captured by the Union army the next day. The Brafferton building of the college was used for a time as quarters for the commanding officer of the Union garrison occupying the town. On September 9, 1862, drunken soldiers of the 5th Pennsylvania Cavalry set fire to the College Building,[26] purportedly in an attempt to prevent Confederate snipers from using it for cover.[non-primary source needed]
 Following the restoration of the Union, Virginia was destitute. The college's 16th president, Benjamin Stoddert Ewell, finally reopened the school in 1869 using his funds, but the college closed again in 1882 due to insufficient funding. In 1888, William & Mary resumed operations under an amended charter when the Commonwealth of Virginia passed an act[27] appropriating $10,000 to support the college as a teacher-training institution. Lyon Gardiner Tyler (son of US President and alumnus John Tyler) became the 17th president of the college following Ewell's retirement. Tyler and his successor J. A. C. Chandler expanded the college. In 1896, Minnie Braithwaite Jenkins was the first woman to attempt to take classes at William & Mary, although her petition was denied.[28] In March 1906, the General Assembly passed an act taking over the college grounds, and it has remained publicly supported ever since. In 1918, it was one of the first universities in Virginia to admit women.[29] Enrollment increased from 104 in 1889 to 1269 students by 1932.
 W. A. R. Goodwin, rector at Bruton Parish Church and professor of biblical literature and religious education at the college, pursued benefactors who could support the restoration of Williamsburg. Goodwin considered Williamsburg ""as the original training and testing ground"" of the United States. Goodwin persuaded John D. Rockefeller Jr. to initiate the restoration of Williamsburg in 1926, leading to the establishment of Colonial Williamsburg.[30] Goodwin had initially only pursued Rockefeller to help fund the construction of Phi Beta Kappa Memorial Hall, but had convinced Rockefeller to participate in a broader restoration effort when he visited William & Mary for the hall's dedication. While the college's administration was less supportive of the restoration efforts than many others in Williamsburg–before the Colonial Williamsburg project, the William & Mary campus was Williamsburg's primary tourist attraction–the college's cooperation was secured.[31] Restoration paid for by Rockefeller's program extended to the college, with the Wren Building restored in 1928–1931, President's House in 1931, and Brafferton in 1931–1932.[32][33]
 In 1930, William & Mary established a branch in Norfolk, Virginia called The Norfolk Division of the College of William & Mary; it eventually became the independent state-supported institution known as Old Dominion University.
 President Franklin D. Roosevelt received an honorary degree from the college on October 20, 1934.[34] In 1935, the Sunken Garden was constructed just west of the Wren Building. The sunken design is from a similar landscape feature at Royal Hospital Chelsea in London, designed by Sir Christopher Wren.
 In 1945, the editor-in-chief of The Flat Hat, Marilyn Kaemmerle, wrote an editorial, ""Lincoln's Job Half-Done..."" that supported the end of  racial segregation, anti-miscegenation laws and white supremacy; the university administration removed her from the newspaper and nearly expelled her.[35] According to Time magazine, in response, over one-thousand William & Mary students held ""a spirited mass meeting protesting infringement of the sacred principles of freedom of the press bequeathed by Alumnus Thomas Jefferson."" She was allowed to graduate, but future editors had to discuss ""controversial writings"" with faculty before printing. The college Board of Visitors apologized to her in the 1980s.[36][37]
 The college admitted Hulon Willis into a graduate program in 1951 because the program was unavailable at Virginia State. However, the college did not open all programs to African-American students until around 1970.[39]
 In 1960, The Colleges of William & Mary, a short-lived five-campus university system, was founded. It included the College of William & Mary, the Richmond Professional Institute, the Norfolk Division of the College of William & Mary, Christopher Newport College, and Richard Bland College.[40] It was dissolved in 1962, with only Richard Bland College remaining officially associated with the College of William & Mary at the present day.
 In 1974, Jay Winston Johns willed Highland, the 535-acre (2.17 km2) historic Albemarle County, Virginia estate of alumnus and U.S. President James Monroe, to the college. The college restored this historic presidential home near Charlottesville and opened it publicly.[41]
 Jefferson Hall, a student dormitory, was destroyed by fire on January 20, 1983, without casualties. The building, including the destroyed west wing, was rebuilt and reopened.[42]
 On July 25, 2012, Eastern Virginia Medical School (EVMS), in nearby Norfolk, Virginia, made a joint announcement with William & Mary that the two schools were considering merging, with the prospect that EVMS would become the William & Mary School of Medicine.[43][44] Any such merger would have to be confirmed by the two schools and then confirmed by the Virginia General Assembly and Governor. Both universities subsequently agreed upon a pilot relationship, supported by a $200,000 grant in the Virginia budget, to examine this possible union in reality.[45]
 Throughout the second half of the 20th century, William & Mary has retained its historic ties to the United Kingdom and that state's royal family. In 1954, Queen Elizabeth The Queen Mother visited William & Mary as part of her tour of the United States, becoming the first member of the royal family to visit the college. In 1957, Queen Elizabeth II and Prince Philip, Duke of Edinburgh, visited the college to commemorate the 350th anniversary of the landing at Jamestown. Queen Elizabeth gave a speech from the balcony of the Wren Building that drew over 20,000 people, the largest crowd ever seen in the city. In 1981, Charles, Prince of Wales, visited to commemorate the 200th anniversary of the Battle of Yorktown. In 1988, the United States Congress selected William & Mary to send a delegation to the United Kingdom for the 300th anniversary of the ascension of King William III and Queen Mary II. Prince Charles would return to the college in 1993 for the 300th anniversary of William & Mary. William & Mary sent a delegation to meet with Queen Elizabeth II that same year. Former Prime Minister Margaret Thatcher would be made the Chancellor of the College of William & Mary that same year. In 2007, Elizabeth II and Prince Philip would visit the college for a second time to recognize the 400th anniversary of the landing at Jamestown.[46] In 2022, a beacon was lit in front of the Wren Building to celebrate the Platinum Jubilee of Queen Elizabeth II.[47]
 The college is on a 1,200-acre (490 ha) campus in Williamsburg, Virginia. In 2011, Travel+Leisure named William & Mary one of the most beautiful college campuses in the United States.[48]
 The Sir Christopher Wren Building is the oldest college building in the United States and a National Historic Landmark.[49] The building, colloquially referred to as the ""Wren Building"", was named upon its renovation in 1931 to honor the English architect Sir Christopher Wren. The basis for the 1930s name is a 1724 history in which mathematics professor Hugh Jones stated the 1699 design was ""first modelled by Sir Christopher Wren"" and then was adapted ""by the Gentlemen there"" in Virginia; little is known about how it looked since it burned within a few years of its completion. Today's Wren Building is based on the design of its 1716 replacement. The college's alum association has suggested Wren's connection to the 1931 building is a viable subject of investigation.[50]
 Two other buildings around the Wren Building compose an area known as ""Ancient"" or ""Historic Campus"":[51] the Brafferton (built within 1723 and originally housing the Indian School, now the President and Provost's offices) and the President's House (built within 1732). In addition to the Ancient Campus, which dates to the 18th century, the college also consists of ""Old Campus"" and ""New Campus"".[52] ""Old Campus"", adjacent to Ancient Campus, surrounds the Sunken Garden.[53]
 Adjoining ""Old Campus"" to the north and west is ""New Campus"". It was constructed primarily between 1950 and 1980, and it consists of academic buildings and dormitories that, while of the same brick construction as ""Old Campus"", fit into the vernacular of modern architecture. Beginning with the college's tercentenary in 1993, the college has embarked on a building and renovation program that favors the traditional architectural style of ""Old Campus"", while incorporating energy-efficient technologies. Several buildings constructed since the 1990s have been LEED certified. Additionally, as the buildings of ""New Campus"" are renovated after decades of use, several have been remodeled to incorporate more traditional architectural elements to unify the appearance of the entire college campus. ""New Campus"" is dominated by William and Mary Hall, Earl Gregg Swem Library, and formerly Phi Beta Kappa Memorial Hall. It also includes the offices and classrooms of the Mathematics, Physics, Psychology, Biology, and Chemistry Departments, the majority of freshman dormitories, the fraternity complex, the majority of the college's athletic fields, and the Muscarelle Museum of Art. The newest addition to ""New Campus"" is Alan B. Miller Hall, the headquarters of the college's Mason School of Business.
 The recent wave of construction at William & Mary has resulted in a new building for the School of Education, not far from Kaplan Hall (formerly William and Mary Hall). The offices and classrooms of the Government, Economics, and Classical Language Departments share John E. Boswell Hall (formerly ""Morton Hall"") on ""New Campus"". These departments have been piecemeal separated and relocated to buildings recently renovated within the ""Old Campus"", such as Chancellors' Hall.[54][relevant? – discuss]
 The vast majority of William & Mary's 1,200 acres (4.9 km2) consists of woodlands and Lake Matoaka, an artificial lake created by colonists in the early 18th century.[55]
 Following the George Floyd protests and associated movements, as well as student and faculty pressure in 2020 and 2021, several buildings, halls, and other entities were renamed. Maury Hall (named for Confederate sailor Matthew Fontaine Maury) on the Virginia Institute of Marine Science campus and Trinkle Hall (named for Governor Elbert Lee Trinkle) of Campus Center were renamed in September 2020 to York River Hall and Unity Hall respectively.[56] In April 2021, three buildings were renamed at following a vote by the Board of Visitors: Morton Hall (named for professor Richard Lee Morton) to John E. Boswell Hall (for LGBT advocate and alum John Boswell), Taliaferro Hall (named for Confederate General William Taliaferro) to Hulon L. Willis Sr. Hall (Hulon Willis Sr. was the first Black student at the college), and Tyler Hall (named for President John Tyler and his son) to its original name of Chancellors' Hall (the hall had been renamed in 1988).[54]
 The Board of Visitors is a corporation established by the General Assembly of Virginia to govern and supervise the operation of the College of William & Mary and of Richard Bland College.[57] The corporation is composed of 17 members appointed by the Governor of Virginia, based upon the recommendations made by the Society of the Alumni, to a maximum of two successive four-year terms. The Board elects a Rector, Vice-Rector, and Secretary, and the Board meets four times annually.[57] The Board appoints a president, related administrative officers, and an honorary chancellor, approving degrees, admission policies, departments, and schools and executing the fiduciary duties of supervising the college's property and finances.[58]
 The Chancellor of the College of William & Mary is largely ceremonial. Until 1776, the position was held by an English subject, usually the Archbishop of Canterbury or the Bishop of London, who served as the college's advocate to the Crown, while a colonial President oversaw the day-to-day activities of the Williamsburg campus. Following the Revolutionary War, General George Washington was appointed as the first American chancellor; later, United States President John Tyler held the post. The college has recently had several distinguished chancellors: former Chief Justice of the United States Warren E. Burger (1986–1993), former British Prime Minister Margaret Thatcher (1993–2000), former U.S. Secretary of State Henry Kissinger (2000–2005), and former U.S. Supreme Court Justice Sandra Day O'Connor (2005–2012).[59] Former U.S. Secretary of Defense Robert M. Gates, himself an alumnus of the college, succeeded O'Connor in February 2012.[60]
 The Board of Visitors delegates to a president the operating responsibility and accountability for the college's administrative, fiscal, and academic performance, as well as representing the college on public occasions such as conferral of degrees.[57] W. Taylor Reveley III, 27th President of the college, served from 2008 to 2018.[61]  In February 2018, The Board of Visitors unanimously elected Katherine A. Rowe as Reveley's successor. Rowe is the first female president to serve the college since its founding.[62]
 Faculty members are organized into separate faculties of the Faculty of Arts and Science as well as those for the respective schools of Business, Education, Law, and Virginia Institute of Marine Science.[57] Each faculty is presided over by a dean, who reports to the provost, and governs itself through separate by-laws approved by the Board of Visitors. The faculty is also represented by a faculty assembly advising the president and provost.[57]
 The Royal Hospital School, an independent boarding school in the United Kingdom, is a sister institution.[63]
 The College of William & Mary is a medium-sized, highly residential, public research university.[64] The focal point of the university is its four-year, full-time undergraduate program which constitutes most of the institution's enrollment. The college has a strong undergraduate arts & sciences focus, with many graduate programs in diverse fields ranging from American colonial history to marine science. The university offers multiple academic programs through its center in the District of Columbia: an undergraduate joint degree program in engineering with Columbia University, as well as a liberal arts joint degree program with the University of St Andrews in Scotland.[65]
 The graduate programs are dominant in STEM fields and the university has a high level of research activity.[64] For the 2016–17 academic year, 1,591 undergraduate, 652 masters, and 293 doctoral degrees were conferred.[66] William & Mary is accredited by the Southern Association of Colleges and Schools.[67]
 William & Mary contains several schools,[68] academic departments, and interdisciplinary research institutes, including:
 William & Mary offers exchange programs with 15 foreign schools, drawing more than 12% of its undergraduates into these programs. It also receives U.S. State Department grants to expand its foreign exchange programs further.[71]
 William & Mary enrolled 6,285 undergraduate and 2,455 postgraduate students in Fall 2018.[66] Women made up 57.6% of the undergraduate and 50.7% of the graduate student bodies.[66]
 Admission to W&M is considered ""most selective"" according to U.S. News & World Report.[73] For the undergraduate class entering fall 2023, William & Mary received 17,548 applications and accepted 5,741, or 32.7%. Of accepted applications, 1,619 enrolled, a yield rate of 28.2%. Of all matriculating students, the average high school GPA is 4.4. The interquartile range for total SAT scores was 1365–1510, while the range for ACT scores was 32–34.[72] 
 Undergraduate tuition for 2016–2017 was $13,127 for Virginia residents and $36,158 for out-of-state students.[74] W&M granted over $20.9 million in need-based scholarships in 2014–2015 to 1,734 undergraduates (27.5% of the undergraduate student body); 37% of the student body received loans, and average student indebtedness was $26,017.[75] Research of William & Mary students published in 2016 and 2017 showed they hailed from overwhelmingly wealthy student family backgrounds, even as compared to other elite public institutions.[76][77] The college is need-blind for domestic applicants.[78]
 In the 2025 U.S. News & World Report rankings, W&M ranks as tied for the 23rd-best public university in the United States, tied for 54th-best national university in the U.S., and tied for 1049th-best university in the world.[89][90]  U.S. News & World Report also rated William & Mary's undergraduate teaching as 4th best (tied with Princeton University) among 73 national universities and 13th best for Undergraduate Research/Creative Projects in its 2021 rankings.[89] In 2025, College Raptor ranked William & Mary's median SAT score #2 of public colleges and universities in the United States.[91] William & Mary is ranked 3rd for four year graduation rates among public colleges and universities.[92]
 For 2019 Kiplinger ranked William & Mary 6th out of 174 best-value public colleges and universities in the U.S.[93] 
 In the 2019 ""America's Top Colleges"" ranking by Forbes, W&M was ranked the 9th best public college and 47th out of the 650 best private and public colleges and universities in the U.S.[94] W&M ranked 3rd for race and class interaction in The Princeton Review's 2018 rankings.[citation needed] The college was ranked as the public college with the smartest students in the nation according to Business Insider's 2014 survey.[95]
 The undergraduate business program was ranked 12th among undergraduate programs by the 2016 Bloomberg Businessweek survey.[96] Also in 2020, was W&M ranked 4th for ""Colleges with the Happiest Students"" by The Princeton Review[97] and 9th in a list of the public universities that ""pay off the most"", according to CNBC.[98] In 2022, Georgetown University's Center on Education and the Workforce ranked William & Mary's undergraduate business program #21 in value among national universities.[99]
 The Omohundro Institute of Early American History and Culture publishes the William and Mary Quarterly, a scholarly journal focusing on colonial history, particularly in North America and the Atlantic World, from the Age of Discovery onward.
 In addition to the Quarterly, W&M, by its mission to provide undergraduates with a thorough grounding in research, W&M also hosts several student journals. The Monitor, the undergraduate journal of International Studies, is published semi-annually. The Lyon Gardiner Tyler Department of History publishes an annual undergraduate history journal, the James Blair Historical Review.
 Non-academic publications include The William & Mary Review – William & Mary's official literary magazine – Winged Nation – a student literary arts magazine, Acropolis – the art and art history magazine, The Flat Hat – the student newspaper, The Botetourt Squat – the student satirical newspaper, The Colonial Echo – William & Mary's yearbook, The DoG Street Journal – a daily online newspaper, and Rocket Magazine – William & Mary's fashion, art, and photography publication.[100][101][102][103][104]
 The college enjoys a temperate climate.[106] In addition to the college's extensive student recreation facilities (which include a large gym, a rock-climbing wall, and many exercise rooms) and programs (facilitating involvement in outdoor recreation, as well as club and intramural sports),[107] the largely wooded campus has its own lake and outdoor amphitheater. The Virginia Beach oceanfront is 60 miles (97 km) away, and Washington, D.C. is a 150-mile (240 km) drive to the north. Also, the beaches of the Delmarva Peninsula are just a few hours away via the Chesapeake Bay Bridge-Tunnel.
 The college's Alma Mater Productions (AMP) hosts concerts, comedians, and speakers on campus and in the 8,600-person capacity Kaplan Arena, as well as putting on many smaller activity-based events.[108] Students produce several publications on campus, including the official student newspaper The Flat Hat, arts and fashion magazine Rocket Magazine,[109] and the satirical newspaper The Botetourt Squat.[110] The school's television station, WMTV, produces content in the categories of cuisine, comedy, travel, and sports. Everyday Gourmet, the former flagship production of the station, was featured in USA Today in 2009.[111] WCWM, the college's student-run public radio station, transmits 24 hours a day on 90.9 FM locally and online [112] and features student-curated and created content; they also put on an annual concert, WCWM Fest, featuring local and touring musicians.[113]
 The college also hosts several prominent student-run culture- and identity-based organizations. These include the Black Student Organization, Catholic Campus Ministry, Hillel (the college's official Jewish student group), Asian American Student Initiative, Latin American Student Union, Lambda Alliance and Rainbow Coalition, and the Middle Eastern Students Association, among many others.[114]
 The college's International Relations Club (IRC) ranked eleventh of twenty-five participants in the 2020–2021 North American College Model U.N.[115]
 William & Mary has several traditions, including the Yule Log Ceremony, at which the president dresses as Santa Claus and reads a rendition of ""How the Grinch Stole Christmas"", the Vice-President of Student Affairs reads ""Twas the Night Before Finals"", and The Gentlemen of the College sing the song ""The Twelve Days of Christmas"".[116] Christmas is a grand celebration at the college; decorated Christmas trees abound on campus. This popular tradition started when German immigrant Charles Minnigerode, a humanities professor at the college in 1842 who taught Latin and Greek, brought one of the first Christmas trees to America. Entering into the social life of post-colonial Virginia, Minnigerode introduced the German custom of decorating an evergreen tree at Christmas at the home of law professor St. George Tucker, thereby becoming another of many influences that prompted Americans to adopt the practice at about that time.[117]
 Incoming first-year students participate in Opening Convocation, at which they pass through the entrance of the Wren Building and are officially welcomed as the newest members of the college. During orientation week, first-year students also have the opportunity to serenade the college president at his home with the Alma Mater song. The Senior Walk is similar in that graduating seniors walk through the Wren Building during their ""departure"" from college. On the last day of classes, Seniors are invited to ring the bell in the cupola of the Wren Building.
 W&M also takes pride in its connections to its colonial past during Charter Day festivities. Charter Day is technically February 8, based on the date (from the Julian Calendar) that the Reverend James Blair, first president of the college, received the charter from the Court of William III and Mary II at Kensington Palace in 1693. Past Charter Day speakers have included former US President John Tyler, Henry Kissinger, Margaret Thatcher, and Robert Gates.[118]
 Another underground tradition at W&M is known as the ""Triathlon"". As reported by The Flat Hat, the tradition - normally performed before graduation - involves completing three activities:[119] jumping the walls of the Governor's Palace in Colonial Williamsburg, streaking through the Sunken Garden, and finally swimming in the Crim Dell. The tradition has been referred to as an underground one and is not sanctioned by the college but is still widely practiced.[119]
 The Student Assembly is the student government organization serving undergraduates and graduates. It allocates a student organization budget and funds services, advocates for student rights, and is the formal student representation to the City of Williamsburg and William & Mary administration.[120] It consists of Executive, Legislative, and Judicial branches. The president and vice president are elected jointly by the student body to lead the Executive Branch, and each class elects one class president and four senators who serve in the Senate (the Legislative Branch). The five graduate schools appoint one to two senators. The Cabinet consists of 10 departments managed by secretaries and undersecretaries.[121]
 William & Mary's honor system was established by alumnus Thomas Jefferson in 1779 and is widely believed to be the nation's first.[122] During the orientation week, every entering student recites the Honor Pledge in the Great Hall of the Wren Building pledging:
 The basis of W&M's Honor Pledge was written over 150 years ago by alum and law professor Henry St. George Tucker Sr.[123] While teaching law at the University of Virginia, Tucker proposed students attach a pledge to all exams confirming on their honor they did not receive any assistance.[124][125] Tucker's honor pledge was the early basis of the Honor System at the University of Virginia.[126] At W&M, the Honor System stands as one of the college's most important traditions; it remains student-administered through the Honor Council with the advice of the faculty and administration of the college. The college's Honor System is codified such that students found guilty of cheating, stealing, or lying are subject to sanctions ranging from a verbal warning to expulsion.[127]
 W&M considers the observance of public laws of equal importance to the observance of its particular regulations. William & Mary's Board of Visitors delegates authority for discipline to its president. The President oversees a hierarchy of disciplinary authorities to enforce local laws as it pertains to William & Mary's interest as well as its internal regulatory system.[128]
 William & Mary has a long history of fraternities and sororities dating back to 1750 and the founding of the F.H.C. Society, the first collegiate fraternity established in what now is the United States.  Phi Beta Kappa, the first ""Greek-letter"" fraternity, was founded at the college in 1776.
 Today, various Greek-letter organizations play an important role in the college community and other social organizations such as theatre and club sports groups.  Overall, about one-third of undergraduate students are active members of one or another of 16 national fraternities and 13 sororities.[129]  William & Mary is also home to several unusual fraternal or similar organizations, including the Nu Kappa Epsilon music sorority[130] and its male counterpart, Phi Mu Alpha Sinfonia; the Alpha Phi Omega co-ed service fraternity; gender-inclusive Phi Sigma Pi and other honor fraternities.
 Several student secret societies exist at the college, including the F.H.C Society or Flat Hat Club, the Seven Society, the 13 Club, the Bishop James Madison Society, and the Wren Society.[131]
  The Queens' Guard was established on February 8, 1961, as a special unit of the Army Reserve Officers' Training Corps and was affiliated with the Pershing Rifles. The Guard was described by former President Davis Young Paschall as ""a unit organized, outfitted with special uniforms, and trained in appropriate drills and ceremonies as will represent the College of William & Mary in Virginia on such occasions and in such events as may be approved by the President."" The uniform of the Guard loosely resembles that of the Scots Guards of the United Kingdom. The baldric is a pleated Stuart tartan in honor of Queen Mary II and Queen Anne.[132] Following a hazing citation in fall 2019 by the college's Community Values & Restorative Practices organization, the Queens' Guard was suspended until at least spring 2022.[133]
 William & Mary has eleven collegiate a cappella groups: The Christopher Wren Singers (1987, co-ed); The Gentlemen of the College (1990, all-male); The Stairwells (1990, all-male); Intonations (1990, all-female); Reveille (1992, all-female); The Accidentals (1992, all-female); DoubleTake (1993, co-ed); The Cleftomaniacs (1999, co-ed); Passing Notes (2002, all-female); The Tribetones (2015, all-female);[134] and the Crim Dell Criers (2019, co-ed).[135][136] Sinfonicron Light Opera Company, founded in 1965, is William & Mary's student-run light opera company, producing musicals (traditionally those by Gilbert & Sullivan) in the early spring of each academic year.[137] Music societies at the college include local chapters of the music honor societies Delta Omicron (co-ed) and Phi Mu Alpha (all-male) as well as Nu Kappa Epsilon (all-female). Nu Kappa Epsilon, founded in 1994 at William & Mary, is ""dedicated to promoting the growth and development of musical activities at the college as well as in the Williamsburg community"".[138]
 Large musical ensembles include a symphony orchestra, wind symphony, and four choral ensembles: The William & Mary Choir,[139] The Botetourt Chamber Singers,[140] The Barksdale Treble Chorus (formerly the William & Mary Women's Chorus), and Ebony Expressions Gospel Choir. The Botetourt Chamber Singers (1974, co-ed) are the student chamber choir.[141] There are several musical ensembles at the college, from Early Music Ensemble to Jazz.[142] Prior to 1996 the college had a marching band, which has since changed into the William & Mary Pep Band.[143]
 William & Mary's radio station, WCWM, has been on the air since 1959.[144]
 William & Mary has multiple campus comedy groups. I.T. (""Improvisational Theatre"") was founded in 1986 and is the oldest group on campus.[145] The sketch comedy ensemble 7th Grade Sketch Comedy has been in existence since 1997.[146] In 2012, Sandbox Improv was formed.[147] Dad Jeans Improv, established in 2016, specializes in long-form improvisation and technical elements.[148]
 Formerly known as the ""Indians"", William & Mary's athletic teams are now known as the ""Tribe"". The college fields NCAA Division I teams for men and women in basketball, cross country, golf, gymnastics, soccer, swimming, tennis, and indoor and outdoor track and field. Also, there are women's field hockey, lacrosse, and volleyball squads, as well as men's baseball and football. In the 2004–05 season, the Tribe garnered five Colonial Athletic Association titles, leading the conference with over 80 titles. That same year, several teams competed in the NCAA Championships, with the football team appearing in the Division I-AA national semifinals. The men's cross country team finished 8th and 5th in Division I NCAA Men's Cross Country Championship in 2006 and 2009, respectively. The William & Mary men's basketball team is one of four original Division I schools that have never been to the NCAA Division I men's basketball tournament.[citation needed]
 In May 2006, the NCAA ruled that the athletic logo, which includes two green and gold feathers, could create an environment offensive to the American Indian community. The college's appeal regarding using the institution's athletic logo to the NCAA Executive Committee was rejected. The ""Tribe"" nickname was found to be neither hostile nor abusive but rather communicates ennobling sentiments of commitment, shared idealism, community, and common cause.[149] The college stated it would phase out the use of the two feathers by the fall of 2007. However, they can still be seen prominently painted on streets throughout the campus.[150]
 In 2018, athletic director Samantha Huge introduced a new brand kit for the department, officially retiring and de-emphasizing the script ""Tribe"" logo.[151]
 The ""Tribe 2025"" plan, a comprehensive plan for the athletics department to raise national prominence, undergo significant facilities upgrades, and achieve higher levels of student involvement and spirit, was presented in 2019.[152] In 2020, William & Mary announced that due to financial concerns, they would be discontinuing seven varsity sports: men's and women's gymnastics, men's and women's swimming, men's indoor and outdoor track and field and volleyball.[153] This decision prompted a petition entitled ""save the Tribe 7"" which received significant support. On October 19, the university reinstated women's gymnastics, women's swimming, and volleyball after notice of an impending lawsuit on the grounds of Title IX violations.[154] President Rowe later announced that the decision to cancel the four men's programs would be put off until the 2021-2022 academic year.[154]
 Since the 17th century, many prominent academics have chosen to teach at William & Mary. Distinguished faculty include the first professor of law in the United States, George Wythe (who taught Henry Clay, John Marshall, and Thomas Jefferson, among others); William Small (Thomas Jefferson's cherished mentor); William and Thomas Dawson, who were also presidents of William & Mary. Also, the founder and first president of the Massachusetts Institute of Technology – William Barton Rogers – taught chemistry at William & Mary (which was also Professor Barton's alma mater). Several members of the socially elite and politically influential Tucker family, including Nathaniel Beverley, St. George, and Henry St. George Tucker Sr. (who penned the original honor code pledge for the University of Virginia that remains in use there today), taught at William & Mary.
 More recently, William & Mary recruited the constitutional scholar William Van Alstyne from Duke Law School. Lawrence Wilkerson, current Harriman Visiting Professor of Government and Public Policy, was chief of staff for Colin Powell. Susan Wise Bauer is an author and founder of Peace Hill Press, who teaches writing and American literature at the college. James Axtell, who teaches history, was inducted into the American Academy of Arts and Sciences as a Fellow in 2004. Iyabo Obasanjo, a previous senator of Nigeria and daughter of former President Olusegun Obasanjo of Nigeria, also serves as faculty in Kinesiology & Health Sciences.[155]
 Professor Benjamin Bolger – the second-most credentialed person in modern history behind Michael Nicholson – taught at W&M.[156]
 Although a historically small college, the alumni of William & Mary include influential and historically significant people. Among its alumni are four of the first ten presidents of the United States,[157] four United States Supreme Court justices, dozens of U.S. senators, members of government, six Rhodes Scholars,[158] and three Marshall Scholars.[159]
"
Apprenticeship,https://en.wikipedia.org/wiki/Apprenticeship,"Apprenticeship is a system for training a new generation of practitioners of a trade or profession with on-the-job training and often some accompanying study (classroom work and reading). Apprenticeships may also enable practitioners to gain a license to practice in a regulated occupation. Most of their training is done while working for an employer who helps the apprentices learn their trade or profession, in exchange for their continued labor for an agreed period after they have achieved measurable competencies.
 Apprenticeship lengths vary significantly across sectors, professions, roles and cultures. In some cases, people who successfully complete an apprenticeship can reach the ""journeyman"" or professional certification level of competence. In other cases, they can be offered a permanent job at the company that provided the placement. Although the formal boundaries and terminology of the apprentice/journeyman/master system often do not extend outside guilds and trade unions, the concept of on-the-job training leading to competence over a period of years is found in any field of skilled labor.
 There is no global consensus on a single term for apprenticeship. Depending on the culture, country and sector, the same or similar definitions are used to describe the terms apprenticeship, internship, and trainee-ship. The latter two terms may be preferred in the health sector. One example is internships in medicine for physicians and trainee-ships for nurses – and western countries. Apprenticeship is the preferred term of the European Commission and the one selected for use by the European Center for the Development of Vocational Training (CEDEFOP), which has developed many studies on the subject. Some non-European countries adapt European apprenticeship practices.[citation needed]
 The system of apprenticeship first developed in the Late Middle Ages and came to be supervised by craft guilds and town governments. A master craftsman was entitled to employ young people as an inexpensive form of labour in exchange for providing food, lodging and formal training in the craft. Most apprentices were males, but female apprentices were found in crafts such as seamstress,[1] tailor, cordwainer, baker and stationer.[2] Apprentices usually began at ten to fifteen years of age, and would live in the master craftsman's household. The contract between the craftsman, the apprentice and, generally, the apprentice's parents would often be governed by an indenture.[3] Most apprentices aspired to becoming master craftsmen themselves on completion of their contract (usually a term of seven years), but some would spend time as a journeyman and a significant proportion would never acquire their own workshop. In Coventry those completing seven-year apprenticeships with stuff merchants were entitled to become freemen of the city.[4]
 Apprenticeship was adopted into military of the West African kingdom of Dahomey. Soldiers in the army were recruited as young as seven or eight years old, as they initially served as shield carriers for regular soldiers. After years of apprenticeship and military experience, the recruits were allowed to join the army as regular soldiers. With a combination of lifelong military experience and monetary incentives, a cohesive and well-disciplined military emerged in Dahomey.[5]
 Apprenticeships can be divided into two main categories: Independent and Cooperative.[7]
 Independent apprenticeships are those organized and managed by employers, without any involvement from educational institutions. They happen dissociated from any educational curricula, which means that, usually, the apprentices are not involved in any educational programme at the same time but, even if they are, there is no relation between the undergoing studies and the apprenticeship.
 Cooperative apprenticeships are those organized and managed in cooperation between educational institutions and employers. They vary in terms of governance, some being more employer lead and others more educational institution lead, but they are always associated with a curriculum and are designed as a mean for students to put theory in practice and master knowledge in a way that empowers them with professional autonomy. Their main characteristics could be summarized into the following:
 Australian Apprenticeships encompass all apprenticeships and traineeships. They cover all industry sectors in Australia and are used to achieve both 'entry-level' and career 'upskilling' objectives. There were 475,000 Australian Apprentices in-training as at 31 March 2012, an increase of 2.4% from the previous year. Australian Government employer and employee incentives may be applicable, while State and Territory Governments may provide public funding support for the training element of the initiative. Australian Apprenticeships combine time at work with formal training and can be full-time, part-time or school-based.[8]
 Australian apprentice and traineeship services are dedicated to promoting retention, therefore much effort is made to match applicants with the right apprenticeship or traineeship. This is done with the aid of aptitude tests, tips, and information on 'how to retain an apprentice or apprenticeship'.[9]
 Information and resources on potential apprenticeship and traineeship occupations are available in over sixty industries.[10]
 The distinction between the terms apprentices and trainees lies mainly around traditional trades and the time it takes to gain a qualification. The Australian government uses Australian Apprenticeships Centres to administer and facilitate Australian Apprenticeships so that funding can be disseminated to eligible businesses, apprentices and trainees, supporting the whole process as it underpins the future skills of Australian industry. Australia also has a fairly unusual safety net in place for businesses and Australian Apprentices with its Group Training scheme. This is where businesses that are not able to employ the Australian Apprentice for the full period until they qualify, are able to lease or hire the Australian Apprentice from a Group Training Organisation. It is a safety net, because the Group Training Organisation is the employer and provides continuity of employment and training for the Australian Apprentice.[11][12]
 In addition to a safety net, Group Training Organisations (GTO) have other benefits such as additional support for both the Host employer and the trainee/apprentice through an industry consultant who visits regularly to make sure that the trainee/apprentice are fulfilling their work and training obligations with their Host employer. There is the additional benefit of the trainee/apprentice being employed by the GTO reducing the Payroll/Superannuation and other legislative requirements on the Host employer who pays as invoiced per agreement.[citation needed]
 Apprenticeship training in Austria is organized in a school system with long-term training parts. It is thus possible to get the Matura needed to enter university. WIFI[13] company-based training of apprentices is complemented by compulsory attendance of a part-time vocational school for apprentices (Berufsschule).[14] It lasts two to four years – the duration varies among the 250 legally recognized apprenticeship trades.[citation needed]
 About 40 percent of all Austrian teenagers enter apprenticeship training upon completion of compulsory education (at age 15). This number has been stable since the 1950s.[15]
 The five most popular trades are: Retail Salesperson (5,000 people complete this apprenticeship per year), Clerk (3,500 / year), Car Mechanic (2,000 / year), Hairdresser (1,700 / year), Cook (1,600 / year).[16] There are many smaller trades with small numbers of apprentices, e.g. ""EDV-Systemtechniker"" (Sysadmin), which is completed by fewer than 100 people a year.[17]
 The Apprenticeship Leave Certificate provides the apprentice with access to two different vocational careers. On the one hand, it is a prerequisite for the admission to the Master Craftsman Exam and for qualification tests, and on the other hand it gives access to higher education via the TVE-Exam or the Higher Education Entrance Exam which are prerequisites for taking up studies at colleges, universities, ""Fachhochschulen"", post-secondary courses and post-secondary colleges.[14]
 The person responsible for overseeing the training inside the company is called ""Lehrherr"" or ""Ausbilder"". An Ausbilder must prove that he has the professional qualifications needed to educate another person, has no criminal record and is an otherwise-respectable person. The law states that ""the person wanting to educate a young apprentice must prove that he has an ethical way of living and the civic qualities of a good citizen"".[18]
 In Canada, apprenticeships tend to be formalized for craft trades and technician level qualifications. At the completion of the provincial exam, they may write the Provincial Standard exam. British Columbia is one province that uses these exams as the provincial exam. This means a qualification for the province will satisfy the whole country. The inter-provincial exam questions are agreed upon by all provinces of the time. At the time there were only four provinces, Nova Scotia, New Brunswick, Upper Canada (now Ontario), and Lower Canada (now Quebec).[citation needed]
 In Canada, each province has its own apprenticeship program, which may be the only route into jobs within compulsory trades.[citation needed]
 Organisations such as the Canadian Council of Directors of Apprenticeship and Employment and Social Development Canada help to oversee the programmes.[citation needed]
 In the Czech Republic, the term ""vocational school"" (učiliště) can refer to the two, three or four years of secondary practical education. Apprenticeship Training is implemented under Education Act (školský zákon). Apprentices spend about 30–60% of their time in companies (sociální partneři školy) and the rest in formal education. Depending on the profession, they may work for two to three days a week in the company and then spend two or three days at a vocational school.[citation needed]
 In France, apprenticeships also developed between the ninth and thirteenth centuries, with guilds structured around apprentices, journeymen and master craftsmen, continuing in this way until 1791, when the guilds were suppressed.[citation needed]
 The first laws regarding apprenticeships were passed in 1851. From 1919, young people had to take 150 hours of theory and general lessons in their subject a year. This minimum training time rose to 360 hours a year in 1961, then 400 in 1986.[citation needed]
 The first training centres for apprentices (centres de formation d'apprentis, CFAs) appeared in 1961, and in 1971 apprenticeships were legally made part of professional training. In 1986 the age limit for beginning an apprenticeship was raised from 20 to 25. From 1987 the range of qualifications achievable through an apprenticeship was widened to include the brevet professionnel (certificate of vocational aptitude), the bac professionnel (vocational baccalaureate diploma), the brevet de technicien supérieur (advanced technician's certificate), engineering diplomas, master's degree and more.[citation needed]
 On January 18, 2005, President Jacques Chirac announced the introduction of a law on a programme for social cohesion comprising the three pillars of employment, housing and equal opportunities. The French government pledged to further develop apprenticeship as a path to success at school and to employment, based on its success: in 2005, 80% of young French people who had completed an apprenticeship entered employment. In France, the term apprenticeship often denotes manual labor but it also includes other jobs like secretary, manager, engineer, shop assistant... The plan aimed to raise the number of apprentices from 365,000 in 2005 to 500,000 in 2009. To achieve this aim, the government is, for example, granting tax relief for companies when they take on apprentices. (Since 1925 a tax has been levied to pay for apprenticeships.) The minister in charge of the campaign, Jean-Louis Borloo, also hoped to improve the image of apprenticeships with an information campaign, as they are often connected with academic failure at school and an ability to grasp only practical skills and not theory. After the civil unrest end of 2005, the government, led by prime minister Dominique de Villepin, announced a new law. Dubbed ""law on equality of chances"", it created the First Employment Contract as well as manual apprenticeship from as early as 14 years of age. From this age, students are allowed to quit the compulsory school system in order to quickly learn a vocation. This measure has long been a policy of conservative French political parties, and was met by tough opposition from trade unions and students.[citation needed]
 Apprenticeships are part of Germany's dual education system, which combines a practical education in a company and a theoretical one in a vocational school. The lessons in vocational school can either happen once or twice a week, or in whole week blocks. During their apprenticeship, apprentices can earn a salary, which gets higher every year, with the average apprentice earning 1,066€ before taxes. The average apprenticeship takes between 2 and 3.5 years. As of 2024, there are 327 officially recognized apprenticeship trades.[19]
 In India, the Apprentices Act was enacted in 1961.[20] It regulates the programme of training of apprentices in the industry so as to conform to the syllabi, period of training etc. as laid down by the Central Apprenticeship Council and to utilise fully the facilities available in industry for imparting practical training with a view to meeting the requirements of skilled manpower for industry.[citation needed]
 The Apprentices Act enacted in 1961 and was implemented effectively in 1962. Initially, the Act envisaged training of trade apprentices. The Act was amended in 1973 to include training of graduate and diploma engineers as ""Graduate"" & ""Technician"" Apprentices. The Act was further amended in 1986 to bring within its purview the training of the 10+2 vocational stream as ""Technician (Vocational)"" Apprentices.[citation needed]
 Overall responsibility is with the Directorate General of Employment & Training (DGE&T) in the Union Ministry of Skill Development and Entrepreneurship.[21]
 In Ireland the apprenticeships are split into two main categories: ""craft"" and ""new"".[23] The main craft trades and professions have been designated by SOLAS and come within the scope of the Statutory Apprenticeship system, which is organised by SOLAS in co-operation with the Department of Education, employers and unions.[23] An Apprenticeship Council is also in place. An apprenticeship provides on-the-job training with an employer.[23] It usually alternates between off-the-job training in an education centre and on-the-job training at an employer's workplace. An apprenticeship generally lasts for 4 years, during which time there are 3 different periods in off-the-job training.[23] This training phase takes place in an Education and Training Board (ETB) Training Centre while the subsequent off-the-job training phases take place in an Institute of Technology.[23] After on-going assessments through on-the-job competence testing as well as off-the-job modular assessments and examinations, if passed successfully the apprentice is awarded an Advanced Certificate in craft (level 6 on the National Framework of Qualifications).[23]
 New apprenticeships in other areas of industry were introduced from 2016, and can lead to an award between Levels 5–10 on the National Framework of Qualifications. Each apprenticeship programme lasts between 2 and 4 years. Industry-led groups which work with education and training providers and other partners, oversee the development and roll-out of new apprenticeships. New apprenticeships in ICT, finance and hospitality include software development, accounting technician and commis chef.[23][24]
 In Liberia, tailor apprenticeships engage with more skilled tailors to learn the craft and the skills that may be taught in more traditional school settings. They learn from master tailors, which gives the apprentices a promised job once their training is completed. 
Apprentices must have a grasp on patterns, measurement, and other mathematics skills. They demonstrate full concept mastery before moving on to the next piece of clothing. Instead of formal testing for evaluation, articles of clothing must meet the quality standards before they can be sold and before the apprentice can begin a new design.[25]
 The Igbo apprentice system is a framework of formal and informal indentured agreements between parties that ultimately facilitate burgeoning entrepreneurial communities within the Igbos. It is an economic model practiced widely by Igbos and originated in South-Eastern Nigeria. Its purposes were and still remains to spur economic growth and stability, and sustainable livelihood by financing and investing in human resources through vocational training.[26]
 In Pakistan, the Apprenticeship Training is implemented under the National Apprenticeship Ordinance 1962 and Apprenticeship Rules 1966, regulating programs across industries in conjunction with Technical and Vocational Education and Training (TVET) institutes. For companies with over fifty workers in apprenticeable trades, it's mandatory to conduct apprenticeship training. All costs, including wages, are covered by the industry. Enforcement is carried out by provincial governments through various TEVTAs, such as Punjab TEVTA, Sindh TEVTA, KP TEVTA, Balochistan TEVTA, and AJK TEVTA.[citation needed]
 The training period varies by trade, from 1 to 4 years. As of 2015, more than 30,000 apprentices were being trained in 2,751 industries across 276 trades. This accounts for less than 10% of the over 350,000 institution-based vocational trainees produced annually.[citation needed]
 In recent years, the Government of Pakistan, through the National Vocational & Technical Training Commission, has begun reforms to modernize the apprenticeship system. Key aspects include:[citation needed]
 In Switzerland, after the end of compulsory schooling, two thirds of young people follow a vocational training.[27] Ninety percent of them are in the dual education system.[27]
 Switzerland has an apprenticeship similarly to Germany and Austria. The educational system is ternar, which is basically dual education system with mandatory practical courses. The length of an apprenticeship can be 2, 3 or 4 years.
 Apprenticeships with a length of 2 years are for persons with weaker school results. The certificate awarded after successfully completing a 2-year apprenticeship is called ""Attestation de formation professionnelle"" (AFP [fr]) in French, ""Eidgenössisches Berufsattest"" (EBA [de]) in German and ""Certificato federale di formazione pratica"" (CFP) in Italian. It could be translated as ""Attestation of professional formation"".[citation needed]
 Apprenticeship with a length of 3 or 4 years are the most common ones. The certificate awarded after successfully completing a 3 or 4-year apprenticeship is called ""Certificat Fédéral de Capacité"" (CFC [fr]) in French, ""Eidgenössisches Fähigkeitszeugnis"" (EFZ [de]) in German and ""Attestato federale di capacità"" (AFC) in Italian. It could be translated as ""Federal Certificate of Proficiency"".[citation needed]
 Some crafts, such as electrician, are educated in lengths of 3 and 4 years. In this case, an Electrician with 4 years apprenticeship gets more theoretical background than one with 3 years apprenticeship. Some languages have different names for a craft depending on the length of the apprenticeship; this can be lost in translation.[citation needed]
 Each of the over 300 nationwide defined vocational profiles has defined framework – conditions as length of education, theoretical and practical learning goals and certification conditions.[citation needed]
 Typically an apprenticeship can commence for individuals once they are aged 15 or 18 after finishing general education. Some apprenticeships have a recommend or required age of 18. There is formally no maximum age, however, for persons above 21 it is hard to find a company due to companies preferring younger ages due to the lower cost of labour.[citation needed]
 In Turkey, apprenticeship has been part of the small business culture for centuries since the time of Seljuk Turks who claimed Anatolia as their homeland in the 11th century.[citation needed]
 There are three levels of apprenticeship. The first level is the apprentice, i.e., the ""çırak"" in Turkish. The second level is pre-master which is called, ""kalfa"" in Turkish. The mastery level is called as ""usta"" and is the highest level of achievement. An 'usta' is eligible to take in and accept new 'ciraks' to train and bring them up. The training process usually starts when the small boy is of age 10–11 and becomes a full-grown master at the age of 20–25. Many years of hard work and disciplining under the authority of the master is the key to the young apprentice's education and learning process.[citation needed]
 In Turkey today there are many vocational schools that train children to gain skills to learn a new profession. The student after graduation looks for a job at the nearest local marketplace usually under the authority of a master.[citation needed]
 Apprenticeships in the United Kingdom are devolved. It has a long tradition in the United Kingdom, dating back to around the 12th century. Apprenticeships flourished in the 14th century and were expanded during the Industrial Revolution. In modern times, apprenticeships were formalised in 1964 by act of parliament and they continue to be in widespread use in all four nations, with hundreds of apprenticeships to choose from.[28]
 Apprenticeship programs in the United States are regulated by the Smith–Hughes Act (1917), The National Industrial Recovery Act (1933), and National Apprenticeship Act, also known as the ""Fitzgerald Act.""[29]
 The number of American apprentices has increased from 375,000 in 2014 to 500,000 in 2016, while the federal government intends to see 750,000 by 2019, particularly by expanding the apprenticeship model to include white-collar occupations such as information technology.[30][31]
 
The modern concept of an internship is similar to an apprenticeship but not as rigorous. Universities still use apprenticeship schemes in their production of scholars: bachelors are promoted to masters and then produce a thesis under the oversight of a supervisor before the corporate body of the university recognises the achievement of the standard of a doctorate. Another view of this system is of graduate students in the role of apprentices, post-doctoral fellows as journeymen, and professors as masters .[citation needed] In the ""Wealth of Nations"" Adam Smith states that:  Also similar to apprenticeships are the professional development arrangements for new graduates in the professions of accountancy, engineering, management consulting, and the law. A British example was training contracts known as 'articles of clerkship'. The learning curve in modern professional service firms, such as law firms, consultancies or accountancies, generally resembles the traditional master-apprentice model: the newcomer to the firm is assigned to one or several more experienced colleagues (ideally partners in the firm) and learns their skills on the job.[citation needed]
"
"Thomas Fairfax, 6th Lord Fairfax of Cameron","https://en.wikipedia.org/wiki/Thomas_Fairfax,_6th_Lord_Fairfax_of_Cameron","

 Thomas Fairfax, 6th Lord Fairfax of Cameron (22 October 1693 – 9 December 1781) was a British peer, military officer and planter. The only member of the British peerage to permntly reside in Britain's Noeck Proprietary in the Colony of Virginia, where he spent the majority of his life. The proprietary had been granted to Fairfax's ancestor John Colepeper, 1st Baron Colepeper by Charles II of England in 1649. 
 On his Virginian estates, Fairfax developed a profitable operation based on the forced labour of several hundred black slaves. A steadfast Loyalist during the American Revolution, he was largely protected from the loss of his property due to Fairfax's friendship with George Washington. Several places in Northern Virginia and the eastern panhandle of West Virginia are named for him, including Fairfax County, Virginia and the City of Fairfax.[1]
 Thomas Fairfax was born on 22 October 1693 in Leeds Castle, Kent. The castle had been owned by his maternal ancestors since the 1630s.[2] Fairfax was the son of Thomas Fairfax, 5th Lord Fairfax of Cameron and Catherine Colepeper, the daughter of Thomas Colepeper, 2nd Baron Colepeper. He succeeded to his father's Scottish peerage in 1709, and was educated at Oriel College, Oxford between 1710 and 1713. In 1721, Fairfax was commissioned into the British Army, serving in the Royal Regiment of Horse Guards until 1733. He was also a contributor to The Spectator, a daily publication founded by Joseph Addison and Richard Steele in 1711 before ceasing publication in the next year.[citation needed]
 In 1719, Fairfax came into possession of the Northern Neck Proprietary in the British colony of Virginia, which had been granted to Fairfax's maternal ancestor John Colepeper, 1st Baron Colepeper by Charles II of England in 1649. The property included a large portion of the Shenandoah and South Branch Potomac valleys, and consisted of approximately 5,282,000 acres (21,380 km2) of land. Struggling to keep up an expensive lifestyle and maintain Leeds Castle, Fairfax relied heavily on the income he derived from the Northern Neck Proprietary, both from the sale of parcels of land to and annual quit-rents from planters who settled in the Northern Neck of Virginia. His affairs in Virginia were handled by Fairfax's resident land agent, Robert Carter I.[3]  
 In the fall of 1732, Fairfax read Carter's obituary in the London monthly The Gentleman's Magazine and was astonished to discover the vast personal wealth Carter had accumulated, which included £10,000 worth of cash, at a time when the governor of Virginia was paid an annual salary of £200. Rather than appoint another Virginian to the position, Fairfax arranged to have his cousin William Fairfax move from Massachusetts to Virginia in 1734 to serve as his resident land agent. Fairfax travelled to Virginia for the first time in 1735 to inspect and manage his estates there, remaining in the colony until 1737. In 1738, Fairfax established approximately thirty farms in the Patterson Creek Manor, a 9,000-acre (36 km2) piece of land granted to him by the Crown.[citation needed]
 The northwestern boundary of the Northern Neck Proprietary, which had been contested by the Privy Council of Great Britain, was marked in 1746 by the Fairfax Stone at the headwaters of the North Branch Potomac River. Returning to North America in 1747, Fairfax first settled at Belvoir, a slave plantation which had been completed by William six years earlier. In the same year, he also set aside land for personal use at Swan Pond Manor. Fairfax also became active in developing his Virginian estates and collecting quit-rents, along with utilising the forced labour of hundreds of black slaves who worked on his estates.[1] He personally bought and sold slaves and, in 1777, engaged in the ""little talked about"" activity of ""bedding down with a negro wench"".[1][4]
 Fairfax was the only British peer who resided in the Thirteen Colonies.[5] In 1748, he became acquainted with George Washington, who was distant relative of the Fairfax family. Impressed with Washington's energy and talents, Fairfax employed him to survey his lands west of the Blue Ridge Mountains, which was Washington's first employment.[6] Fairfax, a lifelong bachelor, moved to the Shenandoah Valley in 1752. At the suggestion of his nephew Thomas, he settled down in a hunting lodge at Greenway Court.[7] Fairfax and Thomas lived together in a style of liberal hospitality, frequently engaging in fox hunts. He also served as both county lieutenant and justice of the peace for Frederick County. 
 During the American Revolution, he kept quiet about his avowed Loyalist views, and was protected by his friendship with Washington. The title to his domain, however, was confiscated by the Virginia Act of 1779. Less than two months after Washington's victory at the Siege of Yorktown, Fairfax died in Greenway Court on 9 December 1781. He was buried in the Christ Episcopal Church in Winchester.[citation needed]
 Lord Fairfax's title descended to his younger brother, Robert Fairfax, 7th Lord Fairfax of Cameron, who was also descended from the 5th Lord Fairfax of Cameron, who died at Leeds Castle in 1793. Since, were it not for the Revolutionary War, his immense domain should also have passed to Robert Fairfax, the latter was awarded £13,758 in 1792, by Act of Parliament for the relief of American Loyalists. A portion of this estate, devised to nephew Denny Martin Fairfax, was later the subject of the landmark U.S. Supreme Court case Martin v. Hunter's Lessee (1816). His younger cousin, son of his manager William Fairfax and half-brother of George William Fairfax, Rev. Bryan Fairfax, would eventually return to England to assert his claim and become the 8th Lord Fairfax of Cameron. 
 Fairfax County, Virginia, and the City of Fairfax, Virginia, are named for Lord Fairfax.[8] Fairfax and Cameron Streets in Alexandria, Virginia, are named for Lord Fairfax. The town's first survey map was made in 1749 by Lord Fairfax's young protégé George Washington. The Fairfax Line and Fairfax Stone both bear Lord Fairfax's name. Lord Fairfax Community College bore his name, but it was changed to Laurel Ridge Community College in July 2021.[9] The Swan Pond Manor Historic District encompasses land Lord Fairfax set aside in 1747 for his personal use.[10] 
"
"Culpeper County, Virginia","https://en.wikipedia.org/wiki/Culpeper_County,_Virginia","
 Culpeper County is a county located along the borderlands of the northern and central region of the Commonwealth of Virginia. As of the 2020 United States Census, the population was 52,552.[1] Its county seat and only incorporated community is Culpeper.[2]
 Culpeper County is included in the Washington–Baltimore–Arlington, DC–MD–VA–WV–PA Combined Statistical Area.
 At the time of European encounter, the inhabitants of future Culpeper County were a Siouan-speaking sub-group of the Manahoac tribe called the Tegninateo.[3] Culpeper County was established in 1749, with territory partitioned from Orange County. The county is named for Thomas Colepeper, 2nd Baron Colepeper, colonial governor of Virginia from 1677 to 1683.
 In May 1749, the first Culpeper Court convened in the home of Robert Tureman, near the present location of the Town of Culpeper. In July 1749, Tureman commissioned 17-year-old George Washington as the first County surveyor.[4] One of his first duties was to lay out the county's courthouse complex, which included the courthouse, jail, stocks, gallows and accessory buildings. By 1752 the complex stood at the present northeast corner of Davis and Main Streets. The courthouse village was named Town of Fairfax for Thomas Fairfax, 6th Lord Fairfax of Cameron (1693–1781).[5]
 During the Virginia convention held in May 1775, the colony was divided into sixteen districts. Each district had instructions to raise a battalion of men ""to march at a minute's notice."" Culpeper, Orange and Fauquier, forming one district, raised 350 men in ""Clayton's old field"" on the Catalpa estate; they were called the Culpeper Minute Men. In December, the Minute Men, marching under their flag depicting a rattlesnake and inscribed with the words ""Liberty or Death"" and ""Don't Tread on Me"", took part in the Battle of Great Bridge, the first Revolutionary battle on Virginia soil. The Culpeper Minute Men reorganized in 1860 in response to the impending Civil War and became part of 13th Infantry's Company B, fighting against the US Government forces. The Culpeper Minutemen were again organized for World War I, and joined the 116th Infantry.
 In 1833, based on the county's growing population and the need of those in the northwestern area for easier access to a county seat, the upper 267 square miles (690 km2) of Culpeper County was partitioned off to create Rappahannock County, Virginia, which was founded by an act of the Virginia General Assembly.
 During the Civil War the Battle of Cedar Mountain took place on August 9, 1862, and the Battle of Brandy Station occurred on June 9, 1863, in Culpeper County.
 Culpeper was the boyhood home of Civil War General A. P. Hill, who fought against Union forces.
 The negative impact of the Massive Resistance campaign against school integration led to the statewide election of a pro-desegregation governor. By the middle of the 1970s,[6] Culpeper was the last county in Virginia to desegregate its public schools. In 2018 Culpeper County Public Schools[7] has six elementary, two middle schools and two high schools. In 1935 the Rotary Club of Culpeper began a college loan fund, which in 1966 became a four-year scholarship based on academic achievement. The group also provides a Technical School scholarship based on academic achievement.[8]
 Culpeper County is home to Commonwealth Park, site for many world-class equestrian events. It was here that actor Christopher Reeve suffered his 1995 accident during a competition.
 The town of Culpeper was rated #10 by Norman Crampton, author of ""The 100 Best Small Towns in America,"" in February 1993.
 In April 2016, the county Board of Supervisors denied a routine request from the Islamic Center of Culpeper for a pump and haul permit to serve their envisioned mosque. This resulted in a lawsuit by the US Department of Justice in December.[9]
 Culpeper County has a civilian workforce of 24,313.[10] 30% of residents live and work within the county while 70% of workers commute out of the locality. The most residents are commuting to Fairfax or Fauquier counties. In comparison, the equivalent of 45% are in-commuters. The most in-commuters are coming from Orange County.[11]
 The Top 10 non-governmental Culpeper employers as of March 2023:[11]
 The northeast border of Culpeper County is defined by the Rappahannock River which flows east-southeastward along its border, while the south border of the county is similarly defined by the meanders of the Rapidan River. The Hazel River flows eastward through the county, discharging into the Rappahannock on the county's east border, while the Thornton River also flows eastward through the county, discharging into the Hazel in the north part of the county. The county is in the foothills of the Blue Ridge Mountains, which are quickly accessed beginning with Old Rag Mountain and the Skyline Drive just up Route 522.[12] The rolling hills generally slope to the south and east, with its highest point near its west corner at 705 ft (215 m) ASL.[13] The county has a total area of 383 square miles (990 km2), of which 379 square miles (980 km2) is land and 3.3 square miles (8.5 km2) (0.9%) is water.[14]
 [12]
 [12]
 Culpeper County is represented by Republicans Bryce E. Reeves, Emmett W. Hanger Jr., and Jill Holtzman Vogel in the Virginia Senate, Republicans Michael J. Webert and Nicholas J. (Nick) Freitas in the Virginia House of Delegates, and Democrat Abigail Spanberger in the U.S. House of Representatives.
 Culpeper County has been a Republican stronghold for several decades. The last time a Democratic presidential candidate carried the county was 1964.
 Recent media investigations regarding law enforcement procurement of military equipment through the ""1033"" program offered by the Defense Logistics Agency identified Culpeper County as having received, as donations, a ""Mine Resistant Vehicle"" in 2013 worth $412,000 and 20 night-vision optics worth an additional $136,000.00.[28]
 As of the 2000 United States Census, there were 34,262 people, 12,141 households, and 9,045 families in the county. The population density was 90.4 people per square mile (34.9 people/km2). There were 12,871 housing units at an average density of 34.0 units per square mile (13.1 units/km2). The racial makeup of the county was 68.27% White, 28.15% Black or African American, 0.33% Native American, 0.66% Asian, 0.01% Pacific Islander, 1.15% from other races, and 1.43% from two or more races. 2.50% of the population were Hispanic or Latino of any race.
 There were 12,141 households, out of which 35.00% had children under the age of 18 living with them, 58.50% were married couples living together, 11.30% had a female householder with no husband present, and 25.50% were non-families. 20.60% of all households were made up of individuals, and 7.90% had someone living alone who was 65 years of age or older. The average household size was 2.68 and the average family size was 3.08.
 The county population contained 25.70% under the age of 18, 8.10% from 18 to 24, 31.10% from 25 to 44, 23.30% from 45 to 64, and 11.90% who were 65 years of age or older. The median age was 36 years. For every 100 females, there were 103.30 males. For every 100 females age 18 and over, there were 103.20 males.
 The median income for a household in the county was $45,290, and the median income for a family was $51,475. Males had a median income of $36,621 versus $25,985 for females. The per capita income for the county was $20,162. About 27.00% of families and 29.20% of the population were below the poverty line, including 38.30% of those under age 18 and 28.60% of those age 65 or over.
 Culpeper County Public Schools
 [12]
 38°29′N 77°58′W﻿ / ﻿38.49°N 77.96°W﻿ / 38.49; -77.96
"
Barbados,https://en.wikipedia.org/wiki/Barbados,"

 Barbados (.mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}UK: /bɑːrˈbeɪdɒs/ bah-BAY-doss; US: /bɑːrˈbeɪdoʊs/ ⓘ bar-BAY-dohss; locally /bɑːrˈbeɪdəs/ bar-BAY-dəss) is an island country in the Atlantic Ocean. It is part of the Lesser Antilles region of the West Indies. Despite not bordering the Caribbean Sea, it is considered to be part of the Caribbean region and is therefore the most easterly of the Caribbean islands. It lies on the boundary of the South American and Caribbean plates. Its capital and largest city is Bridgetown.
 Inhabited by Kalinago people since the 13th century, and prior to that by other Indigenous peoples, Barbados was claimed for the Crown of Castile by Spanish navigators in the late 15th century. It first appeared on a Spanish map in 1511.[7] The Portuguese Empire claimed the island between 1532 and 1536, but abandoned it in 1620 with their only remnants being the introduction of wild boars to supply of meat whenever the island was visited. An English ship, the Olive Blossom, arrived in Barbados on 14 May 1625; its men took possession of the island in the name of King James I. In 1627, the first permanent settlers arrived from England, and Barbados became an English and later British colony.[8] During this period, the colony operated on a plantation economy, relying on the labour of African slaves who worked on the island's plantations. Slavery continued until it was phased out through most of the British Empire by the Slavery Abolition Act 1833.
 On 30 November 1966, Barbados moved toward political independence and assumed the status of a Commonwealth realm, becoming a separate jurisdiction with Elizabeth II as the Queen of Barbados. On 30 November 2021, Barbados transitioned to a republic within the Commonwealth, replacing its monarchy with a ceremonial president.[9][10]
 Barbados's population is predominantly of African ancestry. While it is technically an Atlantic island, Barbados is closely associated with the Caribbean and is ranked as one of its leading tourist destinations.[11]
 The name ""Barbados"" is from either the Portuguese term os barbados or the Spanish equivalent, los barbados, both meaning ""the bearded ones"".[12][13] It is unclear whether ""bearded"" refers to the long, hanging roots of the bearded fig-tree (Ficus citrifolia), a species of banyan indigenous to the island, or to the allegedly bearded Kalinago (Island Caribs) who once inhabited the island, or, more fancifully, to a visual impression of a beard formed by the sea foam that sprays over the outlying coral reefs. In 1519, a map produced by the Genoese mapmaker Visconte Maggiolo showed and named Barbados in its correct position.[citation needed] Furthermore, the island of Barbuda in the Leewards is very similar in name and was once named ""Las Barbudas"" by the Spanish.[citation needed]
 The original name for Barbados in the Pre-Columbian era was Ichirouganaim, according to accounts by descendants of the Indigenous Arawakan-speaking tribes in other regional areas, with possible translations including ""Red land with white teeth""[14] or ""Redstone island with teeth outside (reefs)""[15] or simply ""Teeth"".[16][17][18]
 Colloquially, Barbadians refer to their home island as ""Bim"" or other nicknames associated with Barbados, including ""Bimshire"". The origin is uncertain, but several theories exist. The National Cultural Foundation of Barbados says that ""Bim"" was a word commonly used by slaves, and that it derives from the Igbo term bém from bé mụ́ meaning ""my home, kindred, kind"";[19] the Igbo phoneme [e] in the Igbo orthography is very close to /ɪ/.[20] The name could have arisen due to the relatively large percentage of Igbo slaves from modern-day southeastern Nigeria arriving in Barbados in the 18th century.[21][22] The words ""Bim"" and ""Bimshire"" are recorded in the Oxford English Dictionary and Chambers Twentieth Century Dictionaries. Another possible source for ""Bim"" is reported to be in the Agricultural Reporter of 25 April 1868, where the Rev. N. Greenidge (father of one of the island's most famous scholars, Abel Hendy Jones Greenidge) suggested that Bimshire was ""introduced by an old planter listing it as a county of England"". Expressly named were ""Wiltshire, Hampshire, Berkshire and Bimshire"".[19] Lastly, in the Daily Argosy (of Demerara, i.e. Guyana) of 1652, there is a reference to Bim as a possible corruption of ""Byam"", the name of a Royalist leader against the Parliamentarians. That source suggested the followers of Byam became known as ""Bims"" and that this became a word for all Barbadians.[19]
 Around 700,000 years ago, the island emerged from the ocean as a body of soft rock known as a diapir rose from the mantle beneath its present-day location. This process is still ongoing, raising Barbados at an average rate of 30 centimeters per thousand years.[23] Dozens of inland sea reefs still dominate coastal features within terraces and cliffs on the island.[23]
 Archeological evidence suggests humans may have first settled or visited the island c. 1600 BC.[24][25][26] More permanent Amerindian settlement of Barbados dates to about the 4th to 7th centuries AD, by a group known as the Saladoid-Barrancoid.[27] Settlements of Arawaks from South America appeared by around 800 AD and again in the 12th–13th century.[24] The Kalinago (called ""Caribs"" by the Spanish) visited the island regularly, although there is no evidence of permanent settlement.[28]
 It is uncertain which European nation arrived first in Barbados, which probably would have been at some point in the 15th century or 16th century. One lesser-known source points to earlier revealed works antedating contemporary sources, indicating it could have been the Spanish.[7] Many, if not most, believe the Portuguese, en route to Brazil,[29][30] were the first Europeans to come upon the island. The island was largely ignored by Europeans, though Spanish slave raiding is thought to have reduced the native population, with many fleeing to other islands.[24][31]
 The first English ship, which had arrived on 14 May 1625, was captained by John Powell. The first settlement began on 17 February 1627, near what is now Holetown (formerly Jamestown, after King James I of England),[33] by a group led by John Powell's younger brother, Henry, consisting of 80 settlers and 10 English indentured labourers.[34] Some sources state that some Africans were amongst these first settlers.[24]
 The settlement was established as a proprietary colony and funded by Sir William Courten, a City of London merchant who acquired the title to Barbados and several other islands. The first colonists were actually tenants, and much of the profits of their labour returned to Courten and his company.[35] Courten's title was later transferred to James Hay, 1st Earl of Carlisle, in what was called the ""Great Barbados Robbery"".[citation needed] Carlisle then chose as governor Henry Powell, who established the House of Assembly in 1639, in an effort to appease the planters, who might otherwise have opposed his controversial appointment.[24][36]
 In the period 1640–1660, the West Indies attracted more than two-thirds of the total number of English emigrants to the Americas. By 1650, there were 44,000 settlers in the West Indies, as compared to 12,000 on the Chesapeake and 23,000 in New England. Most English arrivals were indentured. After five years of labour, they were given ""freedom dues"" of about £10, usually in goods. Before the mid-1630s, they also received 5 to 10 acres (2 to 4 hectares) of land, but after that time the island filled and there was no more free land. During the Cromwellian era (1650s) this included a large number of prisoners-of-war, vagrants and people who were illicitly kidnapped, who were forcibly transported to the island and sold as servants. These last two groups were predominantly Irish, as several thousand were infamously rounded up by English merchants and sold into servitude in Barbados and other Caribbean islands during this period, a practice that came to be known as being Barbadosed.[36][37] Cultivation of sugar was thus handled primarily by European indentured labour until it became difficult to bring more indentured servants from England.[38]
 Parish registers from the 1650s show that, for the white population, there were four times as many deaths as marriages. The mainstay of the infant colony's economy was the growth export of tobacco, but tobacco prices eventually fell in the 1630s as Chesapeake production expanded.[36]
 Around the same time, fighting during the War of the Three Kingdoms and the Interregnum spilled over into Barbados and Barbadian territorial waters. The island was not involved in the war until after the execution of Charles I, when the island's government fell under the control of Royalists (ironically the Governor, Philip Bell, remaining loyal to Parliament while the Barbadian House of Assembly, under the influence of Humphrey Walrond, supported Charles II). To try to bring the recalcitrant colony to heel, the Commonwealth Parliament passed an act on 3 October 1650 prohibiting trade between England and Barbados, and because the island also traded with the Netherlands, further Navigation Acts were passed, prohibiting any but English vessels trading with Dutch colonies. These acts were a precursor to the First Anglo-Dutch War. The Commonwealth of England sent an invasion force under the command of Sir George Ayscue, which arrived in October 1651. Ayscue, with a smaller force that included Scottish prisoners, surprised a larger force of Royalists, but had to resort to spying and diplomacy ultimately. On 11 January 1652, the Royalists in the House of Assembly led by Lord Willoughby surrendered, which marked the end of royalist privateering as a major threat.[39] The conditions of the surrender were incorporated into the Charter of Barbados (Treaty of Oistins), which was signed at the Mermaid's Inn, Oistins, on 17 January 1652.[40]
 Starting with Cromwell, a large percentage of the white labourer population were indentured servants and involuntarily transported people from Ireland. Irish servants in Barbados were often treated poorly, and Barbadian planters gained a reputation for cruelty.[41]: 55  The decreased appeal of an indenture on Barbados, combined with enormous demand for labour caused by sugar cultivation, led to the use of involuntary transportation to Barbados as a punishment for crimes, or for political prisoners, and also to the kidnapping of labourers who were deported to Barbados.[41]: 55  Irish indentured servants were a significant portion of the population throughout the period when white servants were used for plantation labour in Barbados, and while a ""steady stream"" of Irish servants entered the Barbados throughout the 17th century, Cromwellian efforts to pacify Ireland created a ""veritable tidal wave"" of Irish labourers who were sent to Barbados during the 1650s.[41]: 56  Due to inadequate historical records, the total number of Irish labourers sent to Barbados is unknown, and estimates have been ""highly contentious"".[41]: 56  While one historical source estimated that as many as 50,000 Irish people were deported to either Barbados or Virginia during the 1650s, this estimate is ""quite likely exaggerated"".[41]: 56  Another estimate that 12,000 Irish prisoners had arrived in Barbados by 1655 has been described as ""probably exaggerated"" by historian Richard B. Sheridan.[42]: 236  According to historian Thomas Bartlett, it is ""generally accepted"" that approximately 10,000 Irish were deported to the West Indies and approximately 40,000 came as voluntary indentured servants, while many also travelled as voluntary, un-indentured emigrants.[43]: 256 
 The introduction of sugar cane from Dutch Brazil in 1640 completely transformed society, the economy and the physical landscape. Barbados eventually had one of the world's biggest sugar industries.[44] One group instrumental in ensuring the early success of the industry was the Sephardic Jews, who had originally been expelled from the Iberian peninsula, to end up in Dutch Brazil.[44] As the effects of the new crop increased, so did the shift in the ethnic composition of Barbados and surrounding islands.[36] The workable sugar plantation required a large investment and a great deal of heavy labour. At first, Dutch traders supplied the equipment, financing, and African slaves, in addition to transporting most of the sugar to Europe.[36][24] In 1644 the population of Barbados was estimated at 30,000, of which about 800 were of African ancestry, with the remainder mainly of English ancestry. These English smallholders were eventually bought out and the island filled up with large sugar plantations worked by African slaves.[24] By 1660 there was near parity with 27,000 Black people and 26,000 White people. By 1666 at least 12,000 white smallholders had been bought out, died, or left the island, many choosing to emigrate to Jamaica or the American Colonies (notably the Carolinas).[24] As a result, Barbados enacted a slave code as a way of legislatively controlling its enslaved Black population.[45] The law's text was influential in laws in other colonies.[46]
 By 1680 there were 20,000 free whites and 46,000 enslaved Africans;[24] by 1724, there were 18,000 free whites and 55,000 enslaved Africans.[36]
 The harsh conditions endured by the slaves resulted in several planned slave rebellions, the largest of which was Bussa's rebellion in 1816 which was rapidly suppressed by the colonial authorities.[24] In 1819, another slave revolt broke out on Easter Day. The revolt was put down in blood, with heads being displayed on stakes. Nevertheless, the brutality of the repression shocked even England and strengthened the abolitionist movement.[47] Growing opposition to slavery led to its abolition in the British Empire in 1833.[24] The plantocracy class retained control of political and economic power on the island, with most workers living in relative poverty.[24]
 The 1780 hurricane killed more than 4,000 people on Barbados.[48][49] In 1854, a cholera epidemic killed more than 20,000 inhabitants.[50]
 Deep dissatisfaction with the situation on Barbados led many to emigrate.[24][51] Things came to a head in the 1930s during the Great Depression, as Barbadians began demanding better conditions for workers, the legalisation of trade unions and a widening of the franchise, which at that point was limited to male property owners.[24] As a result of the increasing unrest the British sent a commission, called the West Indies Royal Commission, or Moyne Commission, in 1938, which recommended enacting many of the requested reforms on the islands.[24] As a result, Afro-Barbadians began to play a much more prominent role in the colony's politics, with universal suffrage being introduced in 1950.[24]
 Prominent among these early activists was Grantley Herbert Adams, who helped found the Barbados Labour Party (BLP) in 1938.[52] He became the first Premier of Barbados in 1953, followed by fellow BLP-founder Hugh Gordon Cummins from 1958 to 1961. A group of left-leaning politicians who advocated swifter moves to independence broke off from the BLP and founded the Democratic Labour Party (DLP) in 1955.[53][54] The DLP subsequently won the 1961 Barbadian general election and their leader Errol Barrow became premier.[citation needed]
 Full internal self-government was enacted in 1961.[24] Barbados joined the short-lived British West Indies Federation from 1958 to 1962, later gaining full independence on 30 November 1966.[24] Errol Barrow became the country's first prime minister. Barbados opted to remain within the Commonwealth of Nations.
 The effect of political independence meant that the United Kingdom government ceased to having sovereignty over Barbados, Elizabeth II, instead, reigning in the country became the Queen of Barbados. The monarch then was represented locally by a governor-general.[55]
 The Barrow government sought to diversify the economy away from agriculture, seeking to boost industry and the tourism sector. Barbados was also at the forefront of regional integration efforts, spearheading the creation of CARIFTA and CARICOM.[24] The DLP lost the 1976 Barbadian general election to the BLP under Tom Adams. Adams adopted a more conservative and strongly pro-Western stance, allowing the Americans to use Barbados as the launchpad for their invasion of Grenada in 1983.[56] Adams died in office in 1985 and was replaced by Harold Bernard St. John; however, St. John lost the 1986 Barbadian general election, which saw the return of the DLP under Errol Barrow, who had been highly critical of the US intervention in Grenada. Barrow, too, died in office, and was replaced by Lloyd Erskine Sandiford, who remained Prime Minister until 1994.[citation needed]
 Owen Arthur of the BLP won the 1994 Barbadian general election, remaining prime minister until 2008.[57] Arthur was a strong advocate of republicanism, though a planned referendum to replace Queen Elizabeth as Head of State in 2008 never took place.[58] The DLP won the 2008 Barbadian general election, but the new Prime Minister David Thompson died in 2010 and was replaced by Freundel Stuart. The BLP returned to power in 2018 under Mia Mottley, who became Barbados's first female prime minister.[59]
 The Government of Barbados announced on 15 September 2020 that it intended to become a republic by 30 November 2021, the 55th anniversary of its independence, resulting in the replacement of the Barbadian monarchy with a president elected through electoral college.[60][61] Barbados would then cease to be a Commonwealth realm, but could maintain membership in the Commonwealth of Nations, like Guyana and Trinidad and Tobago.[62][63][64][65]
 On 20 September 2021, just over a full year after the announcement for the transition was made, the Constitution (Amendment) (No. 2) Bill, 2021 was introduced to the Parliament of Barbados. Passed on 6 October, the Bill made amendments to the Constitution of Barbados, introducing the office of the president of Barbados to replace the role of Elizabeth II as Queen of Barbados.[66] The following week, on 12 October 2021, incumbent Governor-General of Barbados Sandra Mason was jointly nominated by the Prime Minister and Leader of the Opposition as candidate to be the first president of Barbados,[67] and was subsequently elected on 20 October.[68] Mason took office on 30 November 2021.[69] Prince Charles, the heir apparent to the Barbadian Crown at the time, attended the swearing-in ceremony in Bridgetown at the invitation of the Government of Barbados.[70]
 Queen Elizabeth sent a message of congratulations to President Mason and the people of Barbados, saying: ""As you celebrate this momentous day, I send you and all Barbadians my warmest good wishes for your happiness, peace and prosperity in the future.""[71]
 A survey that was conducted between 23 October 2021 and 10 November 2021, by the University of the West Indies showed 34% of respondents being in favour of transitioning to a republic, while 30% were indifferent. Notably, no overall majority was found in the survey; with 24% not indicating a preference and the remaining 12% being opposed to the removal of Queen Elizabeth.[72][73]
 On 20 June 2022, a Constitutional Review Commission was formed and sworn in by Jeffrey Gibson (who at the time was serving temporarily as Acting President of Barbados) to review the Constitution of Barbados.[74]
 The commission was given a 15-month timeline to complete its work, which included consulting the public about the new republic and drafting a constitution.[75] Thus, the CRC engaged the public in a number of public meetings, lectures, and Twitter Spaces.[76] The report was announced delayed by August 2023,[77] with the final report submitted 30 June 2024.[78]
 Barbados is situated in the Atlantic Ocean, east of the other West Indies Islands. Barbados is the easternmost island in the Lesser Antilles. It is 34 kilometres (21 miles) long and up to 23 km (14 mi) wide, covering an area of 439 km2 (169 sq mi).[79] It lies about 168 km (104 mi) east of both the countries of Saint Lucia and Saint Vincent and the Grenadines; 180 km (110 mi) south-east of Martinique and 400 km (250 mi) north-east of Trinidad and Tobago. It is flat in comparison to its island neighbours to the west, the Windward Islands. The island rises gently to the central highland region known as Scotland District, with the highest point being Mount Hillaby 340 m (1,120 ft) above sea level.[24]
 In Barbados forest cover is around 15% of the total land area, equivalent to 6,300 hectares (ha) of forest in 2020, which was unchanged from 1990. In 2020, naturally regenerating forest covered 6,300 hectares (ha) and planted forest covered 0 hectares (ha). Of the naturally regenerating forest 0% was reported to be primary forest (consisting of native tree species with no clearly visible indications of human activity) and around 5% of the forest area was found within protected areas. For the year 2015, 1% of the forest area was reported to be under public ownership, 0% private ownership and 99% with ownership listed as other or unknown.[80][81]
 In the parish of Saint Michael lies Barbados's capital and main city, Bridgetown, containing one third of the country's population.[24] Other major towns scattered across the island include Holetown, in the parish of Saint James; Oistins, in the parish of Christ Church; and Speightstown, in the parish of Saint Peter.[citation needed]
 Barbados lies on the boundary of the South American and the Caribbean Plates.[82] The subduction of the South American Plate beneath the Caribbean Plate scrapes sediment from the South American Plate and deposits it above the subduction zone forming an accretionary prism. The rate of this depositing of material allows Barbados to rise at a rate of about 25 mm (1 in) per 1,000 years.[83] This subduction means geologically the island is composed of coral roughly 90 m (300 ft) thick, where reefs formed above the sediment. The land slopes in a series of ""terraces"" in the west and goes into an incline in the east. A large proportion of the island is circled by coral reefs.[24]
 The erosion of limestone in the northeast of the island, in the Scotland District, has resulted in the formation of various caves and gullies. On the Atlantic east coast of the island coastal landforms, including stacks, have been created due to the limestone composition of the area. Also notable in the island is the rocky cape known as Pico Teneriffe[84] or Pico de Tenerife, which is named after the fact that the island of Tenerife in Spain is the first land east of Barbados according to the belief of the locals.[citation needed]
 The country generally experiences two seasons, one of which includes noticeably higher rainfall. Known as the ""wet season"", this period runs from June to December. By contrast, the ""dry season"" runs from December to May. Annual precipitation ranges between 1,000 and 2,300 mm (40 and 90 in).
From December to May the average temperatures range from 21 to 31 °C (70 to 88 °F), while between June and November, they range from 23 to 31 °C (73 to 88 °F).[85]
 On the Köppen climate classification scale, much of Barbados is regarded as a tropical monsoon climate (Am). However, breezes of 12 to 16 km/h (7 to 10 mph) abound throughout the year and give Barbados a climate which is moderately tropical.[citation needed]
 Infrequent natural hazards include earthquakes, landslips, and hurricanes. Barbados lies outside the Main Development Region for tropical cyclone activity in the Atlantic, and is often spared the worst effects of the region's storms during the rainy season. On average, a major hurricane makes landfall in Barbados about once every 26 years. The last significant hit from a hurricane to cause severe damage to Barbados was Hurricane Janet in 1955; in 2010 the island was struck by Hurricane Tomas, but this caused only minor damage across the country as it was only at Tropical Storm strength at the time of impact.[86]
 Barbados is susceptible to environmental pressures. As one of the world's most densely populated isles, the government worked during the 1990s[87] to aggressively integrate the growing south coast of the island into the Bridgetown Sewage Treatment Plant to reduce contamination of offshore coral reefs.[88][89] As of the first decade of the 21st century, a second treatment plant has been proposed along the island's west coast. Being so densely populated, Barbados has made great efforts to protect its underground aquifers.[90]
 As a coral-limestone island, Barbados is highly permeable to seepage of surface water into the earth. The government has placed great emphasis on protecting the catchment areas that lead directly into the huge network of underground aquifers and streams.[90] On occasion illegal squatters have breached these areas, and the government has removed squatters to preserve the cleanliness of the underground springs which provide the island's drinking water.[91]
 The government has placed a huge emphasis on keeping Barbados clean with the aim of protecting the environment and preserving offshore coral reefs which surround the island.[92] Many initiatives to mitigate human pressures on the coastal regions of Barbados and seas come from the Coastal Zone Management Unit (CZMU).[93][94] Barbados has nearly 90 kilometres (56 miles) of coral reefs just offshore and two protected marine parks have been established off the west coast.[95] Overfishing is another threat which faces Barbados.[96]
 Although on the opposite side of the Atlantic, and some 4,800 kilometres (3,000 miles) west of Africa, Barbados is one of many places in the American continent that experience heightened levels of mineral dust from the Sahara Desert.[97] Some particularly intense dust episodes have been blamed partly for the impacts on the health of coral reefs[98] surrounding Barbados or asthmatic episodes,[99] but evidence has not wholly supported the former claim.[100]
 Access to biocapacity in Barbados is much lower than world average. In 2016, Barbados had 0.17 global hectares[101] of biocapacity per person within its territory, much less than the world average of 1.6 global hectares per person.[102] In 2016 Barbados used 0.84 global hectares of biocapacity per person - their ecological footprint of consumption. This means they use approximately five times as much biocapacity as Barbados contains. As a result, Barbados is running a biocapacity deficit.[101]
 Barbados is host to four species of nesting turtles (green turtles, loggerheads, hawksbill turtles, and leatherbacks) and has the second-largest hawksbill turtle-breeding population in the Caribbean.[103] The driving of vehicles on beaches can crush nests buried in the sand and such activity is discouraged in nesting areas.[104]
 Barbados is also the host to the green monkey. The green monkey is found in West Africa from Senegal to the Volta River. It has been introduced to the Cape Verde islands off north-western Africa, and the West Indian islands of Saint Kitts, Nevis, Saint Martin, and Barbados. It was introduced to the West Indies in the late 17th century when slave trade ships travelled to the Caribbean from West Africa.  The green monkey is considered a very curious and mischievous/troublesome animal by locals.[105][106]
 The 2010 national census conducted by the Barbados Statistical Service reported a resident population of 277,821, of which 144,803 were female and 133,018 were male.[107]
 The life expectancy for Barbados residents as of 2020[update] is 80 years. The average life expectancy is 83 years for females and 79 years for males (2020).[1] Barbados and Japan have the highest per capita occurrences of centenarians in the world.[108]
 The crude birth rate is 12.23 births per 1,000 people, and the crude death rate is 8.39 deaths per 1,000 people. The infant mortality rate was 11.057 infant deaths per 1,000 live births in 2021, according to UNICEF.[109][110]
 Close to 90% of all Barbadians (also known colloquially as ""Bajan"") are of Afro-Caribbean ancestry (""Afro-Bajans"") and mixed ancestry. The remainder of the population includes groups of Europeans (""Anglo-Bajans"" / ""Euro-Bajans"") mainly from the United Kingdom, Ireland, Germany, and Italy.[citation needed] Other European groups consisted of the French, Austrians, Spaniards, and Russians. Asians, predominantly from Hong Kong and India (both Hindu and Muslim) make up less than 1% of the population.
Other groups in Barbados include people from the United States and Canada. Barbadians who return after years of residence in the United States and children born in America to Bajan parents are called ""Bajan Yankees"", a term considered derogatory by some.[111] Generally, Bajans recognise and accept all ""children of the island"" as Bajans, and refer to each other as such.[citation needed]
 The biggest communities outside the Afro-Caribbean community are:
 English is the official language of Barbados, and is used for communications, administration, and public services all over the island. In its capacity as the official language of the country, the standard of English tends to conform to vocabulary, pronunciations, spellings, and conventions akin to, but not exactly the same as, those of British English. For most people, however, Bajan Creole is the language of everyday life. It does not have a standardised written form, but it is used by over 90% of the population.[citation needed]
 Christianity is the largest religion in Barbados, with the largest denomination being Anglican (23.9% of the population in 2019).[121] Other Christian denominations with significant followings in Barbados are the Catholic Church (administered by Roman Catholic Diocese of Bridgetown), Pentecostals (19.5%), Jehovah's Witnesses, the Seventh-day Adventist Church and Spiritual Baptists.[24] The Church of England was the official state religion until its legal disestablishment by the Parliament of Barbados following independence.[121][122] As of 2019, 21% of Barbadians report having no religion, making the non-religious the second largest group after Anglicans.[123] Smaller religions in Barbados include Hinduism, Islam, the Baháʼí Faith,[124] and Judaism.[24]
 The state is considered secular, guaranteeing freedom of religion or belief to all and featuring only symbolic allusions to a higher power in the preamble to the constitution.[123]
 Barbados has been an independent country since 30 November 1966.[125] It functions as a parliamentary republic modelled on the British Westminster system. The head of state is the President of Barbados – presently Sandra Mason – elected by the Parliament of Barbados for a term of four years, and advised on matters of the Barbadian state by the Prime Minister of Barbados, who is head of government. There are 30 representatives within the House of Assembly, the lower chamber of Parliament. In the Senate, the upper chamber of Parliament, there are 21 senators.[126]
 The Constitution of Barbados is the supreme law of the country.[127] Legislation is passed by the Parliament of Barbados but does not have the force of law unless the President grants her assent to that law. The right to withhold assent is absolute and cannot be overridden by Parliament.[128] The Attorney General heads the independent judiciary.[citation needed]
 During the 1990s, at the suggestion of Trinidad and Tobago's Patrick Manning, Barbados attempted a political union with Trinidad and Tobago and Guyana. The project stalled after the then prime minister of Barbados, Lloyd Erskine Sandiford, became ill and his Democratic Labour Party lost the next general election.[129][130] Barbados continues to share close ties with Trinidad and Tobago and with Guyana, claiming the highest number of Guyanese immigrants after the United States, Canada and the United Kingdom.
 Barbados is a party to the Rome Statute of the International Criminal Court.[131]
 Barbados functions as a two-party system. The dominant political parties are the Democratic Labour Party and the incumbent Barbados Labour Party. Since independence on 30 November 1966, the Democratic Labour Party (DLP) has governed from 1966 to 1976; 1986 to 1994; and from 2008 to 2018; and the Barbados Labour Party (BLP) has governed from 1976 to 1986; 1994 to 2008; and from 2018 to present.[citation needed]
 Barbados follows a policy of nonalignment and seeks cooperative relations with all friendly states. Barbados is a full and participating member of the Caribbean Community (CARICOM), CARICOM Single Market and Economy (CSME), the Association of Caribbean States (ACS),[132] the Organization of American States (OAS), the Commonwealth of Nations, and the Caribbean Court of Justice (CCJ). In 2005, Barbados replaced the Judicial Committee of the Privy Council with the Caribbean Court of Justice as its final court of appeal.[133]
 Barbados has been a member of The Forum of Small States (FOSS) since the group's founding in 1992.[134]
 Barbados is an original member (1995) of the World Trade Organization (WTO) and participates actively in its work. It grants at least MFN treatment to all its trading partners. European Union relations and cooperation with Barbados are carried out both on a bilateral and a regional basis. Barbados is party to the Cotonou Agreement, through which, As of December 2007[update], it is linked by an Economic Partnership Agreement with the European Commission. The pact involves the Caribbean Forum (CARIFORUM) subgroup of the African, Caribbean and Pacific Group of States (ACP). CARIFORUM is the only part of the wider ACP-bloc that has concluded the full regional trade-pact with the European Union. There are also ongoing EU-Community of Latin American and Caribbean States (CELAC) and EU-CARIFORUM dialogues.[135]
 Trade policy has also sought to protect a small number of domestic activities, mostly food production, from foreign competition, while recognising that most domestic needs are best met by imports.[136]
 On 6 July 1994, at the Sherbourne Conference Centre, St. Michael, Barbados, representatives of eight countries signed the Double Taxation Relief (CARICOM) Treaties 1994. The countries which were represented were: Antigua and Barbuda, Belize, Grenada, Jamaica, St. Kitts and Nevis, St. Lucia, St. Vincent and the Grenadines, and Trinidad and Tobago.[137]
 On 19 August 1994, a representative of the Government of Guyana signed a similar treaty.[138]
 The Barbados Defence Force has roughly 800 members. Within it, service members aged 14 to 18 years make up the Barbados Cadet Corps. The defence preparations of the island nation are closely tied to defence treaties with the United Kingdom, the United States, the People's Republic of China,[139] and other eastern Caribbean countries.
 The Barbados Police Service is the sole law enforcement agency on the island of Barbados.[140]
 Barbados is divided into 11 parishes:
 Barbados is the 52nd richest country in the world in terms of GDP (Gross Domestic Product) per capita,[141] with a well-developed mixed economy and a moderately high standard of living. According to the World Bank, Barbados is one of 83 high income economies in the world.[142] Despite this, a 2012 self-study in conjunction with the Caribbean Development Bank revealed that 20% of Barbadians live in poverty and nearly 10% cannot meet their basic daily food needs.[143] Barbados was ranked 77th in the Global Innovation Index in 2024.[144]
 Historically, the economy of Barbados was dependent on sugarcane cultivation and related activities, but since the late 1970s and early 1980s it has diversified into the manufacturing and tourism sectors.[24] Offshore finance and information services have become important foreign exchange earners.[145]
 Partly due to the staging of the 2007 Cricket World Cup, the island saw a construction boom, with the development and redevelopment of hotels, office complexes, and homes.[146] This slowed during the 2008 to 2012 world economic crisis and the recession.[147]
 The economy was strong between 1999 and 2000 but entered a recession in 2001 and 2002 due to decreases in tourism and consumer spending and the impact of the 11 September 2001 attacks in the United States and the 7 July 2005 London bombings in the United Kingdom. The economy rebounded in 2003 and grew from 2004 to 2008. The economy went into recession again from 2008 to 2013 before growing from 2014 to 2017. Then it declined to another recession from 2017 to 2019 during the world economic crisis. There were 23 downgrades by both Standard & Poor's and Moody's in 2016, 2017 and 2018. The economy showed signs of recovery with 3 upgrades from Standard and Poor's and Moody's in 2019. From 1 January to 31 March 2020 the economy had started to grow, but then it experienced another decline due to the COVID-19 economic recession.[citation needed]
 Traditional trading partners include Canada, the Caribbean Community (especially Trinidad and Tobago), the United Kingdom, and the United States. Recent government administrations have continued efforts to reduce unemployment, encourage foreign direct investment, and privatise remaining state-owned enterprises. Unemployment dropped to 10.7% in 2003.[1] However, it has since increased to 11.9% in second quarter, 2015.[148]
 The European Union is assisting Barbados with a €10 million program of modernisation of the country's International Business and Financial Services Sector.[149]
 Barbados maintains the third largest stock exchange in the Caribbean region. As of 2009[update], officials at the stock exchange were investigating the possibility of augmenting the local exchange with an International Securities Market (ISM) venture.[150]
 By May 2018, Barbados's outstanding debt had climbed to US$7.5 billion, more than 1.7 times the country's GDP. In June 2018 the government defaulted on its sovereign debt when it failed to make a coupon on Eurobonds maturing in 2035. Outstanding bond debt of Barbados reached US$4.4 billion.[151]
 In October 2019, Barbados concluded restructuring negotiations with a creditor group including investments funds Eaton Vance Management, Greylock Capital Management, Teachers Advisors, and Guyana Bank for Trade and Industry. Creditors will exchange existing bonds for a new debt series maturing in 2029. The new bonds involve a principal ""haircut"" of approximately 26% and include a clause allowing for deferment of principal and capitalization of interest in the event of a natural disaster.[152][153]
 The main hospital on the island is the Queen Elizabeth Hospital; however, Barbados has eight polyclinics across five parishes. There are also well-known medical care centres in Barbados such as Bayview Hospital, Sandy Crest Medical Centre and FMH Emergency Medical Clinic.[154]
 The Barbados literacy rate is ranked close to 100%.[155][24] The mainstream public education system of Barbados is fashioned after the British model. The government of Barbados spends 6.7% of its GDP on education (2008).[1]
 All young people in the country must attend school until age 16. Barbados has over 70 primary schools and over 20 secondary schools throughout the island. There are a number of private schools, including those offering Montessori and International Baccalaureate education. Student enrolment at these schools represents less than 5% of the total enrolment of the public schools.[citation needed]
 Certificate-, diploma- and degree-level education in the country is provided by the Barbados Community College, the Samuel Jackman Prescod Institute of Technology, Codrington College, and the Cave Hill campus and Open Campus of the University of the West Indies. Barbados is also home to several overseas medical schools, such as Ross University School of Medicine and the American University of Integrative Sciences, School of Medicine.[citation needed]
 Barbados Secondary School Entrance Examination: Children who are between the ages of 11 years old and 12 years old on August 31st in the year of the examination are required to write the examination as a means of allocation to secondary school.[156]
 Caribbean Secondary Education Certificate (CSEC) examinations are usually taken by students after five years of secondary school and mark the end of standard secondary education. The CSEC examinations are equivalent to the Ordinary Level (O-Levels) examinations and are targeted toward students 16 and older.[157]
 Caribbean Advanced Proficiency Examinations (CAPE) are taken by students who have completed their secondary education and wish to continue their studies. Students who sit for the CAPE usually possess CSEC or an equivalent certification. The CAPE is equivalent to the British Advanced Levels (A-Levels), voluntary qualifications that are intended for university entrance.[158]
 Barbados is a blend of West African, Portuguese, Creole, Indian and British cultures. Citizens are officially called Barbadians, but are colloquially known as ""Bajans"" (pronounced ˈbājənz). This term evolved from ""Badian"" (a shortening of ""Barbadian"") during the 19th-century.[159][160]
 The largest carnival-like cultural event that takes place on the island is the Crop Over festival, which was established in 1974. As in many other Caribbean and Latin American countries, Crop Over is an important event for many people on the island, as well as the thousands of tourists that flock to there to participate in the annual events.[24] The festival includes musical competitions and other traditional activities, and features the majority of the island's homegrown calypso and soca music for the year. The male and female Barbadians who harvested the most sugarcane are crowned as the King and Queen of the crop.[161] Crop Over gets under way at the beginning of July and ends with the costumed parade on Kadooment Day, held on the first Monday of August. New calypso/soca music is usually released and played more frequently from the beginning of May to coincide with the start of the festival.[citation needed]
 Barbadian art has evolved over the centuries, influenced by the island's complex history, which includes Indigenous cultures, colonial periods, and the subsequent emergence of a vibrant post-colonial identity. The interplay of African, European, and Caribbean influences has given rise to a unique artistic heritage that continues to inspire contemporary artists.[citation needed]
 The latter part of the 20th century and into the 21st century witnessed a cultural renaissance in Barbadian art now documented by Raskal Magazine. Artists began to explore diverse mediums and techniques, blending traditional practices with contemporary expressions. This period of experimentation contributed to the dynamic and multifaceted nature of Barbadian art, reflecting the island's openness to cultural exchange and adaptation.
 Barbadian artists, mindful of their place within the global art community, began to engage with international artistic trends. This global perspective led to a cross-pollination of ideas, as artists drew inspiration from diverse sources while simultaneously contributing to the broader discourse on Caribbean and diasporic art.
 Bajan cuisine is a mixture of African, Indian, Irish, Creole and British influences. A typical meal consists of a main dish of meat or fish, normally marinated with a mixture of herbs and spices, hot side dishes, and one or more salads. A common Bajan side dish could be pickled cucumber, fish cakes, bake, etc. The meal is usually served with one or more sauces.[162] The national dish of Barbados is cou-cou and flying fish with spicy gravy.[163] Another traditional meal is pudding and souse, a dish of pickled pork with spiced sweet potatoes.[164] A wide variety of seafood and meats are also available.[citation needed]
 The Mount Gay Rum visitor's centre in Barbados claims to be the world's oldest remaining rum company, with the earliest confirmed deed from 1703. Cockspur Rum and Malibu are also from the island. Barbados is home to the Banks Barbados Brewery, which brews Banks Beer, a pale lager, as well as Banks Amber Ale.[165] Banks also brews Tiger Malt, a non-alcoholic malted beverage. 10 Saints beer is brewed in Speightstown, St. Peter in Barbados and aged for 90 days in Mount Gay 'Special Reserve' Rum casks. It was first brewed in 2009 and is available in certain Caricom nations.[166]
 In 2009, Rihanna was appointed as an Honorary Ambassador of Youth and Culture for Barbados by the late Prime Minister David Thompson.[167]
 As in other Caribbean countries of British colonial heritage, cricket is very popular on the island. The West Indies cricket team usually includes several Barbadian players. In addition to several warm-up, group stage and few ""Super Eight"" matches, the country hosted the final of the 2007 Cricket World Cup and 2024 ICC Men's T20 World Cup. Barbados has produced many great cricketers including Sir Garfield Sobers, Sir Frank Worrell, Sir Clyde Walcott, Sir Everton Weekes, Gordon Greenidge, Wes Hall, Charlie Griffith, Joel Garner, Desmond Haynes and Malcolm Marshall.[168]
 In Track and Field, sprinter Obadele Thompson won a bronze medal in the 100m at the 2000 Summer Olympic Games. As of August 2022, he was the first Olympics medalist in the Barbados.[169]
 Ryan Brathwaite[170] won a gold medal in the 110 metres hurdles at the 2009 World Championships in Athletics in Berlin.[171]
 Rugby is also popular in Barbados.[172]
 Horse racing takes place at the Historic Garrison Savannah close to Bridgetown. Spectators can pay for admission to the stands. Admission to the Grand Stand costs between US$2.50 and US$5.00.[173]
 Basketball is an increasingly popular sport, played at school or college. The Barbados men's national team has additionally shown some international success, including a fifth-place finish in the 2006 Commonwealth Games.[174]
 Polo is very popular amongst the rich elite on the island and the ""High-Goal"" Apes Hill team is based at the St James's Club.[175]
 In golf, the Barbados Open, played at Royal Westmoreland Golf Club, was an annual stop on the European Seniors Tour from 2000 to 2009. In December 2006 the WGC-World Cup took place at the country's Sandy Lane resort on the Country Club course, an 18-hole course designed by Tom Fazio. The Barbados Golf Club is another course on the island.
 Volleyball is also popular and is mainly played indoors.[176]
 Tennis is gaining popularity and Barbados is home to Darian King, who has achieved a career-high ranking of 106 in May 2017 and has played in the 2016 Summer Olympics and the 2017 US Open.[177]
 Motorsports also play a role, with Rally Barbados occurring each summer and being listed on the FIA NACAM calendar. Also, the Bushy Park Circuit hosted the Race of Champions in 2014.[178]
 The presence of the trade winds along with favourable swells make the southern tip of the island an ideal location for wave sailing (an extreme form of the sport of windsurfing).
 Barbados also hosts several international surfing competitions.[179]
 Netball is also popular with women in Barbados.[180]
 Several players in the National Football League (NFL) are from Barbados, including Robert Bailey, Roger Farmer, Elvis Joseph, Ramon Harewood and Sam Seale.[181]
 Although Barbados is about 34 km (21 mi) across at its widest point, a car journey from Six Cross Roads in St. Philip (south-east) to North Point in St. Lucy (north-central) can take one and a half hours or longer due to traffic. Barbados has half as many registered cars as citizens. In Barbados, drivers drive on the left side of the road.[182]
 Barbados is known for its many roundabouts. One famous roundabout is located east of Bridgetown and holds the Emancipation Statue of the slave Bussa.[183]
 Transport on the island is relatively convenient with ""route taxis"" called ""ZRs"" (pronounced ""Zed-Rs"") travelling to most points on the island. These small buses can at times be crowded, as passengers are generally never turned down regardless of the number. They will usually take the more scenic routes to destinations. They generally depart from the capital Bridgetown or from Speightstown in the northern part of the island.[184]
 Including the ZRs, there are three bus systems running seven days a week (though less frequently on Sundays). There are ZRs, the yellow minibuses and the blue Transport Board buses. A ride on any of them costs Bds$ 3.5.[185] The smaller buses from the two privately owned systems (""ZRs"" and ""minibuses"") can give change; the larger blue buses from the government-operated Barbados Transport Board system cannot, but do give receipts. The Barbados Transport Board buses travel in regular bus routes and scheduled timetables across Barbados. Schoolchildren in school uniform including some Secondary schools ride for free on the government buses and for Bds$ 2.5 on the ZRs. Most routes require a connection in Bridgetown. Barbados Transport Board's headquarters are located at Kay's House, Roebuck Street, St. Michael, and the bus depots and terminals are located in the Fairchild Street Bus Terminal in Fairchild Street and the Princess Alice Bus Terminal (which was formerly the Lower Green Bus Terminal in Jubilee Gardens, Bridgetown, St. Michael) in Princess Alice Highway, Bridgetown, St. Michael; the Speightstown Bus Terminal in Speightstown, St. Peter; the Oistins Bus Depot in Oistins, Christ Church; and the Mangrove Bus Depot in Mangrove, St. Philip. In July 2020, the Barbados Transport Board received 33 BYD electric buses which were obtained not only to add to the aging fleet of diesel buses but also to assist the Government in their goal of eliminating the use of fossil fuels by 2030.[186][187]
 Some hotels also provide visitors with shuttles to points of interest on the island from outside the hotel lobby. There are several locally owned and operated vehicle rental agencies in Barbados but there are no multi-national companies.[citation needed]
 The island's lone airport is the Grantley Adams International Airport. It receives daily flights by several major airlines from points around the globe,[188][189] as well as several smaller regional commercial airlines and charters.[190][191] The airport serves as a southern air-transportation hub for the Caribbean.[192] It underwent a US$100 million upgrade and expansion from 2003 to 2006.[193] In 2023, it began conversion of its former Concorde terminal and museum to a new departure terminal,[194] and in December 2023, Prime Minister Mia Mottley announced the negotiations for a  US$300 million for additional airport development.[195]
 The Bridgetown seaport is the primary port of call for commercial container and cruise traffic.[196][197]
 There was also a helicopter shuttle service, which offered air taxi services to a number of sites around the island, mainly on the West Coast tourist belt. Air and maritime traffic was regulated by the Barbados Port Authority.[citation needed]
 13°10′12″N 59°33′09″W﻿ / ﻿13.17000°N 59.55250°W﻿ / 13.17000; -59.55250
"
Tuberculosis,https://en.wikipedia.org/wiki/Tuberculosis,"



 Tuberculosis (TB), also known colloquially as the ""white death"", or historically as consumption,[7] is a contagious disease usually caused by Mycobacterium tuberculosis (MTB) bacteria.[1] Tuberculosis generally affects the lungs, but it can also affect other parts of the body.[1] Most infections show no symptoms, in which case it is known as latent tuberculosis.[1] Around 10% of latent infections progress to active disease that, if left untreated, kill about half of those affected.[1] Typical symptoms of active TB are chronic cough with blood-containing mucus, fever, night sweats, and weight loss.[1] Infection of other organs can cause a wide range of symptoms.[8]
 Tuberculosis is spread from one person to the next through the air when people who have active TB in their lungs cough, spit, speak, or sneeze.[1][9] People with latent TB do not spread the disease.[1] Active infection occurs more often in people with HIV/AIDS and in those who smoke.[1] Diagnosis of active TB is based on chest X-rays, as well as microscopic examination and culture of bodily fluids.[10] Diagnosis of latent TB relies on the tuberculin skin test (TST) or blood tests.[10]
 Prevention of TB involves screening those at high risk, early detection and treatment of cases, and vaccination with the bacillus Calmette-Guérin (BCG) vaccine.[3][4][5] Those at high risk include household, workplace, and social contacts of people with active TB.[4] Treatment requires the use of multiple antibiotics over a long period of time.[1] Antibiotic resistance is a growing problem, with increasing rates of multiple drug-resistant tuberculosis (MDR-TB).[1]
 In 2018, one quarter of the world's population was thought to have a latent infection of TB.[6] New infections occur in about 1% of the population each year.[11] In 2022, an estimated 10.6 million people developed active TB, resulting in 1.3 million deaths, making it the second leading cause of death from an infectious disease after COVID-19.[1] As of 2018, most TB cases occurred in the WHO regions of South-East Asia (44%), Africa (24%), and the Western Pacific (18%), with more than 50% of cases being diagnosed in seven countries: India (27%), China (9%), Indonesia (8%), the Philippines (6%), Pakistan (6%), Nigeria (4%), and Bangladesh (4%).[12] By 2021, the number of new cases each year was decreasing by around 2% annually.[1] About 80% of people in many Asian and African countries test positive, while 5–10% of people in the United States test positive via the tuberculin test.[13] Tuberculosis has been present in humans since ancient times.[14]
 Tuberculosis has existed since antiquity.[14] The oldest unambiguously detected M. tuberculosis gives evidence of the disease in the remains of bison in Wyoming dated to around 17,000 years ago.[15] However, whether tuberculosis originated in bovines, then transferred to humans, or whether both bovine and human tuberculosis diverged from a common ancestor, remains unclear.[16] A comparison of the genes of M. tuberculosis complex (MTBC) in humans to MTBC in animals suggests humans did not acquire MTBC from animals during animal domestication, as researchers previously believed. Both strains of the tuberculosis bacteria share a common ancestor, which could have infected humans even before the Neolithic Revolution.[17] Skeletal remains show some prehistoric humans (4000 BC) had TB, and researchers have found tubercular decay in the spines of Egyptian mummies dating from 3000 to 2400 BC.[18] Genetic studies suggest the presence of TB in the Americas from about AD 100.[19]
 Before the Industrial Revolution, folklore often associated tuberculosis with vampires. When one member of a family died from the disease, the other infected members would lose their health slowly. People believed this was caused by the original person with TB draining the life from the other family members.[20]
 Although Richard Morton established the pulmonary form associated with tubercles as a pathology in 1689,[21][22] due to the variety of its symptoms, TB was not identified as a single disease until the 1820s.  Benjamin Marten conjectured in 1720 that consumptions were caused by microbes which were spread by people living close to each other.[23] In 1819, René Laennec claimed that tubercles were the cause of pulmonary tuberculosis.[24] J. L. Schönlein first published the name ""tuberculosis"" (German:  Tuberkulose) in 1832.[25][26]
 Between 1838 and 1845, John Croghan, the owner of Mammoth Cave in Kentucky from 1839 onwards, brought a number of people with tuberculosis into the cave in the hope of curing the disease with the constant temperature and purity of the cave air; each died within a year.[27] Hermann Brehmer opened the first TB sanatorium in 1859 in Görbersdorf (now Sokołowsko) in Silesia.[28] In 1865, Jean Antoine Villemin demonstrated that tuberculosis could be transmitted, via inoculation, from humans to animals and among animals.[29] (Villemin's findings were confirmed in 1867 and 1868 by John Burdon-Sanderson.[30])
 Robert Koch identified and described the bacillus causing tuberculosis, M. tuberculosis, on 24 March 1882.[31][32] In 1905, he was awarded the Nobel Prize in Physiology or Medicine for this discovery.[33]
 In Europe, rates of tuberculosis began to rise in the early 1600s to a peak level in the 1800s, when it caused nearly 25% of all deaths.[34] In the 18th and 19th century, tuberculosis had become epidemic in Europe, showing a seasonal pattern.[35][36] Tuberculosis caused widespread public concern in the 19th and early 20th centuries as the disease became common among the urban poor. In 1815, one in four deaths in England was due to ""consumption"". By 1918, TB still caused one in six deaths in France.[citation needed]
 After TB was determined to be contagious, in the 1880s, it was put on a notifiable-disease list in Britain. Campaigns started to stop people from spitting in public places, and the infected poor were ""encouraged"" to enter sanatoria that resembled prisons. The sanatoria for the middle and upper classes offered excellent care and constant medical attention.[28] What later became known as the Alexandra Hospital for Children with Hip Disease (tuberculous arthritis) was opened in London in 1867.[37] Whatever the benefits of the ""fresh air"" and labor in the sanatoria, even under the best conditions, 50% of those who entered died within five years (c. 1916).[28]
 Robert Koch did not believe the cattle and human tuberculosis diseases were similar, which delayed the recognition of infected milk as a source of infection. During the first half of the 1900s, the risk of transmission from this source was dramatically reduced after the application of the pasteurization process. Koch announced a glycerine extract of the tubercle bacilli as a ""remedy"" for tuberculosis in 1890, calling it ""tuberculin"". Although it was not effective, it was later successfully adapted as a screening test for the presence of pre-symptomatic tuberculosis.[38] World Tuberculosis Day is marked on 24 March each year, the anniversary of Koch's original scientific announcement. When the Medical Research Council formed in Britain in 1913, it initially focused on tuberculosis research.[39]
 Albert Calmette and Camille Guérin achieved the first genuine success in immunization against tuberculosis in 1906, using attenuated bovine-strain tuberculosis. It was called bacille Calmette–Guérin (BCG). The BCG vaccine was first used on humans in 1921 in France,[40] but achieved widespread acceptance in the US, Great Britain, and Germany only after World War II.[41]
 By the 1950s mortality in Europe had decreased about 90%.[42] Improvements in sanitation, vaccination, and other public-health measures began significantly reducing rates of tuberculosis even before the arrival of streptomycin and other antibiotics, although the disease remained a significant threat.[42] In 1946, the development of the antibiotic streptomycin made effective treatment and cure of TB a reality. Prior to the introduction of this medication, the only treatment was surgical intervention, including the ""pneumothorax technique"", which involved collapsing an infected lung to ""rest"" it and to allow tuberculous lesions to heal.[43]
 Because of the emergence of multidrug-resistant tuberculosis (MDR-TB), surgery has been re-introduced for certain cases of TB infections. It involves the removal of infected chest cavities (""bullae"") in the lungs to reduce the number of bacteria and to increase exposure of the remaining bacteria to antibiotics in the bloodstream.[44] Hopes of eliminating TB ended with the rise of drug-resistant strains in the 1980s. The subsequent resurgence of tuberculosis resulted in the declaration of a global health emergency by the World Health Organization (WHO) in 1993.[45]
 There is a popular misconception that tuberculosis is purely a disease of the lungs that manifests as coughing.[47] Tuberculosis may infect many organs, even though it most commonly occurs in the lungs (known as pulmonary tuberculosis).[8] Extrapulmonary TB occurs when tuberculosis develops outside of the lungs, although extrapulmonary TB may coexist with pulmonary TB.[8]
 General signs and symptoms include fever, chills, night sweats, loss of appetite, weight loss, and fatigue.[8] Significant nail clubbing may also occur.[48]
 If a tuberculosis infection does become active, it most commonly involves the lungs (in about 90% of cases).[14][49] Symptoms may include chest pain and a prolonged cough producing sputum. About 25% of people may not have any symptoms (i.e., they remain asymptomatic).[14] Occasionally, people may cough up blood in small amounts, and in very rare cases, the infection may erode into the pulmonary artery or a Rasmussen aneurysm, resulting in massive bleeding.[8][50] Tuberculosis may become a chronic illness and cause extensive scarring in the upper lobes of the lungs. The upper lung lobes are more frequently affected by tuberculosis than the lower ones.[8] The reason for this difference is not clear.[13] It may be due to either better air flow,[13] or poor lymph drainage within the upper lungs.[8]
 In 15–20% of active cases, the infection spreads outside the lungs, causing other kinds of TB.[51] These are collectively denoted as extrapulmonary tuberculosis.[52] Extrapulmonary TB occurs more commonly in people with a weakened immune system and young children. In those with HIV, this occurs in more than 50% of cases.[52] Notable extrapulmonary infection sites include the pleura (in tuberculous pleurisy), the central nervous system (in tuberculous meningitis), the lymphatic system (in scrofula of the neck), the genitourinary system (in urogenital tuberculosis), and the bones and joints (in Pott disease of the spine), among others. A potentially more serious, widespread form of TB is called ""disseminated tuberculosis""; it is also known as miliary tuberculosis.[8] Miliary TB currently makes up about 10% of extrapulmonary cases.[53]
 The main cause of TB is Mycobacterium tuberculosis (MTB), a small, aerobic, nonmotile bacillus.[8] The high lipid content of this pathogen accounts for many of its unique clinical characteristics.[54] It divides every 16 to 20 hours, which is an extremely slow rate compared with other bacteria, which usually divide in less than an hour.[55] Mycobacteria have an outer membrane lipid bilayer.[56] If a Gram stain is performed, MTB either stains very weakly ""Gram-positive"" or does not retain dye as a result of the high lipid and mycolic acid content of its cell wall.[57] MTB can withstand weak disinfectants and survive in a dry state for weeks. In nature, the bacterium can grow only within the cells of a host organism, but M. tuberculosis can be cultured in the laboratory.[58]
 Using histological stains on expectorated samples from phlegm (also called sputum), scientists can identify MTB under a microscope. Since MTB retains certain stains even after being treated with acidic solution, it is classified as an acid-fast bacillus.[13][57] The most common acid-fast staining techniques are the Ziehl–Neelsen stain[59] and the Kinyoun stain, which dye acid-fast bacilli a bright red that stands out against a blue background.[60] Auramine-rhodamine staining[61] and fluorescence microscopy[62] are also used.
 The M. tuberculosis complex (MTBC) includes four other TB-causing mycobacteria: M. bovis, M. africanum, M. canettii, and M. microti.[63] M. africanum is not widespread, but it is a significant cause of tuberculosis in parts of Africa.[64][65] M. bovis was once a common cause of tuberculosis, but the introduction of pasteurized milk has almost eliminated this as a public health problem in developed countries.[13][66] M. canettii is rare and seems to be limited to the Horn of Africa, although a few cases have been seen in African emigrants.[67][68] M. microti is also rare and is seen almost only in immunodeficient people, although its prevalence may be significantly underestimated.[69]
 
Other known pathogenic mycobacteria include M. leprae, M. avium, and M. kansasii. The latter two species are classified as ""nontuberculous mycobacteria"" (NTM) or atypical mycobacteria. NTM cause neither TB nor leprosy, but they do cause lung diseases that resemble TB.[70] When people with active pulmonary TB cough, sneeze, speak, sing, or spit, they expel infectious aerosol droplets 0.5 to 5.0 μm in diameter. A single sneeze can release up to 40,000 droplets.[71] Each one of these droplets may transmit the disease, since the infectious dose of tuberculosis is very small (the inhalation of fewer than 10 bacteria may cause an infection).[72]
 People with prolonged, frequent, or close contact with people with TB are at particularly high risk of becoming infected, with an estimated 22% infection rate.[73] A person with active but untreated tuberculosis may infect 10–15 (or more) other people per year.[74] Transmission should occur from only people with active TB – those with latent infection are not thought to be contagious.[13] The probability of transmission from one person to another depends upon several factors, including the number of infectious droplets expelled by the carrier, the effectiveness of ventilation, the duration of exposure, the virulence of the M. tuberculosis strain, the level of immunity in the uninfected person, and others.[75]
 The cascade of person-to-person spread can be circumvented by segregating those with active (""overt"") TB and putting them on anti-TB drug regimens. After about two weeks of effective treatment, subjects with nonresistant active infections generally do not remain contagious to others.[73] If someone does become infected, it typically takes three to four weeks before the newly infected person becomes infectious enough to transmit the disease to others.[76]
 A number of factors make individuals more susceptible to TB infection or disease.[77]
 The most important risk factor globally for developing active TB is concurrent HIV infection; 13% of those with TB are also infected with HIV.[78] This is a particular problem in sub-Saharan Africa, where HIV infection rates are high.[79][80] Of those without HIV infection who are infected with tuberculosis, about 5–10% develop active disease during their lifetimes;[48] in contrast, 30% of those co-infected with HIV develop the active disease.[48]
 Use of certain medications, such as corticosteroids and infliximab (an anti-αTNF monoclonal antibody), is another important risk factor, especially in the developed world.[14]
 Other risk factors include: alcoholism,[14] diabetes mellitus (3-fold increased risk),[81] silicosis (30-fold increased risk),[82] tobacco smoking (2-fold increased risk),[83] indoor air pollution, malnutrition, young age,[77] recently acquired TB infection, recreational drug use, severe kidney disease, low body weight, organ transplant, head and neck cancer,[84] and genetic susceptibility[85] (the overall importance of genetic risk factors remains undefined[14]).
 Tobacco smoking increases the risk of infections (in addition to increasing the risk of active disease and death). Additional factors increasing infection susceptibility include young age.[77]
 About 90% of those infected with M. tuberculosis have asymptomatic, latent TB infections (sometimes called LTBI),[87] with only a 10% lifetime chance that the latent infection will progress to overt, active tuberculous disease.[88] In those with HIV, the risk of developing active TB increases to nearly 10% a year.[88] If effective treatment is not given, the death rate for active TB cases is up to 66%.[74]
 TB infection begins when the mycobacteria reach the alveolar air sacs of the lungs, where they invade and replicate within endosomes of alveolar macrophages.[13][89][90] Macrophages identify the bacterium as foreign and attempt to eliminate it by phagocytosis. During this process, the bacterium is enveloped by the macrophage and stored temporarily in a membrane-bound vesicle called a phagosome. The phagosome then combines with a lysosome to create a phagolysosome. In the phagolysosome, the cell attempts to use reactive oxygen species and acid to kill the bacterium.  However, M. tuberculosis has a thick, waxy mycolic acid capsule that protects it from these toxic substances. M. tuberculosis is able to reproduce inside the macrophage and will eventually kill the immune cell.
 The primary site of infection in the lungs, known as the Ghon focus, is generally located in either the upper part of the lower lobe, or the lower part of the upper lobe.[13] Tuberculosis of the lungs may also occur via infection from the blood stream. This is known as a Simon focus and is typically found in the top of the lung.[91] This hematogenous transmission can also spread infection to more distant sites, such as peripheral lymph nodes, the kidneys, the brain, and the bones.[13][92] All parts of the body can be affected by the disease, though for unknown reasons it rarely affects the heart, skeletal muscles, pancreas, or thyroid.[93]
 Tuberculosis is classified as one of the granulomatous inflammatory diseases. Macrophages, epithelioid cells, T lymphocytes, B lymphocytes, and fibroblasts  aggregate to form granulomas, with lymphocytes surrounding the infected macrophages. When other macrophages attack the infected macrophage, they fuse together to form a giant multinucleated cell in the alveolar lumen. The granuloma may prevent dissemination of the mycobacteria and provide a local environment for interaction of cells of the immune system.[94]
 However, more recent evidence suggests that the bacteria use the granulomas to avoid destruction by the host's immune system.  Macrophages and dendritic cells in the granulomas are unable to present antigen to lymphocytes; thus the immune response is suppressed.[95] Bacteria inside the granuloma can become dormant, resulting in latent infection. Another feature of the granulomas is the development of abnormal cell death (necrosis) in the center of tubercles.  To the naked eye, this has the texture of soft, white cheese and is termed caseous necrosis.[94]
 If TB bacteria gain entry to the blood stream from an area of damaged tissue, they can spread throughout the body and set up many foci of infection, all appearing as tiny, white tubercles in the tissues.[96] This severe form of TB disease, most common in young children and those with HIV, is called miliary tuberculosis.[97] People with this disseminated TB have a high fatality rate even with treatment (about 30%).[53][98]
 In many people, the infection waxes and wanes. Tissue destruction and necrosis are often balanced by healing and fibrosis.[94] Affected tissue is replaced by scarring and cavities filled with caseous necrotic material. During active disease, some of these cavities are joined to the air passages (bronchi) and this material can be coughed up. It contains living bacteria and thus can spread the infection. Treatment with appropriate antibiotics kills bacteria and allows healing to take place. Upon cure, affected areas are eventually replaced by scar tissue.[94]
 Diagnosing active tuberculosis based only on signs and symptoms is difficult,[99] as is diagnosing the disease in those who have a weakened immune system.[100] A diagnosis of TB should, however, be considered in those with signs of lung disease or constitutional symptoms lasting longer than two weeks.[100] A chest X-ray and multiple sputum cultures for acid-fast bacilli are typically part of the initial evaluation.[100] Interferon-γ release assays (IGRA) and tuberculin skin tests are of little use in most of the developing world.[101][102] IGRA have similar limitations in those with HIV.[102][103]
 A definitive diagnosis of TB is made by identifying M. tuberculosis in a clinical sample (e.g., sputum, pus, or a tissue biopsy). However, the difficult culture process for this slow-growing organism can take two to six weeks for blood or sputum culture.[104] Thus, treatment is often begun before cultures are confirmed.[105]
 Nucleic acid amplification tests and adenosine deaminase testing may allow rapid diagnosis of TB.[99] Blood tests to detect antibodies are not specific or sensitive, so they are not recommended.[106]
 The Mantoux tuberculin skin test is often used to screen people at high risk for TB.[100] Those who have been previously immunized with the Bacille Calmette-Guerin vaccine may have a false-positive test result.[107] The test may be falsely negative in those with sarcoidosis, Hodgkin's lymphoma, malnutrition, and most notably, active tuberculosis.[13] Interferon gamma release assays, on a blood sample, are recommended in those who are positive to the Mantoux test.[105] These are not affected by immunization or most environmental mycobacteria, so they generate fewer false-positive results.[108] However, they are affected by M. szulgai, M. marinum, and M. kansasii.[109] IGRAs may increase sensitivity when used in addition to the skin test, but may be less sensitive than the skin test when used alone.[110]
 The US Preventive Services Task Force (USPSTF) has recommended screening people who are at high risk for latent tuberculosis with either tuberculin skin tests or interferon-gamma release assays.[111] While some have recommend testing health care workers, evidence of benefit for this is poor as of 2019[update].[112] The Centers for Disease Control and Prevention (CDC) stopped recommending yearly testing of health care workers without known exposure in 2019.[113]
 Tuberculosis prevention and control efforts rely primarily on the vaccination of infants and the detection and appropriate treatment of active cases.[14] The World Health Organization (WHO) has achieved some success with improved treatment regimens, and a small decrease in case numbers.[14] Some countries have legislation to involuntarily detain or examine those suspected to have tuberculosis, or involuntarily treat them if infected.[114]
 The only available vaccine as of 2021[update] is bacillus Calmette-Guérin (BCG).[115][116] In children it decreases the risk of getting the infection by 20% and the risk of infection turning into active disease by nearly 60%.[117][118]
 It is the most widely used vaccine worldwide, with more than 90% of all children being vaccinated.[14] The immunity it induces decreases after about ten years.[14] As tuberculosis is uncommon in most of Canada, Western Europe, and the United States, BCG is administered to only those people at high risk.[119][120][121] Part of the reasoning against the use of the vaccine is that it makes the tuberculin skin test falsely positive, reducing the test's usefulness as a screening tool.[121] Several vaccines are being developed.[14]
 Intradermal MVA85A vaccine in addition to BCG injection is not effective in preventing tuberculosis.[122]
 Public health campaigns which have focused on overcrowding, public spitting and regular sanitation (including hand washing) during the 1800s helped to either interrupt or slow spread which when combined with contact tracing, isolation and treatment helped to dramatically curb the transmission of both tuberculosis and other airborne diseases which led to the elimination of tuberculosis as a major public health issue in most developed economies.[123][124] Other risk factors which worsened TB spread such as malnutrition were also ameliorated, but since the emergence of HIV a new population of immunocompromised individuals was available for TB to infect.
 During the HIV/AIDS epidemic in the US, up to 35% of those affected by TB were also infected by HIV.[125] Handling of TB-infected patients in US hospitals was known to create airborne TB that could infect others, especially in unventilated spaces.[126]
 Multiple US agencies rolled out new public health rules as a result of the TB spread: the CDC brought in new guidelines mandating HEPA filters and HEPA respirators,[127] NIOSH pushed through new 42 CFR 84 respirator regulations in 1995 (like the N95),[128] and OSHA created a proposed rule for TB in 1997, a result of pressure from groups like the Labor Coalition to Fight TB in the Workplace.[129][130]
 However, in 2003, OSHA dropped their proposed TB rules, citing a decline of TB in the US, and public disapproval.[131]
 The World Health Organization (WHO) declared TB a ""global health emergency"" in 1993,[14] and in 2006, the Stop TB Partnership developed a Global Plan to Stop Tuberculosis that aimed to save 14 million lives between its launch and 2015.[132] A number of targets they set were not achieved by 2015, mostly due to the increase in HIV-associated tuberculosis and the emergence of multiple drug-resistant tuberculosis.[14] A tuberculosis classification system developed by the American Thoracic Society is used primarily in public health programs.[133] In 2015, it launched the End TB Strategy to reduce deaths by 95% and incidence by 90% before 2035. The goal of tuberculosis elimination is being hampered by the lack of rapid testing, short and effective treatment courses, and completely effective vaccines.[134]
 The benefits and risks of giving anti-tubercular drugs to those exposed to MDR-TB is unclear.[135] Making HAART therapy available to HIV-positive individuals significantly reduces the risk of progression to an active TB infection by up to 90% and can mitigate the spread through this population.[136]
 Treatment of TB uses antibiotics to kill the bacteria. Effective TB treatment is difficult, due to the unusual structure and chemical composition of the mycobacterial cell wall, which hinders the entry of drugs and makes many antibiotics ineffective.[137]
 Active TB is best treated with combinations of several antibiotics to reduce the risk of the bacteria developing antibiotic resistance.[14] The routine use of rifabutin instead of rifampicin in HIV-positive people with tuberculosis is of unclear benefit as of 2007[update].[138]
 Acetylsalicylic acid (aspirin) at a dose of 100 mg per day has been shown to improve clinical signs and symptoms, reduce cavitary lesions, lower inflammatory markers, and increase the rate of sputum-negative conversion in patients with pulmonary tuberculosis.[139]
 Latent TB is treated with either isoniazid or rifampin alone, or a combination of isoniazid with either rifampicin or rifapentine.[140][141][142]
 The treatment takes three to nine months depending on the medications used.[75][140][143][142] People with latent infections are treated to prevent them from progressing to active TB disease later in life.[144]
 Education or counselling may improve the latent tuberculosis treatment completion rates.[145]
 The recommended treatment of new-onset pulmonary tuberculosis, as of 2010[update], is six months of a combination of antibiotics containing rifampicin, isoniazid, pyrazinamide, and ethambutol for the first two months, and only rifampicin and isoniazid for the last four months.[14] Where resistance to isoniazid is high, ethambutol may be added for the last four months as an alternative.[14] Treatment with anti-TB drugs for at least 6 months results in higher success rates when compared with treatment less than 6 months, even though the difference is small. Shorter treatment regimen may be recommended for those with compliance issues.[146] There is also no evidence to support shorter anti-tuberculosis treatment regimens when compared to a 6-month treatment regimen.[147] However, results presented in 2020 from an international, randomized, controlled clinical trial indicate that a four-month daily treatment regimen containing high-dose, or ""optimized"", rifapentine with moxifloxacin (2PHZM/2PHM) is as safe and effective as the existing standard six-month daily regimen at curing drug-susceptible tuberculosis (TB) disease.[148]
 If tuberculosis recurs, testing to determine which antibiotics it is sensitive to is important before determining treatment.[14] If multiple drug-resistant TB (MDR-TB) is detected, treatment with at least four effective antibiotics for 18 to 24 months is recommended.[14]
 Directly observed therapy, i.e., having a health care provider watch the person take their medications, is recommended by the World Health Organization (WHO) in an effort to reduce the number of people not appropriately taking antibiotics.[149] The evidence to support this practice over people simply taking their medications independently is of poor quality.[150] There is no strong evidence indicating that directly observed therapy improves the number of people who were cured or the number of people who complete their medicine.[150] Moderate quality evidence suggests that there is also no difference if people are observed at home versus at a clinic, or by a family member versus a health care worker.[150]
 Methods to remind people of the importance of treatment and appointments may result in a small but important improvement.[151] There is also not enough evidence to support intermittent rifampicin-containing therapy given two to three times a week has equal effectiveness as daily dose regimen on improving cure rates and reducing relapsing rates.[152] There is also not enough evidence on effectiveness of giving intermittent twice or thrice weekly short course regimen compared to daily dosing regimen in treating children with tuberculosis.[153]
 Primary resistance occurs when a person becomes infected with a resistant strain of TB. A person with fully susceptible MTB may develop secondary (acquired) resistance during therapy because of inadequate treatment, not taking the prescribed regimen appropriately (lack of compliance), or using low-quality medication.[154] Drug-resistant TB is a serious public health issue in many developing countries, as its treatment is longer and requires more expensive drugs. MDR-TB is defined as resistance to the two most effective first-line TB drugs: rifampicin and isoniazid. Extensively drug-resistant TB is also resistant to three or more of the six classes of second-line drugs.[155] Totally drug-resistant TB is resistant to all currently used drugs.[156] It was first observed in 2003 in Italy,[157] but not widely reported until 2012,[156][158] and has also been found in Iran and India.[159] There is some efficacy for linezolid to treat those with XDR-TB but side effects and discontinuation of medications were common.[160][161] Bedaquiline is tentatively supported for use in multiple drug-resistant TB.[162]
 XDR-TB is a term sometimes used to define extensively resistant TB, and constitutes one in ten cases of MDR-TB. Cases of XDR TB have been identified in more than 90% of countries.[159]
 For those with known rifampicin or MDR-TB, molecular tests such as the Genotype MTBDRsl Assay (performed on culture isolates or smear positive specimens) may be useful to detect second-line anti-tubercular drug resistance.[163][164]
 Progression from TB infection to overt TB disease occurs when the bacilli overcome the immune system defenses and begin to multiply. In primary TB disease (some 1–5% of cases), this occurs soon after the initial infection.[13] However, in the majority of cases, a latent infection occurs with no obvious symptoms.[13] These dormant bacilli produce active tuberculosis in 5–10% of these latent cases, often many years after infection.[48]
 The risk of reactivation increases with immunosuppression, such as that caused by infection with HIV. In people coinfected with M. tuberculosis and HIV, the risk of reactivation increases to 10% per year.[13] Studies using DNA fingerprinting of M. tuberculosis strains have shown reinfection contributes more substantially to recurrent TB than previously thought,[166] with estimates that it might account for more than 50% of reactivated cases in areas where TB is common.[167] The chance of death from a case of tuberculosis is about 4% as of 2008[update], down from 8% in 1995.[14]
 In people with smear-positive pulmonary TB (without HIV co-infection), after 5 years without treatment, 50–60% die while 20–25% achieve spontaneous resolution (cure). TB is almost always fatal in those with untreated HIV co-infection and death rates are increased even with antiretroviral treatment of HIV.[168]
 Roughly one-quarter of the world's population has been infected with M. tuberculosis,[6] with new infections occurring in about 1% of the population each year.[11] However, most infections with M. tuberculosis do not cause disease,[169] and 90–95% of infections remain asymptomatic.[87] In 2012, an estimated 8.6 million chronic cases were active.[170] In 2010, 8.8 million new cases of tuberculosis were diagnosed, and 1.20–1.45 million deaths occurred (most of these occurring in developing countries).[78][171] Of these, about 0.35 million occur in those also infected with HIV.[172] In 2018, tuberculosis was the leading cause of death worldwide from a single infectious agent.[1] The total number of tuberculosis cases has been decreasing since 2005, while new cases have decreased since 2002.[78]
 Tuberculosis[clarification needed] incidence is seasonal, with peaks occurring every spring and summer.[173][174][175][176] The reasons for this are unclear, but may be related to vitamin D deficiency during the winter.[176][177] There are also studies linking tuberculosis to different weather conditions like low temperature, low humidity and low rainfall. It has been suggested that tuberculosis incidence rates may be connected to climate change.[178]
 Tuberculosis is closely linked to both overcrowding and malnutrition, making it one of the principal diseases of poverty.[14] Those at high risk thus include: people who inject illicit drugs, inhabitants and employees of locales where vulnerable people gather (e.g., prisons and homeless shelters), medically underprivileged and resource-poor communities, high-risk ethnic minorities, children in close contact with high-risk category patients, and health-care providers serving these patients.[179]
 The rate of tuberculosis varies with age. In Africa, it primarily affects adolescents and young adults.[180] However, in countries where incidence rates have declined dramatically (such as the United States), tuberculosis is mainly a disease of the elderly and immunocompromised (risk factors are listed above).[13][181] Worldwide, 22 ""high-burden"" states or countries together experience 80% of cases as well as 83% of deaths.[159]
 In Canada and Australia, tuberculosis is many times more common among the Indigenous peoples, especially in remote areas.[182][183] Factors contributing to this include higher prevalence of predisposing health conditions and behaviours, and overcrowding and poverty. In some Canadian Indigenous groups, genetic susceptibility may play a role.[77]
 Socioeconomic status (SES) strongly affects TB risk. People of low SES are both more likely to contract TB and to be more severely affected by the disease. Those with low SES are more likely to be affected by risk factors for developing TB (e.g., malnutrition, indoor air pollution, HIV co-infection, etc.), and are additionally more likely to be exposed to crowded and poorly ventilated spaces. Inadequate healthcare also means that people with active disease who facilitate spread are not diagnosed and treated promptly; sick people thus remain in the infectious state and (continue to) spread the infection.[77]
 The distribution of tuberculosis is not uniform across the globe; about 80% of the population in many African, Caribbean, South Asian, and eastern European countries test positive in tuberculin tests, while only 5–10% of the U.S. population test positive.[13] Hopes of totally controlling the disease have been dramatically dampened because of many factors, including the difficulty of developing an effective vaccine, the expensive and time-consuming diagnostic process, the necessity of many months of treatment, the increase in HIV-associated tuberculosis, and the emergence of drug-resistant cases in the 1980s.[14]
 In developed countries, tuberculosis is less common and is found mainly in urban areas. In Europe, deaths from TB fell from 500 out of 100,000 in 1850 to 50 out of 100,000 by 1950. Improvements in public health were reducing tuberculosis even before the arrival of antibiotics, although the disease remained a significant threat to public health, such that when the Medical Research Council was formed in Britain in 1913 its initial focus was tuberculosis research.[184]
 In 2010, rates per 100,000 people in different areas of the world were: globally 178, Africa 332, the Americas 36, Eastern Mediterranean 173, Europe 63, Southeast Asia 278, and Western Pacific 139.[172]
 In 2023, tuberculosis overtook COVID-19 as the leading cause of infectious disease-related deaths globally, according to a World Health Organization.[185] Around 8.2 million people were newly diagnosed with TB last year, allowing them access to treatment—a record high since WHO’s tracking began in 1995 and an increase from 7.5 million cases in 2022.[186] The report highlights ongoing obstacles in combating TB, including severe funding shortages that hinder efforts toward eradication. Although TB-related deaths decreased slightly to 1.25 million in 2023 from 1.32 million in 2022, the overall number of new cases rose marginally to an estimated 10.8 million.
 Russia has achieved particularly dramatic progress with a decline in its TB mortality rate—from 61.9 per 100,000 in 1965 to 2.7 per 100,000 in 1993;[187][188] however, mortality rate increased to 24 per 100,000 in 2005 and then recoiled to 11 per 100,000 by 2015.[189]
 China has achieved particularly dramatic progress, with about an 80% reduction in its TB mortality rate between 1990 and 2010.[172] The number of new cases has declined by 17% between 2004 and 2014.[159]
 In 2007, the country with the highest estimated incidence rate of TB was Eswatini, with 1,200 cases per 100,000 people. In 2017, the country with the highest estimated incidence rate as a % of the population was Lesotho, with 665 cases per 100,000 people.[190]
 In South Africa, 54,200 people died in 2022 from TB. The incidence rate was 468 per 100,000 people; in 2015, this was 988 per 100,000. The total incidence was 280,000 in 2022; in 2015, this was 552,000.[191]
 As of 2017, India had the largest total incidence, with an estimated 2,740,000 cases.[190] According to the World Health Organization (WHO), in 2000–2015, India's estimated mortality rate dropped from 55 to 36 per 100,000 population per year with estimated 480 thousand people died of TB in 2015.[192][193] In India a major proportion of tuberculosis patients are being treated by private partners and private hospitals. Evidence indicates that the tuberculosis national survey does not represent the number of cases that are diagnosed and recorded by private clinics and hospitals in India.[194]
 In Canada, tuberculosis was endemic in some rural areas as of 1998.[195] The tuberculosis case rate in Canada in 2021 was 4.8 per 100,000 persons. The rates were highest among Inuit (135.1 per 100,000), First Nations (16.1 per 100,000) and people born outside of Canada (12.3 per 100,000).[196]
 In the United States, Native Americans have a fivefold greater mortality from TB,[197] and racial and ethnic minorities accounted for 88% of all reported TB cases.[198] The overall tuberculosis case rate in the United States was 2.9 per 100,000 persons in
2023, representing a 16% increase in cases compared to 2022.[198]
 In 2024, Long Beach, California authorized a public health emergency in response to a local outbreak of TB.[199]
 In 2017, in the United Kingdom, the national average was 9 per 100,000 and the highest incidence rates in Western Europe were 20 per 100,000 in Portugal.
 Tuberculosis has been known by many names from the technical to the familiar.[202] Phthisis (φθίσις) is the Greek word for consumption, an old term for pulmonary tuberculosis;[7] around 460 BCE, Hippocrates described phthisis as a disease of dry seasons.[203] The abbreviation TB is short for tubercle bacillus. Consumption was the most common nineteenth century English word for the disease, and was also in use well into the twentieth century. The Latin root con meaning 'completely' is linked to sumere meaning 'to take up from under'.[204] In The Life and Death of Mr Badman by John Bunyan, the author calls consumption ""the captain of all these men of death.""[205] ""Great white plague"" has also been used.[202]
 Tuberculosis was for centuries associated with poetic and artistic qualities among those infected, and was also known as ""the romantic disease"".[202][206] Major artistic figures such as the poets John Keats, Percy Bysshe Shelley, and Edgar Allan Poe, the composer Frédéric Chopin,[207] the playwright Anton Chekhov, the novelists Franz Kafka, Katherine Mansfield,[208] Charlotte Brontë, Fyodor Dostoevsky, Thomas Mann, W. Somerset Maugham,[209] George Orwell,[210] and Robert Louis Stevenson, and the artists Alice Neel,[211] Jean-Antoine Watteau, Elizabeth Siddal, Marie Bashkirtseff, Edvard Munch, Aubrey Beardsley and Amedeo Modigliani either had the disease or were surrounded by people who did. A widespread belief was that tuberculosis assisted artistic talent. Physical mechanisms proposed for this effect included the slight fever and toxaemia that it caused, allegedly helping them to see life more clearly and to act decisively.[212][213][214]
 Tuberculosis formed an often-reused theme in literature, as in Thomas Mann's The Magic Mountain, set in a sanatorium;[215] in music, as in Van Morrison's song ""T.B. Sheets"";[216] in opera, as in Puccini's La bohème and Verdi's La Traviata;[214] in art, as in Munch's painting of his ill sister;[217] and in film, such as the 1945 The Bells of St. Mary's starring Ingrid Bergman as a nun with tuberculosis.[218]
 In 2014, the WHO adopted the ""End TB"" strategy which aims to reduce TB incidence by 80% and TB deaths by 90% by 2030.[219] The strategy contains a milestone to reduce TB incidence by 20% and TB deaths by 35% by 2020.[220] However, by 2020 only a 9% reduction in incidence per population was achieved globally, with the European region achieving 19% and the African region achieving 16% reductions.[220] Similarly, the number of deaths only fell by 14%, missing the 2020 milestone of a 35% reduction, with some regions making better progress (31% reduction in Europe and 19% in Africa).[220] Correspondingly, also treatment, prevention and funding milestones were missed in 2020, for example only 6.3 million people were started on TB prevention short of the target of 30 million.[220]
 The World Health Organization (WHO), the Bill and Melinda Gates Foundation, and the U.S. government are subsidizing a fast-acting diagnostic tuberculosis test for use in low- and middle-income countries as of 2012.[221][222][223] In addition to being fast-acting, the test can determine if there is resistance to the antibiotic rifampicin which may indicate multi-drug resistant tuberculosis and is accurate in those who are also infected with HIV.[221][224] Many resource-poor places as of 2011[update] have access to only sputum microscopy.[225]
 India had the highest total number of TB cases worldwide in 2010, in part due to poor disease management within the private and public health care sector.[226] Programs such as the Revised National Tuberculosis Control Program are working to reduce TB levels among people receiving public health care.[227][228]
 A 2014 EIU-healthcare report finds there is a need to address apathy and urges for increased funding. The report cites among others Lucica Ditui ""[TB] is like an orphan. It has been neglected even in countries with a high burden and often forgotten by donors and those investing in health interventions.""[159]
 Slow progress has led to frustration, expressed by the executive director of the Global Fund to Fight AIDS, Tuberculosis and Malaria – Mark Dybul: ""we have the tools to end TB as a pandemic and public health threat on the planet, but we are not doing it.""[159] Several international organizations are pushing for more transparency in treatment, and more countries are implementing mandatory reporting of cases to the government as of 2014, although adherence is often variable. Commercial treatment providers may at times overprescribe second-line drugs as well as supplementary treatment, promoting demands for further regulations.[159]
 The government of Brazil provides universal TB care, which reduces this problem.[159] Conversely, falling rates of TB infection may not relate to the number of programs directed at reducing infection rates but may be tied to an increased level of education, income, and health of the population.[159] Costs of the disease, as calculated by the World Bank in 2009 may exceed US$150 billion per year in ""high burden"" countries.[159] Lack of progress eradicating the disease may also be due to lack of patient follow-up – as among the 250 million rural migrants in China.[159]
 There is insufficient data to show that active contact tracing helps to improve case detection rates for tuberculosis.[229] Interventions such as house-to-house visits, educational leaflets, mass media strategies, educational sessions may increase tuberculosis detection rates in short-term.[230] There is no study that compares new methods of contact tracing such as social network analysis with existing contact tracing methods.[231]
 Slow progress in preventing the disease may in part be due to stigma associated with TB.[159] Stigma may be due to the fear of transmission from affected individuals. This stigma may additionally arise due to links between TB and poverty, and in Africa, AIDS.[159] Such stigmatization may be both real and perceived; for example, in Ghana, individuals with TB are banned from attending public gatherings.[232]
 Stigma towards TB may result in delays in seeking treatment,[159] lower treatment compliance, and family members keeping cause of death secret[232] – allowing the disease to spread further.[159] In contrast, in Russia stigma was associated with increased treatment compliance.[232] TB stigma also affects socially marginalized individuals to a greater degree and varies between regions.[232]
 One way to decrease stigma may be through the promotion of ""TB clubs"", where those infected may share experiences and offer support, or through counseling.[232] Some studies have shown TB education programs to be effective in decreasing stigma, and may thus be effective in increasing treatment adherence.[232] Despite this, studies on the relationship between reduced stigma and mortality are lacking as of 2010[update], and similar efforts to decrease stigma surrounding AIDS have been minimally effective.[232] Some have claimed the stigma to be worse than the disease, and healthcare providers may unintentionally reinforce stigma, as those with TB are often perceived as difficult or otherwise undesirable.[159] A greater understanding of the social and cultural dimensions of tuberculosis may also help with stigma reduction.[233]
 The BCG vaccine has limitations and research to develop new TB vaccines is ongoing.[234] A number of potential candidates are currently in phase I and II clinical trials.[234][235] Two main approaches are used to attempt to improve the efficacy of available vaccines. One approach involves adding a subunit vaccine to BCG, while the other strategy is attempting to create new and better live vaccines.[234] MVA85A, an example of a subunit vaccine, is in trials in South Africa as of 2006, is based on a genetically modified vaccinia virus.[236] Vaccines are hoped to play a significant role in treatment of both latent and active disease.[237]
 To encourage further discovery, researchers and policymakers are promoting new economic models of vaccine development as of 2006, including prizes, tax incentives, and advance market commitments.[238][239] A number of groups, including the Stop TB Partnership,[240] the South African Tuberculosis Vaccine Initiative, and the Aeras Global TB Vaccine Foundation, are involved with research.[241] Among these, the Aeras Global TB Vaccine Foundation received a gift of more than $280 million (US) from the Bill and Melinda Gates Foundation to develop and license an improved vaccine against tuberculosis for use in high burden countries.[242][243]
 In 2012 a new medication regimen was approved in the US for multidrug-resistant tuberculosis, using bedaquiline as well as existing drugs. There were initial concerns about the safety of this drug,[244][245][246][247][248] but later research on larger groups found that this regimen improved health outcomes.[249] By 2017 the drug was used in at least 89 countries.[250] Another new drug is delamanid, which was first approved by the European Medicines Agency in 2013 to be used in multidrug-resistant tuberculosis patients,[251] and by 2017 was used in at least 54 countries.[250]
 Steroids add-on therapy has not shown any benefits for active pulmonary tuberculosis infection.[252]
 Mycobacteria infect many different animals, including birds,[253] fish, rodents,[254] and reptiles.[255] The subspecies Mycobacterium tuberculosis, though, is rarely present in wild animals.[256] An effort to eradicate bovine tuberculosis caused by Mycobacterium bovis from the cattle and deer herds of New Zealand has been relatively successful.[257] Efforts in Great Britain have been less successful.[258][259]
 As of 2015[update], tuberculosis appears to be widespread among captive elephants in the US. It is believed that the animals originally acquired the disease from humans, a process called reverse zoonosis. Because the disease can spread through the air to infect both humans and other animals, it is a public health concern affecting circuses and zoos.[260][261]
"
Smallpox,https://en.wikipedia.org/wiki/Smallpox,"
 


 Smallpox was an infectious disease caused by variola virus (often called smallpox virus),  which belongs to the genus Orthopoxvirus.[7][11] The last naturally occurring case was diagnosed in October 1977, and the World Health Organization (WHO) certified the global eradication of the disease in 1980,[10] making smallpox the only human disease to have been eradicated to date.[12]
 The initial symptoms of the disease included fever and vomiting.[5] This was followed by formation of ulcers in the mouth and a skin rash.[5] Over a number of days, the skin rash turned into the characteristic fluid-filled blisters with a dent in the center.[5] The bumps then scabbed over and fell off, leaving scars.[5] The disease was transmitted from one person to another primarily through prolonged face-to-face contact with an infected person or rarely via contaminated objects.[6][13][14] Prevention was achieved mainly through the smallpox vaccine.[9] Once the disease had developed, certain antiviral medications could potentially have helped, but such medications did not become available until after the disease was eradicated.[9] The risk of death was about 30%, with higher rates among babies.[6][15] Often, those who survived had extensive scarring of their skin, and some were left blind.[6]
 The earliest evidence of the disease dates to around 1500 BC in Egyptian mummies.[16][17] The disease historically occurred in outbreaks.[10] It was one of several diseases introduced by the Columbian exchange to the New World, resulting in large swathes of Native Americans dying. In 18th-century Europe, it is estimated that 400,000 people died from the disease per year, and that one-third of all cases of blindness were due to smallpox.[10][18] Smallpox is estimated to have killed up to 300 million people in the 20th century[19][20] and around 500 million people in the last 100 years of its existence.[21] Earlier deaths included six European monarchs, including Louis XV of France in 1774.[10][18] As recently as 1967, 15 million cases occurred a year.[10] The final known fatal case occurred in the United Kingdom in 1978.
 Inoculation for smallpox appears to have started in China around the 1500s.[22][23] Europe adopted this practice from Asia in the first half of the 18th century.[24] In 1796, Edward Jenner introduced the modern smallpox vaccine.[25][26] In 1967, the WHO intensified efforts to eliminate the disease.[10] Smallpox is one of two infectious diseases to have been eradicated, the other being rinderpest (a disease of even-toed ungulates) in 2011.[27][28] The term ""smallpox"" was first used in England in the 16th century to distinguish the disease from syphilis, which was then known as the ""great pox"".[29][30] Other historical names for the disease include pox, speckled monster, and red plague.[3][4][30]
 The United States and Russia retain samples of variola virus in laboratories, which has sparked debates over safety.
 There are two forms of the smallpox. Variola major is the severe and most common form, with a more extensive rash and higher fever. Variola minor is a less common presentation, causing less severe disease, typically discrete smallpox, with historical death rates of 1% or less.[32] Subclinical (asymptomatic) infections with variola virus were noted but were not common.[33] In addition, a form called variola sine eruptione (smallpox without rash) was seen generally in vaccinated persons. This form was marked by a fever that occurred after the usual incubation period and could be confirmed only by antibody studies or, rarely, by viral culture.[33] In addition, there were two very rare and fulminating types of smallpox, the malignant (flat) and hemorrhagic forms, which were usually fatal.
 The initial symptoms were similar to other viral diseases that are still extant, such as influenza and the common cold: fever of at least 38.3 °C (101 °F), muscle pain, malaise, headache and fatigue. As the digestive tract was commonly involved, nausea, vomiting, and backache often occurred. The early prodromal stage usually lasted 2–4 days. By days 12–15, the first visible lesions – small reddish spots called enanthem – appeared on mucous membranes of the mouth, tongue, palate, and throat, and the temperature fell to near-normal. These lesions rapidly enlarged and ruptured, releasing large amounts of virus into the saliva.[34]
 Variola virus tended to attack skin cells, causing the characteristic pimples, or macules, associated with the disease. A rash developed on the skin 24 to 48 hours after lesions on the mucous membranes appeared. Typically the macules first appeared on the forehead, then rapidly spread to the whole face, proximal portions of extremities, the trunk, and lastly to distal portions of extremities. The process took no more than 24 to 36 hours, after which no new lesions appeared.[34] At this point, variola major disease could take several very different courses, which resulted in four types of smallpox disease based on the Rao classification:[35] ordinary, modified, malignant (or flat), and hemorrhagic smallpox. Historically, ordinary smallpox had an overall fatality rate of about 30%, and the malignant and hemorrhagic forms were usually fatal. The modified form was almost never fatal. In early hemorrhagic cases, hemorrhages occurred before any skin lesions developed.[36] The incubation period between contraction and the first obvious symptoms of the disease was 7–14 days.[37]
 At least 90% of smallpox cases among unvaccinated persons were of the ordinary type.[33] In this form of the disease, by the second day of the rash the macules had become raised papules. By the third or fourth day, the papules had filled with an opalescent fluid to become vesicles. This fluid became opaque and turbid within 24–48 hours, resulting in pustules.
 By the sixth or seventh day, all the skin lesions had become pustules. Between seven and ten days the pustules had matured and reached their maximum size. The pustules were sharply raised, typically round, tense, and firm to the touch. The pustules were deeply embedded in the dermis, giving them the feel of a small bead in the skin. Fluid slowly leaked from the pustules, and by the end of the second week, the pustules had deflated and began to dry up, forming crusts or scabs. By day 16–20 scabs had formed over all of the lesions, which had started to flake off, leaving depigmented scars.[38]
 Ordinary smallpox generally produced a discrete rash, in which the pustules stood out on the skin separately. The distribution of the rash was most dense on the face, denser on the extremities than on the trunk, and denser on the distal parts of the extremities than on the proximal. The palms of the hands and soles of the feet were involved in most cases.[33]
 Sometimes, the blisters merged into sheets, forming a confluent rash, which began to detach the outer layers of skin from the underlying flesh. Patients with confluent smallpox often remained ill even after scabs had formed over all the lesions. In one case series, the case-fatality rate in confluent smallpox was 62%.[33]
 Referring to the character of the eruption and the rapidity of its development, modified smallpox occurred mostly in previously vaccinated people. It was rare in unvaccinated people, with one case study showing 1–2% of modified cases compared to around 25% in vaccinated people. In this form, the prodromal illness still occurred but may have been less severe than in the ordinary type. There was usually no fever during the evolution of the rash. The skin lesions tended to be fewer and evolved more quickly, were more superficial, and may not have shown the uniform characteristic of more typical smallpox.[38] Modified smallpox was rarely, if ever, fatal. This form of variola major was more easily confused with chickenpox.[33]
 In malignant-type smallpox (also called flat smallpox) the lesions remained almost flush with the skin at the time when raised vesicles would have formed in the ordinary type. It is unknown why some people developed this type. Historically, it accounted for 5–10% of cases, and most (72%) were children.[3] Malignant smallpox was accompanied by a severe prodromal phase that lasted 3–4 days, prolonged high fever, and severe symptoms of viremia. The prodromal symptoms continued even after the onset of the rash.[3] The rash on the mucous membranes (enanthem) was extensive. Skin lesions matured slowly, were typically confluent or semi-confluent, and by the seventh or eighth day, they were flat and appeared to be buried in the skin. Unlike ordinary-type smallpox, the vesicles contained little fluid, were soft and velvety to the touch, and may have contained hemorrhages. Malignant smallpox was nearly always fatal and death usually occurred between the 8th and 12th day of illness. Often, a day or two before death, the lesions turned ashen gray, which, along with abdominal distension, was a bad prognostic sign.[3] This form is thought to be caused by deficient cell-mediated immunity to smallpox. If the person recovered, the lesions gradually faded and did not form scars or scabs.[39]
 Hemorrhagic smallpox is a severe form accompanied by extensive bleeding into the skin, mucous membranes, gastrointestinal tract, and viscera. This form develops in approximately 2% of infections and occurs mostly in adults.[33] Pustules do not typically form in hemorrhagic smallpox. Instead, bleeding occurs under the skin, making it look charred and black,[33] hence this form of the disease is also referred to as variola nigra or ""black pox"".[40] Hemorrhagic smallpox has very rarely been caused by variola minor virus.[41] While bleeding may occur in mild cases and not affect outcomes,[42] hemorrhagic smallpox is typically fatal.[43] Vaccination does not appear to provide any immunity to either form of hemorrhagic smallpox and some cases even occurred among people that were revaccinated shortly before. It has two forms.[3]
 The early or fulminant form of hemorrhagic smallpox (referred to as purpura variolosa) begins with a prodromal phase characterized by a high fever, severe headache, and abdominal pain.[39] The skin becomes dusky and erythematous, and this is rapidly followed by the development of petechiae and bleeding in the skin, conjunctiva and mucous membranes. Death often occurs suddenly between the fifth and seventh days of illness, when only a few insignificant skin lesions are present. Some people survive a few days longer, during which time the skin detaches and fluid accumulates under it, rupturing at the slightest injury. People are usually conscious until death or shortly before.[43] Autopsy reveals petechiae and bleeding in the spleen, kidney, serous membranes, skeletal muscles, pericardium, liver, gonads and bladder.[41] Historically, this condition was frequently misdiagnosed, with the correct diagnosis made only at autopsy.[41] This form is more likely to occur in pregnant women than in the general population (approximately 16% of cases in unvaccinated pregnant women were early hemorrhagic smallpox, versus roughly 1% in nonpregnant women and adult males).[43] The case fatality rate of early hemorrhagic smallpox approaches 100%.[43]
 There is also a later form of hemorrhagic smallpox (referred to late hemorrhagic smallpox, or variolosa pustula hemorrhagica). The prodrome is severe and similar to that observed in early hemorrhagic smallpox, and the fever persists throughout the course of the disease.[3] Bleeding appears in the early eruptive period (but later than that seen in purpura variolosa), and the rash is often flat and does not progress beyond the vesicular stage. Hemorrhages in the mucous membranes appear to occur less often than in the early hemorrhagic form.[33] Sometimes the rash forms pustules which bleed at the base and then undergo the same process as in ordinary smallpox. This form of the disease is characterized by a decrease in all of the elements of the coagulation cascade and an increase in circulating antithrombin.[34] This form of smallpox occurs anywhere from 3% to 25% of fatal cases, depending on the virulence of the smallpox strain.[36] Most people with the late-stage form die within eight to 10 days of illness. Among the few who recover, the hemorrhagic lesions gradually disappear after a long period of convalescence.[3] The case fatality rate for late hemorrhagic smallpox is around 90–95%.[35] Pregnant women are slightly more likely to experience this form of the disease, though not as much as early hemorrhagic smallpox.[3]
 Smallpox is caused by infection with variola virus, which belongs to the family Poxviridae, subfamily Chordopoxvirinae, genus Orthopoxvirus.
 The date of the appearance of smallpox is not settled. It most probably evolved from a terrestrial African rodent virus between 68,000 and 16,000 years ago.[44] The wide range of dates is due to the different records used to calibrate the molecular clock. One clade was the variola major strains (the more clinically severe form of smallpox) which spread from Asia between 400 and 1,600 years ago. A second clade included both alastrim (a phenotypically mild smallpox) described from the American continents and isolates from West Africa which diverged from an ancestral strain between 1,400 and 6,300 years before present. This clade further diverged into two subclades at least 800 years ago.[45]
 A second estimate has placed the separation of variola  virus from Taterapox (an Orthopoxvirus of some African rodents including gerbils) at 3,000 to 4,000 years ago.[46] This is consistent with archaeological and historical evidence regarding the appearance of smallpox as a human disease which suggests a relatively recent origin. If the mutation rate is assumed to be similar to that of the herpesviruses, the divergence date of variola virus from Taterapox has been estimated to be 50,000 years ago.[46] While this is consistent with the other published estimates, it suggests that the archaeological and historical evidence is very incomplete. Better estimates of mutation rates in these viruses are needed.
 Examination of a strain that dates from c. 1650 found that this strain was basal to the other presently sequenced strains.[47] The mutation rate of this virus is well modeled by a molecular clock. Diversification of strains only occurred in the 18th and 19th centuries.
 Variola virus is  large and brick-shaped and is approximately 302 to 350 nanometers by 244 to 270 nm,[48] with a single linear double stranded DNA genome 186 kilobase pairs (kbp) in size and containing a hairpin loop at each end.[49][50]
 Four orthopoxviruses cause infection in humans: variola, vaccinia, cowpox, and monkeypox. Variola virus infects only humans in nature, although primates and other animals have been infected in an experimental setting. Vaccinia, cowpox, and monkeypox viruses can infect both humans and other animals in nature.[33]
 The life cycle of poxviruses is complicated by having multiple infectious forms, with differing mechanisms of cell entry. Poxviruses are unique among human DNA viruses in that they replicate in the cytoplasm of the cell rather than in the nucleus. To replicate, poxviruses produce a variety of specialized proteins not produced by other DNA viruses, the most important of which is a viral-associated DNA-dependent RNA polymerase.
 Both enveloped and unenveloped virions are infectious. The viral envelope is made of modified Golgi membranes containing viral-specific polypeptides, including hemagglutinin.[49] Infection with either variola major virus or variola minor virus confers immunity against the other.[34]
 The more common, infectious form of the disease was caused by the variola major virus strain, known for its significantly higher mortality rate compared to its counterpart, variola minor. Variola major had a fatality rate of around 30%, while variola minor’s mortality rate was about 1%. Throughout the 18th century, variola major was responsible for around 400,000 deaths annually in Europe alone. Survivors of the disease often faced lifelong consequences, such as blindness and severe scarring, which were nearly universal among those who recovered.[51]
 In the first half of the 20th century, variola major was the primary cause of smallpox outbreaks across Asia and most of Africa. Meanwhile, variola minor was more commonly found in regions of Europe, North America, South America, and certain parts of Africa.[52]
 Variola minor virus, also called alastrim, was a less common form of the virus, and much less deadly. Although variola minor had the same incubation period and pathogenetic stages as smallpox, it is believed to have had a mortality rate of less than 1%, as compared to variola major's 30%. Like variola major, variola minor was spread through inhalation of the virus in the air, which could occur through face-to-face contact or through fomites. Infection with variola minor virus conferred immunity against the more dangerous variola major virus.
 Because variola minor was a less debilitating disease than smallpox, people were more frequently ambulant and thus able to infect others more rapidly. As such, variola minor swept through the United States, Great Britain, and South Africa in the early 20th century, becoming the dominant form of the disease in those areas and thus rapidly decreasing mortality rates. Along with variola major, the minor form has now been totally eradicated from the globe. The last case of indigenous variola minor was reported in a Somali cook, Ali Maow Maalin, in October 1977, and smallpox was officially declared eradicated worldwide in May 1980.[16] Variola minor was also called white pox, kaffir pox, Cuban itch, West Indian pox, milk pox, and pseudovariola.
 The genome of variola major virus is about 186,000 base pairs in length.[53] It is made from linear double stranded DNA and contains the coding sequence for about 200 genes.[54] The genes are usually not overlapping and typically occur in blocks that point towards the closer terminal region of the genome.[55] The coding sequence of the central region of the genome is highly consistent across orthopoxviruses, and the arrangement of genes is consistent across chordopoxviruses.[54][55]
 The center of the variola virus genome contains the majority of the essential viral genes, including the genes for structural proteins, DNA replication, transcription, and mRNA synthesis.[54] The ends of the genome vary more across strains and species of orthopoxviruses.[54] These regions contain proteins that modulate the hosts' immune systems, and are primarily responsible for the variability in virulence across the orthopoxvirus family.[54] These terminal regions in poxviruses are inverted terminal repetitions (ITR) sequences.[55] These sequences are identical but oppositely oriented on either end of the genome, leading to the genome being a continuous loop of DNA.[55] Components of the ITR sequences include an incompletely base paired A/T rich hairpin loop, a region of roughly 100 base pairs necessary for resolving concatomeric DNA (a stretch of DNA containing multiple copies of the same sequence), a few open reading frames, and short tandemly repeating sequences of varying number and length.[55] The ITRs of poxviridae vary in length across strains and species.[55] The coding sequence for most of the viral proteins in variola major virus have at least 90% similarity with the genome of vaccinia, a related virus used for vaccination against smallpox.[55]
 Gene expression of variola virus occurs entirely within the cytoplasm of the host cell, and follows a distinct progression during infection.[55] After entry of an infectious virion into a host cell, synthesis of viral mRNA can be detected within 20 minutes.[55] About half of the viral genome is transcribed prior to the replication of viral DNA.[55] The first set of expressed genes are transcribed by pre-existing viral machinery packaged within the infecting virion.[55] These genes encode the factors necessary for viral DNA synthesis and for transcription of the next set of expressed genes.[55] Unlike most DNA viruses, DNA replication in variola virus and other poxviruses takes place within the cytoplasm of the infected cell.[55] The exact timing of DNA replication after infection of a host cell varies across the poxviridae.[55] Recombination of the genome occurs within actively infected cells.[55] Following the onset of viral DNA replication, an intermediate set of genes codes for transcription factors of late gene expression.[55] The products of the later genes include transcription factors necessary for transcribing the early genes for new virions, as well as viral RNA polymerase and other essential enzymes for new viral particles.[55] These proteins are then packaged into new infectious virions capable of infecting other cells.[55]
 Two live samples of variola major virus remain, one in the United States at the CDC in Atlanta, and one at the Vector Institute in Koltsovo, Russia.[56] Research with the remaining virus samples is tightly controlled, and each research proposal must be approved by the WHO and the World Health Assembly (WHA).[56] Most research on poxviruses is performed using the closely related Vaccinia virus as a model organism.[55] Vaccinia virus, which is used to vaccinate for smallpox, is also under research as a viral vector for vaccines for unrelated diseases.[57]
 The genome of variola major virus was first sequenced in its entirety in the 1990s.[54] The complete coding sequence is publicly available online. The current reference sequence for variola major virus was sequenced from a strain that circulated in India in 1967. In addition, there are sequences for samples of other strains that were collected during the WHO eradication campaign.[54] A genome browser for a complete database of annotated sequences of variola virus and other poxviruses is publicly available through the Viral Bioinformatics Resource Center.[58]
 The WHO currently bans genetic engineering of the variola virus.[59] However, in 2004, a committee advisory to the WHO voted in favor of allowing editing of the genome of the two remaining samples of variola major virus to add a marker gene.[59] This gene, called GFP, or green fluorescent protein, would cause live samples of the virus to glow green under fluorescent light.[60] The insertion of this gene, which would not influence the virulence of the virus, would be the only allowed modification of the genome.[60] The committee stated the proposed modification would aid in research of treatments by making it easier to assess whether a potential treatment was effective in killing viral samples.[60] The recommendation could only take effect if approved by the WHA.[60] When the WHA discussed the proposal in 2005, it refrained from taking a formal vote on the proposal, stating that it would review individual research proposals one at a time.[61] Addition of the GFP gene to the Vaccinia genome is routinely performed during research on the closely related Vaccinia virus.[62]
 The public availability of the variola virus complete sequence has raised concerns about the possibility of illicit synthesis of infectious virus.[63] Vaccinia, a cousin of the variola virus, was artificially synthesized in 2002 by NIH scientists.[64] They used a previously established method that involved using a recombinant viral genome to create a self-replicating bacterial plasmid that produced viral particles.[64]
 In 2016, another group synthesized the horsepox virus using publicly available sequence data for horsepox.[65] The researchers argued that their work would be beneficial to creating a safer and more effective vaccine for smallpox, although an effective vaccine is already available.[65] The horsepox virus had previously seemed to have gone extinct, raising concern about potential revival of variola major and causing other scientists to question their motives.[63] Critics found it especially concerning that the group was able to recreate viable virus in a short time frame with relatively little cost or effort.[65] Although the WHO bans individual laboratories from synthesizing more than 20% of the genome at a time, and purchases of smallpox genome fragments are monitored and regulated, a group with malicious intentions could compile, from multiple sources, the full synthetic genome necessary to produce viable virus.[65]
 Smallpox was highly contagious, but generally spread more slowly and less widely than some other viral diseases, perhaps because transmission required close contact and occurred after the onset of the rash. The overall rate of infection was also affected by the short duration of the infectious stage. In temperate areas, the number of smallpox infections was highest during the winter and spring. In tropical areas, seasonal variation was less evident and the disease was present throughout the year.[33] Age distribution of smallpox infections depended on acquired immunity. Vaccination immunity declined over time and was probably lost within thirty years.[34] Smallpox was not known to be transmitted by insects or animals and there was no asymptomatic carrier state.[33]
 Transmission occurred through inhalation of airborne variola virus, usually droplets expressed from the oral, nasal, or pharyngeal mucosa of an infected person. It was transmitted from one person to another primarily through prolonged face-to-face contact with an infected person.[14]
 Some infections of laundry workers with smallpox after handling contaminated bedding suggested that smallpox could be spread through direct contact with contaminated objects (fomites), but this was found to be rare.[14][35] Also rarely, smallpox was spread by virus carried in the air in enclosed settings such as buildings, buses, and trains.[32] The virus can cross the placenta, but the incidence of congenital smallpox was relatively low.[34] Smallpox was not notably infectious in the prodromal period and viral shedding was usually delayed until the appearance of the rash, which was often accompanied by lesions in the mouth and pharynx. The virus can be transmitted throughout the course of the illness, but this happened most frequently during the first week of the rash when most of the skin lesions were intact.[33] Infectivity waned in 7 to 10 days when scabs formed over the lesions, but the infected person was contagious until the last smallpox scab fell off.[66]
 Concern about possible use of smallpox for biological warfare led in 2002 to Donald K. Milton's detailed review of existing research on its transmission and of then-current recommendations for controlling its spread. He agreed, citing Rao, Fenner and others, that “careful epidemiologic investigation rarely implicated fomites as a source of infection”;  noted that “Current recommendations for control of secondary smallpox infections emphasize transmission ‘by expelled droplets to close contacts (those within 6–7 feet)’”; but warned that the “emphasis on spread via large droplets may reduce the vigilance with which more difficult airborne precautions [i.e. against finer droplets capable of traveling longer distances and penetrating deeply into the lower respiratory tract] are maintained”.[67]
 Once inhaled, the variola virus invaded the mucous membranes of the mouth, throat, and respiratory tract. From there, it migrated to regional lymph nodes and began to multiply. In the initial growth phase, the virus seemed to move from cell to cell, but by around the 12th day, widespread lysis of infected cells occurred and the virus could be found in the bloodstream in large numbers, a condition known as viremia. This resulted in the second wave of multiplication in the spleen, bone marrow, and lymph nodes.
 The clinical definition of ordinary smallpox is an illness with acute onset of fever equal to or greater than 38.3 °C (101 °F) followed by a rash characterized by firm, deep-seated vesicles or pustules in the same stage of development without other apparent cause.[33] When a clinical case was observed, smallpox was confirmed using laboratory tests.
 Microscopically, poxviruses produce characteristic cytoplasmic inclusion bodies, the most important of which are known as Guarnieri bodies, and are the sites of viral replication. Guarnieri bodies are readily identified in skin biopsies stained with hematoxylin and eosin, and appear as pink blobs. They are found in virtually all poxvirus infections but the absence of Guarnieri bodies could not be used to rule out smallpox.[68] The diagnosis of an orthopoxvirus infection can also be made rapidly by electron microscopic examination of pustular fluid or scabs. All orthopoxviruses exhibit identical brick-shaped virions by electron microscopy.[34] If particles with the characteristic morphology of herpesviruses are seen this will eliminate smallpox and other orthopoxvirus infections.
 Definitive laboratory identification of variola virus involved growing the virus on chorioallantoic membrane (part of a chicken embryo) and examining the resulting pock lesions under defined temperature conditions.[69] Strains were characterized by polymerase chain reaction (PCR) and restriction fragment length polymorphism (RFLP) analysis. Serologic tests and enzyme linked immunosorbent assays (ELISA), which measured variola virus-specific immunoglobulin and antigen were also developed to assist in the diagnosis of infection.[70]
 Chickenpox was commonly confused with smallpox in the immediate post-eradication era. Chickenpox and smallpox could be distinguished by several methods. Unlike smallpox, chickenpox does not usually affect the palms and soles. Additionally, chickenpox pustules are of varying size due to variations in the timing of pustule eruption: smallpox pustules are all very nearly the same size since the viral effect progresses more uniformly. A variety of laboratory methods were available for detecting chickenpox in the evaluation of suspected smallpox cases.[33]
 The earliest procedure used to prevent smallpox was inoculation with variola minor virus (a method later known as variolation after the introduction of smallpox vaccine to avoid possible confusion), which likely occurred in India, Africa, and China well before the practice arrived in Europe.[15] The idea that inoculation originated in India has been challenged, as few of the ancient Sanskrit medical texts described the process of inoculation.[71] Accounts of inoculation against smallpox in China can be found as early as the late 10th century, and the procedure was widely practiced by the 16th century, during the Ming dynasty.[72] If successful, inoculation produced lasting immunity to smallpox. Because the person was infected with variola virus, a severe infection could result, and the person could transmit smallpox to others. Variolation had a 0.5–2 percent mortality rate, considerably less than the 20–30 percent mortality rate of smallpox.[33] Two reports on the Chinese practice of inoculation were received by the Royal Society in London in 1700; one by Dr. Martin Lister who received a report by an employee of the East India Company stationed in China and another by Clopton Havers.[73]
 Lady Mary Wortley Montagu observed smallpox inoculation during her stay in the Ottoman Empire, writing detailed accounts of the practice in her letters, and enthusiastically promoted the procedure in England upon her return in 1718.[74] According to Voltaire (1742), the Turks derived their use of inoculation from neighbouring Circassia. Voltaire does not speculate on where the Circassians derived their technique from, though he reports that the Chinese have practiced it ""these hundred years"".[75] In 1721, Cotton Mather and colleagues provoked controversy in Boston by inoculating hundreds. After publishing The present method of inoculating for the small-pox in 1767, Dr Thomas Dimsdale was invited to Russia to variolate the Empress Catherine the Great of Russia and her son, Grand Duke Paul, which he successfully did in 1768. In 1796, Edward Jenner, a doctor in Berkeley, Gloucestershire, rural England, discovered that immunity to smallpox could be produced by inoculating a person with material from a cowpox lesion. Cowpox is a poxvirus in the same family as variola. Jenner called the material used for inoculation vaccine from the root word vacca, which is Latin for cow. The procedure was much safer than variolation and did not involve a risk of smallpox transmission. Vaccination to prevent smallpox was soon practiced all over the world. During the 19th century, the cowpox virus used for smallpox vaccination was replaced by the vaccinia virus. Vaccinia is in the same family as cowpox and variola virus but is genetically distinct from both. The origin of the vaccinia virus and how it came to be in the vaccine are not known.[33]
 The current formulation of the smallpox vaccine is a live virus preparation of the infectious vaccinia virus. The vaccine is given using a bifurcated (two-pronged) needle that is dipped into the vaccine solution. The needle is used to prick the skin (usually the upper arm) several times in a few seconds. If successful, a red and itchy bump develops at the vaccine site in three or four days. In the first week, the bump becomes a large blister (called a ""Jennerian vesicle"") which fills with pus and begins to drain. During the second week, the blister begins to dry up, and a scab forms. The scab falls off in the third week, leaving a small scar.[76]
 The antibodies induced by the vaccinia vaccine are cross-protective for other orthopoxviruses, such as monkeypox, cowpox, and variola (smallpox) viruses. Neutralizing antibodies are detectable 10 days after first-time vaccination and seven days after revaccination. Historically, the vaccine has been effective in preventing smallpox infection in 95 percent of those vaccinated.[77] Smallpox vaccination provides a high level of immunity for three to five years and decreasing immunity thereafter. If a person is vaccinated again later, the immunity lasts even longer. Studies of smallpox cases in Europe in the 1950s and 1960s demonstrated that the fatality rate among persons vaccinated less than 10 years before exposure was 1.3 percent; it was 7 percent among those vaccinated 11 to 20 years prior, and 11 percent among those vaccinated 20 or more years before infection. By contrast, 52 percent of unvaccinated persons died.[78]
 There are side effects and risks associated with the smallpox vaccine. In the past, about 1 out of 1,000 people vaccinated for the first time experienced serious, but non-life-threatening, reactions, including toxic or allergic reaction at the site of the vaccination (erythema multiforme), spread of the vaccinia virus to other parts of the body, and spread to other individuals. Potentially life-threatening reactions occurred in 14 to 500 people out of every 1 million people vaccinated for the first time. Based on past experience, it is estimated that 1 or 2 people in 1 million (0.000198 percent) who receive the vaccine may die as a result, most often the result of postvaccinial encephalitis or severe necrosis in the area of vaccination (called progressive vaccinia).[77]
 Given these risks, as smallpox became effectively eradicated and the number of naturally occurring cases fell below the number of vaccine-induced illnesses and deaths, routine childhood vaccination was discontinued in the United States in 1972 and was abandoned in most European countries in the early 1970s.[10][79] Routine vaccination of health care workers was discontinued in the U.S. in 1976, and among military recruits in 1990 (although military personnel deploying to the Middle East and Korea still receive the vaccination[80]). By 1986, routine vaccination had ceased in all countries.[10] It is now primarily recommended for laboratory workers at risk for occupational exposure.[33] However, the possibility of variola virus being used as a biological weapon has rekindled interest in the development of newer vaccines.[81] The smallpox vaccine is also effective in, and therefore administered for, the prevention of mpox.[82]
 ACAM2000 is a smallpox vaccine developed by Acambis, approved for use in the United States by the U.S. FDA on August 31, 2007. It contains live vaccinia virus, cloned from the same strain used in an earlier vaccine, Dryvax. While the Dryvax virus was cultured in the skin of calves and freeze-dried, ACAM2000s virus is cultured in kidney epithelial cells (Vero cells) from an African green monkey. Efficacy and adverse reaction incidence are similar to Dryvax.[81] The vaccine is not routinely available to the US public; it is, however, used in the military and maintained in the Strategic National Stockpile.[83]
 Smallpox vaccination within three days of exposure will prevent or significantly lessen the severity of smallpox symptoms in the vast majority of people. Vaccination four to seven days after exposure can offer some protection from disease or may modify the severity of the disease.[77] Other than vaccination, treatment of smallpox is primarily supportive, such as wound care and infection control, fluid therapy, and possible ventilator assistance. Flat and hemorrhagic types of smallpox are treated with the same therapies used to treat shock, such as fluid resuscitation. People with semi-confluent and confluent types of smallpox may have therapeutic issues similar to patients with extensive skin burns.[84]
 Antiviral treatments have improved since the last large smallpox epidemics, and as of 2004, studies suggested that the antiviral drug cidofovir might be useful as a therapeutic agent. The drug must be administered intravenously, and may cause serious kidney toxicity.[85] In July 2018, the Food and Drug Administration approved tecovirimat, the first drug approved for treatment of smallpox.[86] However, during treatment viral mutations causing resistance have been known to occur, especially since its use in the 2022–2023 mpox outbreak which jeopardize its effectiveness for smallpox biothreat preparedness.[87]
 In June 2021, brincidofovir was approved for medical use in the United States for the treatment of human smallpox disease caused by variola virus.[88][89]
 The mortality rate from variola minor is approximately 1%, while the mortality rate from variola major is approximately 30%.[90]
 Ordinary type-confluent is fatal about 50–75% of the time, ordinary-type semi-confluent about 25–50% of the time, in cases where the rash is discrete the case-fatality rate is less than 10%. The overall fatality rate for children younger than 1 year of age is 40–50%. Hemorrhagic and flat types have the highest fatality rates. The fatality rate for flat or late hemorrhagic type smallpox is 90% or greater and nearly 100% is observed in cases of early hemorrhagic smallpox.[43] The case-fatality rate for variola minor is 1% or less.[38] There is no evidence of chronic or recurrent infection with variola virus.[38] In cases of flat smallpox in vaccinated people, the condition was extremely rare but less lethal, with one case series showing a 67% death rate.[3]
 In fatal cases of ordinary smallpox, death usually occurs between days 10–16 of the illness. The cause of death from smallpox is not clear, but the infection is now known to involve multiple organs. Circulating immune complexes, overwhelming viremia, or an uncontrolled immune response may be contributing factors.[33] In early hemorrhagic smallpox, death occurs suddenly about six days after the fever develops. The cause of death in early hemorrhagic cases is commonly due to heart failure and pulmonary edema. In late hemorrhagic cases, high and sustained viremia, severe platelet loss and poor immune response were often cited as causes of death.[3] In flat smallpox modes of death are similar to those in burns, with loss of fluid, protein and electrolytes, and fulminating sepsis.[84]
 Complications of smallpox arise most commonly in the respiratory system and range from simple bronchitis to fatal pneumonia. Respiratory complications tend to develop on about the eighth day of the illness and can be either viral or bacterial in origin. Secondary bacterial infection of the skin is a relatively uncommon complication of smallpox. When this occurs, the fever usually remains elevated.[33]
 Other complications include encephalitis (1 in 500 patients), which is more common in adults and may cause temporary disability; permanent pitted scars, most notably on the face; and complications involving the eyes (2% of all cases). Pustules can form on the eyelid, conjunctiva, and cornea, leading to complications such as conjunctivitis, keratitis, corneal ulcer, iritis, iridocyclitis, and atrophy of the optic nerve. Blindness results in approximately 35–40% of eyes affected with keratitis and corneal ulcer. Hemorrhagic smallpox can cause subconjunctival and retinal hemorrhages. In 2–5% of young children with smallpox, virions reach the joints and bone, causing osteomyelitis variolosa. Bony lesions are symmetrical, most common in the elbows, legs, and characteristically cause separation of the epiphysis and marked periosteal reactions. Swollen joints limit movement, and arthritis may lead to limb deformities, ankylosis, malformed bones, flail joints, and stubby fingers.[34]
 Between 65 and 80% of survivors are marked with deep pitted scars (pockmarks), most prominent on the face.
 The earliest credible clinical evidence of smallpox is found in the descriptions of smallpox-like disease in medical writings from ancient India (as early as 1500 BCE),[91][92] and China (1122 BCE),[93] as well as a study of the Egyptian mummy of Ramses V (died 1145 BCE).[92][94] It has been speculated that Egyptian traders brought smallpox to India during the 1st millennium BCE, where it remained as an endemic human disease for at least 2000 years. Smallpox was probably introduced into China during the 1st century CE from the southwest, and in the 6th century was carried from China to Japan.[3] In Japan, the epidemic of 735–737 is believed to have killed as much as one-third of the population.[18][95] At least seven religious deities have been specifically dedicated to smallpox, such as the god Sopona in the Yoruba religion in West Africa. In India, the Hindu goddess of smallpox, Shitala, was worshipped in temples throughout the country.[96]
 A different viewpoint is that smallpox emerged 1588 CE and the earlier reported cases were incorrectly identified as smallpox.[97][47]
 The timing of the arrival of smallpox in Europe and south-western Asia is less clear. Smallpox is not clearly described in either the Old or New Testaments of the Bible or in the literature of the Greeks or Romans. While some have identified the Plague of Athens – which was said to have originated in ""Ethiopia"" and Egypt – or the plague that lifted Carthage's 396 BCE siege of Syracuse – with smallpox,[3] many scholars agree it is very unlikely such a serious disease as variola major would have escaped being described by Hippocrates if it had existed in the Mediterranean region during his lifetime.[42]
 While the Antonine Plague that swept through the Roman Empire in 165–180 CE may have been caused by smallpox,[98] Saint Nicasius of Rheims became the patron saint of smallpox victims for having supposedly survived a bout in 450,[3] and Saint Gregory of Tours recorded a similar outbreak in France and Italy in 580, the first use of the term variola.[3] Other historians speculate that Arab armies first carried smallpox from Africa into Southwestern Europe during the 7th and 8th centuries.[3] In the 9th century the Persian physician, Rhazes, provided one of the most definitive descriptions of smallpox and was the first to differentiate smallpox from measles and chickenpox in his Kitab fi al-jadari wa-al-hasbah (The Book of Smallpox and Measles).[99] During the Middle Ages several smallpox outbreaks occurred in Europe. However, smallpox had not become established there until the population growth and mobility marked by the Crusades allowed it to do so. By the 16th century, smallpox had become entrenched across most of Europe,[3] where it had a mortality rate as high as 30 percent. This endemic occurrence of smallpox in Europe is of particular historical importance, as successive exploration and colonization by Europeans tended to spread the disease to other nations. By the 16th century, smallpox had become a predominant cause of morbidity and mortality throughout much of the world.[3]
 There were no credible descriptions of smallpox-like disease in the Americas before the westward exploration by Europeans in the 15th century CE.[45] Smallpox was introduced into the Caribbean island of Hispaniola in 1507, and into the mainland in 1520, when Spanish settlers from Hispaniola arrived in Mexico, inadvertently carrying smallpox with them. Because the native Amerindian population had no acquired immunity to this new disease, their peoples were decimated by epidemics. Such disruption and population losses were an important factor in the Spanish achieving conquest of the Aztecs and the Incas.[3] Similarly, English settlement of the east coast of North America in 1633 in Plymouth, Massachusetts was accompanied by devastating outbreaks of smallpox among Native American populations,[100] and subsequently among the native-born colonists.[101] Case fatality rates during outbreaks in Native American populations were as high as 90%.[102] Smallpox was introduced into Australia in 1789 and again in 1829,[3] though colonial surgeons, who by 1829 were attempting to distinguish between smallpox and chickenpox (which could be almost equally fatal to Aborigines), were divided as to whether the 1829–1830 epidemic was chickenpox or smallpox.[103] Although smallpox was never endemic on the continent,[3] it has been described as the principal cause of death in Aboriginal populations between 1780 and 1870.[104]
 By the mid-18th century, smallpox was a major endemic disease everywhere in the world except in Australia and small islands untouched by outside exploration. In 18th century Europe, smallpox was a leading cause of death, killing an estimated 400,000 Europeans each year.[105] Up to 10 percent of Swedish infants died of smallpox each year,[18] and the death rate of infants in Russia might have been even higher.[93] The widespread use of variolation in a few countries, notably Great Britain, its North American colonies, and China, somewhat reduced the impact of smallpox among the wealthy classes during the latter part of the 18th century, but a real reduction in its incidence did not occur until vaccination became a common practice toward the end of the 19th century. Improved vaccines and the practice of re-vaccination led to a substantial reduction in cases in Europe and North America, but smallpox remained almost unchecked everywhere else in the world. By the mid-20th century, variola minor occurred along with variola major, in varying proportions, in many parts of Africa. Patients with variola minor experience only a mild systemic illness, are often ambulant throughout the course of the disease, and are therefore able to more easily spread disease. Infection with variola minor virus induces immunity against the more deadly variola major form. Thus, as variola minor spread all over the US, into Canada, the South American countries, and Great Britain, it became the dominant form of smallpox, further reducing mortality rates.[3]
 
 The first clear reference to smallpox inoculation was made by the Chinese author Wan Quan (1499–1582) in his Dòuzhěn xīnfǎ (痘疹心法, ""Pox Rash Teachings"") published in 1549,[106] with earliest hints of the practice in China during the 10th century.[107] In China, powdered smallpox scabs were blown up the noses of the healthy. People would then develop a mild case of the disease and from then on were immune to it. The technique did have a 0.5–2.0% mortality rate, but that was considerably less than the 20–30% mortality rate of the disease itself. Two reports on the Chinese practice of inoculation were received by the Royal Society in London in 1700: one by Dr. Martin Lister who received a report by an employee of the East India Company stationed in China and another by Clopton Havers.[108] Voltaire (1742) reports that the Chinese had practiced smallpox inoculation ""these hundred years"".[75] Variolation had also been witnessed in Turkey by Lady Mary Wortley Montagu, who later introduced it in the UK.[109]
 An early mention of the possibility of smallpox's eradication was made in reference to the work of Johnnie Notions, a self-taught inoculator from Shetland, Scotland. Notions found success in treating people from at least the late 1780s through a method devised by himself despite having no formal medical background.[110][111] His method involved exposing smallpox pus to peat smoke, burying it in the ground with camphor for up to 8 years, and then inserting the matter into a person's skin using a knife, and covering the incision with a cabbage leaf.[112] He was reputed not to have lost a single patient.[112] Arthur Edmondston, in writings on Notions' technique that were published in 1809, stated, ""Had every practitioner been as uniformly successful in the disease as he was, the small-pox might have been banished from the face of the earth, without injuring the system, or leaving any doubt as to the fact.""[113]
 The English physician Edward Jenner demonstrated the effectiveness of cowpox to protect humans from smallpox in 1796, after which various attempts were made to eliminate smallpox on a regional scale. In Russia in 1796, the first child to receive this treatment was bestowed the name ""Vaccinov"" by Catherine the Great, and was educated at the expense of the nation.[114]
 The introduction of the vaccine to the New World took place in Trinity, Newfoundland in 1800 by Dr. John Clinch, boyhood friend and medical colleague of Jenner.[115] As early as 1803, the Spanish Crown organized the Balmis expedition to transport the vaccine to the Spanish colonies in the Americas and the Philippines, and establish mass vaccination programs there.[116] The U.S. Congress passed the Vaccine Act of 1813 to ensure that safe smallpox vaccine would be available to the American public. By about 1817, a robust state vaccination program existed in the Dutch East Indies.[117]
 On March 26, 1806, the Swiss canton Thurgau became the first state in the world to introduce compulsory smallpox vaccinations, by order of the cantonal councillor Jakob Christoph Scherb.[118][119] Half a year later, Elisa Bonaparte issued a corresponding order for her Principality of Lucca and Piombino.[120] Baden followed in 1809, Prussia in 1815, Württemberg in 1818, Sweden in 1816 and the German Empire in 1874 through the Reichs Vaccination Act.[121][122] In Lutheran Sweden, the Protestant clergy played a pioneering role in voluntary smallpox vaccination as early as 1800.[123] The first vaccination was carried out in Liechtenstein in 1801, and from 1812 it was mandatory to vaccinate.[124]
 In British India a program was launched to propagate smallpox vaccination, through Indian vaccinators, under the supervision of European officials.[125] Nevertheless, British vaccination efforts in India, and in Burma in particular, were hampered by indigenous preference for inoculation and distrust of vaccination, despite tough legislation, improvements in the local efficacy of the vaccine and vaccine preservative, and education efforts.[126] By 1832, the federal government of the United States established a smallpox vaccination program for Native Americans.[127] In 1842, the United Kingdom banned inoculation, later progressing to mandatory vaccination. The British government introduced compulsory smallpox vaccination by an Act of Parliament in 1853.[128]
 In the United States, from 1843 to 1855, first Massachusetts and then other states required smallpox vaccination. Although some disliked these measures,[93] coordinated efforts against smallpox went on, and the disease continued to diminish in the wealthy countries. In Northern Europe a number of countries had eliminated smallpox by 1900, and by 1914, the incidence in most industrialized countries had decreased to comparatively low levels.
 Vaccination continued in industrialized countries as protection against reintroduction until the mid to late 1970s. Australia and New Zealand are two notable exceptions; neither experienced endemic smallpox and never vaccinated widely, relying instead on protection by distance and strict quarantines.[129]
 The first hemisphere-wide effort to eradicate smallpox was made in 1950 by the Pan American Health Organization.[130] The campaign was successful in eliminating smallpox from all countries of the Americas except Argentina, Brazil, Colombia, and Ecuador.[129] In 1958 Professor Viktor Zhdanov, Deputy Minister of Health for the USSR, called on the World Health Assembly to undertake a global initiative to eradicate smallpox.[131] The proposal (Resolution WHA11.54) was accepted in 1959.[131] At this point, 2 million people were dying from smallpox every year. Overall, the progress towards eradication was disappointing, especially in Africa and in the Indian subcontinent. In 1966 an international team, the Smallpox Eradication Unit, was formed under the leadership of an American, Donald Henderson.[131] In 1967, the World Health Organization intensified the global smallpox eradication by contributing $2.4 million annually to the effort, and adopted the new disease surveillance method promoted by Czech epidemiologist Karel Raška.[132]
 In the early 1950s, an estimated 50 million cases of smallpox occurred in the world each year.[10] To eradicate smallpox, each outbreak had to be stopped from spreading, by isolation of cases and vaccination of everyone who lived close by.[133] This process is known as ""ring vaccination"". The key to this strategy was the monitoring of cases in a community (known as surveillance) and containment.
 The initial problem the WHO team faced was inadequate reporting of smallpox cases, as many cases did not come to the attention of the authorities. The fact that humans are the only reservoir for smallpox infection (the virus only infected humans and not other animals) and that carriers did not exist played a significant role in the eradication of smallpox. The WHO established a network of consultants who assisted countries in setting up surveillance and containment activities. Early on, donations of vaccine were provided primarily by the Soviet Union and the United States, but by 1973, more than 80 percent of all vaccine was produced in developing countries.[129] The Soviet Union provided one and a half billion doses between 1958 and 1979, as well as the medical staff.[134]
 The last major European outbreak of smallpox was in 1972 in Yugoslavia, after a pilgrim from Kosovo returned from the Middle East, where he had contracted the virus. The epidemic infected 175 people, causing 35 deaths. Authorities declared martial law, enforced quarantine, and undertook widespread re-vaccination of the population, enlisting the help of the WHO. In two months, the outbreak was over.[135] Prior to this, there had been a smallpox outbreak in May–July 1963 in Stockholm, Sweden, brought from the Far East by a Swedish sailor; this had been dealt with by quarantine measures and vaccination of the local population.[136]
 By the end of 1975, smallpox persisted only in the Horn of Africa. Conditions were very difficult in Ethiopia and Somalia, where there were few roads. Civil war, famine, and refugees made the task even more difficult. An intensive surveillance, containment, and vaccination program was undertaken in these countries in early and mid-1977, under the direction of Australian microbiologist Frank Fenner. As the campaign neared its goal, Fenner and his team played an important role in verifying eradication.[138] The last naturally occurring case of indigenous smallpox (Variola minor) was diagnosed in Ali Maow Maalin, a hospital cook in Merca, Somalia, on 26 October 1977.[33] The last naturally occurring case of the more deadly Variola major had been detected in October 1975 in a three-year-old Bangladeshi girl, Rahima Banu.[40]
 The global eradication of smallpox was certified, based on intense verification activities, by a commission of eminent scientists on 9 December 1979 and subsequently endorsed by the World Health Assembly on 8 May 1980.[10][139] The first two sentences of the resolution read:
 The cost of the eradication effort, from 1967 to 1979, was roughly US$300 million. Roughly a third came from the developed world, which had largely eradicated smallpox decades earlier. The United States, the largest contributor to the program, has reportedly recouped that investment every 26 days since in money not spent on vaccinations and the costs of incidence.[141]
 The last case of smallpox in the world occurred in an outbreak in the United Kingdom in 1978.[142] A medical photographer, Janet Parker, contracted the disease at the University of Birmingham Medical School and died on 11 September 1978. Although it has remained unclear how Parker became infected, the source of the infection was established to be the variola virus grown for research purposes at the Medical School laboratory.[143][144] All known stocks of smallpox worldwide were subsequently destroyed or transferred to two WHO-designated reference laboratories with BSL-4 facilities – the United States' Centers for Disease Control and Prevention (CDC) and the Soviet Union's (now Russia's) State Research Center of Virology and Biotechnology VECTOR.[145]
 WHO first recommended destruction of the virus in 1986 and later set the date of destruction to be 30 December 1993. This was postponed to 30 June 1999.[146] Due to resistance from the U.S. and Russia, in 2002 the World Health Assembly agreed to permit the temporary retention of the virus stocks for specific research purposes.[147] Destroying existing stocks would reduce the risk involved with ongoing smallpox research; the stocks are not needed to respond to a smallpox outbreak.[148] Some scientists have argued that the stocks may be useful in developing new vaccines, antiviral drugs, and diagnostic tests;[149] a 2010 review by a team of public health experts appointed by WHO concluded that no essential public health purpose is served by the U.S. and Russia continuing to retain virus stocks.[150] The latter view is frequently supported in the scientific community, particularly among veterans of the WHO Smallpox Eradication Program.[151]
 On March 31, 2003, smallpox scabs were found inside an envelope in an 1888 book on Civil War medicine in Santa Fe, New Mexico.[152] The envelope was labeled as containing scabs from a vaccination and gave scientists at the CDC an opportunity to study the history of smallpox vaccination in the United States.
 On July 1, 2014, six sealed glass vials of smallpox dated 1954, along with sample vials of other pathogens, were discovered in a cold storage room in an FDA laboratory at the National Institutes of Health location in Bethesda, Maryland. The smallpox vials were subsequently transferred to the custody of the CDC in Atlanta, where virus taken from at least two vials proved viable in culture.[153][154] After studies were conducted, the CDC destroyed the virus under WHO observation on February 24, 2015.[155]
 In 2017, scientists at the University of Alberta recreated an extinct horse pox virus to demonstrate that the variola virus can be recreated in a small lab at a cost of about $100,000, by a team of scientists without specialist knowledge.[156] This makes the retention controversy irrelevant since the virus can be easily recreated even if all samples are destroyed. Although the scientists performed the research to help development of new vaccines as well as trace smallpox's history, the possibility of the techniques being used for nefarious purposes was immediately recognized, raising questions on dual use research and regulations.[157][158]
 In September 2019, the Russian lab housing smallpox samples experienced a gas explosion that injured one worker. It did not occur near the virus storage area, and no samples were compromised, but the incident prompted a review of risks to containment.[159]
 In 1763, Pontiac's War broke out as a Native American confederacy led by Pontiac attempted to counter British control over the Great Lakes region.[160][161][162] A group of Native American warriors laid siege to British-held Fort Pitt on June 22.[163] In response, Henry Bouquet, the commander of the fort, ordered his subordinate Simeon Ecuyer to give smallpox-infested blankets from the infirmary to a Delaware delegation outside the fort. Bouquet had discussed this with his superior, Sir Jeffrey Amherst, who wrote to Bouquet stating: ""Could it not be contrived to send the small pox among the disaffected tribes of Indians? We must on this occasion use every stratagem in our power to reduce them."" Bouquet agreed with the proposal, writing back that ""I will try to inocculate  [sic] the Indians by means of Blankets that may fall in their hands"".[164] On 24 June 1763, William Trent, a local trader and commander of the Fort Pitt militia, wrote, ""Out of our regard for them, we gave them two Blankets and an Handkerchief out of the Small Pox Hospital. I hope it will have the desired effect.""[165][160] The effectiveness of this effort to broadcast the disease is unknown. There are also accounts that smallpox was used as a weapon during the American Revolutionary War (1775–1783).[166][167]
 According to a theory put forward in Journal of Australian Studies (JAS) by independent researcher Christopher Warren, Royal Marines used smallpox in 1789 against indigenous tribes in New South Wales.[168] This theory was also considered earlier in Bulletin of the History of Medicine[169] and by David Day.[170] However it is disputed by some medical academics, including Professor Jack Carmody, who in 2010 claimed that the rapid spread of the outbreak in question was more likely indicative of chickenpox – a more infectious disease which, at the time, was often confused, even by surgeons, with smallpox, and may have been comparably deadly to Aborigines and other peoples without natural immunity to it.[171] Carmody noted that in the 8-month voyage of the First Fleet and the following 14 months there were no reports of smallpox amongst the colonists and that, since smallpox has an incubation period of 10–12 days, it is unlikely it was present in the First Fleet; however, Warren argued in the JAS article that the likely source was bottles of variola virus possessed by First Fleet surgeons. Ian and Jennifer Glynn, in The life and death of smallpox, confirm that bottles of ""variolous matter"" were carried to Australia for use as a vaccine, but think it unlikely the virus could have survived till 1789.[104] In 2007, Christopher Warren offered evidence that the British smallpox may have been still viable.[172] However, the only non-Aborigine reported to have died in this outbreak was a seaman called Joseph Jeffries, who was recorded as being of ""American Indian"" origin.[173]
 W. S. Carus, an expert in biological weapons, has written that there is circumstantial evidence that smallpox was deliberately introduced to the Aboriginal population.[174] However Carmody and the Australian National University's Boyd Hunter continue to support the chickenpox hypothesis.[175] In a 2013 lecture at the Australian National University,[176] Carmody pointed out that chickenpox, unlike smallpox, was known to be present in the Sydney Cove colony. He also suggested that all c. 18th century (and earlier) identifications of smallpox outbreaks were dubious because: ""surgeons … would have been unaware of the distinction between smallpox and chickenpox – the latter having traditionally been considered a milder form of smallpox.""[177]
 During World War II, scientists from the United Kingdom, United States, and Japan (Unit 731 of the Imperial Japanese Army) were involved in research into producing a biological weapon from smallpox.[178] Plans of large scale production were never carried through as they considered that the weapon would not be very effective due to the wide-scale availability of a vaccine.[166]
 In 1947, the Soviet Union established a smallpox weapons factory in the Scientific Research Institute of Medicine of the Ministry of Defense in Sergiyev Posad in Zagorsk, 75 km to the northeast of Moscow.[179] An outbreak of weaponized smallpox occurred during testing at a facility on an island in the Aral Sea in 1971. General Prof. Peter Burgasov, former Chief Sanitary Physician of the Soviet Army and a senior researcher within the Soviet program of biological weapons, described the incident:
 Others contend that the first patient may have contracted the disease while visiting Uyaly or Komsomolsk-on-Ustyurt, two cities where the boat docked.[182][183]
 Responding to international pressures, in 1991 the Soviet government allowed a joint U.S.–British inspection team to tour four of its main weapons facilities at Biopreparat. The inspectors were met with evasion and denials from the Soviet scientists and were eventually ordered out of the facility.[184] In 1992, Soviet defector Ken Alibek alleged that the Soviet bioweapons program at Zagorsk had produced a large stockpile – as much as twenty tons – of weaponized smallpox (possibly engineered to resist vaccines, Alibek further alleged), along with refrigerated warheads to deliver it. Alibek's stories about the former Soviet program's smallpox activities have never been independently verified.
 In 1997, the Russian government announced that all of its remaining smallpox samples would be moved to the Vector Institute in Koltsovo.[184] With the breakup of the Soviet Union and unemployment of many of the weapons program's scientists, U.S. government officials have expressed concern that smallpox and the expertise to weaponize it may have become available to other governments or terrorist groups who might wish to use virus as means of biological warfare.[185] Specific allegations made against Iraq in this respect proved to be false.[186]
 Famous historical figures who contracted smallpox include Lakota Chief Sitting Bull, Ramses V,[187] the Kangxi Emperor (survived), Shunzhi Emperor and Tongzhi Emperor of China, Emperor Komei of Japan (died of smallpox in 1867), and Date Masamune of Japan (who lost an eye to the disease). Cuitláhuac, the 10th tlatoani (ruler) of the Aztec city of Tenochtitlan, died of smallpox in 1520, shortly after its introduction to the Americas, and the Incan emperor Huayna Capac died of it in 1527 (causing a civil war of succession in the Inca empire and the eventual conquest by the Spaniards). More recent public figures include Guru Har Krishan, 8th Guru of the Sikhs, in 1664, Louis I of Spain in 1724 (died), Peter II of Russia in 1730 (died),[188] George Washington (survived), Louis XV of France in 1774 (died) and Maximilian III Joseph of Bavaria in 1777 (died).
 Prominent families throughout the world often had several people infected by and/or perish from the disease. For example, several relatives of Henry VIII of England survived the disease but were scarred by it. These include his sister Margaret, his wife Anne of Cleves, and his two daughters: Mary I in 1527 and Elizabeth I in 1562. Elizabeth tried to disguise the pockmarks with heavy makeup. Mary, Queen of Scots, contracted the disease as a child but had no visible scarring.
 In Europe, deaths from smallpox often changed dynastic succession. Louis XV of France succeeded his great-grandfather Louis XIV through a series of deaths of smallpox or measles among those higher in the succession line. He himself died of the disease in 1774. Peter II of Russia died of the disease at 14 years of age. Also, before becoming emperor, Peter III of Russia caught the virus and suffered greatly from it.[citation needed] He was left scarred and disfigured. His wife, Catherine the Great, was spared but fear of the virus clearly had its effects on her. She feared for the safety of her son, Paul, so much that she made sure that large crowds were kept at bay and sought to isolate him. Eventually, she decided to have herself inoculated by a British doctor, Thomas Dimsdale. While this was considered a controversial method at the time, she succeeded. Paul was later inoculated as well. Catherine then sought to have inoculations throughout her empire stating: ""My objective was, through my example, to save from death the multitude of my subjects who, not knowing the value of this technique, and frightened of it, were left in danger."" By 1800, approximately two million inoculations had been administered in the Russian Empire.[189]
 In China, the Qing dynasty had extensive protocols to protect Manchus from Peking's endemic smallpox.
 U.S. Presidents George Washington, Andrew Jackson, and Abraham Lincoln all contracted and recovered from the disease. Washington became infected with smallpox on a visit to Barbados in 1751.[190] Jackson developed the illness after being taken prisoner by the British during the American Revolution, and though he recovered, his brother Robert did not.[190] Lincoln contracted the disease during his presidency, possibly from his son Tad, and was quarantined shortly after giving the Gettysburg address in 1863.[190]
 The famous theologian Jonathan Edwards died of smallpox in 1758 following an inoculation.[191]
 Soviet leader Joseph Stalin fell ill with smallpox at the age of seven. His face was badly scarred by the disease. He later had photographs retouched to make his pockmarks less apparent.[192]
 Hungarian poet Ferenc Kölcsey, who wrote the Hungarian national anthem, lost his right eye to smallpox.[193]
 In the face of the devastation of smallpox, various smallpox gods and goddesses have been worshipped throughout parts of the Old World, for example in China and India. In China, the smallpox goddess was referred to as T'ou-Shen Niang-Niang (Chinese: 痘疹娘娘).[194] Chinese believers actively worked to appease the goddess and pray for her mercy, by such measures as referring to smallpox pustules as ""beautiful flowers"" as a euphemism intended to avert offending the goddess, for example (the Chinese word for smallpox is 天花, literally ""heaven flower"").[195] In a related New Year's Eve custom it was prescribed that the children of the house wear ugly masks while sleeping, so as to conceal any beauty and thereby avoid attracting the goddess, who would be passing through sometime that night.[195] If a case of smallpox did occur, shrines would be set up in the homes of the victims, to be worshipped and offered to as the disease ran its course. If the victim recovered, the shrines were removed and carried away in a special paper chair or boat for burning. If the patient did not recover, the shrine was destroyed and cursed, to expel the goddess from the house.[194]
 In the Yoruba language smallpox is known as ṣọpọná, but it was also written as shakpanna, shopona, ṣhapana, and ṣọpọnọ. The word is a combination of 3 words, the verb ṣán, meaning to cover or plaster (referring to the pustules characteristic of smallpox), kpa or pa, meaning to kill, and enia, meaning human. Roughly translated, it means One who kills a person by covering them with pustules.[196] Among the Yorùbá people of West Africa, and also in Dahomean religion, Trinidad, and in Brazil, The deity Sopona, also known as Obaluaye, is the deity of smallpox and other deadly diseases (like leprosy, HIV/AIDS, and fevers). One of the most feared deities of the orisha pantheon, smallpox was seen as a form of punishment from Shopona.[197] Worship of Shopona was highly controlled by his priests, and it was believed that priests could also spread smallpox when angered.[197] However, Shopona was also seen as a healer who could cure the diseases he inflicted, and he was often called upon by his victims to heal them.[198] The British government banned the worship of the god because it was believed his priests were purposely spreading smallpox to their opponents.[198][197]
 India's first records of smallpox can be found in a medical book that dates back to 400 CE. This book describes a disease that sounds exceptionally like smallpox.[195] India, like China and the Yorùbá, created a goddess in response to its exposure to smallpox. The Hindu goddess Shitala was both worshipped and feared during her reign. It was believed that this goddess was both evil and kind and had the ability to inflict victims when angered, as well as calm the fevers of the already affected.[199][93] Portraits of the goddess show her holding a broom in her right hand to continue to move the disease and a pot of cool water in the other hand in an attempt to soothe patients.[195] Shrines were created where many Indian natives, both healthy and not, went to worship and attempt to protect themselves from this disease. Some Indian women, in an attempt to ward off Shitala, placed plates of cooling foods and pots of water on the roofs of their homes.[200]
 In cultures that did not recognize a smallpox deity, there was often nonetheless a belief in smallpox demons, who were accordingly blamed for the disease. Such beliefs were prominent in Japan, Europe, Africa, and other parts of the world. Nearly all cultures who believed in the demon also believed that it was afraid of the color red. This led to the invention of the so-called red treatment, where patients and their rooms would be decorated in red. The practice spread to Europe in the 12th century and was practiced by (among others) Charles V of France and Elizabeth I of England.[3] Afforded scientific credibility through the studies by Niels Ryberg Finsen showing that red light reduced scarring,[3] this belief persisted even until the 1930s.
"
Virginia militia,https://en.wikipedia.org/wiki/Virginia_militia,"The Virginia militia is an armed force composed of all citizens of the Commonwealth of Virginia capable of bearing arms.  The Virginia militia was established in 1607 as part of the English militia system. Militia service in Virginia was compulsory for all free males. The main purpose of the Crown's militia was to repel invasions and insurrections and to enforce the laws of the colony.
 In 1623, the year following the outbreak of the first major Anglo-Powhatan War in Virginia, the Virginia General Assembly commanded, ""that men go not to work in the ground without their arms; That no man go or send abroad without a sufficient partie well armed."" In 1661 Governor William Berkeley stated, ""All our freemen are bound to be trained every month in their particular counties."" The British county lieutenant system was employed as the population grew; each county had a lieutenant, appointed as the county's chief militia officer.
 The militia system was originally used to defend against Native American tribes in the tidewater area. As the slave population grew in the Virginia Colony, the militia played a role in keeping slaves from running away or from revolting – through the use of militia patrollers.[1] This Virginia militia system was put to the test in 1676 during Bacon's Rebellion. The Crown's militia was victorious over Nathaniel Bacon, who tried to seize power.
 Virginia militia under the command of Colonel James Patton fought an Iroquois war party at the Battle of Galudoghson in December 1742. This was the first military action between Virginia soldiers and Native Americans in western Virginia.[2]
 During the French and Indian War (1754–1763), a formal act came into effect.
 Four companies of Virginia militia participated in the Sandy Creek Expedition in February 1756, however the expedition was forced to turn back due to harsh weather and lack of food.[4]
 In September 1756, Colonel George Washington was unsuccessful in raising a militia in Augusta County to attack the active Indian warriors on the Jackson River.[5] Militia were required to man a series of strategically sited forts on the Virginia frontier.
 In 1774, the American Revolution was at Virginia's doorstep when Royal Governor Lord Dunmore dissolved the Virginia House of Burgesses because of their support of the city of Boston against the closing of the Port of Boston by Lord North.  On May 15, 1776, the Virginia General Assembly voted unanimously for independence and to have a declaration of rights drawn up. Colonel George Mason became the principal author of the Virginia Declaration of Rights which was published on June 12, 1776.[6] Mason drew from his own previous writings upon his founding of the Fairfax County Independent Company of Volunteers on September 21, 1774.[7] This company was a paramilitary organization independent of the Crown's militia. Article 13 of the Virginia Declaration of Rights which established the militia clause as a fundamental right was based upon three solid English rights: the right to revolution, the right to group self-preservation and the right to self-defense. Under Article 13 of the Virginia Declaration of Rights he wrote:
 Shortly after the Revolutionary War began, Kentucky County, Virginia was organized with George Rogers Clark as head of its local militia.  Clark asked Governor Patrick Henry  for permission to lead a secret expedition to capture the nearest British posts, which were located in the Illinois country. Governor Henry commissioned Clark as a lieutenant colonel and authorized him to raise troops for the expedition.[8]  The Illinois campaign began in July 1778, when Clark and about 175 men crossed the Ohio River at Fort Massac and marched to Kaskaskia, taking it on the night of July 4.[9] Cahokia, Vincennes, and several other villages and forts in British territory were subsequently captured without firing a shot, because most of the French-speaking and American Indian inhabitants were unwilling to take up arms on behalf of the British. To counter Clark's advance, Henry Hamilton reoccupied Vincennes with a small force.[10] In February 1779, Clark returned to Vincennes in a surprise winter expedition and retook the town, capturing Hamilton in the process. The winter expedition was Clark's most significant military achievement.[11]
 The Virginia militia system, as a compulsory service composed of the body of the people trained to arms as envisioned by George Mason, remained intact until the end of the American Civil War. During the Civil War, the Virginia militia was the main recruiting body for first the Provisional Army of Virginia and later the Virginia state regiments of the Confederate Army.  After the Civil War, Reconstruction governments forced upon Virginia an all-volunteer militia system in opposition to Virginia's Bill of Rights. The militia became statutorily composed of the volunteer and the unorganized militia.
 In 1971, the Virginia Bill of Rights under Article I, Section 13, was changed to the following by popular vote[12]
 
The current Virginia Militia under Virginia Code states  The Code of Virginia, Section 44-117 states, ""The officers of the Virginia Military Institute, the Virginia Women's Institute for Leadership at Mary Baldwin College, the Fishburne Military School, the Massanutten Military Academy, and the Commandant of Cadets and Assistant Commandants of Cadets of the Virginia Polytechnic Institute and State University shall be commissioned officers of the Virginia militia, unorganized, and subject to the orders of the Governor and the same rules and regulations as to discipline provided for other commissioned officers of the military organizations of the Commonwealth.""[14] 
 Permanent faculty members of the Virginia Military Institute (VMI) are normally offered commissions in the naval or unorganized militia of Virginia. The Superintendent of VMI is normally a Lieutenant General of the unorganized Virginia militia unless the superintendent is a regular US military officer of higher rank. The corps of cadets also serves as cadet members of the unorganized militia. Staff members of the Virginia Tech Corps of Cadets also may hold officer appointments in the Virginia Militia, unless they hold higher rank as active or retired US military officers.  All graduates of the Virginia Tech Corps of Cadets, who complete either a leadership track or ROTC education, are automatically commissioned as Second Lieutenants in the Virginia Militia.[15]
"
Commission (document),https://en.wikipedia.org/wiki/Commission_(document),"
 A commission is a formal document issued to appoint a named person to high office or as a commissioned officer in a territory's armed forces.  A commission constitutes documentary authority that the person named is vested with the powers of that office and is empowered to execute official acts.[1]  A commission often takes the form of letters patent.  
 Commissions are typically issued in the name of or signed by the head of state. In Commonwealth realms, the documentation is referred to as a King's Commission or Queen's Commission (depending on the gender of the reigning monarch). However, in Commonwealth realms other than the United Kingdom, they may be signed by the governor-general, the representative of the monarch of that realm.
 Because the word ""commission"" can also refer generally to an individual's duty, the more specific terms commissioning parchment or commissioning scroll are often used to specify the commissioning document. However the document is not usually in the form of a scroll and is more often printed on paper instead of parchment.  In Canada, there is a differentiation in terminology according to rank; officers are accorded commissioning scripts.[citation needed]
 Here is an example from Canada:
 Canadian Commissioning Scripts, as they are properly called by NDHQ, are signed by the Governor General of Canada and countersigned by the Minister of National Defence, on behalf of the King of Canada.[citation needed]
 Here is an example of the Royal Canadian Navy's Commission from pre-1968:
 Officers in the Swedish Armed Forces have not received written commissions since 1982 when a new employment structure was instituted by law. They are nowadays hired on contracts, as in any other civil service position. Prior to 1982 all officers received written certificates of commission, each signed by the King of Sweden.
 The wording used prior to 1982 in translation would be;
 The following is typical of the wording of a British commission during the reign of Charles III,[2]
 The commission would be signed by the King at the top left of the scroll (although a facsimile signature may be used) and countersigned at the bottom of the scroll by two senior members of the Ministry of Defence.
 Before the Board of Admiralty were merged into the Ministry of Defence in 1964, with the title of Lord High Admiral reverting to the Crown, the naval officer's commission was signed not by the Sovereign but by the Lords Commissioners of the Admiralty, executing the office of Lord High Admiral. The naval officer's commission was worded as follows:[3]
 Similarly the following is the wording of a Lieutenant's Commission from 1800:
 It was signed by two Lords Commissioners of the Admiralty and a Secretary, i.e. a quorum of the Board of Admiralty.
 Article II, section 3, of the U.S. Constitution provides that the President ""shall Commission all the Officers of the United States,"" including officers of the uniformed services as well as civilian officers.  Commissions of officers in the armed services are issued in the name of the President, although authority to sign on the President's behalf is generally exercised by the secretary of the department in which the officer is being commissioned.  This includes not only ""commissioned officers"" but also ""commissioned warrant officers"" (warrant officers in the pay grades of W-2 through W-5).  Warrant officers at the grade of W-1 are appointed by warrant by the secretary of their respective service, except in the Coast Guard where they are appointed by secretarial commission.
 The commission of a newly commissioned officer reads [citation needed]:
 At higher grade levels (Major or Lieutenant Commander and above in the Regular components and Colonel or Captain and above in the Reserve components), appointments and promotions require Senate confirmation, and the wording of the commission reflects that fact:  ""... I have nominated and, by and with the Advice and Consent of the Senate, do appoint...""
 The Constitutional requirement mentioned above, that the President commission all officers of the United States, includes a wide range of civilian officials, including justices of the Supreme Court and other federal judges, the heads of executive departments, subcabinet level officials down to the level of assistant secretary, U.S. attorneys and marshals, diplomatic representatives, and members of the Foreign Service, among others.  Commissions are issued in the name of the President, either under his own signature or that of an official delegated to act on his behalf, and under either the Great Seal of the United States or, the seal of the executive department in which the appointment is made.
 A typical commission for a Presidentially appointed, Senate-confirmed civilian official in the Executive Branch would read:
 For heads of executive departments and independent agencies, the Seal of the United States and the signature of the Secretary of State appears, but if the position is subordinate to the head of a different executive department, the seal of said executive department appears instead of the Seal of the United States and the signature of beforementioned head replaces the Secretary of State.
 For certain positions, other characteristics such as ""prudence"" (for ambassadors) or ""wisdom, uprightness, and learning"" (for judges) may be used in addition to or instead of ""integrity and ability.""  If a position is for a fixed term of years or ""during good behavior,"" the appropriate wording replaces the clause beginning ""during the pleasure of the President.""
 Commissions of officers in the U.S. Foreign Service are also signed by the President.  The commission of a newly commissioned officer reads:
 The commission is countersigned by the Secretary of State, and the singular Great Seal of the United States, entrusted to the Secretary under the 1789 statute creating the Department of State,[4] is affixed.
 Similar to the U.S. Constitution's provisions directing the President to commission executive officers of the U.S. Government, the state constitutions and/or laws provide for state (and sometimes local) officers to be commissioned; for example, Texas law directs the Texas governor to commission most state officers and elected county officers.[5]
 A person applying for a license to be a notary public receives a commission, generally indicating what political jurisdiction (state or District of Columbia) issued it, when it is valid (usually four years from issue) and the signature of the issuing authorities (usually the Governor and countersigned by the Secretary of State).
"
Robert Dinwiddie,https://en.wikipedia.org/wiki/Robert_Dinwiddie,"
 Robert Dinwiddie (1692 – 27 July 1770) was a Scottish colonial administrator who served as the lieutenant governor of Virginia from 1751 to 1758. Since the governors of Virginia remained in Great Britain, he served as the de facto head of the colony of Virginia. Dinwiddie is credited for starting the military career of George Washington.[1]
 Dinwiddie was born at Glasgow before 2 October 1692, the son of Robert Dinwiddie of Germiston and Elizabeth Cumming.[2] His younger brother Lawrence Dinwiddie was later Lord Provost of Glasgow.[3] He was a descendant of the Lordly Dinwiddies of Annandale.
 He matriculated at the university in 1707 before starting work as a merchant. Joining the British colonial service in 1727, Dinwiddie was appointed collector of the customs for Bermuda.  Following an appointment as surveyor general of customs in southern American ports, Dinwiddie became Lieutenant Governor of Virginia, and featured as such in William Makepeace Thackeray's nineteenth-century historical novel The Virginians: A Tale of the Last Century.[4]
 Dinwiddie's actions as lieutenant governor are cited by one historian as precipitating the French and Indian War, commonly held to have begun in 1754. He wanted to limit French expansion in Ohio Country, an area claimed by the Virginia Colony and in which the Ohio Company, of which he was a stockholder,[5] had made preliminary surveys and some small settlements.  This version of history is disputed when one notices that Father Le Loutre's War in Acadia began in 1749 and did not end until the expulsion of the Acadians in 1755.  In fact, Thomas Jefferys, the Royal Geographer of the day, produced a pamphlet out of his Parliamentary testimony that explained the misconduct of the French in what amounted to a Treaty of Utrecht boundary dispute.
 In 1753, Dinwiddie learned the French had built Fort Presque Isle near Lake Erie and Fort Le Boeuf, which he saw as threatening Virginia's interests in the Ohio Valley. In fact, he considered Winchester, Virginia, to be ""Exposed to the enemy""; Cumberland, Maryland, was only to be fortified the next year.[6]
 Dinwiddie sent an eight-man expedition under George Washington to warn the French to withdraw. Washington, then only 21 years old, made the journey in midwinter of 1753–54. Washington arrived at Fort Le Boeuf on 11 December 1753.  Jacques Legardeur de Saint-Pierre, commandant at Fort Le Boeuf, a tough veteran of the west, received Washington politely, but rejected his ultimatum.[7]
 Jacques Saint-Pierre gave Washington three days hospitality at the fort, and then gave Washington a letter for him to deliver to Dinwiddie.  The letter conveyed to Dinwiddie that he would send Dinwiddie's on to Marquis de Duquesne in Quebec and would meantime maintain his post while he awaited the latter's orders.[8][9]
 In January 1754, even before learning of the French refusal to decamp, Dinwiddie sent a small force of Virginia militia to build a fort at the forks of the Ohio River, where the Allegheny and Monongahela rivers merge to form the Ohio (present-day Pittsburgh). The French quickly drove off the Virginians and built a larger fort on the site, calling it Fort Duquesne, in honour of the Marquis de Duquesne, the then-governor of New France.
 Dinwiddie named Joshua Fry to the position of Commander-in-Chief of colonial forces. Fry was given command of the Virginia Regiment and was ordered to take Fort Duquesne, then held by the French. During the advance into the Ohio Country, Fry suddenly fell off his horse and died from his injuries on 31 May 1754 at Fort Cumberland, upon which the command of the regiment fell to Washington.
 In early spring 1754, Dinwiddie sent Washington to build a road to the Monongahela. After having attacked the French at the Battle of Jumonville Glen, Washington retreated and built a small stockade, Fort Necessity, at a spot then called ""Great Meadows"", by the Youghiogheny River, eleven miles southeast of present-day Uniontown. Here he encountered the French in a skirmish on 3 July 1754 and was forced to surrender. Dinwiddie was subsequently active in rallying other colonies in defense against France and ultimately prevailed upon the British to send General Edward Braddock to Virginia with two regiments of regular troops, in part with a letter to Lord Halifax on 25 October 1754 containing these words:[10]
 Braddock met his end at the Battle of Monongahela on 9 July 1755; this unexpected reverse prompted much concern and fear in the colony. Over the next four years, until the defeat of the French at the Battle of the Plains of Abraham in Quebec, and Vaudreuil's capitulation before Amherst at Montreal, the fate of the 13 colonies was uncertain and Dinwiddie's administration was marked by frequent disagreements with the Assembly over the financing of the war. In fact, the friction between the government and the Burgesses would eventually develop into the American Revolution.  In January 1758 he left Virginia, to be replaced by Francis Fauquier, and lived in England until his death at Clifton, Bristol.
 Dinwiddie County, Virginia, which lies 30 miles south of Richmond, is named in honor of Robert Dinwiddie. Dinwiddie Hall, since 1972 a dormitory at the College of William & Mary, is also named in honor of Robert Dinwiddie.[11]
 Dinwiddie retained his links with his alma mater throughout his life: in 1754 he was conferred an honorary degree by the University of Glasgow; and upon his death in 1770 he gave the University of Glasgow Library £100 for the procurement of books.[4]
 Robert Dinwiddie died on 27 July 1770. He is buried in Clifton Parish Church with a monument created by William Tyler RA.[12]
 He married some time before 1738 Rebecca Auchinleck[13] (or Afflick); together they had two children Rebecca Dinwiddie and Elizabeth Dinwiddie.[2]
 His official records, which were compiled by the Virginia Historical Society in 1883, are left in five volumes.  They cover the years of his administration of the Virginia colony, from 1751 to 1758.[14] His archival material resides at several places,[15] and some related to the French and Indian War is visible to web browsers.[16] He was the subject of the book Robert Dinwiddie: Servant of the Crown.[13]
 
"
Ohio River,https://en.wikipedia.org/wiki/Ohio_River,"
 The Ohio River is a 981-mile-long (1,579 km) river in the United States. It is located at the boundary of the Midwestern and Southern United States, flowing in a southwesterly direction from western Pennsylvania to its mouth on the Mississippi River at the southern tip of Illinois. It is the third largest river by discharge volume in the United States and the largest tributary by volume of the north-south flowing Mississippi River, which divides the eastern from western United States.[8] It is also the sixth oldest river on the North American continent. The river flows through or along the border of six states, and its drainage basin includes parts of 14 states. Through its largest tributary, the Tennessee River, the basin includes several states of the southeastern U.S. It is the source of drinking water for five million people.[9]
 The river became a primary transportation route for pioneers during the westward expansion of the early U.S. The lower Ohio River just below Louisville was obstructed by rapids known as the Falls of the Ohio where the elevation falls 26 feet (7.9 m) in 2 miles (3.2 km) restricting larger commercial navigation, although in the 18th and early 19th century its three deepest channels could be traversed by a wide variety of craft then in use. In 1830, the Louisville and Portland Canal (now the McAlpine Locks and Dam) bypassed the rapids, allowing even larger commercial and modern navigation from the Forks of the Ohio at Pittsburgh to the Port of New Orleans at the mouth of the Mississippi on the Gulf of Mexico. Since the ""canalization"" of the river in 1929, the Ohio has not been a natural free-flowing river; today, it is divided into 21 discrete pools or reservoirs by 20 locks and dams for navigation and power generation.[10]
 The name ""Ohio"" comes from the Seneca, Ohi:yo', lit. ""Good River"".[11] In his Notes on the State of Virginia published in 1781–82, Thomas Jefferson stated: ""The Ohio is the most beautiful river on earth. Its current gentle, waters clear, and bosom smooth and unbroken by rocks and rapids, a single instance only excepted.""[12]
 After the French and Indian War, Britain's trans-Appalachian Indian Reserve was divided by the river into colonial lands to the south and Native American lands to the north. In the late 18th century, the river became the southern boundary of the Northwest Territory. The river is sometimes considered as the western extension of the Mason–Dixon line that divided Pennsylvania from Maryland, and thus part of the border between free and slave territory, and between the Northern and Southern United States or Upper South. Where the river was narrow, it was crossed by thousands of slaves escaping to the North for freedom; many were helped by free blacks and whites of the Underground Railroad resistance movement.
 The Ohio River is a climatic transition area, as its water runs along the periphery of the humid subtropical and humid continental climate areas. It is inhabited by fauna and flora of both climates. Today, the Ohio River is one of the most polluted rivers in the United States. In winter, it regularly freezes over at Pittsburgh but rarely farther south towards Cincinnati and Louisville. Further down the river in places like Paducah and Owensboro, Kentucky, closer to its confluence with the Mississippi, the Ohio is ice-free year-round.
 The name ""Ohio"" comes from the Seneca language (an Iroquoian language), Ohi:yo' (roughly pronounced oh-hee-yoh, with the vowel in ""hee"" held longer), a proper name derived from ohiːyoːh (""good river""), therefore literally translating to ""Good River"".[11][13] ""Great river"" and ""large creek"" have also been given as translations.[14][15]
 Native Americans, including the Lenape and Iroquois, considered the Ohio and Allegheny rivers as the same, as is suggested by a New York State road sign on Interstate 86 that refers to the Allegheny River also as Ohi:yo'.[16] Similarly, the Geographic Names Information System lists O-hee-yo and O-hi-o as variant names for the Allegheny.[17]
 An earlier Miami-Illinois language name was also applied to the Ohio River, Mosopeleacipi (""river of the Mosopelea"" tribe). Shortened in the Shawnee language to pelewa thiipi, spelewathiipi or peleewa thiipiiki, the name evolved through variant forms such as ""Polesipi"", ""Peleson"", ""Pele Sipi"" and ""Pere Sipi"", and eventually stabilized to the variant spellings ""Pelisipi"", ""Pelisippi"" and ""Pellissippi"". Originally applied just to the Ohio River, the ""Pelisipi"" name later was variously applied back and forth between the Ohio River and the Clinch River in Virginia and Tennessee.[18][19] In his original draft of the Land Ordinance of 1784, Thomas Jefferson proposed a new state called ""Pelisipia"", to the south of the Ohio River, which would have included parts of present-day Eastern Kentucky, Virginia and West Virginia.[18]
 The river had great significance in the history of the Native Americans, as numerous prehistoric and historic civilizations formed along its valley.[20] For thousands of years, Native Americans used the river as a major transportation and trading route.[21]
 In the five centuries before European colonization, the Mississippian culture built numerous regional chiefdoms and major earthwork mounds in the Ohio Valley like the Angel Mounds near Evansville, Indiana as well as in the Mississippi Valley and the Southeast. The historic Osage, Omaha, Ponca, and Kaw peoples lived in the Ohio Valley. Under pressure over the fur trade from the Iroquois nations to the northeast, they migrated west of the Mississippi River in the 17th century to the territory now defined as Missouri, Arkansas, and Oklahoma.
 Several accounts exist of the discovery and traversal of the Ohio River by Europeans in the latter half of the 17th century: Virginian colonist Abraham Wood's trans-Appalachian expeditions between 1654 and 1664;[22] Frenchman Robert de La Salle's putative Ohio expedition of 1669;[23] and two expeditions of Virginians sponsored by Colonel Wood: the Batts and Fallam expedition of 1671,[24] and the Needham and Arthur expedition of 1673–74.[25][26][27][28]
 In early autumn 1692, loyal English-speaking Dutchman Arnout Viele and a party of eleven companions from Esopus[29]—Europeans, Shawnee, and a few loyal Delaware guides—were sent by the governor of New York to trade with the Shawnee and bring them into the English sphere of influence.[30][31] Viele understood several Native American languages, which made him valuable as an interpreter. He is credited with being the first European to travel and explore western Pennsylvania and the upper Ohio Valley. Viele made contact with Native American nations as far west as the Wabash River, in present-day Indiana.[31]
 He and his company left Albany, traveling southbound and crossing portions of present-day New Jersey and eastern Pennsylvania. They apparently followed the west branch of the Susquehanna River into the mountains, traversing the Tioga River and reaching a tributary of the Allegheny River before floating down to the Shawnee towns along the Ohio River.[31] Viele and his expedition spent most of 1693 exploring the Ohio River and its tributaries in northern Kentucky with their Shawnee hosts.[31] Gerit Luykasse, two of Viele's Dutch traders, and two Shawnee reappeared in Albany in February 1694 ""to fetch powder for Arnout [Viele] and his Company"";[31] their party had been gone for fifteen months, but Viele was away for about two years.[24] He and his companions returned from the Pennsylvania wilderness in August 1694, accompanied by diplomats from ""seven Nations of Indians"" who sought trade with the English (or peace with the powerful Iroquois nations of New York and Pennsylvania), and hundreds of Shawnee who intended to relocate in the Minisink country on the upper Delaware River.[30][31]
 In 1729, Gaspard-Joseph Chaussegros de Léry, a French architect and surveyor whose survey was the first mapping of the Ohio River,[32] led an expedition of French troops from Fort Niagara down the Allegheny and Ohio Rivers as far as the mouth of the Great Miami River near Big Bone Lick and possibly the Falls of the Ohio (present-day Louisville).[29][33][34] Chaussegros de Lery mapped the Great Lakes in 1725, and engineered the Niagara fortifications in 1726.[35][36]
 A map of the Ohio River valley, drawn by Bellin from observations by de Lery, is in Pierre François Xavier de Charlevoix's History of New France.[38][39] The 1744 Bellin map, ""Map of Louisiana"" (French: Carte de La Louisiane), has an inscription at a point south of the Ohio River and north of the Falls: ""Place where one found the ivory of Elephant in 1729"" (French: endroit ou on à trouvé des os d'Elephant en 1729).[40][41] De Lery's men found teeth weighing ten pounds (4.5 kg) with a diameter of five to seven inches (130 to 180 mm), tusks 11 feet (3.4 m) long and 6–7 inches (150–180 mm) in diameter, and thigh bones 5 feet (1.5 m) long.[42] The bones were collected and shipped to Paris, where they were identified as mastodon remains; they are on display at the French National Natural History Museum.[32][35]
 Charles III Le Moyne, second Baron de Longueuil (later the governor of Montreal and interim governor of New France), commanded Fort Niagara from 1726 to 1733.[32] He led an expedition of 442 men, including Native Americans, from Montreal to war against the Chickasaw who occupied territory on the lower part of the Mississippi River in the area claimed as La Louisiane.[43] According to Gaston Pierre de Lévis, Duke de Mirepoix, the expedition used the Ohio River as a corridor to the Mississippi.
 One of the first reported eyewitness accounts of Shannoah, a Shawnee town, was by le Moyne III in July 1739. On their journey down the Ohio River toward the Mississippi, they met with local chiefs in a village on the Scioto River.
 John Howard, a pioneer from Virginia, led a party of five—John Peter Salling (a Pennsylvania German),[45] Josiah Howard (John's son), Charles Sinclair, and John Poteet (Vizt)—from the Virginia mountains to the Mississippi River.[46] The elder Howard had a promised reward of 10,000 acres (4,000 ha) of land for a successful expedition from the Virginia Royal Governor's Council to reinforce British claims in the west. Howard offered equal shares of the 10,000 acres to the four other members of his expedition. The party of five left John Peter Salling's house in August County on March 16, 1742, and traveled west to Cedar Creek (near the Natural Bridge), crossing Greenbrier River and landing at the New River. At New River, the Virginia explorers built a large bull boat frame and covered it with five buffalo skins. The first Englishmen to explore the region then followed the New River for 250 miles (400 km), until it became too dangerous to navigate. At a large waterfall, they traveled overland to the Coal River. Following the Kanawha River, they entered the Ohio River 444 miles (715 km) above the falls. The Virginia pioneers traced the northern boundary of Kentucky for five hundred miles (800 km), reaching the Mississippi River on June 7.[47][48][49][50][51] They descended just below the mouth of the Arkansas River, where they were ambushed by a large company of Native Americans, Blacks and Frenchmen on July 2, 1742; one or two of Howard's men were killed.[52] The rest were brought to New Orleans and imprisoned as spies.[45] After two years in prison, Salling escaped on October 25, 1744, and returned on a southern route to his home in Augusta County, Virginia, in May 1745. John Howard was extradited to France to stand trial. His ship was intercepted by the English and, as a free man, he reported his adventures after landing in London; however, his account has been lost.[47][51] Salling's detailed account of Virginia's adjacent lands was used in Joshua Fry and Peter Jefferson's 1751 map.[47][51]
 In 1749, the Ohio Company was established in Virginia Colony to settle and trade in the Ohio River region. Exploration of the territory and trade with the Indians in the region near the Forks brought white colonists from both Pennsylvania and Virginia across the mountains, and both colonies claimed the territory. The movement across the Allegheny Mountains of Anglo-American settlers and the claims of the area near modern-day Pittsburgh led to conflict with the French, who had forts in the Ohio River Valley. This conflict was called the French and Indian War, and would merge into the global Anglo-French struggle, the Seven Years' War. In 1763, following its defeat in the war, France ceded its area east of the Mississippi River to Britain and its area west of the Mississippi River to Spain in the 1763 Treaty of Paris.
 The 1768 Treaty of Fort Stanwix with several tribes opened Kentucky to colonial settlement and established the Ohio River as a southern boundary for American Indian territory.[53] In 1774 the Quebec Act restored the land east of the Mississippi River and north of the Ohio River to Quebec, in effect making the Ohio the southern boundary of Canada. This appeased French Canadians in Quebec but angered the colonists of the Thirteen Colonies. Lord Dunmore's War south of the Ohio river also contributed to cession of land north to Quebec to prevent colonial expansion onto Native American territory. During the Revolutionary War. in 1776 the British military engineer John Montrésor created a map of the river showing the strategic location of Fort Pitt, including specific navigational information about the Ohio River's rapids and tributaries in that area.[54] However, the 1783 Treaty of Paris gave the entire Ohio Valley to the United States, and numerous white settlers entered the region.
 The economic connection of the Ohio Country to the East was significantly increased in 1818 when the National Road being built westward from Cumberland, Maryland, reached Wheeling, Virginia, (now West Virginia), providing an easier overland connection from the Potomac River to the Ohio River.[55] The Wheeling Suspension Bridge was built over the river at Wheeling from 1847 to 1849, making the trip west easier. For a brief time, until 1851, it was the world's largest suspension bridge. The bridge survived the American Civil War, after having been improved in 1859. It was renovated again in 1872, and remains in use as the oldest vehicular suspension bridge in the U.S.
 Louisville was founded in 1778 as a military encampment on Corn Island (now submerged) by General George Rogers Clark at the only major natural navigational barrier on the river, the Falls of the Ohio. The Falls were a series of rapids where the river dropped 26 feet (7.9 m) in a stretch of about 2 miles (3.2 km). In this area, the river flowed over hard, fossil-rich beds of limestone. The outpost was moved that same year to the south shore where Fort-on-Shore was constructed. It proved insufficient within 3 years, and the mighty Fort Nelson was constructed upriver. The town of Louisville was chartered in 1780, in honor of King Louis XVI of France. The first locks on the river – the Louisville and Portland Canal – were built between 1825 and 1830 to circumnavigate the falls. Fears that Louisville's transshipment industry would collapse proved ill-founded: but the increasing size of steamships and barges on the river meant that the outdated locks could serve only the smallest vessels until well after the Civil War when improvements were made. The U.S. Army Corps of Engineers improvements were expanded again in the 1960s, forming the present-day McAlpine Locks and Dam.
 During the nineteenth century, emigrants from Virginia, North Carolina and Kentucky traveled by the river and settled along its northern bank. Known as butternuts, they formed the dominant culture in the southern portions of Ohio, Indiana and Illinois with a society that was primarily Southern in culture. Largely devoted to agricultural pursuits, they shipped much of their produce along the river to ports such as Cincinnati.[56]
 Because the Ohio River flowed westward, it became a convenient means of westward movement by pioneers traveling from western Pennsylvania. After reaching the mouth of the Ohio, settlers would travel north on the Mississippi River to St. Louis, Missouri. There, some continued on up the Missouri River, some up the Mississippi, and some farther west over land routes. In the early 19th century, river pirates such as Samuel Mason, operating out of Cave-In-Rock, Illinois, waylaid travelers on their way down the river. They killed travelers, stealing their goods and scuttling their boats. The folktales about Mike Fink recall the keelboats used for commerce in the early days of American settlement. The Ohio River boatmen inspired performer Dan Emmett, who in 1843 wrote the song ""The Boatman's Dance"".
 Trading boats and ships traveled south on the Mississippi to New Orleans, and sometimes beyond to the Gulf of Mexico and other ports in the Americas and Europe. This provided a much-needed export route for goods from the west since the trek east over the Appalachian Mountains was long and arduous. The need for access to the port of New Orleans by settlers in the Ohio Valley is one of the factors that led to the United States' Louisiana Purchase in 1803.
 Because the river is the southern border of Ohio, Indiana, and Illinois, it was part of the border between free states and slave states in the years before the American Civil War. One antebellum slave trader reported that they kept slaves chained two-by-two while navigating the Ohio, only when they reached the Mississippi could the slaves be unchained for a time, because ""there was slavery on both sides of the boat.""[57] The expression ""sold down the river"" originated as a lament of Upper South slaves, especially from Kentucky, who were shipped via the Ohio and Mississippi to cotton and sugar plantations in the Deep South. Changes in crops cultivated in the Upper South resulted in slaves available to be sold to the South, where the expansion of cotton plantations was doing very well. Invention of the cotton gin made cultivation of short-staple cotton profitable throughout the Black Belt of this region.[58][59]
 Before and during the Civil War, the Ohio River was called the ""River Jordan"" by slaves crossing it to escape to freedom in the North via the Underground Railroad.[60] More escaping slaves, estimated in the thousands, made their perilous journey north to freedom across the Ohio River than anywhere else across the north-south frontier. Harriet Beecher Stowe's Uncle Tom's Cabin, the bestselling novel that fueled abolitionist work, was the best known of the anti-slavery novels that portrayed such escapes across the Ohio. The times have been expressed by 20th-century novelists as well, such as the Nobel Prize-winning Toni Morrison, whose novel Beloved was adapted as a film of the same name. She also composed the libretto for the opera Margaret Garner (2005), based on the life and trial of an enslaved woman who escaped with her family across the river.
 The colonial charter for Virginia defined its territory as extending to the north shore of the Ohio, so that the riverbed was ""owned"" by Virginia. Where the river serves as a boundary between states today, Congress designated the entire river to belong to the states on the east and south, i.e., West Virginia and Kentucky at the time of admission to the Union, that were divided from Virginia. Thus Wheeling Island, the largest inhabited island in the Ohio River, belongs to West Virginia, although it is closer to the Ohio shore than to the West Virginia shore. Kentucky sued the state of Indiana in the early 1980s because of their construction of the never-completed Marble Hill Nuclear Power Plant in Indiana, which would have discharged its waste water into the river. This would have adversely affected Kentucky's water supplies.
 The U.S. Supreme Court held that Kentucky's jurisdiction (and, implicitly, that of West Virginia) extended only to the low-water mark of 1793 (important because the river has been extensively dammed for navigation so that the present river bank is north of the old low-water mark). Similarly, in the 1990s, Kentucky challenged Illinois's right to collect taxes on a riverboat casino docked in Metropolis, citing its own control of the entire river. A private casino riverboat that docked in Evansville, Indiana, on the Ohio River opened about the same time. Although such boats cruised on the Ohio River in an oval pattern up and down, the state of Kentucky soon protested. Other states had to limit their cruises to going forward, then reversing and going backward on the Indiana shore only. Both Illinois and Indiana have long since changed their laws to allow riverboat casinos to be permanently docked, with Illinois changing in 1999 and Indiana in 2002.
 The Silver Bridge at Point Pleasant, West Virginia, collapsed into the river on December 15, 1967. The collapse killed 46 people who had been crossing when the bridge failed. The bridge had been built in 1929, and by 1967 was carrying too heavy a load for its design.[61] The bridge was rebuilt about one mile downstream and in service as the Silver Memorial Bridge in 1969.
 In the early 1980s, the Falls of the Ohio National Wildlife Conservation Area was established at Clarksville, Indiana.
 The Ohio River as a whole is ranked as the most polluted river in the United States, based on 2009 and 2010 data. The more industrial and regional West Virginia/Pennsylvania tributary, the Monongahela River, ranked 17th for water pollution, behind 16 other American rivers.[62] The Ohio again ranked as the most polluted in 2013, and has been the most polluted river since at least 2001, according to the Ohio River Valley Water Sanitation Commission (ORSANCO). The Commission found that 92% of toxic discharges were nitrates, including farm runoff and waste water from industrial processes such as steel production. The Commission also noted mercury pollution as an ongoing concern, citing a 500% increase in mercury discharges between 2007 and 2013.[63]
 For several decades beginning in the 1950s, the Ohio River was polluted with hundreds of thousands of pounds of PFOA, a fluoride-based chemical used in making teflon, among other things, by the DuPont chemical company from an outflow pipe at its Parkersburg, West Virginia, facility.[64]
 The Ohio was listed among America's Most Endangered Rivers of 2023 on the heels of contamination from a high-profile train derailment, and following years as one of the most polluted watersheds in the United States.[65]
 The Ohio River is extensively industrialized and populated. Regular barge traffic carries cargoes of oil, steel and other industrial goods produced in the region. Major cities located along the northern and southern banks of the river include Pittsburgh, Pennsylvania; Louisville, Kentucky; Evansville, Indiana; and Cincinnati, Ohio.[21]
 The combined Allegheny-Ohio river is 1,310 miles (2,110 km) long and carries the largest volume of water of any tributary of the Mississippi. The Indians and early European explorers and settlers of the region often considered the Allegheny to be part of the Ohio. The forks (the confluence of the Allegheny and Monongahela rivers at what is now Pittsburgh) were considered a strategic military location by colonial French and British, and later independent American military authorities.
 The Ohio River is formed by the confluence of the Allegheny and Monongahela rivers at what is now Point State Park in Pittsburgh, Pennsylvania. From there, it flows northwest through Allegheny and Beaver counties, before making an abrupt turn to the south-southwest at the West Virginia–Ohio–Pennsylvania triple-state line (near East Liverpool, Ohio; Chester, West Virginia; and Ohioville, Pennsylvania). From there, it forms the border between West Virginia and Ohio, upstream of Wheeling, West Virginia.
 The river follows a roughly southwest and then west-northwest course until Cincinnati, before bending to a west-southwest course for most of the remainder of its length. The course forms the northern borders of West Virginia and Kentucky; and the southern borders of Ohio, Indiana and Illinois, until it joins the Mississippi River at the city of Cairo, Illinois. Where the Ohio joins the Mississippi is the lowest elevation in the state of Illinois, at 315 feet (96 m).
 The Mississippi River flows to the Gulf of Mexico on the Atlantic Ocean. Among rivers wholly or mostly in the United States, the Ohio is the second largest by discharge volume and the tenth longest and has the eighth largest drainage basin. It serves to separate the Midwestern Great Lakes states from the Upper South states, which were historically border states in the Civil War.
 The Ohio River is a left (east) and the largest tributary by volume of the Mississippi River in the United States. At the confluence, the Ohio is considerably bigger than the Mississippi, measured by long-term mean discharge. The Ohio River at Cairo is 281,500 cu ft/s (7,960 m3/s);[1] and the Mississippi River at Thebes, Illinois, which is upstream of the confluence, is 208,200 cu ft/s (5,897 m3/s).[66] The Ohio River flow is greater than that of the Mississippi River, so hydrologically the Ohio River is the main stream of the river system.
 The Ohio River is a naturally shallow river that was artificially deepened by a series of dams. The natural depth of the river varied from about 3 to 20 feet (1 to 6 m). The dams raise the water level and have turned the river largely into a series of reservoirs, eliminating shallow stretches and allowing for commercial navigation. From its origin to Cincinnati, the average depth is approximately 15 feet (5 m). The largest immediate drop in water level is below the McAlpine Locks and Dam at the Falls of the Ohio at Louisville, Kentucky, where flood stage is reached when the water reaches 23 feet (7 m) on the lower gauge. However, the river's deepest point is 168 feet (51 m) on the western side of Louisville, Kentucky. From Louisville, the river loses depth very gradually until its confluence with the Mississippi at Cairo, Illinois, where it has an approximate depth of 19 feet (6 m).
 Water levels for the Ohio River from Smithland Lock and Dam upstream to Pittsburgh are predicted daily by the National Oceanic and Atmospheric Administration's Ohio River Forecast Center.[67] The water depth predictions are relative to each local flood plain based upon predicted rainfall in the Ohio River basin in five reports as follows:
 The water levels for the Ohio River from Smithland Lock and Dam to Cairo, Illinois, are predicted by the National Oceanic and Atmospheric Administration's Lower Mississippi River Forecast Center.[68]
 The largest tributaries of the Ohio by discharge volume are:
 By drainage basin area, the largest tributaries are:[69]
 The largest tributaries by length are:[69]
 Major tributaries, in order from the headwaters, include:[69]
 The Ohio's drainage basin covers 203,940 sq mi (528,200 km2), encompassing the easternmost regions of the Mississippi Basin. The Ohio drains parts of 14 states in four regions.
 The Ohio River is a climatic transition area, as its water runs along the periphery of the humid continental and humid subtropical climate areas. It is inhabited by fauna and flora of both climates. In winter, it regularly freezes over at Pittsburgh but rarely farther south toward Cincinnati and Louisville. At Paducah, Kentucky, in the south, at the Ohio's confluence with the Tennessee River, it is ice-free year-round.
 In the 21st century, with the 2016 update of climate zones,[70] the humid subtropical zone has stretched across the river, into the southern portions of Ohio, Indiana, and Illinois.
 From a geological standpoint, the Ohio River is young. Before the river was created, large parts of North America were covered by water forming a saltwater lake about 200 miles across and 400 miles in length. The bedrock of the Ohio Valley was mostly set during this time.[21] The river formed on a piecemeal basis beginning between 2.5 and 3 million years ago. By the movement of glaciers during the earliest ice ages, the contemporary river drainages of the Kanawha, Sandy, Kentucky, Green, Cumberland and Tennessee rivers northward created the Ohio system and the course of early tributaries of the Ohio River, including the Monongahela and the Allegheny rivers, were set.[21] The Teays River was the largest of these rivers. The modern Ohio River flows within segments of the ancient Teays. The ancient rivers were rearranged or consumed.
 The section of the river that runs southwest from Pittsburgh to Cairo, Illinois, is around tens of thousands of years old.
 The upper Ohio River formed when one of the glacial lakes overflowed into a south-flowing tributary of the Teays River. Prior to that event, the north-flowing Steubenville River (no longer in existence) ended between New Martinsville and Paden City, West Virginia. The south-flowing Marietta River (no longer in existence) ended between the present-day cities. The overflowing lake carved through the separating hill and connected the rivers. The floodwaters enlarged the small Marietta valley to a size more typical of a large river. The new large river subsequently drained glacial lakes and melting glaciers at the end of the ice ages. The valley grew during and following the ice age. Many small rivers were altered or abandoned after the upper Ohio River formed. Valleys of some abandoned rivers can still be seen on satellite and aerial images of the hills of Ohio and West Virginia between Marietta, Ohio, and Huntington, West Virginia.
 The middle Ohio River formed in a manner similar to that of the upper Ohio River. A north-flowing river was temporarily dammed by natural forces southwest of present-day Louisville, creating a large lake until the dam burst. A new route was carved to the Mississippi. Eventually, the upper and middle sections combined to form what is essentially the modern Ohio River.
 Along the banks of the Ohio are some of the largest cities in their respective states:[note 1] Pittsburgh, the third-largest city on the river and second-largest in Pennsylvania; Cincinnati, the second-largest city on the river and third-largest in Ohio; Louisville, the largest city on the river and in Kentucky as well; Evansville, the third-largest city in Indiana; Owensboro, the fourth-largest city in Kentucky; and three of the five largest cities in West Virginia—Huntington (second), Parkersburg (fourth), and Wheeling (fifth). Only Illinois, among the border states, has no significant cities on the river. There are hundreds of other cities, towns, villages and unincorporated populated places on the river, most of them very small.
 Cities along the Ohio are also among the oldest cities in their respective states and among the oldest cities in the United States west of the Appalachian Mountains (by date of founding): Old Shawneetown, Illinois, 1748; Pittsburgh, Pennsylvania, 1758; Wheeling, West Virginia, 1769; Huntington, West Virginia, 1775; Louisville, Kentucky, 1779; Clarksville, Indiana, 1783; Maysville, Kentucky, 1784; Martin's Ferry, Ohio, 1785; Marietta, Ohio, 1788; Cincinnati, Ohio, 1788; Manchester, Ohio, 1790; Beaver, Pennsylvania, 1792; and Golconda, Illinois, 1798.
 Other cities of interest include Cairo, Illinois, at the confluence of the Ohio with the Mississippi River and the southernmost and westernmost city on the river; and Beaver, Pennsylvania, the site of colonial Fort McIntosh and the northernmost city on the river. It is 548 miles as the crow flies between Cairo and Pittsburgh, but 981 miles by water. Direct water travel over the length of the river is obstructed by the Falls of the Ohio just below Louisville, Kentucky. The Ohio River Scenic Byway follows the Ohio River through Illinois, Indiana and Ohio ending at Steubenville, Ohio, on the river.
 Before there were cities, there were colonial forts. These forts played a dominant role in the French and Indian War, Northwest Indian War and pioneering settlement of Ohio Country. Many cities got their start at or adjacent to the forts. Most were abandoned by 1800. Forts along the Ohio river include Fort Pitt (Pennsylvania), Fort McIntosh (Pennsylvania), Fort Randolph (West Virginia), Fort Henry (West Virginia), Fort Harmar (Ohio), Fort Washington (Ohio), and Fort-on-Shore and Fort Nelson (Kentucky). Short-lived, special-purpose forts included Fort Steuben (Ohio), Fort Finney (Indiana), Fort Finney (Ohio) and Fort Gower (Ohio). There was also the Newport Barracks (Kentucky) in the 19th century.
 Annual events taking place on or over the river include Thunder Over Louisville, the Madison Regatta, and the Great Steamboat Race.
 Multiple West Virginia stage record fish were caught along the Ohio River.[71][72][73]
"
Lake Erie,https://en.wikipedia.org/wiki/Lake_Erie,"
 Lake Erie (/ˈɪri/ EER-ee) is the fourth-largest lake by surface area of the five Great Lakes in North America and the eleventh-largest globally.[6][10] It is the southernmost, shallowest, and smallest by volume of the Great Lakes[11][12] and also has the shortest average water residence time. At its deepest point, Lake Erie is 210 feet (64 m) deep, making it the only Great Lake whose deepest point is above sea level.[13]
 Located on the International Boundary between Canada and the United States, Lake Erie's northern shore is the Canadian province of Ontario, specifically the Ontario Peninsula, with the U.S. states of Michigan, Ohio, Pennsylvania, and New York on its western, southern, and eastern shores. These jurisdictions divide the surface area of the lake with water boundaries. The largest city on the lake is Cleveland, anchoring the third largest U.S. metro area in the Great Lakes region, after Greater Chicago and Metro Detroit. Other major cities along the lake shore include Buffalo, New York; Erie, Pennsylvania; and Toledo, Ohio.
 Situated below Lake Huron, Erie's primary inlet is the Detroit River. The main natural outflow from the lake is via the Niagara River, which provides hydroelectric power to Canada and the U.S. as it spins huge turbines near Niagara Falls at Lewiston, New York, and Queenston, Ontario.[14] Some outflow occurs via the Welland Canal, part of the Saint Lawrence Seaway, which diverts water for ship passages from Port Colborne, Ontario, on Lake Erie, to St. Catharines on Lake Ontario, an elevation difference of 326 ft (99 m). Lake Erie's environmental health has been an ongoing concern for decades, with issues such as overfishing, pollution, algae blooms, and eutrophication generating headlines.[15][16]
 Lake Erie (42.2° N, 81.2W) has a mean elevation of 571 feet (174 m)[8] above sea level. It has a surface area of 9,990 square miles (25,874 km2)[7] with a length of 241 statute miles (388 km; 209 nmi)[7] and breadth of 57 statute miles (92 km; 50 nmi)[7] at its widest points. It is the shallowest of the Great Lakes with an average depth of 10 fathoms 3 feet or 63 ft (19 m)[7] and a maximum depth of 35 fathoms (210 ft; 64 m)[7][8] Because Erie is the shallowest, it is also the warmest of the Great Lakes,[17] and in 1999 this almost became a problem for two nuclear power plants which require cool lake water to keep their reactors cool.[18] The warm summer of 1999 caused lake temperatures to come close to the 85 °F (29 °C) limit necessary to keep the plants cool.[18] Also because of its shallowness, it is the first to freeze in the winter.[19] The shallowest section of Lake Erie is the western basin where depths average only 25 to 30 feet (7.6 to 9.1 m); as a result, ""the slightest breeze can kick up lively waves"", also known as seiches.[20] The ""waves build very quickly"", according to other accounts.[21][22] The region around the lake is known as the ""thunderstorm capital of Canada"" with ""breathtaking"" lightning displays.[22] Sometimes fierce waves springing up unexpectedly have led to dramatic rescues; in one instance, a Cleveland resident trying to measure the dock near his house became trapped but was rescued by a fire department diver from Avon Lake, Ohio:
 Lake Erie is primarily fed by the Detroit River (from Lake Huron and Lake St. Clair) and drains via the Niagara River and Niagara Falls into Lake Ontario. Navigation downstream is provided by the Welland Canal, part of the Saint Lawrence Seaway. Other major contributors to Lake Erie include Grand River, Huron River, Maumee River, Sandusky River, Cuyahoga River, and Buffalo River. The drainage basin covers 30,140 square miles (78,100 km2).
 Point Pelee National Park, the southernmost point of the Canadian mainland, is located on a peninsula extending into the lake. Lake Erie has 31 islands (13 in Canada, 18 in the U.S.), located generally in the western side of the lake. The largest of these is Pelee Island.
 
 Lake Erie has a lake retention time of 2.6 years,[24] the shortest of all the Great Lakes.[25] The lake's surface area is 9,910 square miles (25,667 km2).[7][26] Lake Erie's water level fluctuates with the seasons as in the other Great Lakes. Generally, the lowest levels are in January and February and the highest in June or July, although there have been exceptions. The average yearly level varies depending on long-term precipitation. Short-term level changes are often caused by seiches that are particularly high when southwesterly winds blow across the length of the lake during storms. These cause water to pile up at the eastern end of the lake. Storm-driven seiches can cause damage onshore. During one storm in November 2003, the water level at Buffalo rose by 7 feet (2.1 m) with waves of 10–15 feet (3.0–4.6 metres) for a rise of 22 feet (6.7 m).[27] Meanwhile, at the western end of the lake, Toledo experienced a similar drop in water level.
 Lake Erie was carved out by glacier ice and went through many phases before its current form that is less than 4,000 years old, which is a short span in geological terms.[29] Before this, the land on which the lake now sits went through several complex stages. A large lowland basin formed over two million years ago as a result of an eastern flowing river that existed well before the Pleistocene ice ages. This ancient drainage system was destroyed by the first major glacier in the area, while it deepened and enlarged the lowland areas, allowing water to settle and form a lake. The glaciers were able to carve away more land on the eastern side of the lowland because the bedrock is made of shale which is softer than the carbonate rocks of dolomite and limestone on the western side. Thus, the eastern and central basins of the modern lake are much deeper than the western basin, which averages only 25 feet (7.6 m) deep and is rich in nutrients and fish.[30] Lake Erie is the shallowest of the Great Lakes because the ice was relatively thin and lacked erosion power when it reached that far south, according to one view.
 As many as three glaciers advanced and retreated over the land, causing temporary lakes to form in the time periods in between each of them (see: Lake Whittlesey, Lake Maumee and Lake Arkona). Because each lake had a different volume of water, their shorelines rested at differing elevations. The last lake to form, Lake Warren, existed between about 13,000 and 12,000 years ago. It was deeper than the current Lake Erie, and its shoreline existed about eight miles (13 km) inland from the modern one. Early Lake Erie went through many phases with its ancient sand dunes visible in the Oak Openings Region in Northwest Ohio. There, the sandy dry lake bed soil was not sufficient to support large trees with the exception of a few species of oaks, forming a rare oak savanna.[31]
 At the time of European contact, there were several Indigenous peoples living around the shores of the eastern end of the lake. The Erie tribe (from whom the lake takes its name) lived along the southern edge, while the Neutrals (also known as Attawandaron) lived along the northern shore. The tribal name ""erie"" is a shortened form of the Iroquoian word erielhonan, meaning ""long tail"".[32] The name may also come from the word eri, meaning ""cherry tree"".[33] Near Port Stanley, there is an Indigenous village dating from the 16th century known as the Southwold Earthworks where as many as 800 Neutral Indigenous peoples once lived; the archaeological remains include double earth walls winding around the grass-covered perimeter.[34] Europeans named the tribe the Neutral Indians since these people refused to fight with other tribes.[34]
 Both the Erie and Neutrals were colonized and assimilated by their hostile eastern neighbors, the Iroquois Confederacy, between 1651 and 1657 during the Beaver Wars.[35] For decades after those wars, the land around eastern Lake Erie was claimed and utilized by the Iroquois as a hunting ground. As the power of the Iroquois waned during the last quarter of the 17th century, several other, mainly Anishinaabe, displaced them from the territories they claimed on the north shore of the lake.[36] There was a legend of an Indigenous woman named Huldah, who, despairing over her lost British lover, hurled herself from a high rock from Pelee Island.[21]
 In 1669, Frenchman Louis Jolliet was the first documented European to sight Lake Erie, although there is speculation that Étienne Brûlé may have come across it in 1615.[37] Lake Erie was the last of the Great Lakes to be explored by Europeans, since the Iroquois who occupied the Niagara River area were in conflict with the French, and they did not allow explorers or traders to pass through; explorers followed rivers out of Lake Ontario and portaged to Lake Huron. British authorities in Canada were nervous about possible expansion by American settlers across Lake Erie, so Colonel Thomas Talbot developed the Talbot Trail in 1809 as a way to stimulate settlement to the area; Talbot recruited settlers from Ireland and Scotland, and there are numerous places named after him in southern Ontario, such as Port Talbot, the Talbot River, and Talbotville.[34]
 During the War of 1812, Oliver Hazard Perry captured an entire British fleet in 1813[38] near Put-in-Bay, Ohio, despite having inferior numbers.[39] American soldiers swept through the Ontario area around Port Rowan burning towns and villages.[34] Generally, however, with the exceptions of the American Revolutionary War and the War of 1812—which involved conflicts between the U.S. and the United Kingdom—relations between the U.S. and Canada have been remarkably friendly with an ""unfortified boundary"" and an agreement ""that has kept all fleets of war off the Great Lakes.""[40]
 In 1837, rebellions broke about between Canadian settlers and the British Colonial government. These primarily concerned political reforms and land allocation issues. Some of the rebels stationed themselves in the U.S. and crossed the ice from Sandusky Bay to Pelee Island wearing ""tattered overcoats and worn-out boots"", and carrying muskets, pitchforks, and swords,[21] but the islanders had already fled.[21] Later, there was a battle on the ice with the Royal 32nd Regiment, with the rebels being driven to retreat.[21]
 Settlers established commercial fisheries on the north coast of the lake around the 1850s.[41] An important business was fishing.[24] In the pre-Civil War years, railways sprouted everywhere, and around 1852 there were railways circling the lake.[42] Maritime traffic picked up, although the lake was usually closed because of ice from December to early April, and ships had to wait for the ice to clear before proceeding.[43] Since slavery had been abolished in Canada in 1833 but was still legal in southern U.S., a Lake Erie crossing was sometimes required for fugitive slaves seeking freedom:
 Prior to modern radar and weather forecasting, merchant ships were often caught up in intense gales:
 There were reports of disasters usually from sea captains passing information to reporters; in 1868, the captain of the Grace Whitney saw a sunken vessel with ""three men clinging to the masthead,"" but he could not help because of the gale and high seas.[46]
 A balloonist named John Steiner of Philadelphia made an ambitious trip across the lake in 1857.[47] His voyage was described in The New York Times:
 In 1885, lake winds were so strong that water levels dropped substantially, sometimes by as much as two feet, so that at ports such as Toledo, watercraft could not load coal or depart the port.[48] During the history of the lake as a fishery, there has been marked battling by opposing interest groups:
 Predictions of the lake being over-fished in 1895 were premature, since the fishery has survived commercial and sport fishing, pollution in the middle of the 20th century, invasive species and other ailments, but state and provincial governments, as well as national governments, have played a greater role as time went by. Business boomed; in 1901, the Carnegie Company proposed building a new harbor near Erie, Pennsylvania, in Elk Creek to accommodate shipments from its tube-plant site nearby.[50] In 1913, a memorial to Commodore Oliver Hazard Perry was built on Put-in-Bay island featuring a Doric column.[40]
 Steamships have operated on Lake Erie since the early 1800s. Large, opulent cruise liners carried passengers between Detroit, Cleveland, Buffalo and other cities on the lake until the rise of the automobile in the 1950s drastically cut demand for their services. The Detroit and Cleveland Navigation company was one of the largest and most renowned of these companies.[51]
 During the Prohibition years from 1919 to 1933, a ""great deal of alcohol crossed Erie""[21] along with ""mobster corpses"" dumped into the Detroit River which sometimes washed up on the beaches of Pelee Island.[52] Notable rum runners included Thomas Joseph McGinty and the Purple Gang.  The Coast Guard attempted to interdict the Canadian liquor with its Rum Patrol,[53][54][55][56][57] and a casino operated on Middle Island.[58]
 During the 20th century, commercial fishing was prevalent but so was the boom in manufacturing industry around the lake, and often rivers and streams were used as sewers to flush untreated sewage which ended up in the lake.[59] Sometimes poorly constructed sanitary systems meant that when old pipes broke, raw sewage would spill directly into the Cuyahoga River and into the lake.[59] A report in Time magazine in 1969 described the lake as a ""gigantic cesspool"" since only three of 62 beaches were rated ""completely safe for swimming"".[59]
 By 1975 the popular commercial fish blue pike had been declared extinct, although the declaration may have been premature.[60] By the 1980s, there were about 130 fishing vessels with about 3,000 workers,[41] but commercial fishing was declining rapidly, particularly from the American side.
 In 2005, the Great Lakes states of Ohio, Michigan, New York, Pennsylvania, Illinois, Indiana, Wisconsin, and Minnesota and the Canadian Provinces of Ontario and Quebec endorsed the Great Lakes-St. Lawrence River Basin Sustainable Water Resources Compact. It was signed into law by President George W. Bush in September 2008. An international water-rights policy overseen by the Great Lakes Commission, the compact aims to prevent diversion of water from Great Lakes to distant states, as well as to set standards for use and conservation. It had support from both political parties, including United States Senator George Voinovich of Ohio and Governor Jennifer Granholm of Michigan, but is not popular in the southwestern states because of frequent drought conditions and water scarcity.
 Like the other Great Lakes, Erie produces lake-effect snow when the first cold winds of winter pass over the warm waters. When the temperatures of the relatively warm surface water and the colder air separate to at least 18 °F (10 °C) to 23 °F (13 °C) apart, then ""lake-effect snow becomes possible:""[61]
 Heavy lake-effect snowfalls can occur when cold air travels 60 miles (97 km) or longer over a large unfrozen lake.[61] Lake-effect snow makes Buffalo and Erie the eleventh and thirteenth snowiest cities in the entire United States respectively, according to data collected from the National Climatic Data Center.[62] Since winds blow primarily west to east along the main axis of the lake, lake-effect snowstorms are more pronounced on the eastern parts of the lake. Buffalo typically gets 95 inches (240 cm) of snow each winter and sometimes ten feet (3.0 m) of snow;[63] the snowiest city is Syracuse, New York, which can receive heavy snowfall from both the lake effect process and large coastal cyclones. A storm around Christmas in 2001 pounded Buffalo with 7 feet (2.1 m) of snow.[64]
 The effects of the warmer lake water is reduced when the lake freezes over.[65] In January 2011, for example, residents of Cleveland were glad when Lake Erie was ""90 percent frozen"" since it meant that the area had ""made it over the hump"" in terms of enduring repeated snowfalls which required much shoveling.[66] Being the shallowest of the Great Lakes, it is the most likely to freeze and frequently does.[67] In contrast, Lake Michigan has never completely frozen over since the warmer and deeper portion is in the south, although it came close to being totally frozen during three harsh winters over the past century.[68] In past years, lake ice was so thick that it was possible to drive over it or go sailing on iceboats.[52] Many lake residents take advantage of the ice and travel; some drive to Canada and back:[69]
 Strong winds have caused lake currents to shift sediment on the bottom, leading to shifting sandbars that have been the cause of shipwrecks.[52] But winds can have a peaceful purpose as well; there have been proposals to place electricity–producing wind turbines in windy and shallow points in the lake and along the coast. Steel Winds, a former steel mill site in Buffalo, has been developed as an urban wind farm housing 14 turbines capable of generating up to 35 megawatts of electricity.[70] A plan by Samsung to build an offshore wind farm on the north shore of the lake, from Port Maitland to Nanticoke for a distance of 15.5 mi (24.9 km),[71] has been met with opposition from residents. Canadians near Leamington and Kingsville have organized protest groups to thwart attempts to bring wind turbines to the lake; reasons against the turbines include spoiling lake views.[72] Plans to install turbines in Pigeon Bay, south of Leamington were met with opposition as well.[73][74] The notion that bird and bat migration may be hurt by the wind turbines has been used to argue against the wind turbines as well.
 The lake is responsible for microclimates that are important to agriculture. Along its north shore is one of the richest areas of Canada's fruit and vegetable production; this southernmost tip, particularly in the area around Leamington, is known as Canada's ""tomato capital"".[72] The area around Port Rowan has special trees that grow because of the ""tempering effect of the lake"", and species include tulip trees, dogwood, sassafras, and sour gum.[34] This area's many greenhouses produce a ""variety of tropical plants rarely cultivated so far north"", including some species of cacti, because of the lake's tempering effect.[34] Along the southeastern shore of Ohio, Pennsylvania, and New York is an important grape-growing region, as are the islands in the lake. Apple orchards are abundant in northeast Ohio to western New York.
 According to one estimate, 34 to 36 inches (860 to 910 mm) of water evaporate each year from the surface of the lake,[6] which allows for rainfall and other precipitation in surrounding areas. Reports are conflicting about the overall effect of global warming on the Great Lakes region, including Lake Erie. One account suggests that climate change is causing greater evaporation of lake water, leading to warmer temperatures, as well as thinner or nonexistent ice in winter, which is fueling concerns that ""Erie appears to be shrinking"" and is the most likely candidate among the five Great Lakes to ""turn into a festering mud puddle.""[52] In 2010, the Windsor Star reported that the lake experienced record-breaking water temperatures, reaching 81 °F (27 °C) in mid-August and compared the lake to a ""bath tub"".[75]
 Lake Erie has a complex ecosystem with many species in interaction. Human activity, such as pollution and maritime ship traffic, can affect this environment in numerous ways. The interactions between new species can sometimes have beneficial effects, as well as harmful effects. Some introductions have been seen as beneficial such as the introduction of Pacific salmon.[52] Occasionally there have been mass die-offs of certain species of fish, sometimes for reasons unknown, such as many numbers of rainbow smelt in May 2010.[76]
 The lake has been plagued with a number of invasive species, including zebra[52] and quagga mussels, the goby, and the grass carp. One estimate was that 180 invasive species are in the Great Lakes, some having traveled in ballast water in international ships.[77] Zebra mussels and gobies have been credited with the increased population and size of smallmouth bass in Lake Erie.[78] In 2008, concerns arose that the ""newest invader swarming in the Great Lakes"", the bloody-red shrimp, might harm fish populations and promote algal blooms.[79]
 Environmentalists and biologists study lake conditions via installations such as the Franz Theodore Stone Laboratory on Gibraltar Island. The lab, which was established in 1895, is the oldest biological field station in the United States. Stone Laboratory was donated to the Ohio State University by Julius Stone in 1925 as part of the university's Ohio Sea Grant College program.[30][77] The Great Lakes Institute of the University of Windsor has experts who study issues such as lake sediment pollution and the flow of contaminants such as phosphorus.[77]
 Other invasive species in Lake Erie include spiny water fleas, fishhook water fleas, sea lamprey, and white perch. The invasive plant species in Lake Erie consist mainly of Eurasian milfoil, Trapa natans and purple loosestrife.[80] The shore of the lake is also host to invasive species of the Phragmites reed genus.[81]
 An ongoing concern is that nutrient overloading from fertilizers and human and animal waste, known as eutrophication, in which additional nitrogen and phosphorus enter the lake, will cause plant life to ""run wild and multiply like crazy"".[15] Since fewer wetlands remain to filter nutrients, and greater channelization of waterways, nutrients in water can cause algal blooms to sprout, with ""low-oxygen dead zones"" in a complex interaction of natural forces.[15] As of the 2010s, much of the phosphorus in the lake comes from fertilizer applied to no-till soybean and corn fields, but washed into streams by heavy rains. The algal blooms result from growth of Microcystis, a toxic blue-green alga that the zebra mussels, which infest the lake, do not eat.[16]
 Periodically, a dead zone, or region of low oxygen, occurs in the lake, the location of which varies. Scientists from the National Oceanic and Atmospheric Administration have been studying the lake's blue-green algal blooms and trying to find ways to predict when they are spreading or where they might make landfall; typically, the blooms arrive late each summer.[82] This problem was extreme in the mid- and late 1960s, and the Lake Erie Wastewater Management Study conducted by the Buffalo District of the U.S. Army Corps of Engineers determined that the eutrophication was caused by point sources such as industrial outfalls and municipal sanitary and storm sewer outfalls, as well as diffuse sources, such as overland runoff from farm and forest land. All of these sources contribute nutrients, primarily phosphorus, to the lake. Growth of organisms in the lake is then spiked to the point that oxygen levels are depleted. Recommendations were made for reducing point-source outflows, and reducing farm contributions of phosphorus by changing fertilizer usage, employing no-till farming, and other conservative practices. Many industrial and municipal sources have since been greatly reduced. The improved farming practices, which were voluntary, were followed for a while, resulting in remarkable recovery of the lake in the 1970s.[83]
 The conservation practices are not monitored and have not been kept up. One recent account suggests that the seasonal algal blooms in Lake Erie were possibly caused by runoff from cities, fertilizers, zebra mussels, and livestock near water.[82] A second report focuses on the zebra mussels as being the cause of dead zones, since they filter so much sediment that this produces an overgrowth of algae.[52] One report suggests the oxygen-poor zone began about 1993 in the lake's central basin and becomes more pronounced during summer, but is somewhat of a mystery why this happens.[84] Some scientists speculate that the dead zone is a naturally occurring phenomenon.[77] Another report cites Ohio's Maumee River as the main source of polluted runoff of phosphorus from industries, municipalities, tributaries and agriculture, and in 2008, satellite images showed the algal bloom heading toward Pelee Island.[77] Two two-year, $2 million studies are trying to understand the ""growing zone"", which was described as a 10-foot-thick layer of cold water at the bottom, 55 feet (17 m) in one area, which stretches 100 miles [160 km] across the lake's center.[84] It kills fish and microscopic creatures of the lake's food chain and fouls the water, and may cause further problems in later years for sport and commercial fishing.[84]
 Algal blooms continued in early 2013, but new farming techniques, climate change, and even a change in Lake Erie's ecosystem make phosphorus pollution more intractable.[85] Blue-green algae (Cyanobacteria) bloom,[86] were problematic in August 2019. According to a news report in August, ""scientists fully expect [it] to overwhelm much of western Lake Erie again this summer"".[87] By August 12, 2019, the bloom extended for roughly 50 kilometres (31 mi).[88] A large bloom does not necessarily mean the cyanobacteria ... will produce toxins"", said Michael McKay, executive director of the Great Lakes Institute for Environmental Research  at the University of Windsor. ""Not enough is being done to stop fertilizer and phosphorus from getting into the lake and causing blooms,"" he added. Water testing was being conducted in August.[89] The largest Lake Erie blooms to date occurred in 2015, exceeding the severity index at 10.5 and in 2011 at a 10, according to the NOAA. In early August, the 2019 bloom was expected to measure 7.5 on the severity index, but could range between 6 and 9.[90] At that time, satellite images depicted a bloom stretching up to 1,300 square kilometres (500 sq mi) on Lake Erie, with the epicenter near Toledo, Ohio.[91]
 The Lake Erie water snake, a subspecies of the northern water snake (Nerodia sipedon), lives in the vicinity of Ohio's Put-in-Bay Harbor and had been placed on the threatened species list.[92] By 2010, the water snake population was over 12,000 snakes.[92] While they have a nonvenomous bite, they are a key predator in the lake's aquatic ecosystem since they feed on mudpuppies, walleye, and smallmouth bass.[92] The snake is helpful in keeping the population of goby fish in check.[92] They mate from late May through early June and can be found in large mating balls with one female bunched within several males.[93]
 In 1999, a local TV station's Doppler weather radar detected millions of mayflies heading for Presque Isle in blue and green splotches on the radar in clouds measuring 10 mi (16 km) long.[94] These insects were a sign of Lake Erie's move back to health, since the mayflies require clean water to thrive.[94] Biologist Masteller of Pennsylvania State University declared the insects to be a ""nice nuisance"" since they signified the lake's return to health after 40 years of absence.[94] Each is 1.5 in (38 mm) long; the three main species of mayflies are Ephemera simulans, Hexagenia rigida, and H. limbata.[94] The insects mate over a 72-hour period from June through September; they fly in masses up to the shore, mate in the air, then females lay up to 8,000 eggs each over the water; the eggs sink back down and the cycle repeats.[94] Sometimes, the clouds of mayflies have caused power outages[95] and roads to become slippery with squashed insects.[94] Zebra mussels filtering extra nutrients from the lake allows the mayfly larvae to thrive.[95]
 Incidents of birds dying from botulism have occurred, in 2000[96] and in 2002.[97] Birds affected included grebes, common and red-breasted mergansers, loons, diving ducks, ring-billed gulls, and herring gulls.[96] One account suggests that bird populations are in trouble, notably the wood warbler, which had population declines around 60% in 2008.[52] Possible causes for declines in bird populations are farming practices, loss of habitats, soil depletion and erosion, and toxic chemicals.[52] In 2006, concerns arose of possible avian influenza (bird flu) after two wild swans on the lake were found diseased, but they did not contain the H5N1 virus.[98] Sightings of a magnificent frigatebird, a tropical bird with a 2 m wingspan, happened over the lake in 2008.[99]
 Lake Erie infamously became very polluted in the 1960s and 1970s as a result of the quantity of heavy industry situated in cities on its shores, with reports of bacteria-laden beaches and fish contaminated by industrial waste.[100] In the 1970s, patches of the lake were declared dead because of industrial waste and sewage from runoffs; as The New York Times reporter Denny Lee wrote in 2004, ""The lake, after all, is where the Rust Belt meets the water.""[20]
 Incidents occurred of the oily surfaces of tributary rivers emptying into Lake Erie catching fire: in 1969, Cleveland's Cuyahoga River erupted in flames,[101] chronicled in a Time magazine article which lamented a tendency to use rivers flowing through major cities as ""convenient, free sewers"";[59] the Detroit River caught fire on another occasion.[52] The outlook was gloomy:
 In December 1970, a federal grand jury investigation led by U.S. Attorney Robert Jones began, of water pollution allegedly being caused by about 12 companies in northeastern Ohio.[102] It was the first grand jury investigation of water pollution in the area.  The grand jury indicted four corporations for polluting Lake Erie and waterways in northeast Ohio. Facing fines were Cleveland Electric Illuminating Co., Shell Oil Co., Uniroyal Chemical Division of Uniroyal Inc. and Olin Corp.[103] United States Attorney General John N. Mitchell gave a press conference December 18, 1970, referencing new pollution control litigation, with particular reference to work with the Environmental Protection Agency, and announcing the filing of a lawsuit that morning against the Jones and Laughlin Steel Corporation for discharging substantial quantities of cyanide into the Cuyahoga River near Cleveland.[104] Jones filed the misdemeanor charges in district court, alleging violations of the 1899 Rivers and Harbors Act.[105]
 Cleveland's director of public utilities Ben Stefanski pursued a massive effort to ""scrub the Cuyahoga""; the effort cost $100 million in bonds, according to one estimate.[59] New sewer lines were built.[59] Clevelanders approved a bond issue by 2 to 1 to upgrade Cleveland's sewage system.[59] Federal officials acted as well: the United States Congress passed the Clean Water Act of 1972,[101][106] and the United States and Canada established water pollution limits in an international water quality agreement. The Corps' LEWMS was also instituted at that time.
 The clearing of the water column is partly the result of the introduction and rapid spread of zebra mussels from Europe, which had the effect of covering the lake bottom, with each creature filtering a liter of fresh water each day, helping to restore the lake to a cleaner state.[20] The 1972 Great Lakes Water Quality Agreement significantly reduced the dumping and runoff of phosphorus into the lake. The lake has since become clean enough to allow sunlight to infiltrate its water and produce algae and sea weed, but a dead zone persists.[107] There have been instances of beach closings at Presque Isle because of unexplained E. Coli contaminations,[108] possibly caused by sewer water overflows after heavy downpours.
 Since the 1970s environmental regulation has led to a great increase in water quality and the return of economically important fish species such as walleye and other biological life.[109] There was substantial evidence that the new controls had substantially reduced levels of DDT in the water by 1979.[110] Cleanup efforts were described in 1979 as a notable environmental success story, suggesting that the cumulative effect of legislation, studies, and bans had reversed the effects of pollution:[110]
 Joint U.S.–Canadian agreements pushed 600 of 864 major industrial dischargers to meet requirements for keeping the water clean.[110] One estimate was that $5 billion was spent to upgrade plants to treat sewage.[110] The change toward cleaner water has been in a positive direction since the 1970s.
 There was a tentative exploratory plan to capture CO2, compress it to a liquid form, and pump it a half-mile (800 m) beneath Lake Erie's surface underneath the porous rock structure.[111] According to chemical engineer Peter Douglas, there is sufficient storage space beneath Lake Erie to hold between 15 and 50 years of liquid CO2 emissions from the 4,000 megawatt Nanticoke coal plant.[111] But there has been no substantial progress on this issue since 2007.
 Lake Erie is home to one of the world's largest freshwater commercial fisheries. Lake Erie's fish populations are the most abundant of the Great Lakes, partially because of the lake's relatively mild temperatures and plentiful supply of plankton, which is the basic building block of the food chain.[41] The lake's fish population accounts for an estimated 50% of all fish inhabiting the Great Lakes.[112] The lake contains steelhead,[113] walleye (known in Canada as pickerel),[41][113] largemouth bass, smallmouth bass,[113] perch,[113] lake trout, king salmon, whitefish, smelt, and many others.[41] The lake consists of a long list of well established introduced species. Common non-indigenous fish species include the rainbow smelt, alewife, white perch and common carp. Non-native sport fish such as rainbow trout and brown trout are stocked specifically for anglers to catch. Attempts failed to stock coho salmon, and its numbers are dwindling. Commercial landings are dominated by yellow perch and walleye, with substantial quantities of rainbow smelt and white bass also taken. Anglers target walleye and yellow perch, with some effort directed at rainbow trout.
 Up until the end of the 1950s, the most commonly caught commercial fish (more than 50% of the commercial catch) was a subspecies of the walleye known as the blue walleye (Sander vitreus glaucus) sometimes erroneously called ""blue pike"". In the 1970s and 1980s, as pollution in the lake declined, counts of walleyes which were caught grew from 112,000 in 1975 to 4.1 million in 1985, with estimates of the numbers of walleyes in the lake at around 33 million in the basin, with many of 8 pounds (3.6 kg) or more.[114] Not all walleyes thrived. The combination of overfishing and the eutrophication of the lake by pollution caused the population to collapse, and in the mid-1980s, the blue walleye was declared extinct. But the Lake Erie walleye was reportedly having record numbers, even in 1989, according to one report.[115]
 There have been concerns about rising levels of mercury in walleye fish; a study by the Canadian Ministry of the Environment noted an ""increasing concentration trend"" but that concentrations were within acceptable limits established by authorities in Pennsylvania.[116] Because of the threat of PCBs, It was recommended, that persons eat no more than one walleye meal per month.[116] Because of these and other concerns, in 1990, the National Wildlife Federation was on the verge of having a ""negative fish consumption advisory"" for walleye and smallmouth bass, which had been the main catch of an $800 million commercial fishing industry.[117]
 The longest fish in Lake Erie is reportedly the sturgeon which can grow to 10 feet (3.0 m) long and weight 300 pounds (140 kg), but it is an endangered species and mostly lives on the bottom of the lake.[118] In 2009, there was a confirmed instance of a sturgeon being caught, which was returned to the lake alive, and there are hopes that the population of sturgeons is resurging.[119]
 Estimates vary about the fishing market for the Great Lakes region. In 2007, one estimate of the total market for fishing in the Great Lakes, including commercial and recreational fishing, was $4 billion annually.[82] Another estimate was more than $7 billion.[41] But since high levels of pollution were discovered in the 1960s and 1970s, there has been continued debate over the desired intensity of commercial fishing. Commercial fishing in Lake Erie has been hurt by pollution as well as government regulations which limit the size of their catch; one report suggested that the numbers of fishing boats and employees had declined by two-thirds in recent decades.[41] Another concern had been that pollution in the lake, as well as toxins found inside fish, were working against commercial fishing interests.[101]
 U.S. fishermen based along Lake Erie lost their livelihood over the past few decades and no longer catch fish such as whitefish for markets in New York.[24] Pennsylvania had a special $3 stamp on fishing licenses to help ""compensate commercial fishermen for their losses"", but this program ended after five years.[24] One blamed the commercial fishing ban on a ""test of wills"" between commercial and recreational fishermen: ""One side needed large hauls. The other feared the lake was being emptied.""[24]
 Commercial fishing is now predominantly based in Canadian communities, with a much smaller fishery—largely restricted to yellow perch—in Ohio. The Ontario fishery is one of the most intensively managed in the world. However, there are reports that some Canadian commercial fishermen are dissatisfied with fishing quotas and have sued the government about this matter, and there have been complaints that the legislative body writing the quotas is dominated by the U.S. and that sport fishing interests are favored at the expense of commercial fishing interests.[120] Cuts of 30 to 45 percent for certain fish were made in 2007.[120] The Lake Erie fishery was one of the first fisheries in the world managed on individual transferable quotas and features mandatory daily catch reporting and intensive auditing of the catch reporting system. Still, the commercial fishery is the target of critics who would like to see the lake managed for the exclusive benefit of sport fishing and the various industries serving the sport fishery. According to one report, the Canadian town of Port Dover is the home of the lake's largest fishing fleet.[34]
 The lake can be thought of as a common asset with multiple purposes including being a fishery. There was direct competition between commercial fishermen and sport fishermen (including charter boats and sales of fishing licenses) throughout the lake's history, with both sides seeking government assistance from either Washington or Ottawa, and trying to make their case to the public through newspaper reporting.[49] But other groups have entered the political process as well, including environmentalists, lakefront property owners, industry owners and workers seeking cost-effective solutions for sewage, ferry boat operators, even corporations making electric-generating wind turbines.
 Management of the fishery is by consensus of all management agencies with an interest in the resource and work under the mandate of the Great Lakes Fishery Commission. The commission makes assessments using sophisticated mathematical modeling systems. The commission has been the focus of considerable recrimination, primarily from angler and charter fishing groups in the U.S. which have had a historical antipathy to commercial fishing interests. This conflict is complex, dating from the 1960s and earlier, with the result in the United States that, in 2011, commercial fishing was mostly eliminated from Great Lakes states. One report suggests that battling between diverse fishing interests began around Lake Michigan and evolved to cover the entire Great Lakes region.[121] The analysis suggests that in the Lake Erie context, the competition between sport and commercial fishing involves universals and that these conflicts are cultural, not scientific, and therefore not resolvable by reference to ecological data.[122]
 The lake supports a strong sport fishery. While commercial fishing declined, sport fishing has remained. The deep cool waters that spawn the best fishing is in the Canadian side of the lake.[123] As a result, a fishing boat that crosses the international border triggers the security concerns of border crossings, and fishermen are advised to carry their passport.[123] If their boat crosses the invisible border line in the lake, upon returning to the American shore, passengers need to report to a local border protection office.[123]
 In 2008, the Pennsylvania Fish and Boat Commission tried stocking the lake with brown trout in an effort to build what's called a put-grow-and-take fishery.[124] There was a report that charter boat fishing increased substantially on the American side, from 46 to 638 charter boats in operation in Ohio alone, during a period from 1975 to 1985 as pollution levels declined and after populations of walleye increased substantially in the lake.[114] In 1984, Ohio sold 27,000 nonresident fishing permits, and sport fishing was described as big business.[114] In 1992, there were accounts of fishermen regularly catching walleye weighing up to 12 pounds (5.4 kg).[125] It is possible to fish off piers in winter for burbot; the burbot make a midwinter spawning run and is reportedly one of Erie's glacial relics.[113]
 In winter when the lake freezes, many fishermen go out on the ice, cut holes, and fish. It is even possible to build bonfires on the ice.[69] But venturing on Lake Erie ice can be dangerous. In a 2009 incident, warming temperatures, winds of 35 miles per hour (56 km/h) and currents pushing eastward dislodged a miles-wide ice floe which broke away from the shore, trapping more than 130 fishermen offshore; one man died while the rest were rescued by helicopters or boats.[126]
 The lake's formerly more extensive lakebed creates a favorable environment for agriculture in the bordering areas of Ontario, Ohio, Michigan, Pennsylvania, and New York. The Lake Erie sections of western New York have a suitable climate for growing grapes, and there are many vineyards and wineries in Chautauqua County and Erie County.[127] The Canadian region of Lake Erie's north shore is becoming a more prominent wine region as well; it has been dubbed the Lake Erie North Shore, or LENS region, and includes Pelee Island,[128] and since it is farther north than comparable wine-growing areas in the world, the length of the days in the summer are longer.[129] A longer growing season because of the lake-moderated temperatures make the risk of early frosts less likely.[129]
 The drainage basin has led to well fertilized soil. The north coast of Ohio is widely referred to as its nursery capital.[130]
 Lake Erie is a favorite for divers since there are many shipwrecks, perhaps 1,400 to 8,000 according to one estimate,[38] of which about 270 are confirmed shipwreck locations.[38] Research into shipwrecks has been organized by the Peachman Lake Erie Shipwreck Research Center, located on the grounds of the Great Lakes Historical Society.[38] Most wrecks are undiscovered but believed to be well preserved and at most 200 feet (61 m) below the water surface.[131] One report suggests there are more wrecks per square mile than any other freshwater location, including wrecks from Indigenous watercraft. There are efforts to identify shipwreck sites and survey the lake floor to map the location of underwater sites, possibly for further study or exploration.[132] While the lake is relatively warmer than the other Great Lakes, there is a thermocline, meaning that as a diver descends, the water temperature drops about 30 degrees Fahrenheit change (17 °C), requiring a wetsuit.[131] One estimate is that Lake Erie has a quarter of all 8,000 estimated shipwrecks in the Great Lakes.[131] They are preserved because the water is cold and salt-free.[131] Divers have a policy of not removing or touching anything at the wreck.[131] The cold conditions make diving difficult, requiring divers with skill and experience.[131] One charter firm from western New York State takes about 1,500 divers to Lake Erie shipwrecks in a typical season from April through October.[131]
 In 1991, the 19th-century paddle steamer Atlantic was discovered.[133] It had sunk in 1852 after a collision with the steamship Ogdensburg, six miles (9.7 km) west of Long Point, Ontario, and survivors from Atlantic were saved by the crew of Ogdensburg.[133][134] One account suggests 130 people drowned[133] while another suggests about 20 drowned.[134] There was speculation that the sunken vessel had been a gambling ship, and therefore there might have been money aboard, but most historians were skeptical.[133]
 In 1998, the wreckage of Adventure became the first shipwreck registered as an ""underwater archaeological site""; when it was discovered that Adventure's propeller had been removed and given to a junkyard. The propeller was reclaimed days before being converted to scrap metal and brought back to the dive site.[38] In 2003, divers discovered the steamer Canobie near Presque Isle, which had sunk in 1921.[132] Other wrecks include the fish tub Neal H. Dow (1910), the ""steamer-cum-barge"" Elderado (1880),[132] W. R. Hanna,[38] Dundee which sank north of Cleveland in 1900,[38] F. H. Prince,[38] and The Craftsman.[38] In 2007, the wreck of the steamship named after ""Mad"" Anthony Wayne was found near Vermilion, Ohio, in 50 feet (15 m) of water; the vessel sank in 1850 after its boilers exploded, and 38 people died.[135] The wreck belongs to the state of Ohio, and salvaging it is illegal, but divers can visit.[135] In addition, there are wrecks of smaller vessels, with occasional drownings of fishermen.[136]
 There are numerous public parks around the lake. In western Pennsylvania, a wildlife reserve was established in 1991 in Springfield Township for hiking, fishing, cross-country skiing and walking along the beach.[137] In Ontario, Long Point is a peninsula on the northwest shore near Port Rowan that extends 20 miles (32 km) into Lake Erie which is a stopover for birds migrating as well as turtles; Long Point Provincial Park is located there and has been designated as a UNESCO Biosphere reserve.[34] In Ontario's Sand Hill Park, east of Port Burwell, there is a 450-foot (140 m) high dune which people climb for picturesque views of the lake.[138] In southern Michigan, Sterling State Park has campgrounds, 1,300 acres (530 ha) for hiking, biking, fishing, boating, with a sand beach for sunbathing, swimming, and picnicking.[139]
 In 1997, The New York Times reporter Donna Marchetti took a bike tour around the Lake Erie perimeter, traveling 40 miles (64 km) per day and staying at bed and breakfasts.[34] She biked through the cities of Cleveland, Erie, Windsor, Detroit and Toledo as well as resort towns, vineyards, and cornfields.[34] The trip highlights were the ""small port towns and rural farmlands of southern Ontario"".[34] There are few bike repair shops in Ontario on the route.[34]
 Lake Erie islands tend to be in the westernmost part of the lake and have different characters. Some of them include:
 Kayaking has become more popular along the lake, particularly in places such as Put-in-Bay, Ohio.[20] There are extensive views with steep cliffs with exotic wildlife and extensive shoreline.[20] Long-distance swimmers have swum across the lake to set records; for example, a 15-year-old amputee swam the 12-mile (19 km) stretch across the lake in 2001.[141] In 2008, 14-year-old Jade Scognamillo swam from New York's Sturgeon Point to Ontario's Crystal Beach and completed the 11.9-mile (19.2-km) swim in five hours, 40 minutes and 35 seconds, and became the youngest swimmer to make the crossing.[142] It is illegal for swimmers younger than 14 to attempt such a crossing.[142] In Port Dover, Ontario, swimmers do high-dives at the annual ""Polar Bear Swim"" on the beach.[143] Currents can pose a problem, and there have been occasional incidents of drownings.[144][145] Since 2011, there have been 228 deaths.[146]
 The lake is dotted by distinct lighthouses. A lighthouse off the coast of Cleveland, beset with cold lake winter spray, has an unusual artistic icy shape, although sometimes ice prevents the light from being seen by maritime vessels.[147]
 There have been unconfirmed reports of persons spotting a creature akin to the Loch Ness Monster, beginning in the 19th century and sometimes called ""Bessie"" or ""South Bay Bessie"".[118][148] There were reports in 1990 of people seeing a ""large creature moving in the water about 1,000 feet (300 m) from their boat"" described as black in color, about 35 feet (11 m) long, with a snakelike head, and moving as fast as a boat.[118] Five other people reported seeing something similar on three separate occasions, but there is no scientific evidence of such a creature.[118] There is a Lake Erie Monster beer and a Cleveland Monsters hockey team.[148]
 There have been sporadic reports of people in Cleveland being able to see the Canadian shoreline as if it were immediately offshore, even though Canada is 50 miles (80 km) from Cleveland. It has been speculated that this is a weather-related phenomenon, working on similar principles as a mirage.[149]
 The lake has been a shipping lane for maritime vessels for centuries.[131][132] Ships headed eastward can take the Welland Canal[150] and a series of eight locks descending 326 feet (99 m) to Lake Ontario which takes about 12 hours.[34] Thousands of ships make this journey each year.[34] During the 19th century, ships could enter the Buffalo River and travel the Erie Canal eastward to Albany then south to New York City along the Hudson River. Generally there is heavy traffic on the lake except during the winter months from January through March when ice prevents vessels from traveling safely.[150]
 In 2007, there was a protest against Ontario's energy policy which allows the shipping of coal in the lake; Greenpeace activists climbed a ladder on a freighter and ""locked themselves to the conveyor belt device that helps to unload the ship's cargo""; three activists were arrested and the ship was delayed for more than four hours, and anti-coal messages were painted on the ship.[151]
 Ferryboats operate in numerous places: such as the passenger-only Jet Express Ferry from Sandusky and Port Clinton to Put-in-Bay and Kelly's Island. The Miller Ferry from Catawba Island to Put-In-Bay and Middle Bass Island, the Kellys Island ferry from Marblehead to Kellys Island, and the Owen Sound Transportation Company from Leamington or Kingsville to Pelee Island and Sandusky.
 However, plans to operate a ferryboat between the U.S. port of Erie and the Ontario port of Port Dover ran into a slew of political problems, including security restrictions on both sides as well as additional fees required to hire border inspectors.[24] The project was abandoned.[24]
 The Great Lakes Circle Tour is a designated scenic road system connecting all of the Great Lakes and the Saint Lawrence River.[152] Drivers can cross from the United States to the Canadian town of Fort Erie by going over the Peace Bridge.[34]
 Since the border between the two nations is largely unpatrolled, it is possible for people to cross undetected from one country to the other, in either direction, by boat. In 2010, Canadian police arrested persons crossing the border illegally from the United States to Canada, near the Ontario town of Amherstburg.[153]
"
Diplomatic rank,https://en.wikipedia.org/wiki/Diplomatic_rank#Special_envoy,"Diplomatic rank is a system of professional and social rank used in the world of diplomacy and international relations. A diplomat's rank determines many ceremonial details, such as the order of precedence at official processions, table seatings at state dinners, the person to whom diplomatic credentials should be presented, and the title by which the diplomat should be addressed.
 The current system of diplomatic ranks was established by the Vienna Convention on Diplomatic Relations (1961).[1] There are three top ranks, two of which remain in use:
 The body of diplomats accredited to a country form the diplomatic corps. Ambassadors have precedence over chargés, and precedence within each rank is determined by the date on which diplomatic credentials were presented.[4] The longest-serving ambassador is the dean of the diplomatic corps, who speaks for the entire diplomatic corps on matters of diplomatic privilege and protocol. In many Catholic countries, the papal nuncio is always considered the dean of the diplomatic corps.
 The ranks established by the Vienna Convention (1961) modify a more elaborate system of ranks that was established by the Congress of Vienna (1815):[5]
 The rank of envoy was short for ""envoy extraordinary and minister plenipotentiary"", and was more commonly known as ""minister"".[2] For example, the ""envoy extraordinary and minister plenipotentiary of the United States to the French Empire"" was known as the ""United States Minister to France"" and addressed as ""Monsieur le Ministre"".[7][8]
 An Ambassador was regarded as the personal representative of his sovereign as well as his government.[9] Only major monarchies would exchange Ambassadors with each other, while smaller monarchies and republics only sent Ministers. Because of diplomatic reciprocity, Great Powers would only send a minister to a smaller monarchy or a republic.[10] For example, in the waning years of the Second French Empire, the United Kingdom sent an ambassador to Paris, while Sweden-Norway and the United States sent ministers.[11]
 The rule that only monarchies could send ambassadors was more honored in the breach than the observance. This had been true even before the Congress of Vienna, as England continued to appoint ambassadors after becoming a republic in 1649.[12] Countries that overthrew their monarchs proved to be unwilling to accept the lower rank accorded to a republic. After the Franco-Prussian War, the French Third Republic continued to send and receive ambassadors.[8] The rule became increasingly untenable as the United States grew into a Great Power. The United States followed the French precedent in 1893, and began to exchange ambassadors with other Great Powers.[2]
 Historically, the order of precedence had been a matter of great dispute. European powers agreed that the papal nuncio and imperial ambassador would have precedence, but could not agree on the relative precedence of the kingdoms and smaller countries. In 1768, the French and Russian ambassadors to Great Britain even fought a duel over who had the right to sit next to the imperial ambassador at a court ball. After several diplomatic incidents between their ambassadors, France and Spain agreed in 1761 to let the date of arrival determine their precedence. In 1760, Portugal attempted to apply seniority to all ambassadors, but the rule was rejected by the other European courts.[12]
 The Congress of Vienna finally put an end to these disputes over precedence. After an initial attempt to divide countries into three ranks faltered on the question of which country should be in each rank, the Congress instead decided to divide diplomats into three ranks. A fourth rank was added by the Congress of Aix-la-Chapelle (1818). Each diplomatic rank had precedence over the lower ranks, and precedence within each rank was determined by the date that their credentials were presented. The papal nuncio could be given a different precedence than the other ambassadors. The Holy Roman Empire had ceased to exist in 1806, so the Austrian ambassador would accumulate seniority along with the other ambassadors.[12][13]
 In modern diplomatic practice, there are a number of diplomatic ranks below Ambassador. Since most missions are now headed by an ambassador, these ranks now rarely indicate a mission's (or its host nation's) relative importance, but rather reflect the diplomat's individual seniority within their own nation's diplomatic career path and in the diplomatic corps in the host nation:
 The term attaché is used for any diplomatic agent who does not fit in the standard diplomatic ranks, often because they are not (or were not traditionally) members of the sending country's diplomatic service or foreign ministry, and were therefore only ""attached"" to the diplomatic mission. The most frequent use is for military attachés, but the diplomatic title may be used for any specific individual or position as required, generally related to a specific or technical field. Since administrative and technical staff benefit from only limited diplomatic immunity, some countries may routinely appoint support staff as attachés. Attaché does not, therefore, denote any rank or position (except in Soviet and post-Soviet diplomatic services, where attaché is the lowest diplomatic rank of a career diplomat). Note that many traditional functionary roles, such as press attaché or cultural attaché, are not formal titles in diplomatic practice, although they may be used as a matter of custom.
 Furthermore, outside this traditional pattern of bilateral diplomacy, as a rule on a permanent residency basis (though sometimes doubling elsewhere), certain ranks and positions were created specifically for multilateral diplomacy:
 Special envoys have been created ad hoc by individual countries, treaties and international organizations including the United Nations. A few examples are provided below:
 Most countries worldwide have some form of internal rank, roughly parallel to the diplomatic ranks, which are used in their foreign service or civil service in general. The correspondence is not exact, however, for various reasons, including the fact that according to diplomatic usage, all Ambassadors are of equal rank, but Ambassadors of more senior rank are typically sent to more important postings. Some countries may make specific links or comparisons to military ranks.
 Officers from the Department of Foreign Affairs and Trade (DFAT) are graded into four broad bands (BB1 to BB4), with the Senior Executive Service (SES Band 1 to SES Band 3) following above.
 Ambassadors, High Commissioners and Consuls-General usually come from the Senior Executive Service, although in smaller posts the head of mission may be a BB4 officer. Generally speaking (and there are variations in ranking and nomenclature between posts and positions), Counsellors are represented by BB4 officers; Consuls and First and Second Secretaries are BB3 officers and Third Secretaries and Vice Consuls are BB2 officers. DFAT only posts a limited number of low-level BB1 staff abroad. In large Australian missions an SES officer who is not the head of mission could be posted with the rank of Minister.
 The Brazilian Foreign Service (Serviço Exterior Brasileiro) is made up of three careers: the Diplomat Career, the Chancery Officer Career and the Chancery Assistant Career.[20][21]
 
There are no ranks in the Chancery Assistant or Chancery Officer careers, nor a hierarchy between careers. However, when working abroad, it is common for Chancery Assistants and Chancery Officers to be assigned to sensitive functions, such as the Vice-Consul, and/or as Head of Sectors such as administration, accounting, communications, processing of political, commercial, diplomatic or consular information.
 There are six ranks in the Diplomat career, in hierarchical order:
 
Embaixador / Embaixadora is the honorary dignity conceded permanently when a Minister of First Class assumes a Post overseas. It can also be a temporary assignment, when carried on by a lower-rank diplomat or Brazilian politician of high level.
 The ranks of the Ministry of Foreign Affairs of the People's Republic of China are defined by the Law on Diplomatic Personnel Stationed Abroad, passed in 2009 by the National People's Congress:[22]
 The following ranks are used in the Egyptian Ministry of Foreign Affairs:
 There are five ranks in the French Diplomatic Service:[23]
(in ascending order)
 There are two additional ranks for ICT specialists (also in ascending order):
 The German Foreign Service uses a rank system[24] that is connected to that of the rest of the civil administration and to military ranks through a common pay table. All ranks also occur in female form.
 The ranks at the Hungarian Foreign Service are the following.:
 In Italy, ranks and functions are not exactly connected: each rank can cover several functions. Moreover, several exceptions apply.
 There are about 30 people who hold the rank of Ambassador. Therefore, most of the about 150 Italian embassies or permanent representations are held by a Minister Plenipotentiary: traditionally, ambassadors are appointed to the most important representations, such as London, Paris, Washington, New Delhi and Peking embassies and representations to the UN in New York City and the EU in Brussels.
 After the merger of the Consular and Diplomatic Corps, the current grades of Mexican career diplomats are (in ascending order)
 There are additional ranks for Administrative specialists and Staff, this civil servants are also part of the Mexican Foreign Service.
 In ascending order, the five ranks of the Portuguese diplomatic career are, as defined in the Statute of the Diplomatic Career (Estatuto da Carreira Diplomática):[25]
 Ministers Plenipotentiary who have been in that rank for three or more years are called ""Minister Plenipotentiary, First Class"" (ministro plenipotenciário de 1.ª classe), those who have been in the rank for less than three years are called ""Minister Plenipotentiary, Second Class"" (ministro plenipotenciário de 2.ª classe). Embassy Secretaries who have been in that rank for six years or more and in the diplomatic career for eight years or more are called ""First Embassy Secretary"" (primeiro-secretário de embaixada), those who have been in the rank for three years or more and for five years or more in the diplomatic career are called ""Second Embassy Secretary"" (segundo-secretário de embaixada), and those who have been in that rank for less than three years are called ""Third Embassy Secretary"" (terceiro-secretário de embaixada).[25]
 The diplomatic ranks in Russian Federation were introduced with enactment of the Federal Law of 27 July 2010 No.205-FZ.[26] Diplomatic ranks are not to be confused with diplomatic positions (posts).
 The Singapore Foreign Service also has a merged Diplomatic and Consular Corps. 
 Its career diplomats and diplomatic support staff are split across two discrete career schemes, namely: (a) Foreign Service Officers; and (b) Foreign Service Administration Specialists. 
 Foreign Service Officers (FSOs) 
 FSOs are selected through multiple rounds of highly competitive written and observational psychometric and neuropsychological evaluations. Being one of the most exclusive and sought-after roles in the entire Singapore Civil Service, FSO candidates are typically drawn from graduates of the world's top universities. This is especially the case for candidates vying to be emplaced on the extremely competitive Political Track, of which only around 20 are recruited nationwide annually.   
 Regardless, most candidates who are eventually selected, possess degrees with First Class Honours from the world's top fifty universities (e.g. the University of Oxford or the University of Cambridge in the United Kingdom, many of the Ivy League institutions in the United States, or Singapore's two most prestigious universities - the National University of Singapore and the Nanyang Technological University).   
 Foreign Service Administration Specialists (FSASes)
 FSASes, on the other hand, while still selected through some manner of written and observational assessments, are typically those bearing more conventional educational qualifications. These include graduates from top universities but without ""good"" honours, or from private and mainstream universities. A large number of FSASes also include Polytechnic graduates (who possess Diplomas).
 Given the above, FSOs typically occupy the managerial positions, while FSASes generally perform more operational roles. [Note: FSOs are typically the diplomats, while FSASes serve as support staff.] 
 Officials from both schemes occupy billets at both the Singapore Ministry of Foreign Affairs, as well as Singapore's Overseas Missions (Embassies/High Commissions/Consulates-General/Consulates) - which number over 50.
 Rank on Post
 FSOs are posted to Singapore's overseas missions at the rank of Second Secretary, while FSASes are posted according to their substantive grades (typically ranging from Assistant Mission Support Officer to Attache - although in rare cases some senior FSASes may be promoted up to the rank of Third/Second/First Secretary). [Note: FSOs and FSASes are on discrete career tracks. Hence, even the rare FSAS who holds a senior diplomatic rank on post, will not enjoy a similar substantive grade or pay to that of an FSO.]
 Regardless of rank, personnel are typically split across three tracks: (a) Political, (b) Administration and Consular, (c) Administration and Technical. Officers on the Political track take precedence over the rest, as all Heads of Mission (HOMs) or Deputy Chiefs of Mission (DCMs) are generally Political Officers. [Note: The Political track is reserved exclusively for FSOs.]
 Other ministries and agencies 
 Personnel seconded from other government agencies receive different protocol-based suffixes and titles from those in the Foreign Service, which differ from the wider public and military services' ranks/grades and titles. For instance, a First Secretary (Economic) would represent a middle-manager of Senior Assistant Director-rank from the Ministry of Trade and Industry. While such persons may hold diplomatic status temporarily, they are not considered to be part of the career Foreign Service.
 After the merger of the Consular and Diplomatic Corps, the current eight grades of Spanish career diplomats are (in ascending order):
 His Majesty's Diplomatic Service differentiates between officers in the ""Senior Management Structure"" (SMS; equivalent to the Senior Civil Service grades of the Home Civil Service) and those in the ""delegated grades"". 
SMS officers are classified into four pay-bands, and will serve in the Foreign, Commonwealth and Development Office in London as (in descending order of seniority) Permanent Under-Secretary (O-10), Directors-General (O-9), Directors (O-8), and Heads of department or deputy directors (O-7).
 Overseas Ambassadors and High Commissioners (in Commonwealth countries) are generally drawn from all four SMS bands (and the D7 delegated grade) depending on the size and importance of the mission, as are Consuls-General, Deputy Heads of Mission, and Counsellors in larger posts. (Deputy Heads of Mission at the most significant Embassies, for example those in Washington and in Paris, are known as Ministers.)
 In the ""delegated grades"", officers are graded by number from 1 to 7; the grades are grouped into bands lettered A‑D (A1 and A2; B3; C4 and C5; and D6 and D7).
 Overseas, A2 grade officers hold the title of Attache; B3‑grade officers are Third Secretaries; C4s are Second Secretaries; and C5s and D6s are First Secretaries. D7 officers are usually Counsellors in larger posts, Deputy Heads of Mission in medium-sized posts, or Heads of Mission in small posts.
 In the United States Foreign Service, the personnel system under which most U.S. diplomatic personnel are assigned, a system of personal ranks is applied which roughly corresponds to these diplomatic ranks. Personal ranks are differentiated as ""Senior Foreign Service"" (SFS) or ""Member of the Foreign Service"".[27] Officers at these ranks may serve as ambassadors and occupy the most senior positions in diplomatic missions. The SFS ranks, in order from highest to lowest, are:
 Members of the Foreign Service consist of five groups, including Foreign Service Officers and Foreign Service Specialists.[28] Like officers in the U.S. military, Foreign Service Officers are members of the Foreign Service who are commissioned by the President.[29] Foreign Service Specialists are technical leaders and experts, commissioned by the Secretary of State.[30] Ranks descend from the highest, FS‑01, equivalent to a full Colonel in the military, to FS‑09, the lowest rank in the U.S. Foreign Service personnel system.[31] (Most entry-level Foreign Service members begin at the FS‑05 or FS‑06 level.) Personal rank is distinct from and should not be confused with the diplomatic or consular rank assigned at the time of appointment to a particular diplomatic or consular mission.
 In a large mission, several Senior Diplomats may serve under the Ambassador as Minister-Counselors, Counselors, and First Secretaries; in a small mission, a diplomat may serve as the lone Counselor of Embassy.
 Most countries' consular corps are composed of career diplomats who are simply posted to Consulates/Consulates-General. In such situations, these career diplomats will hold consular ranks instead (ranking in descending order: consul-general, consul, vice-consul, consular agent; equivalents with consular immunity limited to official acts only include honorary consul-general, honorary consul, and honorary vice-consul. Other titles, including ""vice consul-general"", have existed in the past.) – although they are usually also given a diplomatic rank by the country. Consular ranks and responsibilities differ from country to country, and may also be used concurrently with diplomatic titles if the individual is assigned to an embassy. Diplomatic immunity is generally more limited for consular officials without other diplomatic accreditation, and is broadly limited to immunity with respect to their official duties.
 While in the past, consular officials have often been more distant from the politically sensitive aspects of diplomacy, this is no longer necessarily the case, and career diplomats in consulates often perform the same roles as those in an embassy would. Some countries also routinely provide their embassy officials with consular commissions, including those without formal consular responsibilities, since a consular commission allows the individual to legalize documents, sign certain documents, and undertake certain other necessary functions.
 Depending on the practice of the individual country, ""consular services"" may be limited to services provided for citizens or residents of the sending country, or extended to include, for example, visa services for nationals of the host country.
 Sending nations may also designate incumbents of certain positions as holding consulary authority by virtue of their office, while lacking individual accreditation, immunity and inviolability. For example, 10 U.S.C. §§ 936 and 1044a identify various U.S. military officers (and authorize the service secretaries to identify others) who hold general authority as a notary and consul of the United States for, respectively, purposes of military administration and those entitled to military legal assistance. A nation may also declare that its senior merchant sea captain in a given foreign port—or its merchant sea captains generally—has consulary authority for merchant seamen.
"
Iroquois Confederacy,https://en.wikipedia.org/wiki/Iroquois_Confederacy,"

 The Iroquois (/ˈɪrəkwɔɪ, -kwɑː/ IRR-ə-kwoy, -⁠kwah), also known as the Five Nations, and later as the Six Nations from 1722 onwards; alternatively referred to by the endonym Haudenosaunee[a] (/ˌhoʊdɪnoʊˈʃoʊni/ HOH-din-oh-SHOH-nee;[8] lit. 'people who are building the longhouse') are an Iroquoian-speaking confederacy of Native Americans and First Nations peoples in northeast North America. They were known by the French during the colonial years as the Iroquois League, and later as the Iroquois Confederacy, while the English simply called them the ""Five Nations"". The peoples of the Iroquois included (from east to west) the Mohawk, Oneida, Onondaga, Cayuga, and Seneca. After 1722, the Iroquoian-speaking Tuscarora people from the southeast were accepted into the confederacy, from which point it was known as the ""Six Nations"".
 The Confederacy was likely formed between 1142 and 1660,[9] but there is little widespread consensus on the exact date.[10][11] The Confederacy emerged from the Great Law of Peace, said to have been composed by the Deganawidah the Great Peacemaker, Hiawatha, and Jigonsaseh the Mother of Nations. For nearly 200 years, the Six Nations/Haudenosaunee Confederacy were a powerful factor in North American colonial policy, with some scholars arguing for the concept of the Middle Ground,[12] in that European powers were used by the Iroquois just as much as Europeans used them.[13] At its peak around 1700, Iroquois power extended from what is today New York State, north into present-day Ontario and Quebec along the lower Great Lakes–upper St. Lawrence, and south on both sides of the Allegheny mountains into present-day Virginia and Kentucky and into the Ohio Valley.
 The St. Lawrence Iroquoians, Wendat (Huron), Erie, and Susquehannock, all independent peoples known to the European colonists, also spoke Iroquoian languages. They are considered Iroquoian in a larger cultural sense, all being descended from the Proto-Iroquoian people and language. Historically, however, they were competitors and enemies of the Iroquois Confederacy nations.[14]
 In 2010, more than 45,000 enrolled Six Nations people lived in Canada, and over 81,000 in the United States.[15][16]
 Haudenosaunee (""People of the Longhouse"") is the autonym by which the Six Nations refer to themselves.[17] While its exact etymology is debated, the term Iroquois is of colonial origin. Some scholars of Native American history consider ""Iroquois"" a derogatory name adopted from the traditional enemies of the Haudenosaunee.[18] A less common, older autonym for the confederation is Ongweh’onweh, meaning ""original people"".[19][20][21]
 Haudenosaunee derives from two phonetically similar but etymologically distinct words in the Seneca language: Hodínöhšö:ni:h, meaning ""those of the extended house"", and Hodínöhsö:ni:h, meaning ""house builders"".[22][23][24] The name ""Haudenosaunee"" first appears in English in Lewis Henry Morgan's work (1851), where he writes it as Ho-dé-no-sau-nee. The spelling ""Hotinnonsionni"" is also attested from later in the nineteenth century.[22][25] An alternative designation, Ganonsyoni, is occasionally encountered as well,[26] from the Mohawk kanǫhsyǫ́·ni ""the extended house"", or from a cognate expression in a related Iroquoian language; in earlier sources it is variously spelled ""Kanosoni"", ""akwanoschioni"", ""Aquanuschioni"", ""Cannassoone"", ""Canossoone"", ""Ke-nunctioni"", or ""Konossioni"".[22] More transparently, the Haudenosaunee confederacy is often referred to as the Six Nations (or, for the period before the entry of the Tuscarora in 1722, the Five Nations).[22][b] The word is Rotinonshón:ni in the Mohawk language.[4]
 The origins of the name Iroquois are somewhat obscure, although the term has historically been more common among English texts than Haudenosaunee. Its first written appearance as ""Irocois"" is in Samuel de Champlain's account of his journey to Tadoussac in 1603.[27] Other early French spellings include ""Erocoise"", ""Hiroquois"", ""Hyroquoise"", ""Irecoies"", ""Iriquois"", ""Iroquaes"", ""Irroquois"", and ""Yroquois"",[22] pronounced at the time as [irokwe] or [irokwɛ].[c] Competing theories have been proposed for this term's origin, but none have gained widespread acceptance. By 1978 Ives Goddard wrote: ""No such form is attested in any Indian language as a name for any Iroquoian group, and the ultimate origin and meaning of the name are unknown.""[22]
 Jesuit priest and missionary Pierre François Xavier de Charlevoix wrote in 1744:
 In 1883, Horatio Hale wrote that Charlevoix's etymology was dubious, and that ""no other nation or tribe of which we have any knowledge has ever borne a name composed in this whimsical fashion"".[27] Hale suggested instead that the term came from Huron, and was cognate with the Mohawk ierokwa ""they who smoke"", or Cayuga iakwai ""a bear"". In 1888, J. N. B. Hewitt expressed doubts that either of those words exist in the respective languages. He preferred the etymology from Montagnais irin ""true, real"" and ako ""snake"", plus the French -ois suffix. Later he revised this to Algonquin Iriⁿakhoiw as the origin.[27][28]
 A more modern etymology was advocated by Gordon M. Day in 1968, elaborating upon Charles Arnaud from 1880. Arnaud had claimed that the word came from Montagnais irnokué, meaning ""terrible man"", via the reduced form irokue. Day proposed a hypothetical Montagnais phrase irno kwédač, meaning ""a man, an Iroquois"", as the origin of this term. For the first element irno, Day cites cognates from other attested Montagnais dialects: irinou, iriniȣ, and ilnu; and for the second element kwédač, he suggests a relation to kouetakiou, kȣetat-chiȣin, and goéṭètjg – names used by neighboring Algonquian tribes to refer to the Iroquois, Huron, and Laurentian peoples.[27]
 The Gale Encyclopedia of Multicultural America attests the origin of Iroquois to Iroqu, Algonquian for ""rattlesnake"".[29] The French encountered the Algonquian-speaking tribes first, and would have learned the Algonquian names for their Iroquois competitors.
 The Iroquois Confederacy is believed to have been founded by the Great Peacemaker at an unknown date estimated between 1450 and 1660, bringing together five distinct nations in the southern Great Lakes area into ""The Great League of Peace"".[30] Other research, however, suggests the founding occurred in 1142.[31] Each nation within this Iroquoian confederacy had a distinct language, territory, and function in the League.
 The League is composed of a Grand Council, an assembly of fifty chiefs or sachems, each representing a clan of a nation.[32]
 When Europeans first arrived in North America, the Haudenosaunee (Iroquois League to the French, Five Nations to the British) were based in what is now central and west New York State including the Finger Lakes region, occupying large areas north to the St. Lawrence River, east to Montreal and the Hudson River, and south into what is today northwestern Pennsylvania. At its peak around 1700, Iroquois power extended from what is today New York State, north into present-day Ontario and Quebec along the lower Great Lakes–upper St. Lawrence, and south on both sides of the Allegheny Mountains into present-day Virginia and Kentucky and into the Ohio Valley. From east to west, the League was composed of the Mohawk, Oneida, Onondaga, Cayuga, and Seneca nations. In about 1722, the Iroquoian-speaking Tuscarora joined the League, having migrated northwards from the Carolinas after a bloody conflict with white settlers. A shared cultural background with the Five Nations of the Iroquois (and a sponsorship from the Oneida) led the Tuscarora to becoming accepted as the sixth nation in the confederacy in 1722; the Iroquois become known afterwards as the Six Nations.[33][34]
 Other independent Iroquoian-speaking peoples, such as the Erie, Susquehannock, Huron (Wendat) and Wyandot, lived at various times along the St. Lawrence River, and around the Great Lakes. In the American Southeast, the Cherokee were an Iroquoian-language people who had migrated to that area centuries before European contact. None of these were part of the Haudenosaunee League. Those on the borders of Haudenosaunee territory in the Great Lakes region competed and warred with the nations of the League.
 French, Dutch, and English colonists, both in New France (Canada) and what became the Thirteen Colonies, recognized a need to gain favor with the Iroquois people, who occupied a significant portion of lands west of the colonial settlements. Their first relations were for fur trading, which became highly lucrative for both sides. The colonists also sought to establish friendly relations to secure their settlement borders.
 For nearly 200 years, the Iroquois were a powerful factor in North American colonial policy. Alliance with the Iroquois offered political and strategic advantages to the European powers, but the Iroquois preserved considerable independence. Some of their people settled in mission villages along the St. Lawrence River, becoming more closely tied to the French. While they participated in French-led raids on Dutch and English colonial settlements, where some Mohawk and other Iroquois settled, in general the Iroquois resisted attacking their own peoples.
 The Iroquois remained a large politically united Native American polity until the American Revolution, when the League was divided by their conflicting views on how to respond to requests for aid from the British Crown.[35] After their defeat, the British ceded Iroquois territory without consultation, and many Iroquois had to abandon their lands in the Mohawk Valley and elsewhere and relocate to the northern lands retained by the British. The Crown gave them land in compensation for the five million acres they had lost in the south, but it was not equivalent to earlier territory.
 Modern scholars of the Iroquois distinguish between the League and the Confederacy.[36][37][38] According to this interpretation, the Iroquois League refers to the ceremonial and cultural institution embodied in the Grand Council, which still exists. The Iroquois Confederacy was the decentralized political and diplomatic entity that emerged in response to European colonization, which was dissolved after the British defeat in the American Revolutionary War.[36] Today's Iroquois/Six Nations people do not make any such distinction, use the terms interchangeably, but prefer the name Haudenosaunee Confederacy.
 After the migration of a majority to Canada, the Iroquois remaining in New York were required to live mostly on reservations. In 1784, a total of 6,000 Iroquois faced 240,000 New Yorkers, with land-hungry New Englanders poised to migrate west. ""Oneidas alone, who were only 600 strong, owned six million acres, or about 2.4 million hectares. Iroquoia was a land rush waiting to happen.""[39] By the War of 1812, the Iroquois had lost control of considerable territory.
 Knowledge of Iroquois history stem from Haudenosaunee oral tradition, archaeological evidence, accounts from Jesuit missionaries, and subsequent European historians. Historian Scott Stevens credits the early modern European value of written sources over oral tradition as contributing to a racialized, prejudiced perspective about the Iroquois through the 19th century.[40] The historiography of the Iroquois peoples is a topic of much debate, especially regarding the American colonial period.[41][42]
 French Jesuit accounts of the Iroquois portrayed them as savages lacking government, law, letters, and religion.[43] But the Jesuits made considerable effort to study their languages and cultures, and some came to respect them. A source of confusion for European sources, coming from a patriarchal society, was the matrilineal kinship system of Iroquois society and the related power of women.[44] The Canadian historian D. Peter MacLeod wrote about the Canadian Iroquois and the French in the time of the Seven Years' War: 
 Eighteenth-century English historiography focuses on the diplomatic relations with the Iroquois, supplemented by such images as John Verelst's Four Mohawk Kings, and publications such as the Anglo-Iroquoian treaty proceedings printed by Benjamin Franklin.[45] A persistent 19th and 20th century narrative casts the Iroquois as ""an expansive military and political power ... [who] subjugated their enemies by violent force and for almost two centuries acted as the fulcrum in the balance of power in colonial North America"".[46]
 Historian Scott Stevens noted that the Iroquois themselves began to influence the writing of their history in the 19th century, including Joseph Brant (Mohawk), and David Cusick (Tuscarora, c.1780–1840). John Arthur Gibson (Seneca, 1850–1912) was an important figure of his generation in recounting versions of Iroquois history in epics on the Peacemaker.[47] Notable women historians among the Iroquois emerged in the following decades, including Laura ""Minnie"" Kellogg (Oneida, 1880–1949) and Alice Lee Jemison (Seneca, 1901–1964).[48]
 The Iroquois League was established prior to European contact, with the banding together of five of the many Iroquoian peoples who had emerged south of the Great Lakes.[49][d] Many archaeologists and anthropologists believe that the League was formed about 1450,[50][51] though arguments have been made for an earlier date.[52] One theory argues that the League formed shortly after a solar eclipse on August 31, 1142, an event thought to be expressed in oral tradition about the League's origins.[53][54][55] Some sources link an early origin of the Iroquois confederacy to the adoption of corn as a staple crop.[56]
 Archaeologist Dean Snow argues that the archaeological evidence does not support a date earlier than 1450. He has said that recent claims for a much earlier date ""may be for contemporary political purposes"".[57] Other scholars note that anthropological researchers consulted only male informants, thus losing the half of the historical story told in the distinct oral traditions of women.[58] For this reason, origin tales tend to emphasize the two men Deganawidah and Hiawatha, while the woman Jigonsaseh, who plays a prominent role in the female tradition, remains largely unknown.[58]
 The founders of League are traditionally held to be Dekanawida the Great Peacemaker, Hiawatha, and Jigonhsasee the Mother of Nations, whose home acted as a sort of United Nations. They brought the Peacemaker's Great Law of Peace to the squabbling Iroquoian nations who were fighting, raiding, and feuding with each other and with other tribes, both Algonkian and Iroquoian. Five nations originally joined in the League, giving rise to the many historic references to ""Five Nations of the Iroquois"".[e][49] With the addition of the southern Tuscarora in the 18th century, these original five tribes still compose the Haudenosaunee in the early 21st century: the Mohawk, Onondaga, Oneida, Cayuga, and Seneca.
 According to legend, an evil Onondaga chieftain named Tadodaho was the last converted to the ways of peace by The Great Peacemaker and Hiawatha. He was offered the position as the titular chair of the League's Council, representing the unity of all nations of the League.[59] This is said to have occurred at Onondaga Lake near present-day Syracuse, New York. The title Tadodaho is still used for the League's chair, the fiftieth chief who sits with the Onondaga in council.[60]
 The Iroquois subsequently created a highly egalitarian society. One British colonial administrator declared in 1749 that the Iroquois had ""such absolute Notions of Liberty that they allow no Kind of Superiority of one over another, and banish all Servitude from their Territories"".[61] As raids between the member tribes ended and they directed warfare against competitors, the Iroquois increased in numbers while their rivals declined. The political cohesion of the Iroquois rapidly became one of the strongest forces in 17th- and 18th-century northeastern North America.
 The League's Council of Fifty ruled on disputes and sought consensus. However, the confederacy did not speak for all five tribes, which continued to act independently and form their own war bands. Around 1678, the council began to exert more power in negotiations with the colonial governments of Pennsylvania and New York, and the Iroquois became very adroit at diplomacy, playing off the French against the British as individual tribes had earlier played the Swedes, Dutch, and English.[49]
 Iroquoian-language peoples were involved in warfare and trading with nearby members of the Iroquois League.[49] The explorer Robert La Salle in the 17th century identified the Mosopelea as among the Ohio Valley peoples defeated by the Iroquois in the early 1670s.[62] The Erie and peoples of the upper Allegheny valley declined earlier during the Beaver Wars. By 1676 the power of the Susquehannock[f] was broken from the effects of three years of epidemic disease, war with the Iroquois, and frontier battles, as settlers took advantage of the weakened tribe.[49]
 According to one theory of early Iroquois history, after becoming united in the League, the Iroquois invaded the Ohio River Valley in the territories that would become the eastern Ohio Country down as far as present-day Kentucky to seek additional hunting grounds. They displaced about 1,200 Siouan-speaking tribepeople of the Ohio River valley, such as the Quapaw (Akansea), Ofo (Mosopelea), and Tutelo and other closely related tribes out of the region. These tribes migrated to regions around the Mississippi River and the Piedmont regions of the east coast.[63]
 Other Iroquoian-language peoples,[64] including the populous Wyandot (Huron), with related social organization and cultures, became extinct as tribes as a result of disease and war.[g] They did not join the League when invited and were much reduced after the Beaver Wars and high mortality from Eurasian infectious diseases. While the indigenous nations sometimes tried to remain neutral in the various colonial frontier wars, some also allied with Europeans, as in the French and Indian War, the North American front of the Seven Years' War. The Six Nations were split in their alliances between the French and British in that war.
 In Reflections in Bullough's Pond, historian Diana Muir argues that the pre-contact Iroquois were an imperialist, expansionist culture whose cultivation of the corn/beans/squash agricultural complex enabled them to support a large population. They made war primarily against neighboring Algonquian peoples. Muir uses archaeological data to argue that the Iroquois expansion onto Algonquian lands was checked by the Algonquian adoption of agriculture. This enabled them to support their own populations large enough to resist Iroquois conquest.[65] The People of the Confederacy dispute this historical interpretation, regarding the League of the Great Peace as the foundation of their heritage.[66]
 The Iroquois may be the Kwedech described in the oral legends of the Mi'kmaq nation of Eastern Canada. These legends relate that the Mi'kmaq in the late pre-contact period had gradually driven their enemies – the Kwedech – westward across New Brunswick, and finally out of the Lower St. Lawrence River region. The Mi'kmaq named the last-conquered land Gespedeg or ""last land"", from which the French derived Gaspé. The ""Kwedech"" are generally considered to have been Iroquois, specifically the Mohawk; their expulsion from Gaspé by the Mi'kmaq has been estimated as occurring c. 1535–1600.[67][page needed]
 Around 1535, Jacques Cartier reported Iroquoian-speaking groups on the Gaspé peninsula and along the St. Lawrence River. Archeologists and anthropologists have defined the St. Lawrence Iroquoians as a distinct and separate group (and possibly several discrete groups), living in the villages of Hochelaga and others nearby (near present-day Montreal), which had been visited by Cartier. By 1608, when Samuel de Champlain visited the area, that part of the St. Lawrence River valley had no settlements, but was controlled by the Mohawk as a hunting ground. The fate of the Iroquoian people that Cartier encountered remains a mystery, and all that can be stated for certain is when Champlain arrived, they were gone.[68] On the Gaspé peninsula, Champlain encountered Algonquian-speaking groups. The precise identity of any of these groups is still debated. On July 29, 1609, Champlain assisted his allies in defeating a Mohawk war party by the shores of what is now called Lake Champlain, and again in June 1610, Champlain fought against the Mohawks.[69]
 The Iroquois became well known in the southern colonies in the 17th century by this time. After the first English settlement in Jamestown, Virginia (1607), numerous 17th-century accounts describe a powerful people known to the Powhatan Confederacy as the Massawomeck, and to the French as the Antouhonoron. They were said to come from the north, beyond the Susquehannock territory. Historians have often identified the Massawomeck / Antouhonoron as the Haudenosaunee.
 In 1649, an Iroquois war party, consisting mostly of Senecas and Mohawks, destroyed the Huron village of Wendake. In turn, this ultimately resulted in the breakup of the Huron nation. With no northern enemy remaining, the Iroquois turned their forces on the Neutral Nations on the north shore of Lakes Erie and Ontario, the Susquehannocks, their southern neighbor. Then they destroyed other Iroquoian-language tribes, including the Erie, to the west, in 1654, over competition for the fur trade.[70][page needed] Then they destroyed the Mohicans. After their victories, they reigned supreme in an area from the Mississippi River to the Atlantic Ocean; from the St. Lawrence River to the Chesapeake Bay.
 Michael O. Varhola has argued their success in conquering and subduing surrounding nations had paradoxically weakened a Native response to European growth, thereby becoming victims of their own success.
 The Five Nations of the League established a trading relationship with the Dutch at Fort Orange (modern Albany, New York), trading furs for European goods, an economic relationship that profoundly changed their way of life and led to much over-hunting of beavers.[71]
 Between 1665 and 1670, the Iroquois established seven villages on the northern shores of Lake Ontario in present-day Ontario, collectively known as the ""Iroquois du Nord"" villages. The villages were all abandoned by 1701.[72]
 Over the years 1670–1710, the Five Nations achieved political dominance of much of Virginia west of the Fall Line and extending to the Ohio River valley in present-day West Virginia and Kentucky. As a result of the Beaver Wars, they pushed Siouan-speaking tribes out and reserved the territory as a hunting ground by right of conquest. They finally sold to British colonists their remaining claim to the lands south of the Ohio in 1768 at the Treaty of Fort Stanwix.
 Historian Pekka Hämäläinen writes of the League, ""There had never been anything like the Five Nations League in North America. No other Indigenous nation or confederacy had ever reached so far, conducted such an ambitious foreign policy, or commanded such fear and respect. The Five Nations blended diplomacy, intimidation, and violence as the circumstances dictated, creating a measured instability that only they could navigate. Their guiding principle was to avoid becoming attached to any single colony, which would restrict their options and risk exposure to external manipulation.""[73]
 Beginning in 1609, the League engaged in the decades-long Beaver Wars against the French, their Huron allies, and other neighboring tribes, including the Petun, Erie, and Susquehannock.[71] Trying to control access to game for the lucrative fur trade, they invaded the Algonquian peoples of the Atlantic coast (the Lenape, or Delaware), the Anishinaabe of the boreal Canadian Shield region, and not infrequently the English colonies as well. During the Beaver Wars, they were said to have defeated and assimilated the Huron (1649), Petun (1650), the Neutral Nation (1651),[74][75] Erie Tribe (1657), and Susquehannock (1680).[76] The traditional view is that these wars were a way to control the lucrative fur trade to purchase European goods on which they had become dependent.[77][78] Starna questions this view.[79]
 Recent scholarship has elaborated on this view, arguing that the Beaver Wars were an escalation of the Iroquoian tradition of ""Mourning Wars"".[80] This view suggests that the Iroquois launched large-scale attacks against neighboring tribes to avenge or replace the many dead from battles and smallpox epidemics.
 In 1628, the Mohawk defeated the Mahican to gain a monopoly in the fur trade with the Dutch at Fort Orange (present-day Albany), New Netherland. The Mohawk would not allow northern native peoples to trade with the Dutch.[71] By 1640, there were almost no beavers left on their lands, reducing the Iroquois to middlemen in the fur trade between Indian peoples to the west and north, and Europeans eager for the valuable thick beaver pelts.[71] In 1645, a tentative peace was forged between the Iroquois and the Huron, Algonquin, and French.
 In 1646, Jesuit missionaries at Sainte-Marie among the Hurons went as envoys to the Mohawk lands to protect the precarious peace. Mohawk attitudes toward the peace soured while the Jesuits were traveling, and their warriors attacked the party en route. The missionaries were taken to Ossernenon village, Kanienkeh (Mohawk Nation) (near present-day Auriesville, New York), where the moderate Turtle and Wolf clans recommended setting them free, but angry members of the Bear clan killed Jean de Lalande and Isaac Jogues on October 18, 1646.[81] The Catholic Church has commemorated the two French priests and Jesuit lay brother René Goupil (killed September 29, 1642)[82] as among the eight North American Martyrs.
 In 1649 during the Beaver Wars, the Iroquois used recently-purchased Dutch guns to attack the Huron, allies of the French. These attacks, primarily against the Huron towns of Taenhatentaron (St. Ignace[83]) and St. Louis[84] in what is now Simcoe County, Ontario, were the final battles that effectively destroyed the Huron Confederacy.[85] The Jesuit missions in Huronia on the shores of Georgian Bay were abandoned in the face of the Iroquois attacks, with the Jesuits leading the surviving Hurons east towards the French settlements on the St. Lawrence.[81] The Jesuit Relations expressed some amazement that the Five Nations had been able to dominate the area ""for five hundred leagues around, although their numbers are very small"".[81] From 1651 to 1652, the Iroquois attacked the Susquehannock, to their south in present-day Pennsylvania, without sustained success.
 In 1653 the Onondaga Nation extended a peace invitation to New France. An expedition of Jesuits, led by Simon Le Moyne, established Sainte Marie de Ganentaa in 1656 in their territory. They were forced to abandon the mission by 1658 as hostilities resumed, possibly because of the sudden death of 500 native people from an epidemic of smallpox, a European infectious disease to which they had no immunity.
 From 1658 to 1663, the Iroquois were at war with the Susquehannock and their Lenape and Province of Maryland allies. In 1663, a large Iroquois invasion force was defeated at the Susquehannock main fort. In 1663, the Iroquois were at war with the Sokoki tribe of the upper Connecticut River. Smallpox struck again, and through the effects of disease, famine, and war, the Iroquois were under threat of extinction. In 1664, an Oneida party struck at allies of the Susquehannock on Chesapeake Bay.
 In 1665, three of the Five Nations made peace with the French. The following year, the Governor-General of New France, the Marquis de Tracy, sent the Carignan regiment to confront the Mohawk and Oneida.[86] The Mohawk avoided battle, but the French burned their villages, which they referred to as ""castles"", and their crops.[86] In 1667, the remaining two Iroquois Nations signed a peace treaty with the French and agreed to allow missionaries to visit their villages. The French Jesuit missionaries were known as the ""black-robes"" to the Iroquois, who began to urge that Catholic converts should relocate to the Caughnawaga, Kanienkeh outside of Montreal.[86] This treaty lasted for 17 years.
 Around 1670, the Iroquois drove the Siouan-speaking Mannahoac tribe out of the northern Virginia Piedmont region, and began to claim ownership of the territory. In 1672, they were defeated by a war party of Susquehannock, and the Iroquois appealed to the French Governor Frontenac for support:
 Some old histories state that the Iroquois defeated the Susquehannock but this is undocumented and doubtful.[88] In 1677, the Iroquois adopted the majority of the Iroquoian-speaking Susquehannock into their nation.[89]
 In January 1676, the Governor of New York colony, Edmund Andros, sent a letter to the chiefs of the Iroquois asking for their help in King Philip's War, as the English colonists in New England were having much difficulty fighting the Wampanoag led by Metacom. In exchange for precious guns from the English, an Iroquois war party devastated the Wampanoag in February 1676, destroying villages and food stores while taking many prisoners.[90]
 By 1677, the Iroquois formed an alliance with the English through an agreement known as the Covenant Chain. By 1680, the Iroquois Confederacy was in a strong position, having eliminated the Susquehannock and the Wampanoag, taken vast numbers of captives to augment their population, and secured an alliance with the English supplying guns and ammunition.[91] Together the allies battled to a standstill the French and their allies the Hurons, traditional foes of the Confederacy. The Iroquois colonized the northern shore of Lake Ontario and sent raiding parties westward all the way to Illinois Country. The tribes of Illinois were eventually defeated, not by the Iroquois, but by the Potawatomi.
 In 1679, the Susquehannock, with Iroquois help, attacked Maryland's Piscataway and Mattawoman allies.[92] Peace was not reached until 1685. During the same period, French Jesuit missionaries were active in Iroquoia, which led to a voluntary mass relocation of many Haudenosaunee to the St. Lawrence valley at Kahnawake and Kanesatake near Montreal. It was the intention of the French to use the Catholic Haudenosaunee in the St. Lawrence valley as a buffer to keep the English-allied Haudenosaunee tribes, in what is now upstate New York, away from the center of the French fur trade in Montreal. The attempts of both the English and the French to make use of their Haudenosaunee allies were foiled, as the two groups of Haudenosaunee showed a ""profound reluctance to kill one another"".[93] Following the move of the Catholic Iroquois to the St. Lawrence valley, historians commonly describe the Iroquois living outside of Montreal as the Canadian Iroquois, while those remaining in their historical heartland in modern upstate New York are described as the League Iroquois.[94]
 In 1684, the Governor General of New France, Joseph-Antoine Le Febvre de La Barre, decided to launch a punitive expedition against the Seneca, who were attacking French and Algonquian fur traders in the Mississippi river valley, and asked for the Catholic Haudenosaunee to contribute fighting men.[95] La Barre's expedition ended in fiasco in September 1684 when influenza broke out among the French troupes de la Marine while the Canadian Iroquois warriors refused to fight, instead only engaging in battles of insults with the Seneca warriors.[96] King Louis XIV of France was not amused when he heard of La Barre's failure, which led to his replacement with Jacques-René de Brisay, Marquis de Denonville (Governor General 1685–1689), who arrived in August with orders from the Sun King to crush the Haudenosaunee confederacy and uphold the honor of France even in the wilds of North America.[96] In the same year, the Iroquois again invaded Virginia and Illinois territory and unsuccessfully attacked French outposts in the latter. Trying to reduce warfare in the Shenandoah Valley of Virginia, later that year the Virginia Colony agreed in a conference at Albany to recognize the Iroquois' right to use the North-South path, known as the Great Warpath, running east of the Blue Ridge, provided they did not intrude on the English settlements east of the Fall Line.
 In 1687, the Marquis de Denonville set out for Fort Frontenac (modern Kingston, Ontario) with a well-organized force. In July 1687 Denonville took with him on his expedition a mixed force of troupes de la Marine, French-Canadian militiamen, and 353 Indian warriors from the Jesuit mission settlements, including 220 Haudenosaunee.[96] They met under a flag of truce with 50 hereditary sachems from the Onondaga council fire, on the north shore of Lake Ontario in what is now southern Ontario.[96] Denonville recaptured the fort for New France and seized, chained, and shipped the 50 Iroquois chiefs to Marseilles, France, to be used as galley slaves.[96] Several of the Catholic Haudenosaunee were outraged at this treachery to a diplomatic party, which led to at least 100 of them to desert to the Seneca.[97] Denonville justified enslaving the people he encountered, saying that as a ""civilized European"" he did not respect the customs of ""savages"" and would do as he liked with them. On August 13, 1687, an advance party of French soldiers walked into a Seneca ambush and were nearly killed to a man; however the Seneca fled when the main French force came up. The remaining Catholic Haudenosaunee warriors refused to pursue the retreating Seneca.[96]
 Denonville ravaged the land of the Seneca, landing a French armada at Irondequoit Bay, striking straight into the seat of Seneca power, and destroying many of its villages. Fleeing before the attack, the Seneca moved farther west, east and south down the Susquehanna River. Although great damage was done to their homeland, the Senecas' military might was not appreciably weakened. The Confederacy and the Seneca developed an alliance with the English who were settling in the east. The destruction of the Seneca land infuriated the members of the Iroquois Confederacy. On August 4, 1689, they retaliated by burning down Lachine, a small town adjacent to Montreal. Fifteen hundred Iroquois warriors had been harassing Montreal defenses for many months prior to that.
 They finally exhausted and defeated Denonville and his forces. His tenure was followed by the return of Frontenac for the next nine years (1689–1698). Frontenac had arranged a new strategy to weaken the Iroquois. As an act of conciliation, he located the 13 surviving sachems of the 50 originally taken and returned with them to New France in October 1689. In 1690, Frontenac destroyed Schenectady, Kanienkeh, and in 1693 burned down three other Mohawk villages and took 300 prisoners.[98]
 In 1696, Frontenac decided to take the field against the Iroquois, despite being seventy-six years of age. He decided to target the Oneida and Onondaga, instead of the Mohawk who had been the favorite enemies of the French.[98] On July 6, he left Lachine at the head of a considerable force and traveled to the capital of Onondaga, where he arrived a month later. With support from the French, the Algonquian nations drove the Iroquois out of the territories north of Lake Erie and west of present-day Cleveland, Ohio, regions which they had conquered during the Beaver Wars.[89] In the meantime, the Iroquois had abandoned their villages. As pursuit was impracticable, the French army commenced its return march on August 10. Under Frontenac's leadership, the Canadian militia became increasingly adept at guerrilla warfare, taking the war into Iroquois territory and attacking a number of English settlements. The Iroquois never threatened the French colony again.[99]
 During King William's War (North American part of the War of the Grand Alliance), the Iroquois were allied with the English. In July 1701, they concluded the ""Nanfan Treaty"", deeding the English a large tract north of the Ohio River. The Iroquois claimed to have conquered this territory 80 years earlier. France did not recognize the treaty, as it had settlements in the territory at that time and the English had virtually none. Meanwhile, the Iroquois were negotiating peace with the French; together they signed the Great Peace of Montreal that same year.
 After the 1701 peace treaty with the French, the Iroquois remained mostly neutral. During the course of the 17th century, the Iroquois had acquired a fearsome reputation among the Europeans, and it was the policy of the Six Nations to use this reputation to play-off the French against the British to extract the maximum amount of material rewards.[100] In 1689, the English Crown provided the Six Nations goods worth £100 in exchange for help against the French, in the year 1693 the Iroquois had received goods worth £600, and in the year 1701 the Six Nations had received goods worth £800.[101]
 During Queen Anne's War (North American part of the War of the Spanish Succession), they were involved in planned attacks against the French. Pieter Schuyler, mayor of Albany, arranged for three Mohawk chiefs and a Mahican chief (known incorrectly as the Four Mohawk Kings) to travel to London in 1710 to meet with Queen Anne in an effort to seal an alliance with the British. Queen Anne was so impressed by her visitors that she commissioned their portraits by court painter John Verelst. The portraits are believed to be the earliest surviving oil portraits of Aboriginal peoples taken from life.[102]
 In the early 18th century, the Tuscarora gradually migrated northwards towards Pennsylvania and New York after a bloody conflict with white settlers in North and South Carolina. Due to shared linguistic and cultural similarities, the Tuscarora gradually aligned with the Iroquois and entered the confederacy as the sixth Indian nation in 1722 after being sponsored by the Oneida.[34]
 The Iroquois program toward the defeated tribes favored assimilation within the 'Covenant Chain' and Great Law of Peace, over wholesale slaughter. Both the Lenni Lenape, and the Shawnee were briefly tributary to the Six Nations, while subjected Iroquoian populations emerged in the next period as the Mingo, speaking a dialect like that of the Seneca, in the Ohio region. During the War of Spanish Succession, known to Americans as ""Queen Anne's War"", the Iroquois remained neutral, through leaning towards the British.[98] Anglican missionaries were active with the Iroquois and devised a system of writing for them.[98]
 In 1721 and 1722, Lieutenant Governor Alexander Spotswood of Virginia concluded a new Treaty at Albany with the Iroquois, renewing the Covenant Chain and agreeing to recognize the Blue Ridge as the demarcation between the Virginia Colony and the Iroquois. But, as European settlers began to move beyond the Blue Ridge and into the Shenandoah Valley in the 1730s, the Iroquois objected. Virginia officials told them that the demarcation was to prevent the Iroquois from trespassing east of the Blue Ridge, but it did not prevent English from expanding west. Tensions increased over the next decades, and the Iroquois were on the verge of going to war with the Virginia Colony. In 1743, Governor Sir William Gooch paid them the sum of 100 pounds sterling for any settled land in the Valley that was claimed by the Iroquois. The following year at the Treaty of Lancaster, the Iroquois sold Virginia all their remaining claims in the Shenandoah Valley for 200 pounds in gold.[103]
 During the French and Indian War (the North American theater of the Seven Years' War), the League Iroquois sided with the British against the French and their Algonquian allies, who were traditional enemies. The Iroquois hoped that aiding the British would also bring favors after the war. Few Iroquois warriors joined the campaign. By contrast, the Canadian Iroquois supported the French.
 In 1711, refugees from is now southern-western Germany known as the Palatines appealed to the Iroquois clan mothers for permission to settle on their land.[104] By spring of 1713, about 150 Palatine families had leased land from the Iroquois.[105] The Iroquois taught the Palatines how to grow ""the Three Sisters"" as they called their staple crops of beans, corn and squash and where to find edible nuts, roots and berries.[105] In return, the Palatines taught the Iroquois how to grow wheat and oats, and how to use iron ploughs and hoes to farm.[105] As a result of the money earned from land rented to the Palatines, the Iroquois elite gave up living in longhouses and started living in European style houses, having an income equal to a middle-class English family.[105] By the middle of the 18th century, a multi-cultural world had emerged with the Iroquois living alongside German and Scots-Irish settlers.[106] The settlements of the Palatines were intermixed with the Iroquois villages.[107] In 1738, an Irishman, Sir William Johnson, who was successful as a fur trader, settled with the Iroquois.[108] Johnson who become very rich from the fur trade and land speculation, learned the languages of the Iroquois and become the main intermediary between the British and the League.[108] In 1745, Johnson was appointed the Northern superintendent of Indian Affairs, formalizing his position.[109]
 On July 9, 1755, a force of British Army regulars and the Virginia militia under General Edward Braddock advancing into the Ohio river valley was almost completely destroyed by the French and their Indian allies at the Battle of the Monongahela.[109] Johnson, who had the task of enlisting the League Iroquois on the British side, led a mixed Anglo-Iroquois force to victory at Lac du St Sacrement, known to the British as Lake George.[109] In the Battle of Lake George, a group of Catholic Mohawk (from Kahnawake) and French forces ambushed a Mohawk-led British column; the Mohawk were deeply disturbed as they had created their confederacy for peace among the peoples and had not had warfare against each other. Johnson attempted to ambush a force of 1,000 French troops and 700 Canadian Iroquois under the command of Baron Dieskau, who beat off the attack and killed the old Mohawk war chief, Peter Hendricks.[109] On September 8, 1755, Diskau attacked Johnson's camp, but was repulsed with heavy losses.[109] Though the Battle of Lake George was a British victory, the heavy losses taken by the Mohawk and Oneida at the battle caused the League to declare neutrality in the war.[109] Despite Johnson's best efforts, the League Iroquois remained neutral for next several years, and a series of French victories at Oswego, Louisbourg, Fort William Henry and Fort Carillon ensured the League Iroquois would not fight on what appeared to be the losing side.[110]
 In February 1756, the French learned from a spy, Oratory, an Oneida chief, that the British were stockpiling supplies at the Oneida Carrying Place, a crucial portage between Albany and Oswego to support an offensive in the spring into what is now Ontario. As the frozen waters melted south of Lake Ontario on average two weeks before the waters did north of Lake Ontario, the British would be able to move against the French bases at Fort Frontenac and Fort Niagara before the French forces in Montreal could come to their relief, which from the French perspective necessitated a preemptive strike at the Oneida Carrying Place in the winter.[111] To carry out this strike, the Marquis de Vaudreuil, the Governor-General of New France, assigned the task to Gaspard-Joseph Chaussegros de Léry, an officer of the troupes de le Marine, who required and received the assistance of the Canadian Iroquois to guide him to the Oneida Carrying Place.[112] The Canadian Iroquois joined the expedition, which left Montreal on February 29, 1756, on the understanding that they would only fight against the British, not the League Iroquois, and they would not be assaulting a fort.[113]
 On March 13, 1756, an Oswegatchie Indian traveler informed the expedition that the British had built two forts at the Oneida Carrying Place, which caused the majority of the Canadian Iroquois to want to turn back, as they argued the risks of assaulting a fort would mean too many casualties, and many did in fact abandon the expedition.[114] On March 26, 1756, Léry's force of troupes de le Marine and French-Canadian militiamen, who had not eaten for two days, received much needed food when the Canadian Iroquois ambushed a British wagon train bringing supplies to Fort William and Fort Bull.[115] As far as the Canadian Iroquois were concerned, the raid was a success as they captured 9 wagons full of supplies and took 10 prisoners without losing a man, and for them, engaging in a frontal attack against the two wooden forts as Léry wanted to do was irrational.[116] The Canadian Iroquois informed Léry ""if I absolutely wanted to die, I was the master of the French, but they were not going to follow me"".[117] In the end, about 30 Canadian Iroquois reluctantly joined Léry's attack on Fort Bull on the morning of March 27, 1756, when the French and their Indian allies stormed the fort, finally smashing their way in through the main gate with a battering ram at noon.[118] Of the 63 people in Fort Bull, half of whom were civilians, only 3 soldiers, one carpenter and one woman survived the Battle of Fort Bull as Léry reported ""I could not restrain the ardor of the soldiers and the Canadians. They killed everyone they encountered"".[119] Afterwards, the French destroyed all of the British supplies and Fort Bull itself, which secured the western flank of New France. On the same day, the main force of the Canadian Iroquois ambushed a relief force from Fort William coming to the aid of Fort Bull, and did not slaughter their prisoners as the French did at Fort Bull; for the Iroquois, prisoners were very valuable as they increased the size of the tribe.[120]
 The crucial difference between the European and First Nations way of war was that Europe had millions of people, which meant that British and French generals were willing to see thousands of their own men die in battle to secure victory as their losses could always be made good; by contrast, the Iroquois had a considerably smaller population, and could not afford heavy losses, which could cripple a community. The Iroquois custom of ""Mourning wars"" to take captives who would become Iroquois reflected the continual need for more people in the Iroquois communities. Iroquois warriors were brave, but would only fight to the death if necessary, usually to protect their women and children; otherwise, the crucial concern for Iroquois chiefs was always to save manpower.[121] The Canadian historian D. Peter MacLeod wrote that the Iroquois way of war was based on their hunting philosophy, where a successful hunter would bring down an animal efficiently without taking any losses to his hunting party, and in the same way, a successful war leader would inflict losses on the enemy without taking any losses in return.[122]
 The Iroquois only entered the war on the British side again in late 1758 after the British took Louisbourg and Fort Frontenac.[110] At the Treaty of Fort Easton in October 1758, the Iroquois forced the Lenape and Shawnee who had been fighting for the French to declare neutrality.[110] In July 1759, the Iroquois helped Johnson take Fort Niagara.[110] In the ensuing campaign, the League Iroquois assisted General Jeffrey Amherst as he took various French forts by the Great Lakes and the St. Lawrence valley as he advanced towards Montreal, which he took in September 1760.[110] The British historian Michael Johnson wrote the Iroquois had ""played a major supporting role"" in the final British victory in the Seven Years' War.[110] In 1763, Johnson left his old home of Fort Johnson for the lavish estate, which he called Johnson Hall, which become a center of social life in the region.[110] Johnson was close to two white families, the Butlers and the Croghans, and three Mohawk families, the Brants, the Hills, and the Peters.[110]
 After the war, to protect their alliance, the British government issued the Royal Proclamation of 1763, forbidding white settlement beyond the Appalachian Mountains. American colonists largely ignored the order, and the British had insufficient soldiers to enforce it.[123]
 Faced with confrontations, the Iroquois agreed to adjust the line again in the Treaty of Fort Stanwix (1768). Sir William Johnson, 1st Baronet, British Superintendent of Indian Affairs for the Northern District, had called the Iroquois nations together in a grand conference in western New York, which a total of 3,102 Indians attended.[39] They had long had good relations with Johnson, who had traded with them and learned their languages and customs. As Alan Taylor noted in his history, The Divided Ground: Indians, Settlers, and the Northern Borderland of the American Revolution (2006), the Iroquois were creative and strategic thinkers. They chose to sell to the British Crown all their remaining claim to the lands between the Ohio and Tennessee rivers, which they did not occupy, hoping by doing so to draw off English pressure on their territories in the Province of New York.[39]
 During the American Revolution, the Iroquois first tried to stay neutral. The Reverend Samuel Kirkland, a Congregational minister working as a missionary, pressured the Oneida and the Tuscarora for a pro-American neutrality while Guy Johnson and his cousin John Johnson pressured the Mohawk, the Cayuga and the Seneca to fight for the British.[124] Pressed to join one side or the other, the Tuscarora and the Oneida sided with the colonists, while the Mohawk, Seneca, Onondaga, and Cayuga remained loyal to Great Britain, with whom they had stronger relationships. Joseph Louis Cook offered his services to the United States and received a Congressional commission as a lieutenant colonel—the highest rank held by any Native American during the war.[125] The Mohawk war chief Joseph Brant together with John Butler and John Johnson raised racially mixed forces of irregulars to fight for the Crown.[126] Molly Brant had been the common-law wife of Sir William Johnson, and it was through her patronage that her brother Joseph came to be a war chief.[127]
 The Mohawk war chief Joseph Brant, other war chiefs, and British allies conducted numerous operations against frontier settlements in the Mohawk Valley, including the Cherry Valley massacre, destroying many villages and crops, and killing and capturing inhabitants. The destructive raids by Brant and other Loyalists led to appeals to Congress for help.[127] The Continentals retaliated and in 1779, George Washington ordered the Sullivan Campaign, led by Col. Daniel Brodhead and General John Sullivan, against the Iroquois nations to ""not merely overrun, but destroy"", the British-Indian alliance. They burned many Iroquois villages and stores throughout western New York; refugees moved north to Canada. By the end of the war, few houses and barns in the valley had survived the warfare. In the aftermath of the Sullivan expedition, Brant visited Quebec City to ask General Sir Frederick Haildmand for assurances that the Mohawk and the other Loyalist Iroquois would receive a new homeland in Canada as compensation for their loyalty to the Crown if the British should lose.[127]
 The American Revolution caused a great divide between the colonists between Patriots and Loyalists and a large proportion (30–35%) who were neutral; it caused a divide between the colonies and Great Britain, and it also caused a rift that would break the Iroquois Confederacy. At the onset of the Revolution, the Iroquois Confederacy's Six Nations attempted to take a stance of neutrality. However, almost inevitably, the Iroquois nations eventually had to take sides in the conflict. It is easy to see how the American Revolution would have caused conflict and confusion among the Six Nations. For years they had been used to thinking about the English and their colonists as one and the same people. In the American Revolution, the Iroquois Confederacy now had to deal with relationships between two governments.[128]
 The Iroquois Confederation's population had changed significantly since the arrival of Europeans. Disease had reduced their population to a fraction of what it had been in the past.[129] Therefore, it was in their best interest to be on the good side of whoever would prove to be the winning side in the war, for the winning side would dictate how future relationships would be with the Iroquois in North America. Dealing with two governments made it hard to maintain a neutral stance, because the governments could get jealous easily if the Confederacy was interacting or trading more with one side over the other, or even if there was simply a perception of favoritism. Because of this challenging situation, the Six Nations had to choose sides. The Oneida and Tuscarora decided to support the American colonists, while the rest of the Iroquois League (the Cayuga, Mohawk, Onondaga, and Seneca) sided with the British and their Loyalists among the colonists.
 There were many reasons that the Six Nations could not remain neutral and uninvolved in the Revolutionary War. One of these is simple proximity; the Iroquois Confederacy was too close to the action of the war to not be involved. The Six Nations were very discontented with the encroachment of the English and their colonists upon their land. They were particularly concerned with the border established in the Proclamation of 1763 and the Treaty of Fort Stanwix in 1768.[130]
 During the American Revolution, the authority of the British government over the frontier was hotly contested. The colonists tried to take advantage of this as much as possible by seeking their own profit and claiming new land. In 1775, the Six Nations were still neutral when ""a Mohawk person was killed by a Continental soldier"".[131] Such a case shows how the Six Nations' proximity to the war drew them into it. They were concerned about being killed, and about their lands being taken from them. They could not show weakness and simply let the colonists and British do whatever they wanted. Many of the English and colonists did not respect the treaties made in the past. ""A number of His Majesty's subjects in the American colonies viewed the proclamation as a temporary prohibition which would soon give way to the opening of the area for settlement ... and that it was simply an agreement to quiet the minds of the Indians"".[130] The Six Nations had to take a stand to show that they would not accept such treatment, and they looked to build a relationship with a government that would respect their territory.
 In addition to being in close proximity to the war, the new lifestyle and economics of the Iroquois Confederacy since the arrival of the Europeans in North America made it nearly impossible for the Iroquois to isolate themselves from the conflict. By this time, the Iroquois had become dependent upon the trade of goods from the English and colonists and had adopted many European customs, tools, and weapons. For example, they were increasingly dependent on firearms for hunting.[128] After becoming so reliant, it would have been hard to even consider cutting off trade that brought goods that were a central part of everyday life.
 As Barbara Graymont stated, ""Their task was an impossible one to maintain neutrality. Their economies and lives had become so dependent on each other for trading goods and benefits it was impossible to ignore the conflict. Meanwhile, they had to try and balance their interactions with both groups. They did not want to seem as they were favoring one group over the other, because of sparking jealousy and suspicion from either side"". Furthermore, the English had made many agreements with the Six Nations over the years, yet most of the Iroquois' day-to-day interaction had been with the colonists. This made it a confusing situation for the Iroquois because they could not tell who the true heirs of the agreement were, and could not know if agreements with England would continue to be honored by the colonists if they were to win independence.
 Supporting either side in the Revolutionary War was a complicated decision. Each nation individually weighed their options to come up with a final stance that ultimately broke neutrality and ended the collective agreement of the Confederation. The British were clearly the most organized, and seemingly most powerful. In many cases, the British presented the situation to the Iroquois as the colonists just being ""naughty children"". On the other, the Iroquois considered that ""the British government was three thousand miles away. This placed them at a disadvantage in attempting to enforce both the Proclamation of 1763 and the Treaty at Fort Stanwix 1768 against land hungry frontiersmen.""[132] In other words, even though the British were the strongest and best organized faction, the Six Nations had concerns about whether they would truly be able to enforce their agreements from so far away.
 The Iroquois also had concerns about the colonists. The British asked for Iroquois support in the war. ""In 1775, the Continental Congress sent a delegation to the Iroquois in Albany to ask for their neutrality in the war coming against the British"".[131] It had been clear in prior years that the colonists had not been respectful of the land agreements made in 1763 and 1768. The Iroquois Confederacy was particularly concerned over the possibility of the colonists winning the war, for if a revolutionary victory were to occur, the Iroquois very much saw it as the precursor to their lands being taken away by the victorious colonists, who would no longer have the British Crown to restrain them.[25] Continental army officers such as George Washington had attempted to destroy the Iroquois.[129]
 On a contrasting note, it was the colonists who had formed the most direct relationships with the Iroquois due to their proximity and trade ties. For the most part, the colonists and Iroquois had lived in relative peace since the English arrival on the continent a century and a half before. The Iroquois had to determine whether their relationships with the colonists were reliable, or whether the English would prove to better serve their interests. They also had to determine whether there were really any differences between how the English and the colonists would treat them.
 The war ensued, and the Iroquois broke their confederation. Hundreds of years of precedent and collective government was trumped by the immensity of the American Revolutionary War. The Oneida and Tuscarora decided to support the colonists, while the rest of the Iroquois League (the Cayuga, Mohawk, Onondaga, and Seneca) sided with the British and Loyalists. At the conclusion of the war the fear that the colonists would not respect the Iroquois' pleas came true, especially after the majority of the Six Nations decided to side with the British and were no longer considered trustworthy by the newly independent Americans. In 1783 the Treaty of Paris was signed. While the treaty included peace agreements between all of the European nations involved in the war as well as the newborn United States, it made no provisions for the Iroquois, who were left to be treated with by the new U.S. government as it saw fit.[128]
 After the Revolutionary War, the ancient central fireplace of the League was re-established at Buffalo Creek. The U.S. and the Iroquois signed the Treaty of Fort Stanwix in 1784, under which the Iroquois ceded much of their historical homeland to the Americans, which was followed by another treaty in 1794 at Canandaigua which they ceded even more land to the Americans.[133] The governor of New York state, George Clinton, was constantly pressuring the Iroquois to sell their land to white settlers, and as alcoholism became a major problem in the Iroquois communities, many did sell their land to buy more alcohol, usually to unscrupulous agents of land companies.[134] At the same time, American settlers continued to push into the lands beyond the Ohio river, leading to a war between the Western Confederacy and the U.S.[133] One of the Iroquois chiefs, Cornplanter, persuaded the remaining Iroquois in New York state to remain neutral and not to join the Western Confederacy.[133] At the same time, American policies to make the Iroquois more settled started to have some effect. Traditionally, for the Iroquois farming was woman's work and hunting was men's work; by the early 19th century, American policies to have the men farm the land and cease hunting were having effect.[135] During this time, the Iroquois living in New York state become demoralized as more of their land was sold to land speculators while alcoholism, violence, and broken families became major problems on their reservations.[135] The Oneida and the Cayuga sold almost all of their land and moved out of their traditional homelands.[135]
 By 1811, Methodist and Episcopalian missionaries established missions to assist the Oneida and Onondaga in western New York. However, white settlers continued to move into the area. By 1821, a group of Oneida led by Eleazer Williams, son of a Mohawk woman, went to Wisconsin to buy land from the Menominee and Ho-Chunk and thus move their people further westward.[136] In 1838, the Holland Land Company used forged documents to cheat the Seneca of almost all of their land in western New York, but a Quaker missionary, Asher Wright, launched lawsuits that led to one of the Seneca reservations being returned in 1842 and another in 1857.[135] However, as late as the 1950s both the U.S. and New York governments confiscated land belonging to the Six Nations for roads, dams and reservoirs with the land being given to Cornplanter for keeping the Iroquois from joining the Western Confederacy in the 1790s being forcibly purchased by eminent domain and flooded for the Kinzua Dam.[135]
 Captain Joseph Brant and a group of Iroquois left New York to settle in the Province of Quebec (present-day Ontario). To partially replace the lands they had lost in the Mohawk Valley and elsewhere because of their fateful alliance with the British Crown, the Haldimand Proclamation gave them a large land grant on the Grand River, at Six Nations of the Grand River First Nation. Brant's crossing of the river gave the original name to the area: Brant's Ford. By 1847, European settlers began to settle nearby and named the village Brantford. The original Mohawk settlement was on the south edge of the present-day Canadian city at a location still favorable for launching and landing canoes. In the 1830s many additional Onondaga, Oneida, Seneca, Cayuga, and Tuscarora relocated into the Indian Territory, the Province of Upper Canada, and Wisconsin.
 Many Iroquois (mostly Mohawk) and Iroquois-descended Métis people living in Lower Canada (primarily at Kahnawake) took employment with the Montreal-based North West Company during its existence from 1779 to 1821 and became voyageurs or free traders working in the North American fur trade as far west as the Rocky Mountains. They are known to have settled in the area around Jasper's House[137] and possibly as far west as the Finlay River[138] and north as far as the Pouce Coupe and Dunvegan areas,[139] where they founded new Aboriginal communities which have persisted to the present day claiming either First Nations or Métis identity and indigenous rights. The Michel Band, Mountain Métis,[140] and Aseniwuche Winewak Nation of Canada[141] in Alberta and the Kelly Lake community in British Columbia all claim Iroquois ancestry.
 During the 18th century, the Catholic Canadian Iroquois living outside of Montreal reestablished ties with the League Iroquois.[142] During the American Revolution, the Canadian Iroquois declared their neutrality and refused to fight for the Crown despite the offers of Sir Guy Carleton, the governor of Quebec.[142] Many Canadian Iroquois worked for both the Hudson's Bay Company and the Northwest Company as voyageurs in the fur trade in the late 18th and early 19th centuries.[142] In the War of 1812, the Canadian Iroquois again declared their neutrality.[142] The Canadian Iroquois communities at Oka and Kahnaweke were prosperous settlements in the 19th century, supporting themselves via farming and the sale of sleds, snowshoes, boats, and baskets.[142] In 1884, about 100 Canadian Iroquois were hired by the British government to serve as river pilots and boatmen for the relief expedition for the besieged General Charles Gordon in Khartoum in the Sudan, taking the force commanded by Field Marshal Wolsely up the Nile from Cairo to Khartoum.[142] On their way back to Canada, the Canadian Iroquois river pilots and boatmen stopped in London, where they were personally thanked by Queen Victoria for their services to Queen and Country.[142] In 1886, when a bridge was being built at the St. Lawrence, a number of Iroquois men from Kahnawke were hired to help built and the Iroquois workers proved so skilled as steelwork erectors that since that time, a number of bridges and skyscrapers in Canada and the U.S. have been built by the Iroquois steelmen.[142]
 During World War I, it was Canadian policy to encourage men from the First Nations to enlist in the Canadian Expeditionary Force (CEF), where their skills at hunting made them excellent as snipers and scouts.[143] As the Iroquois Six Nations were considered the most warlike of Canada's First Nations, and, in turn, the Mohawk the most warlike of the Six Nations, the Canadian government especially encouraged the Iroquois, particularly the Mohawks, to join.[144] About half of the 4,000 or so First Nations men who served in the CEF were Iroquois.[145] Men from the Six Nations reservation at Brantford were encouraged to join the 114th Haldimand Battalion (also known as ""Brock's Rangers) of the CEF, where two entire companies including the officers were all Iroquois. The 114th Battalion was formed in December 1915 and broken up in November 1916 to provide reinforcements for other battalions.[143] A Mohawk from Brantford, William Forster Lickers, who enlisted in the CEF in September 1914 was captured at the Second Battle of Ypres in April 1915, where he was savagely beaten by his captors as one German officer wanted to see if ""Indians could feel pain"".[146] Lickers was beaten so badly that he was left paralyzed for the rest of his life, though the officer was well pleased to establish that Indians did indeed feel pain.[146]
 The Six Nations council at Brantford tended to see themselves as a sovereign nation that was allied to the Crown through the Covenant Chain going back to the 17th century and thus allied to King George V personally instead of being under the authority of Canada.[147] One Iroquois clan mother in a letter sent in August 1916 to a recruiting sergeant who refused to allow her teenage son to join the CEF under the grounds that he was underage, declared the Six Nations were not subject to the laws of Canada and he had no right to refuse her son because Canadian laws did not apply to them.[147] As she explained, the Iroquois regarded the Covenant Chain as still being in effect, meaning the Iroquois were only fighting in the war in response to an appeal for help from their ally, King George V, who had asked them to enlist in the CEF.[147]
 The complex political environment which emerged in Canada with the Haudenosaunee grew out of the Anglo-American era of European colonization. At the end of the War of 1812, Britain shifted Indian affairs from the military to civilian control. With the creation of the Canadian Confederation in 1867, civil authority, and thus Indian affairs, passed to Canadian officials with Britain retaining control of military and security matters. At the turn of the century, the Canadian government began passing a series of Acts which were strenuously objected to by the Iroquois Confederacy. During World War I, an act attempted to conscript Six Nations men for military service. Under the Soldiers Resettlement Act, legislation was introduced to redistribute native land. Finally in 1920, an Act was proposed to force citizenship on ""Indians"" with or without their consent, which would then automatically remove their share of any tribal lands from tribal trust and make the land and the person subject to the laws of Canada.[148]
 The Haudenosaunee hired a lawyer to defend their rights in the Supreme Court of Canada. The Supreme Court refused to take the case, declaring that the members of the Six Nations were British citizens. In effect, as Canada was at the time a division of the British government, it was not an international state, as defined by international law. In contrast, the Iroquois Confederacy had been making treaties and functioning as a state since 1643 and all of their treaties had been negotiated with Britain, not Canada.[148] As a result, a decision was made in 1921 to send a delegation to petition the King George V,[149] whereupon Canada's External Affairs division blocked issuing passports. In response, the Iroquois began issuing their own passports and sent Levi General,[148] the Cayuga Chief ""Deskaheh"",[149] to England with their attorney. Winston Churchill dismissed their complaint claiming that it was within the realm of Canadian jurisdiction and referred them back to Canadian officials.
 On December 4, 1922, Charles Stewart, Superintendent of Indian Affairs, and Duncan Campbell Scott, Deputy Superintendent of the Canadian Department of Indian Affairs traveled to Brantford to negotiate a settlement on the issues with the Six Nations. After the meeting, the Native delegation brought the offer to the tribal council, as was customary under Haudenosaunee law. The council agreed to accept the offer, but before they could respond, the Royal Canadian Mounted Police conducted a liquor raid on the Iroquois' Grand River territory. The siege lasted three days[148] and prompted the Haudenosaunee to send Deskaheh to Washington, D/C., to meet with the chargé d'affaires of the Netherlands asking the Dutch Queen to sponsor them for membership in the League of Nations.[149] Under pressure from the British, the Netherlands reluctantly refused sponsorship.[150]
 Deskaheh and the tribal attorney proceeded to Geneva and attempted to gather support. ""On 27 September 1923, delegates representing Estonia, Ireland, Panama and Persia signed a letter asking for communication of the Six Nations' petition to the League's assembly,"" but the effort was blocked.[148] Six Nations delegates traveled to the Hague and back to Geneva attempting to gain supporters and recognition,[149] while back in Canada, the government was drafting a mandate to replace the traditional Haudenosaunee Confederacy Council with one that would be elected under the auspices of the Canadian Indian Act. In an unpublicized signing on September 17, 1924, Prime Minister Mackenzie King and Governor-General Lord Byng of Vimy signed the Order in Council, which set elections on the Six Nations reserve for October 21. Only 26 ballots were cast.
 The long-term effect of the Order was that the Canadian government had wrested control over the Haudenosaunee trust funds from the Iroquois Confederation and decades of litigation would follow.[148] In 1979, over 300 Indian chiefs visited London to oppose Patriation of the Canadian Constitution, fearing that their rights to be recognized in the Royal Proclamation of 1763 would be jeopardized. In 1981, hoping again to clarify that judicial responsibilities of treaties signed with Britain were not transferred to Canada, several Alberta Indian chiefs filed a petition with the British High Court of Justice. They lost the case but gained an invitation from the Canadian government to participate in the constitutional discussions which dealt with protection of treaty rights.[149]
 In 1990, a long-running dispute over ownership of land at Oka, Quebec, caused a violent stand-off. The Mohawk reservation at Oka had become dominated by a group called the Mohawk Warrior Society that engaged in practices that American and Canadian authorities considered smuggling across the U.S.-Canada border, and were well armed with assault rifles. On July 11, 1990, the Mohawk Warrior Society tried to stop the building of a golf course on land claimed by the Mohawk people, which led to a shoot-out between the Warrior Society and the Sûreté du Québec that left a policeman dead.[151] In the resulting Oka Crisis, the Warrior Society occupied both the land that they claimed belonged to the Mohawk people and the Mercier bridge linking the Island of Montreal to the south shore of the St. Lawrence River.[151] On August 17, 1990, Quebec Premier Robert Bourassa asked for the Canadian Army to intervene to maintain ""public safety"", leading to the deployment of the Royal 22e Régiment to Oka and Montreal.[151] The stand-off ended on September 26, 1990, with a melee between the soldiers and the warriors.[151] The dispute over ownership of the land at Oka continues.[as of?]
 In the period between World War II and The Sixties, the U.S. government followed a policy of Indian Termination for its Native citizens. In a series of laws, attempting to mainstream tribal people into the greater society, the government strove to end the U.S. government's recognition of tribal sovereignty, eliminate trusteeship over Indian reservations, and implement state law applicability to native persons. In general, the laws were expected to create taxpaying citizens, subject to state and federal taxes as well as laws, from which Native people had previously been exempt.[152]
 On August 13, 1946, the Indian Claims Commission Act of 1946, Pub. L. No. 79-726, ch. 959, was passed. Its purpose was to settle for all time any outstanding grievances or claims the tribes might have against the U.S. for treaty breaches, unauthorized taking of land, dishonorable or unfair dealings, or inadequate compensation. Claims had to be filed within a five-year period, and most of the 370 complaints that were submitted[153] were filed at the approach of the five-year deadline in August 1951.[154]
 On July 2, 1948, Congress enacted [Public Law 881] 62 Stat. 1224, which transferred criminal jurisdiction over offenses committed by and against ""Indians"" to the State of New York. It covered all reservations' lands within the state and prohibited the deprivation of hunting and fishing rights which may have been guaranteed to ""any Indian tribe, band, or community, or members thereof."" It further prohibited the state from requiring tribal members to obtain fish and game licenses.[155] Within 2 years, Congress passed [Public Law 785] 64 Stat. 845, on September 13, 1950[156] which extended New York's authority to civil disputes between Indians or Indians and others within the State. It allowed the tribes to preserve customs, prohibited taxation on reservations,[157] and reaffirmed hunting and fishing rights. It also prohibited the state from enforcing judgments regarding any land disputes or applying any State laws to tribal lands or claims prior to the effective date of the law September 13, 1952.[156] During congressional hearings on the law, tribes strongly opposed its passage, fearful that states would deprive them of their reservations. The State of New York disavowed any intention to break up or deprive tribes of their reservations and asserted that they did not have the ability to do so.[158]
 On August 1, 1953, U.S. Congress issued a formal statement, House concurrent resolution 108, which was the formal policy presentation announcing the official federal policy of Indian termination. The resolution called for the ""immediate termination of the Flathead, Klamath, Menominee, Potawatomi, and Turtle Mountain Chippewa, as well as all tribes in the states of California, New York, Florida, and Texas."" All federal aid, services, and protection offered to these Native peoples were to cease, and the federal trust relationship and management of reservations would end.[159] Individual members of terminated tribes were to become full U.S. citizens with all the rights, benefits and responsibilities of any other U.S. citizen. The resolution also called for the Interior Department to quickly identify other tribes who would be ready for termination in the near future.[160]
 Beginning in 1953, a Federal task force began meeting with the tribes of the Six Nations. Despite tribal objections, legislation was introduced into Congress for termination.[161] The proposed legislation involved more than 11,000 Indians of the Iroquois Confederation and was divided into two separate bills. One bill dealt with the Mohawk, Oneida, Onondaga, Cayuga and Tuscarora tribes, and the other dealt with the Seneca.[162] The arguments the Six Nations made in their hearings with committees were that their treaties showed that the U.S. recognized that their lands belonged to the Six Nations, not the U.S., and that ""termination contradicted any reasonable interpretation that their lands would not be claimed or their nations disturbed"" by the federal government.[163] The bill for the Iroquois Confederation died in committee without further serious consideration.[161]
 On August 31, 1964,[164] H. R. 1794 An Act to authorize payment for certain interests in lands within the Allegheny Indian Reservation in New York was passed by Congress and sent to the president for signature. The bill authorized payment for resettling and rehabilitation of the Seneca Indians who were being dislocated by the construction of the Kinzua Dam on the Allegheny River. Though only 127 Seneca families (about 500 people) were being dislocated, the legislation benefited the entire Seneca Nation, because the taking of the Indian land for the dam abridged a 1794 treaty agreement. In addition, the bill provided that within three years, a plan from the Interior Secretary should be submitted to Congress withdrawing all federal supervision over the Seneca Nation, though technically civil and criminal jurisdiction had lain with the State of New York since 1950.[165]
 Accordingly, on September 5, 1967, a memo from the Department of the Interior announced proposed legislation was being submitted to end federal ties with the Seneca.[166][167] In 1968 a new liaison was appointed from the BIA for the tribe to assist the tribe in preparing for termination and rehabilitation.[168] The Seneca were able to hold off termination until President Nixon issued[169] his Special Message to the Congress on Indian Affairs in July 1970.[170] No New York tribes then living in the state were terminated during this period.
 One tribe that had formerly lived in New York did lose its federal recognition. The Emigrant Indians of New York included the Oneida, Stockbridge-Munsee, and Brothertown Indians of Wisconsin.[171] In an effort to fight termination and force the government into recognizing their outstanding land claims in New York, the three tribes filed litigation with the Claims Commission in the 1950s.[172] They won their claim on August 11, 1964.[171] Public Law 90-93 81 Stat. 229 Emigrant New York Indians of Wisconsin Judgment Act established federal trusteeship to pay the Oneida and Stockbridge-Munsee, effectively ending Congressional termination efforts for them. Though the law did not specifically state the Brothertown Indians were terminated, it authorized all payments to be made directly to each enrollee, with special provisions for minors to be handled by the Secretary. The payments were not subject to state or federal taxes.[173]
 Beginning in 1978, the Brothertown Indians submitted a petition to regain federal recognition.[172] In 2012 the Department of the Interior, in the final determination on the Brothertown petition, found that Congress had terminated their tribal status when it granted them citizenship in 1838 and therefore only Congress could restore their tribal status.[174] They are still[when?] seeking Congressional approval.[175]
 For the Haudenosaunee, grief for a loved one who died was a powerful emotion. They believed that if it was not attended to, it would cause all sorts of problems for the grieving who would go mad if left without consolation.[176] Rituals to honor the dead were very important and the most important of all was the condolence ceremony to provide consolation for those who lost a family member or friend.[177] Since it was believed that the death of a family member also weakened the spiritual strength of the surviving family members, it was considered crucially important to replace the lost family member by providing a substitute who could be adopted, or alternatively could be tortured to provide an outlet for the grief.[178] Hence the ""mourning wars"".
 One of the central features of traditional Iroquois life were the ""mourning wars"", when their warriors would raid neighboring peoples in search of captives to replace those Haudenosaunee who had died.[179] War for the Haudenosaunee was primarily undertaken for captives. They were not concerned with such goals as expansion of territory or glory in battle, as were the Europeans.[180] They did, however, go to war to control hunting grounds, especially as the fur trade became more lucrative.
 A war party was considered successful if it took many prisoners without suffering losses in return; killing enemies was considered acceptable if necessary, but disapproved of as it reduced the number of potential captives.[180] Taking captives were considered far more important than scalps. Additionally, war served as a way for young men to demonstrate their valor and courage. This was a prerequisite for a man to be made a chief, and it was also essential for men who wanted to marry. Haudenosaunee women admired warriors who were brave in war.[181] In the pre-contact era, war was relatively bloodless, as First Nations peoples did not have guns and fought one another in suits of wooden armor.[182] In 1609, the French explorer Samuel de Champlain observed several battles between the Algonquin and the Iroquois that resulted in hardly any deaths. This seemed to be the norm for First Nations wars.[182] At a battle between the Algonquin and the Iroquois by the shores of Lake Champlain, the only people killed were two Iroquois warriors hit by bullets from Champlain's musket, in a demonstration to his Algonquin allies.
 The clan mothers would demand a ""mourning war"" to provide consolation and renewed spiritual strength for a family that lost a member to death. Either the warriors would go on a ""mourning war"" or would be marked by the clan mothers as cowards forever, which made them unmarriageable.[179] At this point, the warriors would usually leave to raid a neighboring people in search of captives.[183] The captives were either adopted into Haudenosaunee families to become assimilated, or were to be killed after bouts of ritualized torture as a way of expressing rage at the death of a family member. The male captives were usually received with blows, passing through a kind of gantlet as they were brought into the community. All captives, regardless of their sex or age, were stripped naked and tied to poles in the middle of the community. After having sensitive parts of their bodies burned and some of their fingernails pulled out, the prisoners were allowed to rest and given food and water. In the following days, the captives had to dance naked before the community, when individual families decided for each if the person was to be adopted or killed. Women and children were more often adopted than were older men. If those who were adopted into the Haudenosaunee families made a sincere effort to become Haudenosaunee, then they would be embraced by the community, and if they did not, then they were swiftly executed.[184]
 Those slated for execution had to wear red and black facial paint and were ""adopted"" by a family who addressed the prisoner as ""uncle"", ""aunt"", ""nephew"" or ""niece"" depending on their age and sex, and would bring them food and water. The captive would be executed after a day-long torture session of burning and removing body parts, which the prisoner was expected to bear with stoicism and nobility (an expectation not usually met) before being scalped alive. Hot sand was applied to the exposed skull and they were finally killed by cutting out their hearts. Afterward, the victim's body was cut and eaten by the community. The practice of ritual torture and execution, together with cannibalism, ended some time in the early 18th century. By the late-18th-century, European writers such as Filippo Mazzei and James Adair were denying that the Haudenosaunee engaged in ritual torture and cannibalism, saying they had seen no evidence of such practices during their visits to Haudenosaunee villages.[185]
 In 1711, Onondaga chief Teganissorens told Sir Robert Hunter, governor of New York: ""We are not like you Christians, for when you have prisoners of one another you send them home, by such means you can never rout one another"".[180] The converse of this strategy was that the Iroquois would not accept losses in battle, as it defeated the whole purpose of the ""mourning wars"", which was to add to their numbers, not decrease them. The French during their wars with the Haudenosaunee were often astonished when a war party that was on the verge of victory over them could be made to retreat by killing one or two of their number. The European notion of a glorious death in battle had no counterpart with the Haudenosaunee.[180]
 Death in battle was accepted only when absolutely necessary, and the Iroquois believed the souls of those who died in battle were destined to spend eternity as angry ghosts haunting the world in search of vengeance.[186] For this reason, those who died in battle were never buried in community cemeteries, as it would bring the presence of unhappy ghosts into the community.[187]
 The Haudenosaunee engaged in tactics that the French, the British, and the Americans all considered to be cowardly, until the Americans adopted similar guerrilla tactics. The Haudenosaunee preferred ambushes and surprise attacks, would almost never attack a fortified place or attack frontally, and would retreat if outnumbered. If Kanienkeh was invaded, the Haudenosaunee would attempt to ambush the enemy, or alternatively they would retreat behind the wooden walls of their villages to endure a siege. If the enemy appeared too powerful, as when the French invaded Kanienkeh in 1693, the Haudenosaunee burned their villages and their crops, and the entire population retreated into the woods to wait for the French to depart.[187] The main weapons for the Iroquois were bows and arrows with flint tips and quivers made from corn husks.[188] Shields and war clubs were made from wood.[189] After contact was established with Europeans, the Native Americans adopted such tools as metal knives and hatchets, and made their tomahawks with iron or steel blades.[189] It has been posited that the tomahawk was not used extensively in battle, but instead became associated with the Haudenosaunee through European depictions that sought to portray natives as savage and threatening.[190] Before taking to the field, war chiefs would lead ritual purification ceremonies in which the warriors would dance around a pole painted red.[189]
 European infectious diseases such as smallpox devastated the Five Nations in the 17th century, causing thousands of deaths, as they had no acquired immunity to the new diseases, which had been endemic among Europeans for centuries. The League began a period of ""mourning wars"" without precedent; compounding deaths from disease, they nearly annihilated the Huron, Petun and Neutral peoples.[191] By the 1640s, it is estimated that smallpox had reduced the population of the Haudenosaunee by least 50%. Massive ""mourning wars"" were undertaken to make up these losses.[192] The American historian Daniel Richter wrote it was at this point that war changed from being sporadic, small-scale raids launched in response to individual deaths, and became ""the constant and increasing undifferentiated symptom of societies in demographic crisis"".[192] The introduction of guns, which could pierce the wooden armor, made First Nations warfare bloodier and more deadly than it had been in the pre-contact era. This ended the age when armed conflicts were more brawls than battles as Europeans would have understood the term.[182] At the same time, guns could only be obtained by trading furs with the Europeans. Once the Haudenosaunee exhausted their supplies of beaver by about 1640, they were forced to buy beaver pelts from Indians living further north, which led them to attempt to eliminate other middlemen to monopolize the fur trade in a series of ""beaver wars"".[193] Richter wrote
 From 1640 to 1701, the Five Nations was almost continuously at war, battling at various times the French, Huron, Erie, Neutral, Lenape, Susquenhannock, Petun, Abenaki, Ojibwa, and Algonquin peoples, fighting campaigns from Virginia to the Mississippi and all the way to what is now northern Ontario.[194]
 Despite taking thousands of captives, the Five Nations populations continued to fall, as diseases continued to take their toll. French Jesuits, whom the Haudenosaunee were forced to accept after making peace with the French in 1667, encouraged Catholic converts to move to mission villages in the St. Lawrence river valley near Montreal and Quebec.[195] In the 1640s, the Mohawk could field about 800 warriors. By the 1670s, they could field only 300 warriors, indicating population decline.[196]
 The Iroquois League traditions allowed for the dead to be symbolically replaced through captives taken in ""mourning wars"", the blood feuds and vendettas that were an essential aspect of Iroquois culture.[197] As a way of expediting the mourning process, raids were conducted to take vengeance and seize captives. Captives were generally adopted directly by the grieving family to replace the member(s) who had been lost.
 This process not only allowed the Iroquois to maintain their own numbers, but also to disperse and assimilate their enemies. The adoption of conquered peoples, especially during the period of the Beaver Wars (1609–1701), meant that the Iroquois League was composed largely of naturalized members of other tribes. Cadwallader Colden wrote,
 Those who attempted to return to their families were harshly punished; for instance, the French fur trader Pierre-Esprit Radisson was captured by an Iroquois raiding party as a teenager, was adopted by a Mohawk family, and ran away to return to his family in Trois-Rivières. When he was recaptured, he was punished by having his fingernails pulled out and having one of his fingers cut to the bone.[198] But Radisson was not executed, as his adoptive parents provided gifts to the families of the men whom Radisson had killed when he escaped, given as compensation for their loss. Several Huron who escaped with Radisson and were recaptured were quickly executed.[198]
 By 1668, two-thirds of the Oneida village[which?] were assimilated Algonquian and Huron. At Onondaga there were Native Americans of seven different nations, and among the Seneca eleven.[89] They also adopted European captives,[199] as did the Catholic Mohawk in settlements outside Montreal. This tradition of adoption and assimilation was common to native people of the Northeast.
 At the time of first European contact the Iroquois lived in a small number of large villages scattered throughout their territory. Each nation had between one and four villages at any one time, and villages were moved approximately every five to twenty years as soil and firewood were depleted.[200] These settlements were surrounded by a palisade and usually located in a defensible area such as a hill, with access to water.[201] Because of their appearance with the palisade, Europeans termed them castles. Villages were usually built on level or raised ground, surrounded by log palisades and sometimes ditches.[202]
 Within the villages the inhabitants lived in longhouses. Longhouses varied in size from 15 to 150 feet long and 15 to 25 feet in breadth.[202] Longhouses were usually built of layers of elm bark on a frame of rafters and standing logs raised upright.[202] In 1653, Dutch official and landowner Adriaen van der Donck described a Mohawk longhouse in his Description of New Netherland:
 Usually, between 2 and 20 families lived in a single longhouse with sleeping platforms being 2 feet above the ground and food left to dry on the rafters.[202] A castle might contain twenty or thirty longhouses. In addition to the castles the Iroquois also had smaller settlements which might be occupied seasonally by smaller groups, for example for fishing or hunting.[201] Living in the smoke-filled longhouses often caused conjunctivitis.[188]
 Total population for the five nations has been estimated at 20,000 before 1634. After 1635 the population dropped to around 6,800, chiefly due to the epidemic of smallpox introduced by contact with European settlers.[200] The Iroquois lived in extended families divided clans headed by clan mothers that grouped into moieities (""halves""). The typical clan consisted of about 50 to 200 people.[203] The division of the Iroquois went as follows:
 Government was by the 50 sachems representing the various clans who were chosen by the clan mothers.[203] Assisting the sachems were the ""Pinetree Chiefs"" who served as diplomats and the ""War Chiefs"" who led the war parties; neither the ""Pinetree Chiefs"" or the ""War Chiefs"" were allowed to vote at council meetings.[204]
 By the late 1700s The Iroquois were building smaller log cabins resembling those of the colonists, but retaining some native features, such as bark roofs with smoke holes and a central fireplace.[205] The main woods used by the Iroquois to make their utensils were oak, birch, hickory and elm.[202] Bones and antlers were used to make hunting and fishing equipment.[206]
 The Iroquois are a mix of horticulturalists, farmers, fishers, gatherers and hunters, though traditionally their main diet has come from farming. For the Iroquois, farming was traditionally women's work and the entire process of planting, maintaining, harvesting and cooking was done by women.[188] Gathering has also traditionally been the job of women and children. Wild roots, greens, berries and nuts were gathered in the summer. During spring, sap is tapped from the maple trees and boiled into maple syrup, and herbs are gathered for medicine. After the coming of Europeans, the Iroquois started to grow apples, pears, cherries, and peaches.[188]
 Historically, the main crops cultivated by the Iroquois were corn, beans, and squash, which were called the three sisters (De-oh-há-ko) and in Iroquois tradition were considered special gifts from the Creator.[188] These three crops could be ground up into hominy and soups in clay pots (later replaced by metal pots after contact with Europeans).[188] Besides the ""Three Sisters"", the Iroquois diet also included artichokes, leeks, cucumbers, turnips, pumpkins, a number of different berries such blackberries, blueberries, gooseberries, etc. and wild nuts.[188] Ramson, a species of wild onion, is also a part of traditional Iroquois cuisine,[207] as well as northern redcurrant,[208] American groundnut,[209] and broadleaf toothwort.[210]
 Using these ingredients they prepared meals of boiled cornbread and cornmeal sweetened with maple syrup, known today as Indian pudding. Cornmeal was also used to make samp, a type of porridge with beans and dried meat. Reports from early American settlers mention Iroquois extracting corn syrup that was used as a sweetener for cornmeal dumplings.[211]
 The Iroquois hunted mostly deer but also other game such as wild turkey and migratory birds. Muskrat and beaver were hunted during the winter. Archaeologists have found the bones of bison, elk, deer, bear, raccoon, and porcupines at Iroquois villages.[188] Fishing was also a significant source of food because the Iroquois had villages mostly in the St.Lawrence and Great Lakes areas. The Iroquois used nets made from vegetable fiber with weights of pebbles for fishing.[188] They fished salmon, trout, bass, perch and whitefish until the St. Lawrence became too polluted by industry. In the spring the Iroquois netted, and in the winter fishing holes were made in the ice.[212] Starting about 1620, the Iroquois started to raise pigs, geese and chickens, which they had acquired from the Dutch.[188]
 In 1644 Johannes Megapolensis described Mohawk traditional wear.
 On their feet the Iroquois wore moccasin, ""true to nature in its adjustment to the foot, beautiful in its materials and finish, and durable as an article of apparel.""[25]
 Moccasins of a sort were also made of corn husks.
 In 1653 Dutch official Adriaen van der Donck wrote:
 During the 17th century, Iroquois clothing changed rapidly as a result of the introduction of scissors and needles obtained from the Europeans, and the British scholar Michael Johnson has cautioned that European accounts of Iroquois clothing from the latter 17th century may not have entirely reflected traditional pre-contact Iroquois clothing.[189] In the 17th century women normally went topless in the warm months while wearing a buckskin skirt overlapping on the left while in the winter women covered their upper bodies with a cape-like upper garment with an opening for the head.[213] By the 18th century, cloth colored red and blue obtained from Europeans became the standard material for clothing with the men and women wearing blouses and shirts that usually decorated with beadwork and ribbons and were often worn alongside silver brooches.[214]
 By the late 18th century, women were wearing muslin or calico long, loose-fitting overdresses.[214] The tendency of Iroquois women to abandon their traditional topless style of dressing in the warm months reflected European influence.[214] Married women wore their hair in a single braid held in place by a comb made of bone, antler or silver while unmarried wore their hair in several braids.[214] Warriors wore moccasins, leggings and short kilts and on occasion wore robes that were highly decorated with painted designs.[214] Initially, men's clothing was made of buckskin and were decorated with porcupine quill-work and later on was made of broadcloth obtained from Europeans.[214] The bodies and faces of Iroquois men were heavily tattooed with geometric designs and their noses and ears were pieced with rings made up of wampun or silver.[214] On the warpath, the faces and bodies of the warriors were painted half red, half black.[214] The men usually shaved most of their hair with leaving only a tuft of hair in the center, giving the name Mohawk to their hair style.[214] A cap made of either buckskin or cloth tied to wood splints called the Gus-to-weh that was decorated with feathers was often worn by men.[214] Buckskin ammunition pouches with straps over the shoulder together with belts or slashes that carried powder horn and tomahawks were usually worn by warriors.[214] Quilled knife cases were worn around the neck.[215] Chiefs wore headdresses made of deer antler.[214] By the 18th century, Iroquois men normally wore shirts and leggings made of broadcloth and buckskin coats.[214] In the 17th and 18th centuries silver armbands and gorgets were popular accessories.[214]
 By the 1900s most Iroquois were wearing the same clothing as their non-Iroquois neighbors. Today most nations only wear their traditional clothing to ceremonies or special events.[216]
 Men wore a cap with a single long feather rotating in a socket called a gustoweh. Later, feathers in the gustoweh denote the wearer's tribe by their number and positioning. The Mohawk wear three upright feathers, the Oneida two upright and one down. The Onondaga wear one feather pointing upward and another pointing down. The Cayuga have a single feather at a 45-degree angle. The Seneca wear a single feather pointing up, and the Tuscarora have no distinguishing feathers.[217][218]
 Writing in 1851 Morgan wrote that women's outfits consisted of a skirt (gä-kä'-ah) ""usually of blue broadcloth, and elaborately embroidered with bead-work. It requires two yards of cloth, which is worn with the selvedge at the top and bottom; the skirt being secured about the waist and descending nearly to the top of the moccasin."" Under the skirt, between the knees and the moccasins, women wore leggings (gise'-hǎ), called pantalettes by Morgan, ""of red broadcloth, and ornamented with a border of beadwork around the lower edge ... In ancient times the gise'-hǎ was made of deer-skin and embroidered with porcupine-quill work."" An over-dress (ah-de-a'-da-we-sa) of muslin or calico was worn over the skirt, it is ""gathered slightly at the waist, and falls part way down the skirt ... In front it is generally buttoned with silver broaches."" The blanket (e'yose) is two or three yards of blue or green broadcloth ""it falls from the head or neck in natural folds the width of the cloth, as the selvedges are at the top and bottom, and it is gathered round the person like a shawl.""[25]
 The women wore their hair very long and tied together at the back, or ""tied at the back of the head and folded into a tress of about a hand's length, like a beaver tail ... they wear around the forehead a strap of wampum shaped like the headband that some was worn in olden times."" ""The men have a long lock hanging down, some on one side of the head, and some on both sides. On the top of their heads they have a streak of hair from the forehead to the neck, about the breadth of three fingers, and this they shorten until it is about two or three fingers long, and it stands right on end like a cock's comb or hog's bristles; on both sides of this cock's comb they cut all the hair short, except for the aforesaid locks, and they also leave on the bare places here and there small locks, such as aree in sweeping brushes and then they are in fine array.""[201] This is the forerunner to what is today called a ""Mohawk hairstyle"".
 The women did not paint their faces. The men ""paint their faces red, blue, etc.""[201]
 Societies, often called ""medicine societies"", ""medicine lodges"",[219] or ""curing societies"",[220] played an important role in Iroquois social organization. Lewis H. Morgan says that each society ""was a brotherhood into which new members were admitted by formal initiation.""[221] Originally the membership seems to have been on the basis of moiety, but by 1909 all societies seems to have been open to all men regardless of kinship.
 It is believed that ""most of the societies are of ancient origin and that their rituals have been transmitted with little change for many years."" ""Each society has a legend by which its origin and peculiar rites are explained.""[219] As part of his religious revolution, Handsome Lake ""sought to destroy the societies and orders that conserved the older religious rites.""[219] A council of chiefs proclaimed, some time around 1800, that all animal and mystery societies should immediately dissolve, but through a defect in the form of the order the societies decided it was not legally binding and ""went underground"" becoming secret societies.[221] Reviled by the ""New Religion"" of Handsome Lake, they were also rejected by the Christian Iroquois as holding pagan beliefs. Gradually, however, the societies came more into the open as hostility lessened.[219]
 A number of societies are known, of which the False Face Society is the most familiar. Others were the Little Water Society, the Pygmy Society, the Society of Otters, the Society of Mystic Animals, the Eagle Society, the Bear Society, the Buffalo Society, the Husk Faces, and the Woman's Society—which despite its name had male membership. The Sisters of the Deo-ha-ko was an organization of women.[219]
 During healing ceremonies, a carved ""False Face Mask"" is worn to represent spirits in a tobacco-burning and prayer ritual. False Face Masks are carved in living trees, then cut free to be painted and decorated.[222] False Faces represent grandfathers of the Iroquois, and are thought to reconnect humans and nature and to frighten illness-causing spirits.[223]
 The Iroquois today have several different medicine societies.[224] The False Face Company conducts rituals to cure sick people by driving away spirits; the Husk Face Society is made up of those who had dreams seen as messages from the spirits and the Secret Medicine Society likewise conducts rituals to cure the sick.[206] There are 12 different types of masks worn by the societies.[206] The types of masks are:
 The ""crooked face"" masks with the twisted mouths, the masks with the spoon lips and the whistling masks are the ""Doctor"" masks.[206] The other masks are ""Common Face"" or ""Beggar"" masks that are worn by those who help the Doctors.[188]
 The Husk Face Society performs rituals to communicate with the spirits in nature to ensure a good crop, the False Face Society performs rituals to chase away evil spirits, and the Secret Medicine Society performs rituals to cure diseases.[225] The grotesque masks represent the faces of the spirits that the dancers are attempting to please.[206] Those wearing Doctor masks blow hot ashes into the faces of the sick to chase away the evil spirits that are believed to be causing the illness.[206] The masked dancers often carried turtle shell rattles and long staffs.[188]
 Both male and female healers were knowledgeable in the use of herbs to treat illness, and could dress wounds, set broken bones, and perform surgery. Illness was believed to have a spiritual as well as a natural component, so spells, dances, ceremonies were used in addition to more practical treatments.[226] There are three types of practitioners of traditional medicine: The ""Indian doctor"" or healer, who emphasizes the physical aspect of curing illness, the fortune-teller, who uses spiritual means to determine the cause of the patient's ailments and the appropriate cure, and the witch.[227]
 It was believed that knowledge of healing was given by supernatural creatures in the guise of animals.[228]
 In recent times, traditional medicine has co-existed with western medicine, with traditional practices more prevalent among followers of the Gaihwi:io (Longhouse Religion). People may resort to traditional practices for certain types of ailments, and to western medicine for other types, or they may use both traditional and western medicine to treat the same ailment as a form of double security.
 The Iroquois societies are active in maintaining the practice of traditional medicine.[227]
 The Iroquois have historically followed a matriarchal system. Men and women have traditionally had separate roles but both hold real power in the Nations. No person is entitled to 'own' land, but it is believed that the Creator appointed women as stewards of the land. Traditionally, the Clan Mothers appoint leaders, as they have raised children and are therefore held to a higher regard. By the same token, if a leader does not prove sound, becomes corrupt or does not listen to the people, the Clan Mothers have the power to strip him of his leadership.[229] The chief of a clan can be removed at any time by a council of the women elders of that clan. The chief's sister has historically been responsible for nominating his successor.[230] The clan mothers, the elder women of each clan, are highly respected.
 The Iroquois have traditionally followed a matrilineal system, and hereditary leadership passes through the female line of descent, that is, from a mother to her children. The children of a traditional marriage belong to their mother's clan and gain their social status through hers. Her brothers are important teachers and mentors to the children, especially introducing boys to men's roles and societies. If a couple separates, the woman traditionally keeps the children.[230] It is regarded as incest by the Iroquois to marry within one's matrilineal clan, but considered acceptable to marry someone from the same patrilineal clan.[231]
 The teachings of Handsome Lake also expanded to influence the wider Iroquois society. The power centered around the mode of food production and the social sphere in general. Handsome Lake's teaching tried to center the nuclear family and transferred the women's sphere to be relegated to the home while the men's sphere focused on horticulture. Also, the Handsome Lake code shifted from the family structure from the maternal one to one that centers around the patriarch.[232]
 Moreover, several other factors influenced the position of Iroquois women. The exhaustion of the beavers' population led to men traveling for longer distances; this resulted in women having a more influential role in their societies because of the long absence of men. Another factor that influenced women's position shift was the reorganization of the political structure. The changes were influential as elected representatives instead of women-appointed sachems.[233]
 The status of Iroquois women inspired and had an impact on the early Feminist American movement. This was seen in the Seneca Fall Convention of 1848, the first feminist convention. For example, Matilda Gage, a prominent member of the convention, wrote extensively about the Iroquois throughout her life. Elizabeth Cady lived in close proximity to the Seneca tribe of the Iroquois and had a relative and a neighbor who was adopted by the Seneca tribe as well.[234]
 Women also held an important position to be Agoianders or to elect them. The Agoianders positions was to watch over the public treasury and hold the chief accountable.[235]
 Historically women have held the dwellings, horses and farmed land, and a woman's property before marriage has stayed in her possession without being mixed with that of her husband. The work of a woman's hands is hers to do with as she sees fit.
 Historically, at marriage, a young couple lived in the longhouse of the wife's family (matrilocality). A woman choosing to divorce a shiftless or otherwise unsatisfactory husband is able to ask him to leave the dwelling and take his possessions with him.[236]
 Like many cultures, the Iroquois' spiritual beliefs changed over time and varied across tribes. Generally, the Iroquois believed in numerous deities, including the Great Spirit, the Thunderer, and the Three Sisters (the spirits of beans, maize, and squash). The Great Spirit was thought to have created plants, animals, and humans to control ""the forces of good in nature"", and to guide ordinary people.[222] Orenda was the Iroquoian name for the magical potence found in people and their environment.[237] The Iroquois believed in the orenda, the spiritual force that flowed through all things, and believed if people were respectful of nature, then the orenda would be harnessed to bring about positive results.[238] There were three types of spirits for the Iroquois: 1) Those living on the earth 2) Those living above the earth and 3) the highest level of spirits controlling the universe from high above with the highest of those beings known variously as the Great Spirit, the Great Creator or the Master of Life.[238]
 Sources provide different stories about Iroquois creation beliefs. Brascoupé and Etmanskie focus on the first person to walk the earth, called the Skywoman or Aientsik. Aientsik's daughter Tekawerahkwa gave birth to twins, Tawiskaron, who created vicious animals and river rapids, while Okwiraseh created ""all that is pure and beautiful"".[239] After a battle where Okwiraseh defeated Tawiskaron, Tawiskaron was confined to ""the dark areas of the world"", where he governed the night and destructive creatures.[239] Other scholars present the ""twins"" as the Creator and his brother, Flint.[240] The Creator was responsible for game animals, while Flint created predators and disease. Saraydar (1990) suggests the Iroquois do not see the twins as polar opposites but understood their relationship to be more complex, noting ""Perfection is not to be found in gods or humans or the worlds they inhabit.""[241]
 Descriptions of Iroquois spiritual history consistently refer to dark times of terror and misery prior to the Iroquois Confederacy, ended by the arrival of the Great Peacemaker. Tradition asserts that the Peacemaker demonstrated his authority as the Creator's messenger by climbing a tall tree above a waterfall, having the people cut down the tree, and reappearing the next morning unharmed.[241] The Peacemaker restored mental health to a few of the most ""violent and dangerous men"", Ayonhwatha and Thadodaho, who then helped him bear the message of peace to others.[242]
 After the arrival of the Europeans, some Iroquois became Christians, among them the first Native American Saint, Kateri Tekakwitha, a young woman of Mohawk-Algonquin parents. The Seneca sachem Handsome Lake, also known as Ganeodiyo,[223] introduced a new religious system to the Iroquois in the late 18th century,[243] which incorporated Quaker beliefs along with traditional Iroquoian culture.[222] Handsome Lake's teachings include a focus on parenting, appreciation of life, and peace.[223] A key aspect of Handsome Lake's teachings is the principle of equilibrium, wherein each person's talents combined into a functional community. By the 1960s, at least 50% of Iroquois followed this religion.[222]
 Dreams play a significant role in Iroquois spirituality, providing information about a person's desires and prompting individuals to fulfill dreams. To communicate upward, humans can send prayers to spirits by burning tobacco.[222]
 Condolence ceremonies are conducted by the Iroquois for both ordinary and important people, but most notably when a hoyane (sachem) died. Such ceremonies were still held on Iroquois reservations as late as the 1970s.[222] After death, the soul is thought to embark on a journey, undergo a series of ordeals, and arrive in the sky world. This journey is thought to take one year, during which the Iroquois mourn for the dead. After the mourning period, a feast is held to celebrate the soul's arrival in the skyworld.
 ""Keepers of the faith"" are part-time specialists who conduct religious ceremonies. Both men and women can be appointed as keepers of the faith by tribe elders.[222]
 The Haudenosaunee thanksgiving address is a central prayer in Haudenosaunee tradition recited daily in the beginning of school days as well as social, cultural, and political events.[244] The address gives thanks to the parts of nature necessary to ecosystem sustainability and emphasizes the ideology that all animals and plants within an ecosystem are connected and each plays a vital role in it.[245]
 The phrasing of the address may vary depending on the speaker but is usually composed of 17 main sections and ends with a closing prayer. The 17 main sections are: 1) The people, 2) The Earth Mother, 3) The waters, 4) The fish, 5) plants, 6) food plants,7) medicine herbs, 8) animals, 9) trees, 10) birds, 11) four winds, 12) The Thunderers, 13) The Sun, 14) Grandmother Moon, 15) The stars, 16) The Enlightened Teachers, and 17) The Creator. Within each section, gratitude is given for the gifts that section provides to humanity.
 The address serves as a pledge of gratitude as well as a ""scientific inventory of the natural world"".[246] By describing living and non-living elements of the ecosystem and their functions, uses and benefits, the pledge instills early concepts of traditional ecological knowledge within grade school children and onward.
 The Iroquois traditionally celebrate several major festivals throughout the year.[25] These usually combine a spiritual component and ceremony, a feast, a chance to celebrate together, sports, entertainment and dancing. These celebrations have historically been oriented to the seasons and celebrated based on the cycle of nature rather than fixed calendar dates.
 For instance, the Mid-winter festival, Gi'-ye-wä-no-us-quä-go-wä (""The supreme belief"") ushers in the new year. This festival is traditionally held for one week around the end of January to early February, depending on when the new moon first occurs that year.[247]
 Iroquois ceremonies are primarily concerned with farming, healing, and thanksgiving. Key festivals correspond to the agricultural calendar, and include Maple, Planting, Strawberry, Green Maize, Harvest, and Mid-Winter (or New Year's), which is held in early February.[222] The ceremonies were given by the Creator to the Iroquois to balance good with evil.[241] In the 17th century, Europeans described the Iroquois as having 17 festivals, but only 8 are observed today. The most important of the ceremonies were the New Year Festival, the Maple Festival held in late March to celebrate spring, the Sun Shooting Festival which also celebrates spring, the Seed Dance in May to celebrate the planting of the crops, the Strawberry Festival in June to celebrate the ripening of the strawberries, the Thunder Ceremony to bring rain in July, the Green Bean Festival in early August, the Green Corn Festival in late August and the Harvest Festival in October. Of all the festivals, the most important were the Green Corn Festival to celebrate the maturing of the corn and the New Year Festival. During all of the festivals, men and women from the False Face Society, the Medicine Society and the Husk Face Society dance wearing their masks in attempt to humor the spirits that controlled nature. The most important of the occasions for the masked dancers to appear were the New Year Festival, which was felt to be an auspicious occasion to chase the malevolent spirits that were believed to cause disease.[238]
 Iroquois art from the 16th and 17th centuries as found on bowls, pottery and clay pipes show a mixture of animal, geometrical and human imagery.[215] Moose hair was sometimes attached to tumplines or burden straps for decorative effect.[215] Porcupine quillwork was sewn onto bags, clothing and moccasins, usually in geometrical designs.[215] Other designs included the ""great turtle"" upon North America was said to rest; the circular ""skydome"" and wavy designs.[215] Beads and clothes often featured semi-circles and waves which meant to represent the ""skydome"" which consisted of the entire universe together with the supernatural world above it, parallel lines for the earth and curved lines for the ""celestial tree"".[215] Floral designs were first introduced in the 17th century, reflecting French influence, but did not become truly popular until the 19th century.[215] Starting about 1850 the Iroquois art began to frequently feature floral designs on moccasins, caps, pouches and pincushions, which were purchased by Euro-Americans.[248] The British historian Michael Johnson described the Iroquois artwork meant to be sold to whites in the 19th century as having a strong feel of ""Victoriana"" to them.[248] Silver was much valued by the Iroquois from the 17th century onward, and starting in the 18th century, the Iroquois became ""excellent silversmiths"", making silver earrings, gorgets and rings.[248]
 At harvest time, Iroquois women would use corn husks to make hats, dolls, rope and moccasins.[188]
 The favorite sport of the Iroquois is lacrosse (O-tä-dä-jish′-quä-äge in Seneca).[25] Historically, a version was played between two teams of six or eight players, made up of members of two sets of clans (Wolf, Bear, Beaver, and Turtle on one side vs. Deer, Snipe, Heron, and Hawk on the other among the Senecas). The goals were two sets of poles roughly 450 yards (410 m) apart.[note 1] The poles were about 10 feet (3.0 m) high and placed about 15 feet (4.6 m) apart.[note 2] A goal was scored by carrying or throwing a deer-skin ball between the goal posts using netted sticks—touching the ball with hands was prohibited. The game was played to a score of five or seven. The modern version of lacrosse remains popular among the Haudenasaunee to this day.[249]
 The First Nations Lacrosse Association is recognized by World Lacrosse as a sovereign state for international lacrosse competitions. It is the only sport in which the Iroquois field national teams and the only indigenous people's organization sanctioned for international competition by any world sporting governing body.
 A popular winter game was the snow-snake game.[25] The ""snake"" was a hickory pole about 5–7 feet (1.5–2.1 m) long and about .25 inches (0.64 cm) in diameter, turned up slightly at the front and weighted with lead. The game was played between two sides of up to six players each, often boys, but occasionally between the men of two clans. The snake, or Gawa′sa, was held by placing the index finger against the back end and balancing it on the thumb and other fingers. It was not thrown but slid across the surface of the snow. The side whose snake went the farthest scored one point. Other snakes from the same side which went farther than any other snake of the opposing side also scored a point; the other side scored nothing. This was repeated until one side scored the number of points which had been agreed to for the game, usually seven or ten.
 The Peach-stone game (Guskä′eh) was a gambling game in which the clans bet against each other.[25] Traditionally it was played on the final day of the Green Corn, Harvest, and Mid-winter festivals. The game was played using a wooden bowl about one foot in diameter and six peach-stones (pits) ground to oval shape and burned black on one side. A ""bank"" of beans, usually 100, was used to keep score and the winner was the side who won them all. Two players sat on a blanket-covered platform raised a few feet off the floor. To play the peach stones were put into the bowl and shaken. Winning combinations were five of either color or six of either color showing.
 Players started with five beans each from the bank. The starting player shook the bowl; if he shook a five the other player paid him one bean, if a six five beans. If he shook either he got to shake again. If he shook anything else the turn passed to his opponent. All his winnings were handed over to a ""manager"" or ""managers"" for his side. If a player lost all of his beans another player from his side took his place and took five beans from the bank. Once all beans had been taken from the bank the game continued, but with the draw of beans now coming from the winnings of the player's side, which were kept out of sight so that no one but the managers knew how the game was going. The game was finished when one side had won all the beans.
 The game sometimes took quite a while to play, depending on the starting number of beans, and games lasting more than a day were common.
 Each clan has a group of personal names which may be used to name members. The clan mother is responsible for keeping track of those names not in use, which may then be reused to name infants. When a child becomes an adult he takes a new ""adult"" name in place of his ""baby"" name. Some names are reserved for chiefs or faith keepers, and when a person assumes that office he takes the name in a ceremony in which he is considered to ""resuscitate"" the previous holder. If a chief resigns or is removed he gives up the name and resumes his previous one.[250]
 Although the Iroquois are sometimes mentioned as examples of groups who practiced cannibalism, the evidence is mixed as to whether such a practice could be said to be widespread among the Six Nations, and to whether it was a notable cultural feature. Some anthropologists have found evidence of ritual torture and cannibalism at Iroquois sites, for example, among the Onondaga in the sixteenth century.[251] However, other scholars, such as anthropologist William Arens in his controversial book, The Man-Eating Myth, have challenged the evidence, suggesting the human bones found at sites point to funerary practices, asserting that if cannibalism was practiced among the Iroquois, it was not widespread.[252] Modern anthropologists seem to accept the probability that cannibalism did exist among the Iroquois,[253] with Thomas Abler describing the evidence from the Jesuit Relations and archaeology as making a ""case for cannibalism in early historic times ... so strong that it cannot be doubted.""[253] Scholars are also urged to remember the context for a practice that now shocks the modern Western society. Sanday reminds us that the ferocity of the Iroquois' rituals ""cannot be separated from the severity of conditions ... where death from hunger, disease, and warfare became a way of life"".[254]
 The missionaries Johannes Megapolensis, François-Joseph Bressani, and the fur trader Pierre-Esprit Radisson present first-hand accounts of cannibalism among the Mohawk. A common theme is ritualistic roasting and eating the heart of a captive who has been tortured and killed.[201] ""To eat your enemy is to perform an extreme form of physical dominance.""[255]
 Haudenosaunee peoples participated in ""mourning wars"" to obtain captives.[256][257][101] Leland Donald suggests that captives and slaves were interchangeable roles.[258] There have been archaeological studies to support that Haudenosaunee peoples did in fact have a hierarchical system that included slaves.[259] The term 'slave' in Haudenosaunee culture is identified by spiritual and revengeful purposes, not to be mistaken for the term in the African slave trade.[260] However, once African slavery was introduced into North America by European settlers, some Iroquois, such as Mohawk chief Joseph Brant, owned African slaves.[261]
 The Iroquois have absorbed many other individuals from various peoples into their tribes as a result of adopting war captives and giving refuge to displaced peoples. When such adoptees become fully assimilated, they are considered full members of their adoptive families, clans, and tribes. Historically, such adoptees have married into the tribes, and some have become chiefs or respected elders.
 Slaves brought onto Haudenosaunee territory were mainly adopted into families or kin groups that had lost a person.[257] Although if that person had been vital for the community they ""were usually replaced by other kin-group members"" and ""captives were ... adopted to fill lesser places"".[262] During adoption rituals, slaves were to reject their former life and be renamed as part of their ""genuine assimilation"".[263] The key goal of Haudenosaunee slavery practices was to have slaves assimilate to Haudenosaunee culture to rebuild population after one or many deaths.[262] Children[264] and Indigenous peoples of neighbouring villages[265] to the Haudenosaunee are said to have been good slaves because of their better ability to assimilate. That being said, the role of a slave was not a limited position and whenever slaves were available for capture they were taken, no matter their age, race, gender etc.[266]
 Once adopted, slaves in Haudenosaunee communities had potential to move up in society.[267] Since slaves were replacing dead nation members, they took on the role of that former member if they could prove that they could live up to it.[267] Their rights within the aforementioned framework were still limited though, meaning slaves performed chores or labor for their adoptive families.[264] Also, there are a few cases where slaves were never adopted into families and their only role was to perform tasks in the village.[257] These types of slaves may have been used solely for exchange.[268] Slave trade was common in Haudenosaunee culture and it aimed to increase Haudenosaunee population.[269]
 Slaves were often tortured once captured by the Haudenosaunee. Torture methods consisted of, most notably, finger mutilation, among other things.[270][271] Slaves endured torture not only on their journey back to Haudenosaunee nations, but also during initiation rituals and sometimes throughout their enslavement.[272] Finger mutilation was common as a sort of marking of a slave.[273] In ""Northern Iroquoian Slavery"", Starna and Watkins suggest that sometimes torture was so brutal that captives died before being adopted.[274] Initial torture upon entry into the Haudenosaunee culture also involved binding, bodily mutilation with weapons, and starvation, and for female slaves: sexual assault.[275][276][272] Starvation may have lasted longer depending on the circumstance. Louis Hennepin was captured by Haudenosaunee peoples in the 17th century and recalled being starved during his adoption as one of ""Aquipaguetin's"" replacement sons.[277] Indigenous slaves were also starved by their captors, such as Hennepin was.[276] The brutality of Haudenosaunee slavery was not without its purposes; torture was used to demonstrate a power dynamic between the slave and the ""master"" to constantly remind the slave that they were inferior.[278][279]
 Language played another role in Haudenosaunee slavery practices. Slaves were often referred to as ""domestic animals"" or ""dogs"" which were equivalent to the word to ""slave"".[280] This use of language suggests that slaves were dehumanized, that slaves were ""domesticated"" and another that slaves were to be eaten as Haudenosaunee peoples ate dogs.[281][282] Jacques Bruyas wrote a dictionary of the Mohawk language where the word gatsennen is defined as ""Animal domestique, serviteur, esclave"" (English: ""domestic animal, butler, slave"").[283] There are also more language accounts of slaves being compared to animals (mostly dogs) in Oneida and Onondaga language.[280] This language serves as a proof not only that slavery did exist, but also that slaves were at the bottom of the hierarchy.[284]
 Haudenosaunee slavery practices changed after European contact. With the arrival of European-introduced infectious diseases came the increase in Haudenosaunee peoples taking captives as their population kept decreasing.[285][286] During the 17th century, Haudenosaunee peoples banded together to stand against settlers.[287] By the end of the century, Haudenosaunee populations were made up mostly of captives from other nations.[268] Among the Indigenous groups targeted by the Haudenosaunee were the Wyandot who were captured in such large numbers that they lost their independence for a large period of time.[268][288] ""Mourning wars"" became essential to rebuilding their numbers, while at the same time Haudenosaunee warriors began launching raids on European colonial settlements.[268][289] Similarly to Indigenous slaves, European slaves were tortured by the Haudenosaunee using finger mutilation and sometimes cannibalism.[270] European captives did not make good slaves because they resisted even more than Indigenous captives and did not understand rituals such as renaming and forgetting their past.[290] For this reason most European captives were either used as ransom or murdered upon arrival to Haudenosaunee territory.[268] Many Europeans who were not captured became trading partners with the Haudenosaunee.[285] Indigenous slaves were now being traded among European settlers and some slaves even ended up in Quebec households.[285] Eventually, European contact led to adoptees outnumbering the Haudenosaunee in their own communities. The difficulty of controlling these slaves in large numbers ended Haudenosaunee slavery practices.[268]
 The Grand Council of the Six Nations is an assembly of 56 Hoyenah (chiefs) or sachems. Sachemships are hereditary within a clan. When a position becomes vacant a candidate is selected from among the members of the clan and ""raised up"" by a council of all sachems. The new sachem gives up his old name and is thereafter addressed by the title.
 Today, the seats on the Council are distributed among the Six Nations as follows:
 When anthropologist Lewis Henry Morgan studied the Grand Council in the 19th century, he interpreted it as a central government. This interpretation became influential, but Richter argues that while the Grand Council served an important ceremonial role, it was not a government in the sense that Morgan thought.[36][37][38] According to this view, Iroquois political and diplomatic decisions are made on the local level and are based on assessments of community consensus. A central government that develops policy and implements it for the people at large is not the Iroquois model of government.
 Unanimity in public acts was essential to the Council. In 1855, Minnie Myrtle observed that no Iroquois treaty was binding unless it was ratified by 75% of the male voters and 75% of the mothers of the nation.[291] In revising Council laws and customs, a consent of two-thirds of the mothers was required.[291] The need for a double supermajority to make major changes made the Confederacy a de facto consensus government.[292]
 The women traditionally held real power, particularly the power to veto treaties or declarations of war.[291] The members of the Grand Council of Sachems were chosen by the mothers of each clan. If any leader failed to comply with the wishes of the women of his tribe and the Great Law of Peace, the mother of his clan could demote him, a process called ""knocking off the horns"". The deer antlers, an emblem of leadership, were removed from his headgear, thus returning him to private life.[291][293]
 Councils of the mothers of each tribe were held separately from the men's councils. The women used men as runners to send word of their decisions to concerned parties, or a woman could appear at the men's council as an orator, presenting the view of the women. Women often took the initiative in suggesting legislation.[291]
 The term ""wampum"" refers to beads made from purple and white mollusk shells on threads of elm bark.[189] Species used to make wampum include the highly prized quahog clam which produces the famous purple colored beads. For white colored beads the shells from the channeled whelk, knobbed whelk, lightning whelk, and snow whelk are used.[294]
 Wampum was primarily used to make wampum belts by the Iroquois, which Iroquois tradition claims was invented by Hiawatha to console chiefs and clan mothers who lost family members to war.[189] Wampum belts played a major role in the Condolence Ceremony and in the raising of new chiefs.[189] Wampum belts are used to signify the importance of a specific message being presented. Treaty making often involved wampum belts to signify the importance of the treaty.[189] A famous example is ""The Two Row Wampum"" or ""Guesuenta"", meaning ""it brightens our minds"", which was originally presented to the Dutch settlers, and then French, representing a canoe and a sailboat moving side-by-side along the river of life, not interfering with the other's course. All non-Native settlers are, by associations, members of this treaty. Both chiefs and clan mothers wear wampum belts as symbol of their offices.[189]
 ""The Covenant Belt"" was presented to the Iroquois at the signing of the Canandaigua Treaty. The belt has a design of thirteen human figures representing symbolically the Thirteen Colonies of the U.S. The house and the two figures directly next to the house represent the Iroquois people and the symbolic longhouse. The figure on the left of the house represent the Seneca Nation who are the symbolic guardians of the western door (western edge of Iroquois territory) and the figure to the right of the house represents the Mohawk who are the keepers of the eastern door (eastern edge of Iroquois territory).[294]
 The Hiawatha belt is the national belt of the Iroquois and is represented in the Iroquois Confederacy flag. The belt has four squares and a tree in the middle which represents the original Five Nations of the Iroquois. Going from left to right the squares represent the Seneca, Cayuga, Oneida and Mohawk. The Onondaga are represented by an eastern white pine which represents the Tree of Peace. Traditionally the Onondaga are the peace keepers of the confederacy. The placement of the nations on the belt represents the actually geographical distribution of the six nations over their shared territory, with the Seneca in the far west and the Mohawk in the far east of Iroquois territory.[294]
 The Haudenosaunee flag created in the 1980s is based on the Hiawatha Belt ... created from purple and white wampum beads centuries ago to symbolize the union forged when the former enemies buried their weapons under the Great Tree of Peace.""[295] It represents the original five nations that were united by the Peacemaker and Hiawatha. The tree symbol in the center represents an Eastern White Pine, the needles of which are clustered in groups of five.[296]
 Historians in the 20th century have suggested the Iroquois system of government influenced the development of the U.S. government,[297][298] although the extent and nature of this influence has been disputed.[299] Bruce Johansen proposes that the Iroquois had a representative form of government.[300]
 Consensus has not been reached on how influential the Iroquois model was to the development of U.S. documents such as the Articles of Confederation and the U.S. Constitution.[301] The influence thesis has been discussed by historians such as Donald Grinde[302] and Bruce Johansen.[303] In 1988, the U.S. Congress passed a resolution to recognize the influence of the Iroquois League upon the Constitution and Bill of Rights.[304] In 1987, Cornell University held a conference on the link between the Iroquois' government and the U.S. Constitution.[305]
 Scholars such as Jack N. Rakove challenge this thesis. Stanford University historian Rakove writes, ""The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois"" and notes that there are ample European precedents to the democratic institutions of the U.S.[306] In reply, journalist Charles C. Mann wrote that while he agreed that the specific form of government created for the U.S. was ""not at all like"" that of the Iroquois, available evidence does support ""a cultural argument – that the well-known democratic spirit had much to do with colonial contact with the Indians of the eastern seaboard, including and especially the Iroquois,"" and (quoting Rakove) ""that prolonged contact between the aboriginal and colonizing populations were important elements [sic] in the shaping of colonial society and culture.""[307] Historian Francis Jennings noted that supporters of the thesis frequently cite the following statement by Benjamin Franklin, made in a letter from Benjamin Franklin to James Parker in 1751:[300] ""It would be a very strange thing, if six Nations of ignorant savages should be capable of forming a Scheme for such a Union ... and yet that a like union should be impracticable for ten or a Dozen English Colonies,"" but he disagrees that it establishes influence. Rather, he thinks Franklin was promoting union against the ""ignorant savages"" and called the idea ""absurd"".[89]
 The anthropologist Dean Snow has stated that although Franklin's Albany Plan may have drawn inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from that source. He argues that ""... such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception.""[308]
 Similarly, the anthropologist Elisabeth Tooker has concluded that ""there is virtually no evidence that the framers borrowed from the Iroquois."" She argues that the idea is a myth resulting from a claim made by linguist and ethnographer J.N.B. Hewitt that was exaggerated and misunderstood after his death in 1937.[309] According to Tooker, the original Iroquois constitution did not involve representative democracy and elections; deceased chiefs' successors were selected by the most senior woman within the hereditary lineage in consultation with other women in the tribe.[309]
 The Haudenosaunee people, living mainly in present-day New York and Pennsylvania, had many encounters with European colonial powers, primarily the English, Dutch, and French.
 The Dutch respected Haudenosaunee land claims and were peaceful with the Haudenosaunee, specifically the Mohawk people. Trying to avoid their own Black Legend, the Dutch established trade and an allyship with the Mohawk people. By the 1640s Dutch traders were exporting thousands of furs a year, most of which were traded from the Mohawks. The Mohawks used their monopoly over the Fort Orange (Albany) market to set prices. Many of the furs the Mohawks sold were stolen from other indigenous enemies around the St Lawrence River region and then traded to the Dutch. While the Dutch had strong relations with the Mohawks, they fell into conflict with other indigenous peoples like the Delawares.[310]
 Initially, English rule around the Haudenosaunee strengthened their position. In the mid-1670s, New York governor Sir Edmund Andros allied with the Haudenosaunee in what was known as the Covenant Chain. During the Covenant Chain, the English and Haudenosaunee reinforced each other. The English and Haudenosaunee would join to fight Native rivals and the French. Andros accepted the Haudenosaunee land claim in the vast area stretching to the Ohio River. Starting in the 1680s, natives around the Great Lakes and Ohio Valley would regroup and with French aid pushed the Haudenosaunee back east. The Haudenosaunee would continue to support the English during the Seven Years' War from 1754 to 1763. English respect of Haudenosaunee land claims was starting to diminish and by the end of the 18th century, the Haudenosaunee would adopt a policy of neutrality with the European empires while continuing to profit off the fur trade.[310]
 The Grand Council of the Iroquois Confederacy declared war on Germany in 1917 during World War I and again in 1942 in World War II.[311]
 The Haudenosaunee government has issued passports since 1923, when Haudenosaunee authorities issued a passport to Cayuga statesman Deskaheh (Levi General) to travel to the League of Nations headquarters.[312]
 More recently, passports have been issued since 1997.[313] Before 2001 these were accepted by various nations for international travel, but with increased security concerns across the world since the September 11 attacks, this is no longer the case.[314] 
In 2010, the Iroquois Nationals lacrosse team was allowed by the U.S. to travel on their own passports to the 2010 World Lacrosse Championship in England only after the personal intervention of Secretary of state Hillary Clinton. However, the British government refused to recognize the Iroquois passports and denied the team members entry into the United Kingdom.[315][316]
 The Onondaga Nation spent $1.5 million on a subsequent upgrade to the passports designed to meet 21st-century international security requirements.[317]
 The first five nations listed below formed the original Five Nations (listed from east to west, as they were oriented to the sunrise); the Tuscarora became the sixth nation in 1722.
 Within each of the six nations, people belonged to a number of matrilineal clans. The number of clans varies by nation, currently from three to eight, with a total of nine different clan names.
 Modern scholarly estimates of the 17th century population of the Iroquois have ranged from 5,500[319] to more than 100,000.[320] When it comes to eye-witness estimates (that is, contemporary estimates) Marc Lescarbot estimated the Iroquois in year 1609 at 8,000 warriors (that is around 40,000 people) and baron L. A. de Lahontan estimated the Iroquois population around year 1690 at 70,000 people (on average 14,000 in each of five tribes).[321] Iroquois territory in the 16th century and at the beginning of the 17th century was over 75,000 square km (over 29,000 square mi).[322] John R. Swanton enumerated a total of 226 Iroquois villages and towns (but most were not occupied at the same time as the Iroquois moved villages every five to twenty years).[323][324] On the contrary Lewis H. Morgan in his 1851 book estimated the Iroquois population in year 1650 at 25,000 people, including 10,000 Seneca, 5,000 Mohawk, 4,000 Onondaga, 3,000 Oneida and 3,000 Cayuga.[321] The Seneca were also estimated at 13,000 in year 1672 and 15,000 in year 1687.[321] In 1713–1722, the Iroquois population was augmented when the Tuscarora migrated north to New York and joined them as the sixth nation.[325]
 More recent estimates by Snow and Jones of the Iroquois population have been about 20,000. Jones' estimate applies to the period preceding the first known epidemics of Old World diseases impacting the Iroquois in the mid-17th century. After an archaeological investigation and dating of all 125 Iroquois villages known to have been occupied between 1500 and 1700 (fewer than 226 listed by Swanton occupied at any time), Jones estimated the total pre-epidemic Iroquois population at 20,000 in 1620–1634. In the post-epidemic period from 1634 to 1660 he estimates the total Iroquois population at 8,000. The latter figure does not include the thousands of people adopted into the Iroquois from conquered ethnic groups.[326] The Iroquois had a liberal and successful adoption policy that allowed them to recoup their population losses and gave them an adaptive advantage over their foes who were unable to do the same. In 1658, the Jesuits noted that the Iroquois contained more adopted foreigners than natives of the country.[327]
 In 1779 between 40 and 60 Iroquois towns and villages were destroyed by the Sullivan Expedition in a scorched earth operation. More than 5,000 Iroquois fled to British Canada and an unknown number remained in the U.S. According to one estimate 4,500 died in the aftermath of the expedition, including many who fled to Canada.[328][329][330]
 In 1907 there were 17,630 Iroquois[331] and in 1923 there were 8,696 Iroquois in the USA and 11,355 in Canada, for a total of 20,051.[332]
 According to data compiled in 1995 by Doug George-Kanentiio, a total of 51,255 Six Nations people lived in Canada. These included 15,631 Mohawk in Quebec; 14,051 Mohawk in Ontario; 3,970 Oneida in Ontario; and a total of 17,603 of the Six Nations at the Grand River Reserve in Ontario.[333] More recently according to the Six Nations Elected Council, some 12,436 on the Six Nations of the Grand River reserve, the largest First Nations reserve in Canada,[334] as of December 2014 and 26,034 total in Canada.[335]
 In 1995, tribal registrations among the Six Nations in the U.S. numbered about 30,000 in total, with the majority of 17,566 in New York. The remainder were more than 10,000 Oneida in Wisconsin, and about 2200 Seneca-Cayuga in Oklahoma.[333] As the nations individually determine their rules for membership or citizenship, they report the official numbers. (Some traditional members of the nations refuse to be counted.)[333] There is no federally recognized Iroquois nation or tribe, nor are any Native Americans enrolled as Iroquois.
 In the 2000 U.S. census, 80,822 people identified as having Iroquois ethnicity (which is similar to identifying as European), with 45,217 claiming only Iroquois ancestry. There are the several reservations in New York: Cayuga Nation of New York(~450,[336]) St. Regis Mohawk Reservation (3,288),[336] Onondaga Reservation (468),[336] Oneida Indian Nation (~ 1000[336]), Seneca Nation of New York (533[336]) and the Tuscarora Reservation (1,138 in 2000[336]). Some lived at the Oneida Nation of Wisconsin: some 21,000, according to the 2000 census. Seneca-Cayuga Nation in Oklahoma has more than 5,000 people in 2011.[337] In the 2010 Census, 81,002 persons identified as Iroquois, and 40,570 as Iroquois only across the U.S.[338] Including the Iroquois in Canada, the total population numbered over 125,000 as of 2009.[14]
 In the 2020 U.S. census in total 113,814 people identified as Iroquois.[339]
 Several communities exist to this day of people descended from the tribes of the Iroquois confederacy.
"
Tanacharison,https://en.wikipedia.org/wiki/Tanacharison,"Tanacharison (/ˌtænəxəˈrɪsən/; c. 1700 – 4 October 1754), also called Tanaghrisson (/ˌtænəˈɡrɪsən, ˌtænəxˈrɪsən/), was a Native American leader who played a pivotal role in the beginning of the French and Indian War. He was known to European-Americans as the Half-King, a title also used to describe several other historically important Native American leaders. His name has been spelled in a variety of ways.[a]
 Little is known of Tanacharison's early life. He was born into the Catawba tribe whose lands and villages were along what now called the Catawba River in South Carolina (Not Buffalo, NY).
 Tanacharison first appears in historical records in 1747, living in Logstown (near present Ambridge, Pennsylvania), a multi-ethnic village about 20 miles (30 kilometers) downstream from the forks of the Ohio River. Those Iroquois who had migrated to the Ohio Country were generally known as 'Mingos,' and Tanacharison emerged as a Mingo leader at this time. He also represented the Six Nations at the 1752 Treaty of Logstown, where he was referred to as ""Thonariss, called by the English the half King"".[2] At this treaty, he spoke on behalf of the Six Nations' Grand Council, but also made clear that the council's ratification was required, in accordance with the Iroquois system of government.
 According to the traditional interpretation, the Grand Council had named Tanacharison as leader or ""half-king"" (a sort of viceroy) to conduct diplomacy with other tribes, and to act as spokesman to the British on their behalf. However, some modern historians have doubted this interpretation, asserting that Tanacharison was merely a village leader, whose actual authority extended no further than his own village. In this view, the title ""half king"" was probably a British invention, and his ""subsequent lofty historical role as a Six Nations 'regent' or 'viceroy' in the Ohio Country was the product of later generations of scholars.""[3]
 In 1753, the French began the military occupation of the Ohio Country, driving out British traders and constructing a series of forts. British colonies, however, also claimed the Ohio Country. Robert Dinwiddie, the lieutenant governor of Virginia, sent a young George Washington to travel to the French outposts and demand that the French vacate the Ohio Country. On his journey, Washington's party stopped at Logstown to ask Tanacharison to accompany them as a guide and as a ""spokesman"" for the Ohio Indians. Tanacharison agreed to return the symbolic wampum he had received from French captain Philippe-Thomas Chabert de Joncaire. Joncaire's first reaction, on learning of this double cross, was to mutter of Tanacharison, ""He is more English than the English."" But Joncaire masked his anger and insisted that Tanacharison join him in a series of toasts. By the time the keg was empty, Tanacharison was too drunk to hand back the wampum.[4] 
 Tanacharison traveled with Washington to meet with Jacques Legardeur de Saint-Pierre, the French commander of Fort Le Boeuf in what is now Waterford, Pennsylvania. There he tried to return the wampum to Saint-Pierre, ""who evaded taking it, & made many fair Promises of Love & Friendship; said he wanted to live in Peace & trade amicably with them; as a Proof of which, he wou’d send some Goods immediately down to the Logstown for them.""[5]  The French refused to vacate, however, and to Washington's great consternation, they tried to court Tanacharison as an ally. Although fond of their brandy, he remained a strong francophobe.
 Tanacharison had requested that the British construct a ""strong house"" at the Forks of the Ohio and early in 1754 he placed the first log of an Ohio Company stockade there, railing against the French when they captured it.  He was camped at Half King's Rock on May 27, 1754 when he learned of a nearby French encampment and sent word urging an attack to Washington at the Great Meadows, about five miles (8 km) east of Chestnut Ridge in what is now Fayette County, Pennsylvania (near Uniontown). Washington immediately ordered 40 men to join Tanacharison and at sunset followed with a second group, seven of whom got lost in heavy rain that night.  It was dawn on May 28 before Washington reached the Half King's Rock.
 After a hurried war council, the English and Tanacharison's eight or nine warriors set off to surround and attack the French in the Battle of Jumonville Glen, who quickly surrendered.  The French commander, Ensign Joseph Coulon de Jumonville, was among the wounded. In one of several disputed and contradictory accounts, it is claimed that Tancharison uttered the French words, ""Tu n'es pas encore mort, mon père!"" (Thou art not yet dead, my father), then sank his tomahawk in Jumonville's skull, washed his hands with the brains, ""and scalped him,"" but not before eating a portion of Jumonville's brain.[6] Only one of the wounded French soldiers was not killed and scalped among a total of ten dead, 21 captured, and one missing, a man named Monceau who had wandered off to relieve himself that morning.
 Monceau witnessed the French surrender before walking barefoot to the Monongahela River and paddling down it to report to Contrecoeur, commanding at Fort Duquesne.  Tanacharison sent a messenger to Contrecoeur the following day with news that the British had shot Jumonville and, but for the Indians, would have killed all the French.  A third (and more accurate) account of the Jumonville Glen encounter was told to Jumonville's half-brother, Captain Louis Coulon de Villiers, by a deserter at the mouth of Redstone Creek during his expedition to avenge his brother's murder.
 Washington was without Indian allies on July 3, 1754 at the battle of Fort Necessity, his hastily erected stockade at the Great Meadows. Tanacharison scornfully called the fort ""that little thing upon the meadow"" and complained that Washington would not listen to advice, and that Washington treated the Indians like slaves. He and another Seneca leader, Queen Aliquippa, had taken their people to Wills Creek.[clarification needed] Outnumbered and with supplies running low, Washington surrendered the fort, later blaming Captains George Croghan and Andrew Montour for ""involving the country in great calamity"".[7]
 Tanacharison had a long relationship with George Croghan, a fur trader, interpreter, and diplomat among the Native Americans who had been appointed a member of the Iroquois' Onondaga Council. Tanacharison had been ""one of the sachems who had confirmed Croghan in his land grant of 1749,""[8] 200,000 acres minus about two square miles at the Forks of the Ohio for a British fort.  Thomas Penn and Pennsylvania planned to build a stone fort, but Croghan realized that his deeds would be invalid if in Pennsylvania, and had Andrew Montour testify before the Assembly in 1751 that the Indians did not want the fort and that it had all been Croghan's idea, scuttling the project.[9]
 
In 1752, Croghan was on the Indian council that granted Virginia's Ohio Company permission to build the fort.  Tanacharison's introduction of Croghan to the Virginia commissioners suggests that Croghan organized and led the 1748 Ohio Indian Confederation, which appointed Croghan as the colony's representative in negotiations, and that Pennsylvania recognized as independent of the Six Nations: The Ohio Company fort was surrendered to the French by Croghan's half-brother, Edward Ward, and commanded by his business partner, William Trent. The role of Croghan (who was Pittsburgh's president judge for Virginia and chairman of Pittsburgh's Committee of Safety after Pontiac's War) remains uncertain, since Croghan was later declared a traitor by General Edward Hand and exiled from the frontier.[citation needed]
 Shortly after the battle of Jumonville Glen, Tanacharison moved his people and the old queen Aliquippa east to Croghan's Aughwick plantation in the Aughwick Valley near present Shirleysburg, Pennsylvania.  There Tanacharison became seriously ill and was taken to the farm of John Harris at Paxtang, Pennsylvania (near present-day Harrisburg, Pennsylvania).  He would take no active part in the remainder of the war and died of pneumonia on October 4, 1754. Aliquippa died on December 23, 1754.[citation needed]
"
Logstown,https://en.wikipedia.org/wiki/Logstown,"

 The riverside village of Logstown (1726?, 1727–1758) also known as Logg's Town, French: Chiningue[1]: 356  (transliterated to Shenango) near modern-day Baden, Pennsylvania, was a significant Native American settlement in Western Pennsylvania and the site of the 1752 signing of the Treaty of Logstown between the Ohio Company, the Colony of Virginia, and the Six Nations, which occupied the region.  Being an unusually large settlement, and because of its strategic location in the Ohio Country, an area contested by France and England, Logstown was an important community for all parties living along the Ohio and tributary rivers. Logstown was a prominent trade and council site for the contending British and French colonial governments, both of which made abortive plans to construct forts near the town.[2] Logstown was burned in 1754 and although it was rebuilt, in the years following the French and Indian War it became poor and was eventually abandoned.
 Logstown is located in Harmony Township, about 14 miles northwest of the Forks of the Ohio (now in downtown Pittsburgh) in an area on the east bank of the Ohio River opposite Aliquippa. The site is also due north of the Pittsburgh International Airport. Today the site is marked with a stone bearing a brass plaque placed there by the Fort McIntosh chapter of the Daughters of the American Revolution  in 1932, memorializing the visit of Major George Washington to the town in November, 1753.[3]: 8 [4]
 A few sources claim that in 1747 the French built about 30 log cabins, some with stone chimneys, on a plateau above the original Logstown village,[5][6]: 326  and that these log cabins supposedly gave the town its name.[7] However, George P. Donehoo says that the name ""was probably due to the fact that large numbers of logs were left upon the flat after the floods in the Ohio River.""[8][9]
 Donehoo and several other sources report that the original Lenape name of the village was Maughwawame,[10]: 42  which translates to ""extensive flats.""[1]: 356 [9]
 The French referred to the town as ""Chiningue"" which Father Joseph Pierre de Bonnecamps notes was their designation and not a Native name: ""We called it Chiningue, from its vicinity to a river of that name.""[11]: 142–43  Donehoo says that chiningue is a French word for beaver, but also suggests that it may be a corruption of ochenango, a Seneca word meaning ""large bull thistles.""[9]
 The original village was settled by Shawnees, possibly as early as 1725[12] or 1730[13] on low-lying land less than a mile north of present-day Ambridge in Beaver County, Pennsylvania.[5]  The population grew as groups of Lenape, Cayugas, Senecas, and Shawnees migrated west into the Ohio River Valley seeking to escape a smallpox epidemic in 1733 and a drought in 1741, creating a multi-ethnic community.[14][15] In August 1744 the town's population increased significantly when Kakowatcheky arrived with his band of Shawnee warriors and their families from Wyoming Valley, Pennsylvania.[16] Kakowatcheky is sometimes credited with founding Logstown.[1]: 355–56 [17]: 88  Another early resident was Opessa Straight Tail, who moved to Logstown some time before 1750.[18] The town's population varied from approximately 200 to 500 people. In 1749 Céloron de Blainville observed fifty cabins housing about sixty warriors,[19]: 30  suggesting a population of 200 to 250 total, while in late 1758 George Croghan noted forty houses for about one hundred and twenty warriors,[20]: 97–98  suggesting a total population of 350 to 500.
 In late April 1745, the Pekowi Shawnee leader Peter Chartier and about 400 Shawnees, including Meshemethequater and Neucheconeh, stopped at Logstown to visit Kakowatcheky and to try to persuade him to join them. Chartier was angry with the Provincial government of Pennsylvania for their failure to control the sale of alcohol in Shawnee communities, and his plan at that time was to bring as many Shawnees as he could over to French protection. He was on his way to Lower Shawneetown to address the Shawnees living there. Kakowatcheky, however, refused to join him, and Chartier and his people left Logstown after a brief stay.[13]
 Most sources agree that the main section of the town was built on the broad, flat floodplain along the east bank of the Ohio River, with a few structures located on the west bank. Gardens and cornfields were planted on both sides of the river, on fertile, alluvial flatlands, where the town's residents cultivated maize, beans, squash, gourds, tobacco, and sunflowers. One source states that ""in the year 1752...the Shawanese Inhabited Loggs Town, on the West Side of the Ohio, and tended Corn on the East Side of the River.""[21]: 975  On the east bank of the river, a few homes were built on a grassy terrace above the floodplain, the so-called ""upper town.""[22]: 103–106 
 In 1747, the Six Nations Confederacy Haudenosaunee sent two headmen as emissaries to live in Logstown and supervise the Iroquois allies: Tanacharison,[23][24] a Seneca, and Scarouady, an Oneida.[25]: 45  Tanacharison oversaw the Delawares and Scarouady supervised the Shawnees.[24]
 The provincial government of Pennsylvania was anxious to keep Native Americans in the Ohio Valley from being influenced by the French. As early as 1731, agents from Montreal had visited communities along the Ohio River, distributing goods and urging the tribes to send emissaries to Quebec to establish alliances.[1]: 183–84  On September 18, 1747, George Croghan wrote to Thomas Lawrence in Philadelphia, a member of the Pennsylvania Provincial Council, that one of his men, who had just come ""down from ye Woods,"" had informed him that ""the Indians at this side of the Lake Erie are making war very briskly against the French, but is very impatient to hear from their brothers, ye English, expecting a present of powder and lead; which, if they don't get, I am of opinion, by the best accounts, that they will turn to the French.""[1]: 328–29  In November 1747 Scarouady and other Iroquois leaders visited Philadelphia to sign the ""Treaty Between the President and Council of the Province of Pennsylvania and the Ohio Indians,""[26] promising a military alliance against the French in exchange for supplies and trade goods.[27] The Council obtained £200 worth of goods and sent Croghan to Logstown in April, 1748 to cement the terms of the treaty and secure the tribes' loyalty to the British.[13]: 132  Conrad Weiser was to follow in August with more gifts. Croghan met in council with 1500 men at Logstown, and finding the Council's gifts insufficient for all of them, he added an additional £224 in powder, lead, knives, flints, brass wire, and tobacco from his own stock. This gesture was influential in maintaining the alliance between the British and the Ohio tribes.[28]
 In late July, fifty-five representatives of the Six Nations, Delawares, Shawnees, Nanticokes, and Twightwees met at the courthouse in Lancaster, Pennsylvania and signed a peace treaty with the Pennsylvania Provincial Council.[29] This treaty guaranteed commercial access to tribes across the Ohio Valley as far west as the Wabash River, an unprecedented diplomatic achievement for the English.[30][31]
 In 1748, the colony of Pennsylvania sent Conrad Weiser, Pennsylvania's ambassador to the Six Nations, to Logstown.[27]  Arriving on August 27, he set up his headquarters in Croghan's trading post and visited the surrounding villages.[32]: 347–358   Soon a large number of Delaware, Shawnee, Iroquois and Wyandot Indians gathered at Logstown,[33] including the Wyandot chief Orontony and five other leaders from Kuskusky, who ""behav’d like People of good Sense & Sincerity.""[34]: 43  Weiser met each tribe separately and then in a general council he announced that King George's War had ended and that England and France signed a peace treaty. As a result, the English had no more war supplies for them and he distributed gifts instead.[35] The chiefs complied with his request for a count of their warriors in the Ohio Valley region:[5]: 7 
 Weiser was accompanied by ""English traders, of whom there were above twenty,"" intending to formally establish trade with tribes represented at the council, to create a stronger relationship that would further exclude the French from operating in the region. During the council, a trader from Maryland named Nolan arrived with 30 gallons of rum and began to sell it to the Indians, much to the dismay of Weiser and Croghan, who were afraid that violence would erupt if the Indians drank too much. Several of the Logstown leaders were also unhappy, as they had petitioned the Pennsylvania provincial government as early as 1734 to restrict the sale of alcohol in Native American communities because of the social and economic problems it caused.[1] Croghan eventually decided to break open the kegs and spill the rum, in accordance with a newly-enacted Pennsylvania statute issued by Lieutenant-Governor George Thomas.[36][31]: 191–2 
 After the gifts had been distributed, the chiefs told Weiser and Croghan, ""Our brethren the White Men have indeed tied our hearts to theirs. We at present can but return thanks with an empty hand till another opportunity serves to do it sufficiently...In the meantime, look upon us as your true brothers.""[31]
 Among those accompanying Weiser was Benjamin Franklin's illegitimate son, William Franklin, only nineteen at the time, probably sent by his father as a part of his education. Franklin kept a journal of his trip which Lewis Evans used in making his 1749 map.[37] William's journey subsequently inspired his father's keen interest in the frontier.[28]
 In 1749, the Comte de La Galissonière wanted to strengthen French control over the Ohio Country, and in August he ordered the military commander at Detroit, Pierre Joseph Céloron de Blainville to travel down the Ohio River to demonstrate French dominance. Leading a force of eight officers, six cadets, an armorer, 20 soldiers, 180 Canadians, 30 Iroquois and 25 Abenakis,[11] Céloron moved down the river on a flotilla of 23 large boats and birch-bark canoes, on his ""lead plate expedition,"" burying lead plates at six locations where major tributaries entered the Ohio and nailing copper plates bearing royal arms to trees to claim the territory for New France.[38]
 Céloron arrived at Logstown on August 8, 1749. The Shawnee chief Kakowatcheky, fearing an assault, rallied the town's population in its defense. According to William Trent, ""the Indians ran to their arms and hoisted the English Colors. Cawcaw-wi-cha-ke, the Shawnese King about 114 years of age, set his back against the flag staff with his gun in his hand and desired the young men to kill them all.""[39]: 358  Céloron was enraged by the sight of the British flag, but noted the armed warriors, writing in his journal:
 The Jesuit priest Joseph Pierre de Bonnecamps, who accompanied Céloron, wrote about Logstown, which he called by its French name: ""The village of Chiningué is quite new; it is hardly more than five or six years since it was established. The savages who live there are almost all Iroquois; they count about sixty warriors.""[40]: 176 
 Céloron reported that he was informed that warriors in Logstown had planned to attack his camp during the first night, but that his well-armed force, sentinels, and carefully planned encampment discouraged them from doing so. Later that evening ""the Chiefs, accompanied by thirty or forty braves, came to salute me."" They apologized for the English flag, saying that it had been put on display by some young men ""for show...and without perceiving the consequences,"" adding that ""our heart is entirely French.""[11]
 In contrast, William Trent recorded that as the warriors arrived, ""every man discharged his gun loaded with ball & large shot into the ground between the Frenchmen's legs which almost blinded them & covered them with dirt. The Indians then came to the English traders [living in the town] and asked them if they should kill them, the English took pity on them, seeing Monsieur Céloron & his people much dejected & trembling with fear as they were sure of certain death should the traders advise them to it.""[39]: 359 
 Céloron described Logstown and its inhabitants briefly:
 Céloron discovered some British traders living in Logstown. Incensed, he warned them to leave this territory which belonged to France.[41] and wrote a scolding note to the governor of Pennsylvania,[42]: 26  which stated in part:
 During the night, Céloron was warned by Chabert de Joncaire that preparations were again being made in Logstown to attack the French camp, and he gave orders to his men to prepare for battle. He then sent Joncaire (who had lived in a Seneca village and spoke the language fluently)[43][44]: 298  to advise the chiefs that the French were aware of their plans. Céloron writes that the warriors did not attack, but ""filed before my camp and made the accustomed salute."" Chiefs from the village visited the French camp the next day with pipes of peace, and Céloron reprimanded them for contemplating violence, adding: ""I know how to make war, and those who have made war with us ought to know it, too.""[11]: 32 
 The following day, 10 August 1749, Céloron delivered a prepared message from the Marquis de La Galissonière, the Governor of New France, which described how the English were deceiving the Ohio tribes and planning their ""total ruin,"" adding: ""I know the English only inspire you with evil sentiments, and, besides, intend, through their establishments on the Beautiful River, which belongs to me, to take it from me.""[11]: 33  The aged chief Kakowatcheky, listening in the audience, was apparently outraged. George Croghan, who arrived in Logstown a few days after Céloron had left, told Richard Peters that
 Afterwards, Céloron called the English traders who were living in Logstown to meet with him, ""to whom I addressed a summons to retire into their own territory with all their servants...They answered...that they would do so, that they knew well they had no right to trade on the Beautiful River."" Bonnecamps wrote in his own diary:
 Céloron then distributed gifts and departed from Logstown on 12 August, proceeding downriver to Lower Shawneetown. The expulsion of the British traders and Céloron's condescending attitude irritated the Shawnees, some of whom returned to their home villages, ""tearing down and trampling underfoot with contempt"" the French copper plates as they went.""[46]: 666 [47]: 155 
 Governor James Hamilton sent George Croghan to Logstown as soon as they learned of Céloron's visit, to find out how the Indians had reacted to the French expedition. Croghan arrived in late August, only a few days after Céloron's departure, and reported that the Indians had told Céloron ""that the land was their own, and while there were any Indians in those parts they would continue to trade with the English,"" adding that “to separate them from their brothers, the English, would be like cutting a man in two halves and then expecting him to live.”[1]: 359 
 In September, 1750 the Ohio Company ordered Christopher Gist to survey lands along the Ohio to find an area of 200,000 acres that the Company could take possession of, according to a 1749 grant from King George II of England.[48] Gist was instructed to
 Gist arrived in Logstown on 25 November, describing the path of the Ohio River as it appeared before reaching the town: ""Down the River...to Loggs Town, the Lands these last 8 [miles] very rich, the Bottoms above a Mile wide, but on the SE side, scarce a Mile wide, the Hills high and steep. In the Loggs Town, I found scarce any Body but a Parcel of reprobate Indian Traders, the Chiefs of the Indians being out a hunting."" In the town he found the people suspicious of his reasons for being there, as it was already clear to the Natives that the colonial governments were hoping to take possession of the land: ""The People in this Town, began to enquire my Business, and because I did not readily inform them, they began to suspect me, and said, I was come to settle the Indian's Lands and they knew I should never go Home again safe."" Gist invented a reason for his visit, that ""I had a Message to deliver the Indians from the King, by Order of the President of Virginia,"" which ""obtained me Quiet and Respect among them."" Gist spent one night in the town and left the next day, observing the river downstream from the town: ""The Bottoms upon the River below the Logg's Town very rich but narrow, the high Land pretty good but not very rich.""[20]
 After Céloron returned and reported his experiences, the new Governor-General of New France, the Marquis de la Jonquière, decided to send Philippe-Thomas Chabert de Joncaire back to Logstown to establish a permanent French base there.[43] In early July 1750, Joncaire set out from Montreal with a staff of eight cadets and four soldiers, in addition to two Cayuga guides. They traveled with two canoes loaded with goods, including powder and shot, intended as gifts for the Indians and for trade. They proceeded down the Allegheny to Logstown, where Joncaire had orders to establish a trading-house, two stories high, its walls fitted with crénelés, (loopholes) for defense. Joncaire was directed to explore the region, to learn all he could about the Monongahela River, to find a new route from southern Ohio to Lake Erie, to visit Lower Shawneetown and establish relations with the chiefs there, and finally, to report back to Céloron in Detroit.[44]: 359–60 
 
Throughout September and October the Pennsylvania government received reports that a Frenchman named ""Jean Coeur,"" or ""John Ceur"" was traveling up and down the Ohio River, distributing gifts and gaining influence with the Indians.[32] Croghan returned to Logstown again in November, 1750, to tend to his trading post there. He wrote to Governor James Hamilton on 16 November: ""Yesterday, Andrew Montour and I got to this Town, where we found thirty warriors of the Six Nations...They told us that they saw John Coeur [Joncaire] about one hundred and fifty miles up this River at an Indian Town, where he intends to build a Fort if he can get liberty from the Ohio Indians. He has five canoes loaded with goods, and is very generous in making presents to all the chiefs of the Indians that he meets with.""[1]: 361 
 Alarmed by these continued attempts of the French to maintain influence over the Ohio tribes, the Pennsylvania government purchased gifts and sent Croghan and Montour back to Logstown. They arrived on 18 May 1751, and were welcomed warmly. Two days after they arrived, ""Mr. loncoeur and one Frenchman more"" arrived, accompanied by forty Iroquois warriors. On 21 May Joncaire called a council with the leaders of Logstown, and Croghan was also there. Joncaire requested that the leaders respond to Céloron's speech of August, 1749, challenging them to end all trade relations with the English. In his letter to Governor Hamilton, Croghan noted, ""To enforce that speech he gave them a very large belt of wampum,""[49] a symbol of the importance of his message.[50] Keeshequeatama, Speaker for the Six Nations, replied:
 He then returned the belt of wampum, symbolically rejecting the French challenge to end trade with the English.
 On 25 May Croghan met with Joncaire, who apologized for urging the leaders of Logstown to end trade with the English, saying that he was following orders from the Governor of Canada, but added that ""he was sure the French could not accomplish their designs with the Six Nations, without it could be done by force; which, he said, he believed they [the French] would find to be as difficult as the method they had just tried, and would meet with the like success.""[49]: 59 [33]
 At another meeting with the town's leaders on 28 May, the Speaker of the Six Nations addressed Joncaire directly, saying, ""Is it not our land (stamping on the ground, and putting his finger to Joncair's nose)? What right has Onontio (the Governor of New France) to our lands? I desire you may go home directly off our lands and tell Onontio to send us word immediately what was his reason for using our Brothers so, or what he means by such proceedings, that we may know what to do; for I can assure Onontio that we, the Six Nations, will not take such usage.""[49]: 69 
 On 4 June 1751 Joncaire wrote directly to Governor Hamilton from Logstown, in French, with a warning:
 Joncaire apparently abandoned the idea of constructing a blockhouse, and Governor Duquesne began preparations to send French and Canadian troupes de la marine to the south shore of Lake Erie, under the command of Paul Marin de la Malgue,[51] to build a road and construct a series of forts (Fort Presque Isle, Fort Le Boeuf, Fort Machault), and later, Fort Duquesne.[52]
 The Ohio Iroquois had been reluctant to allow the English to build forts in the region. As early as March 23, 1731, Seneca chiefs sent a message to Governor Patrick Gordon: ""It is [our] land but your people may trade there but not build Stone or Timber houses, but of Bark.""[53] Twenty years later, the English began working to obtain permission to build forts.
 George Croghan was in Logstown in November, 1750 when the residents mentioned to him that Joncaire was exploring the idea of building a French fort nearby. In his letter of 16 November to Governor Hamilton, Croghan then adds: ""We have seen but very few of the Chiefs of the Indians they being all out a hunting, but those we have seen are of opinion that their Brothers the English ought to have a Fort on this River to secure the Trade.""[49]: 54  Governor Hamilton was evidently anxious to pursue this opportunity, and wanted Croghan to obtain approval from the Logstown sachems for the construction of an English fort, but told Croghan that no official request to build a fort should be made. Instead, Croghan was instructed to find out how the Indians felt about having an English stronghold on the Ohio.[54]
 On 29 May 1751, at a council meeting at Logstown between George Croghan, Andrew Montour and representatives of the Six Nations, Croghan reported the following statement from Iroquois speaker Toanahiso:
 Governor Hamilton used this statement as evidence to the Pennsylvania Provincial Council that they should pay for the construction of a fort at a site selected by the sachems at Logstown, arguing that unless the fort were built, the English might lose not only Indian support, but control over the fur trade in Ohio. But Andrew Montour contradicted Croghan's account, stating that the Indians had never requested a fort but had only agreed to consider the idea.[54] Montour doubted that they would allow a fort to be built near Logstown. As a result, the Provincial Council decided not to provide funding for a fort, arguing that fair dealings and occasional presents would hold the Indians as allies.[36]: 547  At the Treaty of Logstown in June 1752, Tanacharison agreed to the construction of a fort upriver from Logstown, but his figurehead authority as half-king did not allow him to speak for the Onondaga Council. The following summer, Virginia's Ohio Company obtained permission from the Six Nations to build Fort Prince George, but construction did not begin until February of 1754.[55]: 54 
 In 1749 the British Crown awarded the Ohio Company a grant of 500,000 acres in the Ohio Country between the Monongahela and the Kanawha Rivers, provided that the company would settle 100 families within seven years.[56] The Ohio Company was also required to construct a fort and provide a garrison to protect the settlement at their own expense.[57] The Treaty of Logstown was intended to open up land for settlement so that the Ohio Company could meet the seven-year deadline, and to obtain explicit permission to construct a fort.[48][58]: 123–144 
 Between 1 and 13 June 1752, the British held a council at Logstown with representatives of the Six Nations, and the Lenape and Shawnee who had been tributary to them. Colonel Joshua Fry, James Patton, and Lunsford Lomax represented the Colony of Virginia,[59] and Christopher Gist, William Trent,[60] and William Beverley represented the Ohio Company.[61]
 One of the main purposes of the Logstown treaty conference was to confirm the 1744 Lancaster Treaty in which the Six Nations supposedly gave up territory to Virginia, along the Ohio River on the southeast, as there was anxiety on the part of the colonial authorities as to whether the Indians were still willing to abide by the treaty.[62] The Ohio Company and the Virginia commissioners also wanted the Ohio tribes to grant permission to build a fort at the forks of the Monongahela and Allegheny rivers and to allow new English settlements to be established on a half-million acres of unsettled land to the west and north of the Ohio River. The Company wanted to open trade with the Ohio Indians, which the French had forbidden.[15] The Virginia and Pennsylvania delegates reminded the Delawares and Shawnees, ""We advise and exhort you to beware of French Councils, and that you will adhere to a strict friendship with us (the English colonies and the Six Nations).""[61]
 At first, the Iroquois sachem, Tanacharison (referred to in the treaty as ""Thonariss, called by the English the half King""),[24] reminded the Virginia officials that ""the lands then sold [by the Six Nations at Lancaster in 1744] were to extend no further to the sunset [west] than the hill on the other [eastern] side of Allagany Hill,"" but he was eventually forced to cede Iroquois lands beyond the Alleghenies, granting access to the territory the colonial authorities wanted. The Virginia representatives also tried to pretend that Indians would still have access to these lands, stating in the treaty: ""Be assur'd that the King, our Father, by purchasing your Lands, had never any Intention of takeing them from you, but that we might live together as one People, & keep them from the French, who wou'd be bad Neighbours.""[61]
 After much urging from Andrew Montour, Tanacharison reluctantly agreed to allow a British fort to be built at the mouth of the Monongahela River,[63]: 104  the site of present-day Pittsburgh.
 
At the conclusion of the Logstown conference, Tanacharison promised that existing settlements southeast of the Ohio River ""shall be unmolested by us, and that we will, as far as in our power, assist and Protect the British Subjects there.""[61][64] In spite of Tanacharison's promises, the Ohio Indians did not agree to allow English settlements in the Ohio region,[9] and Scarouady, the Oneida half king to the Shawnees, warned, ""we [the Ohio Indians] intend to keep our country clear of settlements.""[63]: 103  Tanacharison himself had regrets, and a year later he told the French ""we live in a country in between [the French and the English colonies], therefore the land belongs to neither one nor t'other.""[63]: 104  Technically, the treaty signed by the ""Half-king"" was not binding on the part of the Onondaga Council, although the colonial commissioners and the Ohio Company hoped that they would support the treaty, or at least agree to consider additional treaties in the coming months.[50]
 Since the death of Sassoonan in 1747, the Lenape had been without an effective leader. Sassoonan had selected Pisquetomen as his successor, but James Logan saw him as an obstinate and independent obstructionist to Pennsylvania's political agenda.[65]: 209  Logan also wanted a leader with the determination to bring those Lenape who had migrated to Ohio back to the Susquehanna region, and felt that Pisquetomen would be unable and unwilling to attempt this.[65]: 246  Logan and Weiser actively tried to promote Lappapitton as Sassoonan's successor, but Lappapitton declined out of respect for Pisquetomen.[66] The Iroquois instructed Tanacharison to decide on a leader acceptable to all parties, and at Logstown Tanacharison presented Shingas as his choice, arguing ""that is our right to give you a King"" to represent the Lenape in ""all publick Business"" between the Lenape, the Six Nations, and the British.[61]: 164  Tanacharison announced to the Virginia commissioners, ""we have given our Cousins, the Delawars, a King, who lives there, we desire you will look upon him as a Chief of that Nation."" Shingas was absent from the treaty conference, so Tamaqua ""stood proxy for his brother and was presented with a lace hat and jacket and suit.""[61]: 167 
 The French wanted to maintain control of the Ohio Valley because it lay between their two great provinces of Canada and La Louisiane, and English control of the region would make French commerce, defense and communication slower, more expensive, and less secure.[50] They responded to the news of the treaty by sending troops to construct and garrison a series of forts, intended to solidify their military presence in the Ohio region, intimidate the Native American inhabitants, and keep the British out. Francois Bigot, intendant of New France since 1748, wrote this summary of the French plan on 26 October 1752:
 In late 1753 (the exact date is unknown) the sachems at Logstown received a letter from Jacques Legardeur de Saint-Pierre stating: 
 In late 1753, Virginia Governor Dinwiddie appointed newly-commissioned Major George Washington as a special envoy to the French commander at Fort Le Boeuf to demand that the French vacate the Ohio Valley territory, which the British had claimed.[69] Washington was also ordered to make peace with the Iroquois Confederacy and to gather intelligence about the French forces.[70]: 15–16 
 Washington left Williamsburg, Virginia on 30 October with eight men, heading to Logstown to meet with Iroquois allies.[71]: 18  On his way, he stopped at the homestead of Christopher Gist near Wills Creek and Gist joined them.[72]
 Arriving in Logstown on 23 November, Washington held council with Shingas, Scarouady, and Tanacharison, who had recently returned from a journey to Fort Le Boeuf himself.[72] The chiefs provided Washington with information about the best route to Fort Le Boeuf, and called a council of sachems. Washington explained his mission, and received assurances that the Indians and the English ""were brothers."" Tanacharison told Washington that ""he cou’d not consent to our going without a Guard, for fear some Accident shou’d befall us,"" and volunteered to accompany Washington, along with Kaghswaghtaniunt (White Thunder),[73] Guyasuta, and Jeskakake, on his journey to Fort Le Boeuf.[24] Their purpose was to return three belts of wampum sent by the French as a symbol of friendship. Returning the wampum was a gesture intended to show that the sachems at Logstown were allied with the English.[50] Washington wrote in his diary that ""I knew that returning of Wampum was the abolishing of Agreements; & giving this up was shaking of all Dependence upon the French.""[72]
   While at Logstown, Washington encountered four French deserters who had fled from a French military supply convoy from Fort de Chartres in Illinois, under the command of Captain Demazilière and headed towards Lake Erie, where a French military force under the command of Paul Marin de la Malgue[74] was building a road between Fort Presque Isle, Fort Le Boeuf, and Fort Machault.[52][75]: 156  The deserters had learned of La Malgue's sudden death on 29 October and had taken refuge in Lower Shawneetown. They were on their way to Philadelphia in the company of an English trader.[72]
 Washington and his men left Logstown on 30 November and reached Venango at French Creek on 4 December, where they were warmly greeted by Philippe-Thomas Chabert de Joncaire, who was in command of the French troops at Venango. Joncaire provided Washington's men with wine and brandy, and the Indians, when intoxicated, declared their loyalty to the French. It took Washington three days to persuade them to move on to Fort Le Boeuf, where they met the French commander Jacques Legardeur de Saint-Pierre. Tanacharison tried to return the wampum to Saint-Pierre, ""who evaded taking it, & made many fair Promises of Love & Friendship; said he wanted to live in Peace & trade amicably with them; as a Proof of which, he wou’d send some Goods immediately down to the Logstown for them."" The French refused to consider leaving the area, and gave Washington a reply to deliver personally to Williamsburg.[72]
 After Washington's return to Williamsburg, Governor Dinwiddie wrote to Governor James De Lancey of New York about Washington's mission, stating:
 Tanacharison returned to Logstown on 15 January 1754,[24]: 74  escorted by a French detachment under Michel Maray de La Chauvignerie[76] which set up a temporary post nearby.[24] George Croghan had arrived in Logstown the day before, accompanied by the trader John Patten, and observed the arrival of the French troops, ""an Ensign, a Sergeant, and Fifteen Soldiers.""[49]: 74  The next day, as Patten was walking around the town, the French commandant ordered him to be arrested. Tanacharison and Croghan protested vigorously, and Croghan noted that the residents of Logstown seemed very opposed to the presence of French soldiers in the town. The French decided to ""board their Canoes and set off to a small Town of the Six Nations about two Miles below the Log's Town, where [La Chauvignerie] intends to stay till the Rest of their Army come down."" Correspondence between La Chauvignerie and his superior at Fort Le Boeuf, Saint-Pierre, describe the French soldiers as suffering from hunger and cold, as firewood was difficult to find. La Chauvignerie writes on February 10: ""We are on the eve of being without food...The scarcity of wood which prevails in this place causes us all to be exposed to the harshness of the weather...I shall take every care to keep the tribes as peaceful as possible until a reinforcement arrives.""[76]: 123 
 Saint-Pierre was evidently planning to construct a fort near Logstown and to drive away any English settlers. Governor Duquesne wrote orders to Captain Michel Péan:
 The French fort at Logstown was to be built by Contrecoeur, whose original orders had been to proceed down the Allegheny and Ohio and establish a military base there. The French had been planning to build a fort at Logstown since 1753, and had sent a sizeable French force to the south shore of Lake Erie to build roads and clear the rivers of rocks and driftwood so that boats could bring supplies. Problems with supplies and illness among the troops had slowed progress,[76] however, and the sudden death of Marin, the commander, at Fort Le Boeuf on 29 October forced the French to postpone the project.[67]: 22–23 
 Then on 4 March 1754, La Chauvignerie discovered English soldiers building Fort Prince George at the confluence of the Ohio and the Monongahela rivers.[55][76]: 129  Contrecoeur seized it on 18 April 1754, and razed it to build Fort Duquesne.[78] The French then decided that a fort at Logstown was unnecessary, particularly because of the lack of trees for lumber.[79]: 57 
 A few days before Washington's surrender at Fort Necessity on 3 July 1754, Scarouady burned down Logstown. Washington's journal entry for 26 June 1754, reads: ""An Indian arrived bearing the news that Monacatoocha (Scarouady) had burned his village, Logstown, and was gone by water with his people to Red-Stone, and might be expected there in two days.""[80][81]: 128  Henry Wilson Temple reports that the town's inhabitants destroyed it ""fearing lest they might be punished for their alliance with the French.""[78]: 257  About 200 of the town's Iroquois, Shawnee and Lenape residents moved to Fort Cumberland, and later to the Aughwick Valley near present-day Shirleysburg, Pennsylvania.[1]: 377 
 In March, 1755, French forces began rebuilding the village. Joseph Gaspard Chaussegros de Lery passed the site of the town on 5 April 1755 and refers to it as the ""Little Chaouanon Village.""[75]: 180  Progress was initially slow. Charles Stuart, who was taken captive by a group of Lenape and Shawnee warriors during the Great Cove massacre in November 1755, was taken to Logstown in December and reported: ""When we came to Loggs town we found all the Cabbins waste but Three.""[82]: 67 
 In December, 1755, George Croghan hired a Lenape Indian named Jo Hickman to visit Kittanning and Logstown and bring back information on the number of warriors and European prisoners in each place, as Indian raids on settlements had become frequent, and the Pennsylvania Provincial Council was contemplating sending a military force to attack one or both of these communities. At Logstown Hickman observed ""about 100 Indians and 30 English prisoners.""[83][84]: 449 
 Christian Frederick Post visited the town in December, 1758, and wrote in his journal: ""I came to Logs Town, situated on a hill. On the east end is a great piece of low land, where the Old Log's Town used to stand. In the New Log's Town, the French have built about thirty houses for the Indians.""[1]: 377 [85]: 57 
 John McCullough was 8 years old when he was captured by Lenape warriors in July, 1756, and brought to ""Shenango,"" (a corruption of Chiningué). In his captivity narrative he reports living there with a Lenape family for two and a half years and states that Logstown ""lay in a semi-circular form, round the bend of a creek."" He refers to an upper town and a lower town. In late 1758, he moved to ""Kseek-he-ooing"" (possibly Saucunk) and was released in December, 1764, along with over 200 other captives, by order of Colonel Henry Bouquet.[86]
 In late 1758, Logtown's inhabitants were invited to establish a new town on the Upper Scioto, at Pickaway Plains, by the former residents of Lower Shawneetown, who had abandoned their village in November 1758.[87]: 339–340  On 26 November, George Croghan and Andrew Montour proceeded down the river to Shingas's Town (Saucunk). In his journal, Croghan writes: 
 Henry Bouquet passed through the area in 1764, en route with 1,500 troops to Ohio, writing in his journal:
 George Croghan returned to the area in 1765. His journal entry for 16 May says:
 George Washington returned to the area and noted in his journal that on 21 October 1770 he ""breakfasted at Logstown"" with George Croghan and Alexander McKee,[1]: 380  but says nothing of the community or its inhabitants.[89] On 5 September 1772, the Reverend David McClure visited John Gibson, a trader, at ""his house in Logs Town, which was the only house there.""[1]: 380 
 Arthur Lee, an Indian Commissioner, visited Fort McIntosh in 1784. His journal entry for December 17 begins: ""We embarked on the Monongahela, and soon entered the Ohio...Four miles down the River brings you to Montour's Island...The next place is Loggstown, which was formerly a settlement on both sides of the Ohio, and the place where the Treaty of Lancaster was confirmed by the Western Indians.""[1]: 382 
 In 1792, General Anthony Wayne established a military training base for the newly formed Legion of the United States on ground situated where Logstown's ""upper town"" had been.[22][78] Legionville became the first basic training camp for regular Army recruits, and was the first facility established expressly for this purpose.[90] In March, 1793 the Seneca leader Guyasuta, a former resident of Logstown, was invited to Legionville to meet with General Wayne for peace talks.[21]: 997  The site was vacated in 1793 after the troops left to fight in the Northwest Indian War.[91]
 The only known archaeological studies of the Logstown site took place in 1940[92] and 1942.[93] The 1955 Annals of the Carnegie Museum states: ""One unsuccessful attempt has been made [in 1942] to locate the late Historic village known as Logstown.""[94]: 164 
 In 2011 an archaeological survey of the Beaver Creek area noted that the Logstown site has never been 
 In 2019 the National Park Service conducted a survey of the route traveled by George Washington from Williamsburg, Virginia to Fort LeBoeuf between October, 1753 and January, 1754, to determine the feasibility of designating this a National Historic Trail. The report notes that ""The site of Logs Town, an Indian village that Washington visited in 1753, also has...potential for an archeological survey.""[3]: 28 
"
Town Destroyer,https://en.wikipedia.org/wiki/Town_Destroyer,"Conotocaurius (Town Destroyer, Seneca: Hanödaga꞉nyas) was a nickname given to George Washington by Iroquois peoples in 1753. The name in its original language(s) has been given variously as Conotocarius, Conotocaurious, Caunotaucarius, Conotocarious, Hanodaganears, and Hanadahguyus. It has also been translated as ""Town Taker"", ""Burner of Towns"", ""Devourer of Villages"", or ""he destroys the town"".[1]
 Washington was given the name in 1753 by the Seneca leader Tanacharison. The nickname had previously been given to his great-grandfather John Washington in the late seventeenth century. He had participated in an effort to suppress Indigenous peoples defending themselves[dubious – discuss] in Virginia and Maryland. It involved members of both the Susquehannah and the Piscataway, an Algonquian tribe that lived across the Potomac River from Mount Vernon. Following the massacre of five chiefs who had come out to negotiate under a flag of truce to the colonizers, the Susquehannahs gave John Washington an Algonquian name that translated to ""town taker"" or ""devourer of villages."" The elder Washington's reputation was remembered and when they met his great-grandson in 1753 they called George Washington by the same name, Conotocarious.[2][3]
 Washington referred to himself as ""Conotocaurious"" in a letter he wrote to Andrew Montour dated October 10, 1755, in which he tried to manipulate the Oneida to resettle on the Potomac:
 In 1779 during the American Revolutionary War, the Sullivan Expedition, under Washington's orders,[6] destroyed over 40 Iroquois villages in New York, partially in response to Iroquois participation in attacks on the Wyoming Valley in July 1778 and Cherry Valley in November 1778.[7] In 1790, the Seneca chief Cornplanter told President Washington: ""When your army entered the country of the Six Nations, we called you Town Destroyer and to this day when your name is heard our women look behind them and turn pale, and our children cling close to the necks of their mothers.""[8][9]
"
John Washington,https://en.wikipedia.org/wiki/John_Washington,"

 John Washington (1633 – 1677) was an English-born merchant, planter, politician and military officer. Born in Tring, Hertfordshire, he subsequently immigrated to the English colony of Virginia and became a member of the planter class. In addition to serving in the Virginia militia and owning several slave plantations, Washington also served for many years in the House of Burgesses, representing Westmoreland County. He was the first member of the Washington family to live in North America and was a paternal great-grandfather of George Washington, the first president of the United States.[2][a]
 John Washington was born to rector Lawrence Washington and Amphillis Twigden, about 1633 (when his father resigned his fellowship at Oxford that required him to remain unmarried), likely at his maternal grandparents' home in Tring, Hertfordshire.[1][3][4] However, as an adult, John Washington gave his age in a Virginia deposition as 45, which would put his birth two years earlier.[5][6] Before his marriage Lawrence had been a don at the University of Oxford.[7] He had been born at Sulgrave Manor near Banbury in Oxfordshire.[8] 
 When John was eight, his father enrolled him in Charterhouse School in London to begin preparing for an academic career, but the boy never attended the school. In 1633 the senior Washington had left Oxford to become the rector of All Saints Parish in Purleigh, Essex. During the English Civil War, in 1643 Parliamentary Puritans stripped Anglican Rev. Washington of that clerical position, alleging misconduct that was disputed. Rev. Lawrence Washington then became vicar of an impoverished parish in Little Braxted, Essex, where he died in January 1652. His widow returned to her parents' family home in Tring, Hertfordshire, and in 1655 John became administrator of his widowed mother's estate.[9]
 John Washington was apprenticed with a London merchant through the help of his Sandys relatives.[citation needed] He gained a valuable education in colonial trade, as England had colonies in the Caribbean and North America. He served as Master's Mate on board a tobacco ship when he first came to Virginia.[10]
 In 1656 John Washington invested with Edward Prescott in a merchant ship which transported tobacco from North America to European markets. He secured tobacco contracts in Europe, joined Prescott's ship (the Sea Horse of London) in Denmark, and sailed as second mate for the Colony of Virginia.[11][12][13] A storm on February 28, 1657, caused the ship (fully laden with tobacco for the return journey) to run aground in the Potomac River at a shoal near its confluence with Mattox Creek. Although the vessel was repaired, Washington elected to remain in the colony. However, when he asked for his wages, Prescott said he owed him money instead, so Colonel Nathaniel Pope (his future father-in-law discussed below) gave Prescott beaver skins to settle the alleged debt.[14][15][16] However, his cousin, James Washington, the son of Robert Washington (1616 - 1674), who worked in the London-Rotterdam trade of the Merchant Adventurers, who had also sailed on that voyage, returned on Prescott's ship.[17][18]
 Complicating matters, this John Washington also had a younger brother, Lawrence Washington, who became a merchant, married Mary Jones of Luton in Bedfordshire in England, then also emigrated from England to the Virginia Colony, where he died.[19] That Lawrence also had a son named John Washington (usually distinguished as ""of Chotank"", the name of his plantation in King George County, Virginia). That John Washington raised the children of his cousin Lawrence Washington (1659-1698) (this man's firstborn son): John Washington (1692-1746) and Augustine Washington (1693-1743) when they returned from England.[20]  
 Washington initially lived at the home of Col. Nathaniel Pope, who had emigrated from England to Maryland about twenty years earlier, then moved across the Potomac River to Virginia where he became a planter on the Northern Neck and a justice of the peace for what was then Northumberland County in 1651 and four years later Lt.Col. of the local militia.[21] During his stay, Washington fell in love with his host's daughter Anne, whom he married late in 1658 or early in 1659. She gave birth to their first son, Laurence in October 1659. Around that time, Washington learned that his nemesis Capt. Prescott had hanged a woman as a witch, and brought murder charges against him in the Maryland General Court; however, the trial conflicted with Laurence's baptism, so Prescott went free for lack of evidence.[22]
 Col. Pope gave the couple a wedding gift of 700 acres (2.8 km2) on Mattox Creek, as well as a loan of 80 pounds for startup expenses, which he forgave in his will, which was filed in April 1660.[23][16] In 1664, Washington bought 100 acres on Bridges Creek near the confluence with the Potomac River, and settled there, in what is now part of George Washington Birthplace National Monument.[24][25] Washington became a successful planter, depending on the labour of Black slaves and white indentured servants to cultivate tobacco as a commodity crop as well as kitchen crops needed to support his household and workers. By 1668 he was growing tobacco, with holdings of 5,000 acres (20 km2).[26] His will disposed of more than 8,500 acres (34 km2) of land.[27]
 Washington's first public office was vestryman of the local Appomattox Parish church in 1661 (although the parish would cease to exist four years later after a reorganization).[28] Washington also served as trustee of Westmoreland County estates and guardian of children.[29] In 1661, Washington also became the county coroner and in 1662 became one of the judges of the county court (with administrative as well as judicial responsibilities) and Major of the militia -- both signifying his acceptance into the gentry.[30][31]
 Westmoreland County voters first elected Washington as one of their representatives in the House of Burgesses in 1665, and he continually won re-election until his death more than a decade later. He served alongside planters Isaac Allerton, Gerrard Fowke and his cousin Nicholas Spencer.[32]
 In 1672, Washington received promotion to lieutenant colonel in the local militia, as relations with Native Americans again became troubled.[33] (Settlers in the Northern Neck area had been massacred in 1622 and 1644)[34] In 1675 (by which time Washington's rank had increased to colonel), he and fellow Virginia planter and militia officer Isaac Allerton and Maryland Major Trueman led retaliation against Maryland natives who had killed three Virginia colonists after a trade dispute. During a planned parley with the disgruntled opposition and their allied American Indian leaders, Maryland militia killed at least five surrendered or parleying Doeg and Susquehannock warriors.[35][36][37] For his efforts suppressing Native Americans, the Susquehannock gave John the nickname of ""Town Destroyer"".[38] Some eight decades later, during the French-Indian War, the Seneca would bestow the same title upon Washington's great-grandson, George, for both his own prowess in warfare against the tribes, and in remembrance of the destruction incurred by his ancestor.[39]
 The incident and resultant raids later contributed to Bacon's Rebellion in 1676, during which Col. Washington supported  Governor William Berkeley.[40] During the rebellion, Bacon's forces plundered Washington's estate, among others.[41] Following Bacon's death and the suppression of Bacon's Rebellion, an investigating commission criticized Governor William Berkeley, who returned to England, so John's cousin Nicholas Spencer who had traveled with Berkeley to Virginia, became Virginia's acting governor. However, Washington died within months, as discussed below.
 John Washington married three times. He married Anne Pope in late 1658.[7] 
They had the following children[42] together:
 After Anne Pope's death, Washington married a widow named Anne, who had survived husbands Walter Brodhurst and Henry Brett, but did not have children with Washington. Her maiden name is unknown.[48][6]
 After his second wife's death, John Washington married Frances Gerard (a daughter of Thomas Gerard, and widow of Thomas Speke, Valentine Peyton, and John Appleton). This third marriage occurred about 10 May 1676 when a ""joynture"" was recorded between Mrs. Frances Appleton and John Washington in Westmoreland County, Virginia.[49]
 Although the exact date of Washington's death has not been recorded, it occurred after he attended a meeting concerning taxes and the suppressed rebellion on August 14, 1677. Washington's will was admitted to probate on September 26, 1677.[50][51] His estate consisted of more than 8,500 acres.[52] John and his first wife Anne Pope are buried near present-day Colonial Beach, Virginia, at what is now called the George Washington Birthplace National Monument. His vault is the largest in the small family burial plot.
 During John’s lifetime, the name of the local parish of the Anglican Church (the established church in colonial Virginia, and thereby also a tax district of the county) was changed to Washington in his honor.[16]
"
Susquehannock,https://en.wikipedia.org/wiki/Susquehannock,"The Susquehannock, also known as the Conestoga, Minquas, and Andaste, were an Iroquoian people who lived in the lower Susquehanna River watershed in what is now Pennsylvania. Their name means “people of the muddy river.”
 The Susquehannock were first described by John Smith, who explored the upper reaches of Chesapeake Bay in 1608. The Susquehannocks were active in the fur trade and established close trading relationships with Virginia, New Sweden, and New Netherland. They were in conflict with Maryland until a treaty was negotiated in 1652, and were the target of intermittent attacks by the Haudenosaunee (Iroquois).
 By the 1670s, their population had declined sharply as a result of disease and war. The Susquehannock abandoned their town on the Susquehanna River and moved south into Maryland. They erected a palisaded village on Piscataway Creek, but in September 1675, the Susquehannock were besieged by militias from Maryland and Virginia. The survivors of the siege scattered, and those who returned to the north were absorbed by the Haudenosaunee.
 In the late 1680s, a group of Susquehannock and Seneca established a settlement on the Conestoga River in present-day Lancaster County, Pennsylvania, where they became known as the Conestoga. The population of this community gradually declined, and in 1763, the last members were massacred by the vigilante group known as the Paxton Boys. While there are a significant number of Indigenous people alive today of Susquehannock ancestry, the Susquehannock as a distinct cultural entity are considered extinct.
 The Susquehannock were an Iroquoian speaking people. Little of the language has been preserved. The chief source is the Vocabula Mahakuassica compiled by the Swedish missionary Johannes Campanius during the 1640s. Campanius's vocabulary contains about 100 words and is sufficient to show that Susquehannock is a Northern Iroquoian language, closely related to the languages of the Haudenosaunee and in particular that of the Onondaga. The language is considered extinct as of 1763 when the last remnant community of the Susquehannock was massacred at Lancaster, Pennsylvania.[3]
 The Europeans who colonized the Mid-Atlantic coast of North America typically adopted the names that were used by the coastal Algonquian-speaking peoples for interior tribes. The Europeans adapted and transliterated these exonyms to fit their own languages and spelling systems, and tried to capture the sounds of the names. What the Susquehannock called themselves is not known.[4]
 In the late 15th and early 16th centuries the Susquehannock lived in scattered hamlets on the North Branch of the Susquehanna River in what is now Bradford County, Pennsylvania, and Tioga County, New York. Of Northern Iroquoian ancestry, the Susquehannock became culturally and linguistically distinct before 1500.[7]
 A southward migration towards Chesapeake Bay began in the second half of the 16th century, possibly the result of conflict with the Haudenosaunee to the north. The shortening of the growing season during the Little Ice Age, and the desire to be closer to sources of trade goods may also have been factors.[8] The Susquehannock assimilated the Shenks Ferry people in the lower Susquehanna River valley, and established a palisaded village in present-day Lancaster County, Pennsylvania.[9] An archaeological excavation in 1931 revealed that the village (known as the Schultz Site) contained at least 26 longhouses.[10] The Schultz site was largely abandoned c. 1600 due to overcrowding and depletion of local resources. A larger fortified town was constructed near what is today Washington Boro. The town is estimated to have been 250,000 square feet in size with a population of about 1,700 people.[7]
 Several smaller Susquehannock sites have been found in the upper Potomac River valley in what is now Maryland and West Virginia that date roughly from 1590 to 1610.[11] Archaeological evidence also exists for a palisaded settlement 30 miles upstream of Washington Boro in what is now Cumberland County that was occupied from about 1610 to 1620.[12]
 The first recorded European contact with the Susquehannock was in 1608 when English explorer John Smith met with a group of about 60 ""gyant-like"" warriors and ""weroances"" at the mouth of the Susquehanna River, two days journey downriver from their settlement at Washington Boro. Smith wrote of the Susquehannock, ""They can make neere 600 able and mighty men, and are pallisadoed [palisaded] in their Townes to defend them from the Massawomekes, their mortal enemies."" Smith also recorded that some of the Susquehannock were in possession of hatchets, knives, and brass ornaments of French origin.[7]
 Significant Susquehannock involvement in the fur trade began in the 1620s. Because of their location on the Susquehanna River, the Susquehannock had access to English traders on the Chesapeake, as well as Dutch and Swedish traders on Delaware Bay. Furs, primarily beaver, were traded for cloth, glass beads, brass kettles, hawk bells, axes, hoes, and knives. Although many Europeans were hesitant to trade firearms for furs, the Susquehannocks began to obtain muskets in the 1630s.[13]
 In 1626, a group of Susquehannock travelled to New Amsterdam seeking to establish a trading relationship with the Dutch. Isaack de Rasière, the Secretary of New Netherland noted that the Lenape living on the Delaware River were unable to supply furs because of Susquehannock raids.[7] The following year the Dutch established Fort Nassau on the east side of the Delaware River opposite the mouth of the Schuylkill River.[14]
 To trade with the Dutch, the Susquehannock had to pass through Lenape territory. English explorer Thomas Yonge (Yong) noted that in 1634 the ""people of the river"" were at war with the Minquas who had ""killed many of them, destroyed their corne, and burned their houses.""[15] By 1638, however, the Lenape and the Susquehannock had reached an accommodation, with the later having been given access to trading posts on the Delaware. It is said that the Lenape became ""subject and tributary"" to the Susquehannock[10] but this is disputed.[16]
 Contact with English settlers on the Chesapeake was limited until English merchant William Claiborne began trading with the Susquehannock c. 1630. Claiborne established a settlement on Kent Island in 1631 to facilitate this trade, and later erected an outpost on Palmer's Island near the mouth of the Susquehanna River.[17]
 Relations with the English deteriorated following the establishment of the Province of Maryland in 1634. The new colony formed an alliance with the Piscataway, who were the frequent target of Susquehannock raids. The founding of the colony also disrupted Claiborne's trade alliance with the Susquehannock as he refused to acknowledge Maryland's authority. When a legal dispute forced Claiborne to return to England in 1637, Maryland seized Kent Island.[18]
 The focus of Susquehannock trade now turned to the newly established colony of New Sweden on Delaware Bay. Swedish settlers had built Fort Christina on the west side of the bay near the mouth of the Schuylkill River in 1638. This gave them the advantage over the Dutch in the fur trade with the Susquehannock.[19]
 Following a raid on a Jesuit mission in 1641, the Governor of Maryland declared the Susquehannock ""enemies of the province."" A few attempts were made to organize a military campaign against the Susquehannock, however, it was not until 1643 that an ill-fated expedition was mounted. The Susquehannock inflicted numerous casualties on the English and captured two of their cannon. 15 prisoners were taken and afterwards tortured to death.[16]
  Raids on Maryland and the Piscataway continued intermittently until 1652. In the winter of 1652, the Susquehannock were attacked by the Mohawk, and although the attack was repulsed, it led to the Susquehannock negotiating the Articles of Peace and Friendship with Maryland.[16] The Susquehannock relinquished their claim to territory on either side of Chesapeake Bay, and reestablished their earlier trading relationship with the English.[20][21]
 In 1660, the Susquehannock used their influence to help end the First Esopus War between the Esopus and the Dutch.[16]
 An Oneida raid on the Piscataway in 1660 led Maryland to expand its treaty with the Susquehannock into an alliance. The Maryland assembly authorized armed assistance, and described the Susquehannock as ""a Bullwarke and Security of the Northern Parts of this Province."" 50 men were sent to help defend the Susquehannock village. Muskets, lead and powder were acquired from both Maryland and New Netherland. Despite suffering a smallpox epidemic in 1661, the Susquehannock easily withstood a siege by 800 Seneca, Cayuga and Onondaga in May 1663, and destroyed an Onondaga war party in 1666.[16]
 The Susquehannock abandoned their village on the east side of the Susquehanna c. 1665 and moved across the river to the west side. Their new village appears on Augustin Herrman's 1670 map of Virginia and Maryland. The Jesuit Relations for 1671 reported that the Susquehannock had 300 warriors,[22] and described a rout of a Seneca and Cayuga raiding party by a group of Susquehannock adolescents.[16]
 By the 1670s, epidemics and years of war with the Haudenosaunee had taken their toll on the Susquehannock. In 1675, they left their village on the Susquehanna River and moved south into Maryland.
 Two reasons for the move have been proposed. Most historians believe that the Haudenosaunee inflicted a major defeat on the Susquehannock c. 1674 since the Jesuit Relations for 1675 reports that the Seneca ""utterly defeated ... their ancient and redoubtable foes.""[7] Historian Francis Jennings, however, proposed that the Susquehannock were coerced by Maryland into moving. Jennings argued that the Haudenosaunee could not have mounted an attack in 1674 since a munitions shortage in New France meant that the French were unable to supply them with muskets, lead and powder.[16]
 Although Governor Charles Calvert of Maryland wanted the Susquehannock to settle on the Potomac River above the Great Falls, the tribe instead chose to occupy a site on Piscataway Creek where they erected a palisaded fort. In July 1675, a group of Virginians chasing Doeg raiders crossed the Potomac into Maryland and mistakenly killed several Susquehannock. Subsequent raids in Virginia and Maryland were blamed on the tribe. In September 1675, a thousand-man expedition against the Susquehannock was mounted by militia from Virginia and Maryland led by John Washington and Thomas Truman. After arriving at the Susquehannock town, Truman and Washington summoned five sachems to a parley but then had them summarily executed. Sorties during the ensuing six-week siege resulted in 50 English deaths. In early November the Susquehannock escaped the siege under cover of darkness, killing ten of the militia as they slept.[23]
 Most of Susquehannock crossed the Potomac into Virginia and took refuge in the Piedmont of Virginia. Two encampments were established on the Meherrin River near the village of the Siouan-speaking Occaneechi on the Roanoke River In January 1776, the Susquehannock raided plantations on the upper Rappahannock River, killing 36 colonists, and at the falls of the James River. Nathaniel Bacon, unhappy with Governor Sir William Berkeley's response to the raids, organized a volunteer militia to hunt down the Susquehannock. Bacon persuaded the Occaneechi to attack the closest Susquehannock encampment. After the Occaneechi returned with Susquehannock prisoners, Bacon turned on his allies and indiscriminately massacred Occaneechi men, women and children.[23]
 The Susquehannock who survived the Occaneechi attack moved downriver and may have merged with the Meherrin.[24]
 Other Susquehannock refugees fled to hunting camps on the North Branch of the Potomac or took refuge with the Lenape. Some of these refugees returned to the lower Susquehanna River valley in 1676 and established a palisaded village near the site of their previous village.[24]
 In March 1677, Susquehannock refugees living among the Lenape were invited to settle with the Haudenosaunee. While 26 families chose to remain with the Lenape, the remainder merged with the Cayuga, Oneida and Onondaga, and were joined by some of the Susquehannock from the village on the Susquehanna River. Roughly three years later the village was abandoned when the remaining inhabitants also joined the Haudenosaunee.[24]
 In the late 1680s, a group of Susquehannock and Seneca established a village near the Conestoga River in what is now Manor Township, Lancaster County, Pennsylvania where they became known as the Conestoga. They were later joined by a number of Oneida and Cayuga families. In 1700, William Penn, founder of the Province of Pennsylvania, visited the Conestoga and obtained from them a deed for their lands in the Susquehanna River watershed. In return, a tract of land in Manor Township was set aside for their use. This was confirmed by treaty in 1701.[7]
 For the next few decades, Conestoga Town, as it came to be known, was an important trading center, and a meeting place for negotiations between Pennsylvania and various Indigenous groups. Its importance, however, waned as the focus of the fur trade and European settlement moved west. The population declined due to out-migration, and the remaining Conestoga became increasing impoverished and dependent on the Pennsylvania government, who occasionally provided clothing and provisions.[7] By the 1740s, Seneca had become the dominant language with only a few Conestoga still able to speak the ""ancient tongue."".[25]
 The Conestoga remained neutral during the Seven Years' War and Pontiac's War. They bartered brooms and baskets, fished, and tended their gardens. By 1763, only seven men, five women and eight children lived in Conestoga Town.[26]
 In December 1763, the Paxton Boys, in response to raids by the Lenape and Shawnee during Pontiac's War attacked Conestoga Town in the mistaken belief that the inhabitants were aiding and abetting the attacks. The Paxton Boys slaughtered the six Conestoga they found there, and burned the settlement to the ground. Fourteen of the Conestoga had been absent from the village and were given shelter in the Lancaster workhouse. Two weeks later, however, the Paxton Boys broke into the workhouse and slaughtered the remaining Conestoga including women and children.[26]
 Two former two inhabitants, a couple named Michael and Mary, escaped the massacre as they were living on Christian Hershey's farm near Manheim. Their burial site is recorded in the Historical Marker Database, listed as part of Kreider Homestead.[27]
 In 1768, John Penn, the Governor of Pennsylvania paid the Haudenosaunee £200 in goods for the 500 acres of land on which Conestoga Town had stood. In 1775, Cayuga relatives of the Conestoga leader Sheehays received an additional payment of £300.[25]
 In 1845, six Conestoga descendants living among the Oneida in New York commissioned  Peter Doxtater to obtain restitution for land that had originally belonged to their ancestors in Lancaster County. Doxtater, whose maternal grandmother had lived at Conestoga Town before the massacre, later turned over all legal negotiations to Christian Shenk, an attorney in Lancaster County.[28]
 An 1869 property deed shows that Doxtater bequeathed 200 acres in Lancaster County to Huldah Hall, who had been a missionary teacher among the Oneida. Hall advocated for the Conestoga descendants, and may have lobbied for the 1872 joint resolution of the United States Congress.[29] The resolution was introduced by Representative Holland Duell of New York would have recognized the remaining ""Conestoga Indians"" and would have returned their land on the Manor Township tract. This resolution states that a remnant of Conestoga had been with the Oneida during the massacre of 1763, and that their descendants should have use of the land set aside for them in perpetuity. The resolution died in committee.[30]
 In 1941, a bill was introduced by Ray E. Taylor and William E. Habbyshaw of the Pennsylvania House of Representatives to provide a reservation for the Susquehannock in Dauphin County. The bill was triggered by the claims of ""Chief Fireway"" who said he was the ""sole surviving chief"" of 85–100 Susquehannock in Pennsylvania. The bill made arrangements for tribal members to lease land for a nominal fee and establish a central community in their historic homelands. Under the provisions of the bill, the tract of land would have been called ""The Susquehannock Indian Reservation"".[31] While this appropriation bill for $20,000 was passed unopposed in the state legislature, it was vetoed by Governor Arthur James, who was advised by the Pennsylvania Historical Commission that the last of the Susquehannocks had died in the 1763 massacre.[1][2]
 The Conestoga-Susquehannock Tribe, an organization in Pennsylvania that self-identifies as a tribe, offers membership to those who can show documented descent from a known Susquehannock or the 1845 land claimants, for example, those descended from Skenandoa, a war leader of the Oneida during the Revolutionary War.[32] The organization is neither a federally recognized tribe[33] nor a state-recognized tribe.[34] 
 Those with partial Susquehannock ancestry ""may be included among today's Seneca–Cayuga Nation"" as well as other recognized Haudenosaunee nations in Canada and the United States.[35]
 Little ethnographic information is available about the Susquehannock due to their relative isolation from European settlement. It is widely assumed that their culture was similar to that of other Northern Iroquoian peoples: clan-based, matrilineal, semi-sedentary, and horticultural.[7]
 The Susquehannock lived in semi-permanent palisaded villages that were built on river terraces and surrounded by agricultural fields. Although John Smith named six villages on his 1612 map, archaeological evidence indicates that at any one time the Susquehannock had just one or two large settlements in the lower Susquehanna River valley.[7] Roughly every 25 years, when soil fertility and nearby resources became depleted, they would move to a new location and begin anew. Until c. 1665 these villages were located on the east side of the Susquehanna River, however, from c. 1665 to 1675 the Susquehannock occupied a village on the west side of Susquehanna known as the Upper Leibhart site.[36]
 Susquehannock villages contained numerous longhouses surrounded by a double palisade. Each bark-covered shelter was up to 80 feet (24 m) in length and housed as many as 60 individuals. Multiple families related through the female family line would live in one longhouse. Sons lived within this extended family household until they married, upon which time they would move to their wife's family's longhouse.[36]
 Archaeological evidence from trash and burn pits indicates that the Susquehannock had a varied and seasonal diet. Maize, beans and squash were staple foods, with maize-based meals, usually in the form of soup, making up nearly half of their caloric intake. Deer was the most common animal protein but elk, black bear, fish, freshwater mussels, wild turkey and waterfowl were also eaten. Wild plants, fruits, and nuts supplemented their diets.[37]
 Iroquoian people called maize, beans and squash the Three Sisters. In a technique known as companion planting, maize and climbing beans were planted together in mounds, with squash planted between the mounds.[36] Dried crops were kept in circular or bell-shaped subterranean storage pits lined with bark and dried grasses.[7]
 Susquehannock women made shell-tempered pottery of various sizes primarily for cooking. Three different pottery types, corresponding to three different phases of occupation in what is now Lancaster County, have been identified. Schultz Incised is a high-collared, cordmarked pottery type that was produced until c. 1600. The collars are marked with incised lines that form geometric patterns. Schultz Incised has also been found at sites near Tioga Point. Washington Boro Incised, produced between 1600 and 1635, is similar in some respects to Schultz Incised, however, the collar is not as wide. Known as ""face pots"" their distinguishing feature is the presence of two to four expressionless human faces on the collars. In the mid-17th century, as European goods became more common, pot design became simpler, and many of the pots used for cooking were replaced by brass kettles. Strickler Cordmarked, produced between 1635 and 1680 lacked the collars, geometric designs and face effigies of the earlier pottery types.[38]
 While Susquehannock women cultivated crops and managed the household, the men engaged in extended periods of travel for hunting, trading, and raids against neighbouring tribes. They also constructed and tended the fishing weirs that were used to catch American shad and eels.[36]
 The Susquehannock relied on a network of footpaths to cross their territory. Of particular importance was the Great Minquas Path between the Susquehanna River and the Delaware River which the Susquehannock used to reach Dutch and Swedish trading posts.[39] For fishing and carrying cargoes of meat, pelts and people across the Susquehanna River, dugout canoes were used.[40]
 The Susquehannock typically buried their dead in individual graves in cemeteries located outside the palisade walls. A number of multiple burials have also been found, especially at the Strickler site which was occupied from c. 1645 to c. 1665. These burials typically were of an adult and one or more children. Bodies were flexed and usually accompanied by a variety of grave goods such as bead or shell necklaces, pendants, tobacco pipes, combs, knives, clay pots, brass kettles, and occasionally gun parts.[41]
 Among the gifts that Smith received from the Susquehannock in 1608 were several long-stemmed clay pipes. Tobacco was an important aspect of Susquehannock culture, but its use did not become widespread until the mid-16th century. Almost all graves dating from this period, including those of women and older children, contained pipes among the grave goods. The vocabulary compiled by Campanius includes words specifically meaning ""smoking tobacco"", as well as a word for ""pipe for smoking tobacco."" Pipes were either formed from clay or carved from soapstone. The bowls were frequently decorated with geometric designs or with human or animal effigies.[7]
 Smith described the Susquehannock as ""gyant-like people,"" however, osteoarchaeological evidence from burial sites in the lower Susquehanna River valley has not shown that the Susquehannock were exceptionally tall compared to Europeans and other Indigenous groups.[7] A recent reevaluation of the skeletal remains in the collection of Franklin & Marshall College has provided an average height for Susquehannock adult males of 174.7 centimeters (68.8 inches),[42] however, skeletal remains in England show a similar average height for adult males in the early 17th century.[43] Smith's description was based on an arranged meeting he had with 60 adult males who were likely chosen because they were physically intimidating.[42]
 A number of locations have been named after the Susquehannock:
 Barry Kent's Jacob My Friend: His 17th Century Account of the Susquehannock Indians is a historical novel about Dutch fur-trader and interpreter Jacob Young who married a Susquehannock woman and had several children.
 A graphic novel, documentary, and teaching material, under the title Ghost River, a project of the Library Company of Philadelphia and supported by The Pew Center for Arts & Heritage, addresses the Paxton massacres of 1763 and provides ""interpreters and new bodies of evidence to highlight the Indigenous victims and their kin.""[44]
"
Fort Le Boeuf,https://en.wikipedia.org/wiki/Fort_Le_Boeuf,"
 Fort Le Bœuf (often referred to as Fort de la Rivière au Bœuf) was a fort established by the French during 1753 on a fork of French Creek (in the drainage area of the River Ohio), in present-day Waterford, in northwest Pennsylvania. The fort was part of a line that included Fort Presque Isle, Fort Machault, and Fort Duquesne.
 The fort was located about 15 miles (24 km) from the shores of Lake Erie, on the banks of LeBoeuf Creek, for which the fort was named. The French portaged supplies and trade goods from Lake Erie overland to Fort Le Bœuf. From there they traveled by raft and canoe down French Creek to the rivers Allegheny, Ohio and Mississippi.
 Today, the site of the fort is occupied by the Fort LeBoeuf Museum,[1] operated by the Fort LeBoeuf Historical Society.
 Captain Paul Marin de la Malgue began construction on 11 July 1753;[2] Jacques Legardeur de Saint-Pierre began command of the fort on 3 December 1753. This fort was the second of a series of posts that the French built between spring 1753 and summer 1754 to assert their possession of the Ohio Country. These four posts Fort Presque Isle, Fort LeBoeuf, Fort Machault, and Fort Duquesne ran from Lake Erie to the Forks of the Ohio; they represented the last links of France's effort to connect its dominions in Canada with those in the Illinois Country and Louisiana. They seized and occupied the  British trading post of John Frazier, a Scots, at the Lenape village of Venango at the junction of French Creek and the Allegheny River (where Franklin, Pennsylvania developed). Leaving a force to garrison the new posts, the French command returned to Canada for the winter.[3]
 Fort LeBoeuf (modern Waterford, Pennsylvania developed here) guarded the southern end of the portage road, known as the Venango Path, between Lake Erie and French Creek, which flowed to the Allegheny River and ultimately to the River Ohio. It served as a French trading post and garrison until 1759, when the capture of Fort Niagara forced the French to abandon the Ohio Country.[4]
 Robert Dinwiddie, the governor of Virginia, sent the 21-year-old George Washington, a major in the Virginia militia, to Fort Le Boeuf with seven escorts, in order to deliver a message to the French demanding that they leave the Ohio Country. Dinwiddie was responding to news of the French building forts in the Ohio Country. Washington took explorer Christopher Gist along as his guide; during the trip, Gist saved the young Washington's life on two occasions.  Washington and Gist arrived at Fort Le Boeuf on 11 December 1753. Jacques Legardeur de Saint-Pierre, commandant at Fort Le Boeuf, a tough veteran of the west, received Washington politely, but contemptuously rejected his blustering ultimatum.[5]
 Jacques Legardeur de Saint-Pierre provided Washington with three days rest and hospitality at the fort, and then gave Washington a letter for him to deliver to Governor Dinwiddie. The letter ordered the Governor of Virginia to deliver his demand to the Major General of New France in the capital, Quebec City.[6]
 During his stay, Washington noted that the fort had one hundred men, numerous officers, and birch canoes and 70 pine canoes, many unfinished. He described the fort as on a south or west fork of French Creek, near the water, and almost surrounded by it. Four houses composed the sides. The bastions were made of piles driven into the ground, standing more than 12 feet (3.7 m) high, and sharpened at the top. Port holes for cannon and loop-holes for small-arms were cut into the bastions. Each bastion mounted eight six-pound cannon and one four-pound cannon guarded the gate. Inside the bastions stood a guard-house, chapel, doctor's lodging and the commander's private stores. Outside the fort were several log barracks, some covered with bark, others with boards. In addition, there were stables, a smithy and other buildings.
 The French and Indian War began in North America on 28 May 1754 with the Battle of Jumonville Glen. (It was the regional front of the Seven Years' War between Britain and France in Europe.) Some four years later, on 25 July 1759, the French surrendered Fort Niagara to the British. 
 During August 1759, the commander of Fort Presque Isle sent an order to Fort Le Boeuf and Fort Machault for the officers and troops to abandon their positions and move north. Before they left, they burned the forts to make them unavailable to the British. The British rebuilt these forts and gave them British names, as they renamed the former Fort Duquesne at the Forks of the Ohio as Fort Pitt. 
 During Pontiac's Rebellion, on 18 June 1763, a war party of Native Americans burned Fort Le Boeuf. The survivors escaped to Fort Venango (formerly Fort Machault), but it too was burned, so they continued to Fort Pitt.
 On 1 August 1794, Major Ebenezer Denny reported to Governor Thomas Mifflin from Le Boeuf. He described a fortification with four blockhouses, manned by riflemen. The two rear blockhouses had a six-pound cannon on the second floor, as well as swivel guns over the gates.
 When Judge Vincent settled in Waterford during 1797, he wrote, ""There are no remains of the old French fort excepting the traces on the ground...""
"
Jacques Legardeur de Saint-Pierre,https://en.wikipedia.org/wiki/Jacques_Legardeur_de_Saint-Pierre,"Jacques Legardeur de Saint-Pierre (October 24, 1701 - September 8, 1755) was a Canadian colonial military commander and explorer who held posts throughout North America in the 18th century, just before and during the French and Indian War.
 He traced his lineage to a number of New France's prominent families. He was a grandson of Jean-Baptiste Legardeur de Repentigny (who had been elected the first mayor of Quebec City on October 17, 1663, and founded Repentigny, Quebec in 1670[2][3]) and a great-grandson of explorer Jean Nicollet de Belleborne.[1]  Most immediately however, his father Jean-Paul was an adventurer and had founded a post at Chagouamigon in what is now Wisconsin in 1718.[1] It is believed that Jacques spent a number of years there with his father where he obtained an excellent knowledge of the Indian languages and the business conducted in the trading posts.
 In 1724 he began military service as a second ensign with the colonial regular troops. Because of his skills as an interpreter, his early active duty involved building loyalty and support among  the Ojibwa, Cree, and Sioux to assist the French in future campaigns against other Indian tribes. From 1734 to 1737, he was commandant at Fort Beauharnois (on Lake Pepin, along the present day Wisconsin-Minnesota border) and caught in the middle of tribal rivalries. Fearing for himself and his garrison, he abandoned and burned the fort in May 1737.
 From 1737 to 1740 he campaigned against the Chickasaws and drew praise for his skills in dealing with the Indians. In 1741 he made lieutenant and was commandant of the Miami post near what is now known as Fort Wayne, Indiana for a short while. From 1745–1747 he was stationed at Montreal and carried out many military assignments, including a raid on the British colonial settlement of Saratoga, New York. In 1747 he and Louis de la Corne fought the British and their Indian allies in the Lachine area.
 From 1748–1750 he served at Fort Michilimackinac during which time he was promoted to captain. The whole period was spent negotiating a fragile peace between the warring Indian nations. In 1750 Governor La Jonquière appointed him Western commander to lead in the search for the western sea, a project that had been headed by Pierre Gaultier de Varennes et de La Vérendrye who had died in 1749 while planning a new expedition. He was soon involved in a dispute with two of  the La Vérendrye sons, Louis-Joseph Gaultier de La Vérendrye and Pierre Gaultier de La Vérendrye, who had been actively involved in this project with their father. To his credit, Saint-Pierre recognized their past role and apologized. During his tenure, he was quite active. He travelled several times to the area of the Red and Winnipeg rivers and Fort St. Charles on Lake of the Woods. He no doubt headquartered at Fort La Reine. At least one important post was established under his command; that being Fort La Jonquière on the Saskatchewan River (probably in the Nipawin, Saskatchewan area). The western sea explorations were not expanded much beyond that point. The story goes that when Fort La Reine was invaded by a group of Assiniboines he saved the fort by standing at the door of the powder magazine with a blazing brand and threatening to blow everyone up if they did not leave.[4]
 Returning from the western forts in 1753, Saint-Pierre was assigned to the Ohio Country, where the French and Canadians were building a strong presence in order to deal with the economic threat posed by British fur traders. Paul Marin de la Malgue constructed two forts, the main one being Fort Le Boeuf. After Marin's death, Legardeur took over Marin's command. The French occupation drew attention from the Virginia Colony, and its lieutenant governor, Robert Dinwiddie, sent a young George Washington with a written demand that the French leave the disputed territory.  Much has been made of this event because of the later fame of the messenger; history records that Washington was impressed by Saint-Pierre in this encounter.[citation needed]  After Washington in 1754 led an expedition that ambushed a Canadian party, Saint-Pierre was in the party sent from Fort Duquesne that defeated Washington at Fort Necessity. In 1755 Jacques led a large contingent of militia and Indians from Montreal into a battle at Lac du Saint-Sacrement (as Lake George, New York was then known) and was immediately killed. These actions contributed to beginning of the French and Indian War with the eventual formal declarations of war in spring 1756.[5]
 Pierre's service to France in North America was valuable and extensive. He was a logical successor to La Vérendrye in the western forts and fur trade. He was awarded the cross of Saint Louis for his endeavors in military action. He was married but had no children. His widow remarried in 1757, to Luc de la Corne.
"
Virginia Regiment,https://en.wikipedia.org/wiki/Virginia_Regiment,"The Virginia Regiment was an infantry unit of the Virginia Provincial Forces raised in 1754 by the Virginia General Assembly and Governor Robert Dinwiddie for service in the French and Indian War. The sole provincial unit raised by the British colony of Virginia during the conflict, it initially consisted of 300 men under the command of Colonel George Washington and fought in the battles of Jumonville Glen and Fort Necessity. After the Virginia Regiment's defeat at Fort Necessity, the General Assembly voted to double the size of the unit, which participated in the failed Braddock Expedition to capture Fort Duquesne from the French. 
 Under orders from General Edward Braddock, the unit was re-organized into two carpenter companies, six ranger companies, and one troop of mounted rangers, fighting at the Battle of the Monongahela in 1755. The Virginia Regiment was subsequently expanded into two regiments for the 1758 Forbes Expedition. As a result of the outbreak of the Anglo-Cherokee War in 1762, the unit remained on the Virginia frontier for longer than expected, but was disbanded by Governor Francis Fauquier in 1762. Although Washington resigned from the regiment in 1758, upset over not being made an officer in the British Army, the experience he gained in the conflict greatly helped him during the American Revolutionary War.
 The Anglo-French conflict over the Ohio Country led to raising of the first provincial regiment in the British colony of Virginia. In 1754, the Virginia General Assembly voted to raise a regiment of 300 men and send it to the confluence of the Alleghany and Monongahela rivers. After the battle of Fort Necessity, the General Assembly voted to increase the size of the regiment from five companies to ten.[1][2] The Virginian provincial troops who participated in the Braddock Expedition of 1755 and suffered defeat at the Battle of the Monongahela were unregimented: at the behest of General Edward Braddock, they had been organized into two companies of carpenters, six companies of rangers, and one troop of mounted rangers, about 450 men in all. The remaining 350 men from the original ten companies of the Virginia Regiment had been allocated to the two regular regiments of the expedition.[3][4]
 After the defeat of the expedition, the Virginia Regiment was immediately reformed, with the General Assembly voting in 1755 to increase its size again, to 1,500 men organized in 16 companies. The actual strength of the Regiment in 1756 was 1,400 men, but in 1757 it was reduced to 1,000 men. In 1758, Virginia raised two additional regiments of a thousand men each for the Forbes Expedition. The enlistment period for the first regiment expired in May 1759, and for the second in December 1758. After the fall of Fort Duquesne, the General Assembly voted in 1759 to fill the one regiment still in service, and to raise a force of another 500 men that would remain in the province for its immediate defense. The regiment would remain in service until May 1760.[5][6]
 With the outbreak of the Anglo-Cherokee War, the General Assembly prolonged the Regiment's service, adding 300 men in three companies as frontier guards. It remained on the Cherokee frontier until early 1762, when Governor Francis Fauquier disbanded it. When, later in 1762, the British government indicated its wish for Virginia to raise a regiment which would be put on the British establishment, the General Assembly instead voted to re-raise the Virginia Regiment. This re-raised Regiment was finally disbanded in May 1763, just before the outbreak of Pontiac's War, as the province could not maintain it without a supply of paper money, which the Board of Trade had disallowed.[7]
 Most recruits were characterized by Washington as ""loose, Idle Persons ... quite destitute of House, and Home.""[8] Hampered by frequent desertions because of poor supplies, extremely low pay and hazardous duty, Virginia Regiment recruiters went to Pennsylvania and Maryland for men. Washington said of them, "" and not a few... have Scarce a Coat, or Waistcoat, to their Backs ...""[8] Later drafts pulled only those who could not provide a substitute or pay the £10 exemption fee, ensuring that only Virginia's poor would be drafted. White males between 16 and 50 were permitted to serve, although the regiment's size rolls report men as young as 15 and as old as 60 in the ranks, along with references to a small number of drafts with partial African and Native American ancestry.[citation needed]
 The First Virginia Regiment is memorialized in a statue in Meadow Park, a triangular park in Richmond’s (VA) Fan District by sculptor Ferruccio Legnaioli. Dedicated on 1 May 1930, to commemorate the regiment for fighting in seven American Wars, including the Civil War when they served in the Confederate Army. The statue is a seven foot high bronze standing figure of a colonial infantryman that lists the founding date of the Regiment (1754) at its base. The figure is mounted on a pedestal eight feet high which is lined with bronze plaques describing the history and service of the Regiment through seven wars.[9]
 The statue was pulled down from its pedestal during the night of 19–20 June 2020. It was the fifth statue toppled in Richmond during a series of civil rights protests.[10]
 Source:[14]
 
"
Point State Park,https://en.wikipedia.org/wiki/Point_State_Park,"
 Point State Park (locally known as The Point) is a Pennsylvania state park which is located on 36 acres (150,000 m2) in Downtown Pittsburgh, Allegheny County, Pennsylvania, US, at the confluence of the Allegheny and Monongahela rivers, forming the Ohio River.
 Built on land that was acquired via eminent domain from industrial enterprises during the 1950s, this park opened in August 1974[4] after construction was completed on its iconic fountain. Pittsburgh settled on the current design after rejecting an alternative plan for a Point Park Civic Center designed by Frank Lloyd Wright.
 The park also includes the outlines and remains of two of the oldest structures in Pittsburgh, Fort Pitt and Fort Duquesne. The Fort Pitt Museum, which is housed in the Monongahela Bastion of Fort Pitt, commemorates the French and Indian War (1754–63), during which the area soon to become Pittsburgh became a major battlefield. It was designated as a National Historic Landmark in 1960 for its role in the strategic struggles between Native Americans, French colonists, and British colonists, for control of the Ohio River watershed.[5]
 Today, the park provides recreational space for workers, visitors, and residents in downtown Pittsburgh, and also acts as the site for major cultural events in the city, including the Venture Outdoors Festival, Three Rivers Arts Festival and Three Rivers Regatta. The park is operated by the Pennsylvania Bureau of State Parks.[6]
 The location of the fountain at the tip of the Point previously served as a connector for two old bridges, the Manchester Bridge (over the Allegheny River) and Point Bridge (over the Monongahela). Both were removed in 1970 to make way for the fountain.
 In April 2009, the fountain was turned off for a $9.6 million upgrade and refurbishment; it went online again at the opening of the Three Rivers Arts Festival on June 7, 2013.[7]
 The fountain also serves as the western terminus for the Great Allegheny Passage, a 150-mile hiker-biker trail beginning at the 184.5 milepost of the Cumberland, MD terminus of the Chesapeake & Ohio Canal National Historical Park, which begins in the Georgetown area of Washington, DC, thus forming in total a 350-mile recreational trail between DC and Pittsburgh.
 On October 11, 2006, Michael DeBerardinis, Secretary of the Pennsylvania Department of Conservation and Natural Resources, announced a $25 million plan to renovate Point State Park. The plans called for improving the green spaces within the park, expanding recreational opportunities, preserving historical installations, and updating outdated amenities. Those estimated costs grew to a final figure of $36 million [2]. The project was scheduled to be completed within four years, with the majority of the work to be finished in time for Pittsburgh's 250th anniversary celebration in 2008.[8]
 Sections of the park had fallen into disuse since it was established in the summer of 1974. The homeless had used the trenches surrounding the foundations of the remains of Fort Pitt as a temporary shelter for years. Graffiti on the structures of the park had become a major problem. Sections of the park were littered with fence posts, cut logs, plastic drums, and rolled up snow drift fencing. The walkways were cracked and beginning to fall apart. The restoration project aimed to reestablish the park as a recreational destination.[8]
 Plans for the 2006 improvements to the park included installing new pumps and pipes in the fountain, establishing a seating area around the fountain and a wading area for children, restoring the river walk with steps that lead into the river, building kiosks for information and concessions, renovating the rest rooms, water taxi landings and surrounding docks, and installing wireless internet access hubs.[8]
 These plans were not put into place without some controversy. On January 25, 2007, thirteen members of two local labor unions were arrested for blocking access by contractors to the work sites at the remnants of Fort Pitt. The unions were protesting the use of four non-represented workers by the contractor. In addition, advocates for historical preservation disagreed with the decision to bury the remnants of the fort's walls, which could damage the bricks and remove the walls from public access.[9]
 Point State Park was reopened to the public in the spring of 2008. The renovation process took a year and a half to complete.[10]
 The confluence of the Allegheny and Monongahela rivers, creating the Ohio River, has greatly impacted the history of Point State Park. This confluence was referred to as the Forks of the Ohio,[12] which remains the official landmark-designated name for the site. It was once at the center of river travel, trade, and even wars throughout the pioneer history of Western Pennsylvania. During the mid-18th century, the armies of France and the Great Britain carved paths through the wilderness to control the point area and trade on the rivers. The French built Fort Duquesne in 1754 on foundations of Fort Prince George, which had been built by the colonial forces of Virginia.[13]
 The French held Fort Duquesne during the French and Indian War, and it became one of the focal points for that war because of its strategic riverside location in disputed territory. The French held the fort successfully early in the war, turning back the 1755 expedition led by General Edward Braddock. A smaller attack by James Grant in September 1758 was repulsed, but with heavy losses. Two months later, on November 25, the Forbes Expedition, under General John Forbes, captured the site after the French destroyed Fort Duquesne the day before. The British built the temporary Mercer's Fort in 1759, and later a much larger fort on the site, Fort Pitt.[13]
 The Forbes Expedition was successful where the Braddock expedition had failed because of the Treaty of Easton, in which local American Indians agreed to abandon their alliance with the French. American Indians, primarily Delawares and Shawnee, made this agreement with the understanding that the British military would leave the area after the war. The Indians did not want British army garrisons in their territory. The British, however, built Fort Pitt on the site, naming it after William Pitt the Elder.[13]
 As a result, in 1763 local Delawares and Shawnees took part in Pontiac's Rebellion, an effort to drive the British from the region. The Indians' siege of Fort Pitt began on June 22, 1763, but the fort was too strong to be taken by force. During a diplomatic meeting at the fort, the commander of Fort Pitt Simon Ecuyer gave Delaware Indian emissaries blankets that had been exposed to smallpox in hopes of infecting them, as suggested by trader William Trent. The use of blankets to spread smallpox was discussed and approved the next month between British General Jeffery Amherst and his subordinate Colonel Henry Bouquet, who was marching to Fort Pitt with a force of 460 soldiers.[14] Smallpox was highly contagious among the Native Americans, and — together with measles, influenza, chicken pox, and other Old World diseases — was a major cause of death since the arrival of Europeans and their animals. A reported outbreak that began the spring before left as many as one hundred Native Americans dead in Ohio Country from 1763 to 1764. It is not clear, however, whether the smallpox was a result of the Fort Pitt incident or the virus was already present among the Delaware people as outbreaks happened on their own every dozen or so years[15] and the delegates were met again later and they seemingly had not contracted smallpox.[16][17][18] On August 1, 1763, most of the Indians broke off the siege to intercept an approaching force under Colonel Bouquet, resulting in the Battle of Bushy Run. Bouquet fought off the attack and relieved Fort Pitt on August 20.[13]
 After Pontiac's War, Fort Pitt was no longer necessary to the British Crown, and was abandoned to the locals in 1772. At that time, the Pittsburgh area was claimed by both Virginia and Pennsylvania, and a power struggle for the region commenced. Virginians took control of Fort Pitt, and for a brief while in the 1770s it was called Fort Dunmore, in honour of Virginia's Governor Lord Dunmore. The fort was a staging ground in Dunmore's War of 1774.[13]
 During the American Revolutionary War, Fort Pitt was the headquarters for the western theatre of the war.[13]
 A small brick building called the Blockhouse—actually an outbuilding known as a redoubt—remains in Point State Park, the only intact remnant of Fort Pitt. It was erected in 1764, and is believed to be the oldest building, not only in Pittsburgh, but in Western Pennsylvania. Used for many years as a house, the blockhouse was purchased and has been preserved for many years by the Daughters of the American Revolution, who open it to the public.[13]
 During the city's early history, the Point became a hub for industry and transportation. By the 1930s it was occupied by warehouses and railroad yards. Frank Lloyd Wright commented in 1935 that the city had wasted the potential of its rivers and hilly landscape. In 1945, the situation in this regard was even worse; four years of World War II and eight years of the Great Depression had permitted urban blight to make substantial inroads. The total assessed property value in the Golden Triangle was at a record low and falling. At the point, rarely used railroad facilities included 15 acres (61,000 m2) of yards (60000 m2) and half a mile (800 m) of elevated train tracks.[13]
 City leaders wanted to alleviate traffic congestion in the city. The Manchester Bridge and Point Bridge met at the point in a way that left no room for the interchange required by the volume of traffic. The growth of the city had made parking a serious problem as well. Robert Moses, who had developed the traffic scheme for New York City, was brought in and published a traffic plan in 1939. He offered a circular park surrounded by approach roads connecting the bridges to the city. His plan retained the existing bridges, but most of the experts brought in to examine the problem called for the bridges to be relocated away from the Point, creating more space for adjoining access roads and interchanges.
 Although the site had substantial problems, Robert Alberts observes that ""the condition, in the view of the city planner, was almost perfect"". The Point had few property owners who needed to be bought out, few residents who would need relocation, and few structures worth preserving. Architects and planners could treat it as a tabula rasa.[13]
 During the course of the Second World War, federal and local authorities established three goals for the site: ""the creation of a park commemorating the site's history, improved traffic circulation through the construction of new roads and bridges, and designation of a portion of the site for new office buildings, intended to stimulate private interest in the Golden Triangle"". The Allegheny Conference on Community Development became a driving force for these changes. Edgar Kaufmann, a Pittsburgh department store owner, sat on the board of the Conference and became chair of the 28-member committee convened to look into the Point Park problem.[citation needed]
 Kaufmann wanted a plan for the Point that was more urban and developed than the park others were imagining. In particular, Kaufmann was a major supporter of Pittsburgh Civic Light Opera and wanted to provide it with a permanent building. He brought in Wright, by that time a preeminent architect, who had done numerous other projects for Kaufmann in the past, including Kaufmann's landmark home at Fallingwater and an unbuilt design for a parking garage.[citation needed]
 Plans for Point Park Civic Center fell through and ultimately, the site was turned into a park with historic and recreational aspects, Point State Park. The Fort Pitt blockhouse remained intact, and three of the five bastions of the fort have been restored. The state acquired almost all property for the site by 1949, at a cost of $7,588,500 ($ 97.2 million in 2025); the park was finally completed in August 1974. Areas adjoining the park were condemned to permit commercial development, most notably Gateway Center.
 Moving downstream about 100 yards:
 The following state parks are within 30 miles (48 km) of Point State Park:[19][20][21]
[22]
"
Fort Duquesne,https://en.wikipedia.org/wiki/Fort_Duquesne,"Fort Duquesne (/djuːˈkeɪn/ dew-KAYN, .mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}French: [dykɛːn]; originally called Fort Du Quesne[citation needed]) was a fort established by the French in 1754, at the confluence of the Allegheny and Monongahela rivers. It was later taken over by the British, and later the Americans, and developed as Pittsburgh in the U.S. state of Pennsylvania. Fort Duquesne was destroyed by the French before its British conquest during the Seven Years' War, known as the French and Indian War on the North American front. The British replaced it, building Fort Pitt between 1759 and 1761. The site of both forts is now occupied by Point State Park, where the outlines of the two forts have been laid in granite slabs.[2]
 Fort Duquesne, built at the confluence of the Allegheny and Monongahela rivers which forms the Ohio River, was considered strategically important for controlling the Ohio Country,[3] both for settlement and for trade. The English merchant William Trent had established a highly successful trading post at the forks as early as the 1740s, to do business with a number of nearby Native American villages. Both the French and the British were keen to gain advantage in the area.
 As the area was within the drainage basin of the Mississippi River, the French had claimed it as theirs. They controlled New France (Quebec), the Illinois Country along the Mississippi, and La Louisiane, the ports of New Orleans and Mobile, Alabama.
 In the early 1750s, the French began construction of a line of forts, starting with Fort Presque Isle on Lake Erie in present-day Erie, Pennsylvania, followed by Fort Le Boeuf, about 15 miles south in present-day Waterford, Pennsylvania, and Fort Machault, on the Allegheny River in Venango County in present-day Franklin, Pennsylvania.
 Robert Dinwiddie, Lieutenant Governor of the Virginia Colony, thought these forts threatened extensive claims to the land area by Virginians (including himself) of the Ohio Company.
 In late autumn 1753, Dinwiddie dispatched a young Virginia militia officer named George Washington to the area to deliver a letter to the French commander at Fort Le Boeuf, asking them to leave. Washington was also to assess French strength and intentions. After reaching Fort Le Boeuf in December, Washington was politely rebuffed by the French.
 Following Washington's return to Mount Vernon in January 1754, Dinwiddie sent Virginians to build Fort Prince George at the Forks of the Ohio. Work began on the fort on February 17. By April 18, a much larger French force of five hundred under the command of Claude-Pierre Pécaudy de Contrecœur arrived at the forks, forcing the small British garrison to surrender. The French knocked down the tiny British fort and built Fort Duquesne, named in honor of Marquis Duquesne, the governor-general of New France. The fort was built on the same model as the French Fort Frontenac on Lake Ontario.[4]
 Washington, who was lieutenant colonel in the newly created Virginia Regiment, set out on April 2, 1754, with a small force to build a road to, and then defend, Fort Prince George. Washington was at Wills Creek in north central Maryland when he received news of the fort's surrender. On May 28,[5] Washington encountered a Canadian scouting party near a place now known as Jumonville Glen (several miles east of present-day Uniontown). Washington attacked the French Canadians, killing 10 in the early morning hours, and took 21 prisoners, of whom many were ritually killed by the Native American allies of the British.  On May 31, Washington replaced Colonel Joshua Fry as commander of the Virginia Regiment after Colonel Fry died en route to Wills Creek.[6]
 The Battle of Jumonville Glen is widely considered the formal start of the French and Indian War, the North American front of the Seven Years' War.[7][8]
 Washington ordered construction of Fort Necessity at a large clearing known as the Great Meadows. On 3 July 1754, the counterattacking French and Canadians forced Washington to surrender Fort Necessity. After disarming them, they released Washington and his men to return home.
 Although Fort Duquesne's location at the forks looked strong on a map—controlling the confluence of three rivers—the reality was rather different.  The site was low, swampy, and prone to flooding.  In addition, the position was dominated by highlands across the Monongahela River, which would allow an enemy to bombard the fort with ease.  Pécaudy de Contrecœur was preparing to abandon the fort in the face of Braddock's advance in 1755. He was able to retain it due to the advancing British force being annihilated (see below).  When the Forbes expedition approached in 1758, the French had initial success in the Battle of Fort Duquesne against the English vanguard, but were forced to abandon the fort in the face of the much superior size of Forbes' main force.
 The French held the fort successfully early in the war, turning back the expedition led by General Edward Braddock during the 1755 Battle of the Monongahela. George Washington served as one of General Braddock's aides.  A smaller attack by James Grant in September 1758 was repulsed with heavy losses.
 Two months later, on November 25, 1758, the Forbes Expedition under the Scotsman General John Forbes took possession Fort Duquesne after the French destroyed and abandoned the site.[9]
 Fort Duquesne was built at the point of land of the confluence of the Allegheny and Monongahela Rivers, where they form the Ohio River. Since the late 20th century, this area of Downtown Pittsburgh has been preserved as Point State Park. The park includes a brick outline of the fort's walls, as well as outlines to mark the later Fort Pitt.
 In May 2007, Thomas Kutys, an archaeologist with A.D. Marble & Company, a Cultural Resource Management firm based in Conshohocken, Pennsylvania, discovered a stone and brick drain on the Fort Duquesne site. It is thought to have drained one of the fort's many buildings.  Due to its depth in the ground, this drain may be all of the fort that has survived.  The entire northern half of the former fort site was disrupted and destroyed by the heavy industrial development of the area during the 19th century.[10]
 On November 25, 1958, the 200th anniversary of the capture of Fort Duquesne, the U.S. Post Office issued a 4-cent Fort Duquesne bicentennial commemorative stamp. It was first released for sale at the post office in Pittsburgh. The design was reproduced from a composite drawing, using various figures taken from an etching by T.B. Smith and a painting portraying the British occupation of the site as the Fort Duquesne blockhouse burns in the background.
 Colonel Washington is depicted on horseback in the center, while General Forbes, who was debilitated by intestinal disease, is shown lying on a stretcher. The stamp also depicts Colonel Henry Bouquet, who was second in command to the ailing Forbes, and other figures who represent the Virginia militia and provincial army.[11]
 Fort Duquesne is the subject of, or referenced, in:
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}40°26′29.9″N 80°00′39.4″W﻿ / ﻿40.441639°N 80.010944°W﻿ / 40.441639; -80.010944
"
Fort Necessity National Battlefield,https://en.wikipedia.org/wiki/Fort_Necessity_National_Battlefield,"

 Fort Necessity National Battlefield is a National Battlefield in Fayette County, Pennsylvania, United States, which preserves the site of the Battle of Fort Necessity.  The battle, which took place on July 3, 1754, was an early battle of the French and Indian War, and resulted in the surrender of British colonial forces under Colonel George Washington, to the French and Indians, under Louis Coulon de Villiers.
 The site also includes the Mount Washington Tavern, once one of the inns along the National Road, and in two separate units the grave of British General Edward Braddock, killed in 1755, and the site of the Battle of Jumonville Glen.
 After returning to the Great Meadows in northwestern Virginia, and what is now Fayette County, Pennsylvania, George Washington decided it prudent to reinforce his position. Supposedly named by Washington as Fort Necessity or Fort of Necessity, the structure protected a storehouse for supplies such as gunpowder, rum, and flour. The crude palisade they erected was built more to defend supplies in the fort's storehouse from Washington's own men, whom he described as ""loose and idle"", than as a planned defense against a hostile enemy.  The sutler of Washington's force was John Fraser, who earlier had been second-in-command at Fort Prince George.  Later he served as chief scout to General Edward Braddock and then chief teamster to the Forbes Expedition.
 By June 13, 1754, Washington had under his command 295 colonials and the nominal command of 100 additional regular British army troops from South Carolina. Washington spent the remainder of June 1754 extending the wilderness road further west and down the western slopes of the Allegheny range into the valley of the Monongahela River. He wanted to create a river crossing point roughly 41 mi (66 km) away, near Redstone Creek and Redstone Old Fort.
 This was a prehistoric Native American earthwork mound on a bluff overlooking the river crossing.  The aboriginal mound structure may have once been part of a fortification. Five years later in the war, in 1759, Fort Burd was constructed at Redstone Old Fort. The area eventually became the site of Nemacolin Castle and Brownsville, Pennsylvania—an important western jumping-off point for travelers crossing the Alleghenies in the late 18th and early 19th centuries.
 To reach the Ohio River basins' navigable waters as soon as possible on the Monongahela River, Washington chose to follow Nemacolin's Trail, a Native American trail which had been somewhat improved by colonists, with Nemacolin's help.  He preferred this to following the ridge-hopping, high-altitude path traversed by the western part of the route that was later chosen for Braddock's Road.  It jogged to the north near the fort and passed over another notch near Confluence, Pennsylvania, into the valley and drainage basin of the Youghiogheny River. The Redstone destination at the terminus of Nemacolin's Trail was a natural choice for an advanced base.  The location was one of the few known good crossing points where both sides of the wide deep river had low accessible banks; steep sides were characteristic of the Monongahela River valley.
 Late in the day on July 3, Washington did not know the French situation. Believing his situation was impossible, he accepted surrender terms which allowed the peaceful withdrawal of his forces, which he completed on July 4, 1754.[4] The French subsequently occupied the fort and then burned it. Washington did not speak French, and stated later that if he had known that he was confessing to the ""assassination"" of Joseph Coulon de Jumonville, he would not have signed the surrender document.
 During the Great Depression of the 20th century, attempts to preserve the location of Fort Necessity were undertaken.  On March 4, 1931, Congress declared the location a National Battlefield Site under management of the War Department. Transferred to the National Park Service in 1933, the park was redesignated a National Battlefield on August 10, 1961. As with all historic sites administered by the National Park Service, the battlefield was listed on the National Register of Historic Places on October 15, 1966.
 Subsequent archaeological research helped to uncover the majority of the original fort position, shape and design. A replica of the fort was constructed on site in the 1970s. A new visitor center, which also is home to a National Road interpretive center, opened on October 8, 2005. The battlefield and fort are currently being improved; the battlefield itself has seen much vegetation growth, and the general public are asked to stay out of the battlefield grounds. As the remains of the casualties of the battle were never truly recovered, the battlefield is treated as hallowed ground.
 On a hillside adjacent to the battlefield and within the boundaries of the park is Mount Washington Tavern, a classic example of the many inns once lining the National Road, the United States' first federally funded highway.
 The land on which the tavern was built was originally owned by George Washington.  In 1770 he purchased the site on which he had commanded his first battle.  Around the 1830s, Judge Nathanial Ewing of Uniontown constructed the tavern. James Sampey acquired the tavern in 1840.  It was operated by his family until the railroad construction boom caused the National Road to decline in popularity, rendering the inn unprofitable.
 In 1855, it was sold to the Fazenbaker family.  They used it as a private home for the next 75 years, until the Commonwealth Of Pennsylvania  purchased the property in 1932.  In 1961 the National Park Service purchased the property from the state, making the building a part of Fort Necessity.  The Mount Washington Tavern demonstrates the standard features of an early American tavern, including a simple barroom that served as a gathering place, a more refined parlor that was used for relaxation, and bedrooms in which numerous people would crowd to catch up on sleep.
 In a separate unit of the park, lying about one mile (1.6 km) northwest of the battlefield, is the grave of General Edward Braddock. The British commander led a major expedition to the area in 1755 which included the construction of Braddock's Road, a useful but inadequate wilderness road through western Pennsylvania. Braddock was severely wounded in the Battle of the Monongahela as the British advanced toward Fort Duquesne.
 He and his forces fled along the wilderness road to a site near Great Meadows. Braddock died on July 13, 1755, and was buried in an elaborate ceremony officiated by George Washington. He was buried under the road in order to hide the location of his grave from the enemy French and Indians.[5] In 1804 Braddock's remains were discovered by men making repairs to the wilderness road.[citation needed] A marker was erected in 1913.
"
Battle of Jumonville Glen,https://en.wikipedia.org/wiki/Battle_of_Jumonville_Glen,"
 The Battle of Jumonville Glen, also known as the Jumonville affair, was the opening battle of the French and Indian War,[5] fought on May 28, 1754, near present-day Hopwood and Uniontown in Fayette County, Pennsylvania.  A company of provincial troops from Virginia under the command of Lieutenant Colonel George Washington, and a small number of Mingo warriors led by the chieftain Tanacharison (also known as the ""Half King""), ambushed a force of 35 French Canadians under the command of Joseph Coulon de Jumonville.
 A larger French Canadian force had driven off a small crew attempting to construct Fort Prince George under the auspices of the Ohio Company at present-day Pittsburgh, Pennsylvania, land claimed by the French. A British colonial force led by George Washington was sent to protect the fort under construction. The French Canadians sent Jumonville to warn Washington about encroaching on French-claimed territory. Washington was alerted to Jumonville's presence by Tanacharison, and they joined forces to ambush the French Canadian camp. Washington's force killed Jumonville and some of his men in the ambush and captured most of the others. The exact circumstances of Jumonville's death are a subject of historical controversy and debate.
 Since Britain and France were not then at war, the event had international repercussions, and was a contributing factor in the start of the Seven Years' War in 1756, also known as the French and Indian War in the United States.  After the action, Washington retreated to Fort Necessity, where Canadian forces from Fort Duquesne compelled his surrender.  The terms of Washington's surrender included a statement (written in French, a language that Washington did not read) that admitted that Jumonville was assassinated. That document and others were used by the French and the Canadians to level accusations that Washington had ordered Jumonville's slaying.
 Throughout the 1740s and early 1750s, British and French Canadian traders had increasingly come into contact in the Ohio Country, including the upper watershed of the Ohio River in what is now western Pennsylvania.[6] Authorities in New France became more aggressive in their efforts to expel British traders and colonists from the area and in 1753 began construction of a series of fortifications in the area.[7]
 The French action drew the attention of the British but also the Indian tribes of the area. Despite good Franco-Indian relations, British traders had become highly successful in convincing the Indians to trade with them in preference to the Canadians, and the planned large-scale advance was not well received by all.[8]  In particular, Tanacharison, a Mingo chief also known as the ""Half King,"" became decidedly anti-French as a consequence. In a meeting with Paul Marin de la Malgue, commander of the French and Canadian construction force, de la Malgue reportedly lost his temper, and shouted at the Indian chief, ""I tell you, down the river I will go. If the river is blocked up, I have the forces to burst it open and tread under my feet all that oppose me. I despise all the stupid things you have said.""[9] He then threw down some wampum that Tanacharison had offered as a goodwill gesture.[9] Marin died not long after, when command of the operations was turned over to Jacques Legardeur de Saint-Pierre.[10]
 Virginia Royal Governor Robert Dinwiddie sent Major George Washington to the Ohio Country (a territory that was claimed by several of the British colonies, including Virginia) as an emissary in December 1753 to tell the French to leave. Saint-Pierre politely informed Washington that he was there pursuant to orders, that Washington's letter should have been addressed to his commanding officer in Canada and that he had no intention of leaving.[11]
 Washington returned to Williamsburg and informed Governor Dinwiddie that the French refused to leave.[12] Dinwiddie commissioned Washington a lieutenant colonel, and ordered him to begin raising a provincial regiment to hold the Forks of the Ohio, a site Washington had identified as a fine location for a fortress.[13]  The Governor also issued a captain's commission to Ohio Company employee William Trent, with instructions to raise a small force and immediately begin construction of the fort. Dinwiddie issued these instructions on his own authority, without even asking for funding from the Virginia House of Burgesses until after the fact.[14]  Trent's company arrived on site in February 1754, and began construction of a storehouse and stockade with the assistance of Tanacharison and the Mingos.[14][15] The same month, an 800-strong French Canadian militia, as well as French troupes de la marine, departed Montreal for the Ohio River Valley under the command of Claude-Pierre Pécaudy de Contrecœur, a Canadian who took over command from Saint-Pierre.[16] When Contrecœur learned of Trent's activity, he led a force of about 500 men (consisting of troupes de la marine, militia, and Indians) to drive them off (rumors reaching Trent's men put its size at 1,000). On April 16, Contrecœur's force arrived at the forks. The next day, Trent's force of 36 men, led by Ensign Edward Ward in Trent's absence, agreed to leave the site.[17] The French then began construction of the fort they called ""Fort Duquesne"".[18]
 In March 1754, Dinwiddie ordered Washington back to the frontier with instructions to ""act on the [defensive], but in Case any Attempts are made to obstruct the Works or interrupt our [settlements] by any Persons whatsoever, You are to restrain all such Offenders, & in Case of resistance to make Prisoners of or kill & destroy them"".[19]  Historian Fred Anderson describes Dinwiddie's instructions, which were issued without the knowledge or direction of the British government in London, as ""an invitation to start a war"".[19]  Washington was ordered to gather up as many supplies and provincial troops as he could along the way. By the time he left for the frontier on April 2, he had recruited fewer than 160 men.[20]
 Along their march through the forests of the frontier, Washington was joined by more men at Winchester.[21] He then learned from Captain Trent of the French advance. Trent also brought a message from Tanacharison, who promised warriors to assist the British.[21] To keep Tanacharison's support, Washington decided not to turn back, choosing instead to advance. He reached a place known as the Great Meadows (now in Fayette County, Pennsylvania), about 37 miles (60 km) south of the forks, began to construct a small fort and awaited further news or instructions.[22]
 Contrecœur operated under orders that forbade attacks by his force unless they were provoked. On May 23, he sent Joseph Coulon de Villiers de Jumonville with 35 soldiers (principally French Canadian recruits)[23] to see if Washington had entered French territory and with a summons to order Washington's troops out. The summons was similar in nature to the one that Washington had delivered to them four months earlier.[2]
 On May 27, Washington was informed by Christopher Gist, a settler who had accompanied him on the 1753 expedition, that a French Canadian party numbering about 50 was in the area. In response, Washington sent 75 men with Gist to find them.[24] That evening, Washington received a message from Tanacharison, informing him that he had found the Canadian camp, and that the two of them should meet. Despite the fact that he had just sent another group in pursuit of the French Canadians, Washington went with a detachment of 40 men to meet with Tanacharison. The Mingo leader had with him 12 warriors, two of whom were boys.[1]  After discussing the matter, both leaders agreed to make an attack on the Canadians. The attackers took up positions behind rocks around the Canadian camp, counting not more than 40 Canadians.[1]
 Exactly what happened next has been a subject of controversy and debate. The few primary accounts of the affair agree on a number of facts and disagree on others.  They agree that the battle lasted about 15 minutes, that Jumonville was killed, and that most of his party were either killed or taken prisoner.[25] According to French Canadian records, most of the dead were French Canadians: Desroussel and Caron from Québec City, Charles Bois from Pointe-Claire, Jérôme from La Prairie, L'Enfant from Montréal, Paris from Mille-Isles, Languedoc and Martin from Boucherville, and LaBatterie from Trois-Rivières.[26]
 Washington's accounts of the battle exist in several versions; they are consistent with one another, but the details are compressed, according to historian Fred Anderson, with the intent to obscure post-battle atrocities.[27] Washington wrote in his diary, ""We were advanced pretty near to them ... when they discovered us; whereupon I ordered my company to fire ... [Wagonner's] Company ... received the whole Fire of the French, during the greatest Part of the Action, which only lasted a Quarter of an Hour, before the Enemy was routed.  We killed Mr. de Jumonville, the commander ... also nine others; we wounded one, and made Twenty-one Prisoners"".[28]
 
Contrecœur prepared an official report of the action that was based on two sources. Most of it came from a Canadian named Monceau, who escaped the action but apparently did not witness Jumonville's slaying: .mw-parser-output .block-indent{padding-left:3em;padding-right:0;overflow:hidden}   Contrecœur's second source was an Indian from Tanacharison's camp, who reported that ""Mr. de Jumonville was killed by a Musket-Shot in the Head, whilst they were reading the Summons"".[29] The same Indian claimed that the Indians then rushed in to prevent Washington's men from slaughtering the Frenchmen.[29]
 A third account was made by a private named John Shaw, who was in Washington's regiment but not present at the affair. His account, based on detailed accounts from others who were present, was made in a sworn statement on August 21; the details on Tanacharison's role in the affair are confirmed in a newspaper account printed on June 27.[30]  In his account, the French were surrounded while some still slept. Alerted by a noise, one of the Frenchmen 
   Shaw's narrative is substantially correct on a number of other details, including the size and composition of both forces.  Shaw also claimed to have seen and counted the dead, numbering 13 or 14.[31]
 Anderson documents a fourth account by a Virginian deserter, named Denis Kaninguen, and speculates that Kaninguen was one of Tanacharison's followers.[32] His report to the French commanders echoed that of Shaw: ""notwithstanding the discharge of musket fire that [Washington] had made upon him, he [Washington] intended to read [the summons] and had withdrawn himself to his people, whom he had [previously] ordered to fire upon the French. That [Tanacharison], a savage, came up to [the wounded Jumonville] and had said, ""Tu n'es pas encore mort, mon père!"" [Thou art not yet dead, my father!] and struck several hatchet blows with which he killed him.""[32]  Anderson notes that Kaninguen apparently understood what Tanacharison said, and understood it to be a ritual slaying.[33]  Kaninguen reported that 30 men were taken prisoner, and 10 to 12 had been killed.[33] The Virginians suffered only one killed and two or three wounded.[1][4]
 Washington wrote a letter to his brother after the battle in which he said ""I can with truth assure you, I heard bullets whistle and believe me, there was something charming in the sound.""[34] Following the battle, Washington returned to the Great Meadows and pushed onward the construction of a fort, which was called Fort Necessity. The dead were left on the field or buried in shallow graves, where they were later found by the French.[35]
 On June 28, 1754, a combined force of 600 French, French Canadian, and Indian soldiers, under the command of Jumonville's brother, Louis Coulon de Villiers, left Fort Duquesne.[36] On July 3, they captured Fort Necessity in the Battle of Fort Necessity and forced Washington to negotiate a withdrawal under arms.[37] The capitulation document that Washington signed was written in French, which Washington did not know how to read, and it may have been poorly translated for him by a Dutchman who spoke neither English nor French well, and it included language claiming that Jumonville and his men had been assassinated.[38] French officers included Pierre-Jacques Drouillon de Macé, and two cadets, Michel-Ignace Dandonneau, sieur du Sablé, and René Amable Boucher de Boucherville.[39] They were removed to Virginia, ultimately being shipped to London by Dinwiddie, along with 19 of the militiamen, arriving on June 10, 1755.[40][41] An exception was Michel Pépin, called ""La Force,"" a skilled interpreter with whom Washington was previously acquainted.[42][43] After the Battle of Fort Necessity, La Force was to held hostage in Williamsburg, as Captains Robert Stobo and Jacob Van Braam had been taken as hostages by the French. 
 When news of the two battles reached England in August, the government of the Duke of Newcastle, after several months of negotiations, sent an army expedition the following year to dislodge the French.[44]  Major General Edward Braddock was chosen to lead the expedition.[45] He was defeated at the Battle of the Monongahela, and the French remained in control of Fort Duquesne until 1758, when an expedition under General John Forbes finally succeeded in taking the fort.[46]
 Word of the British military plans had leaked to France well before Braddock's departure for North America, and King Louis XV dispatched a much larger body of troops to Canada in 1755.[47] Although they arrived too late to participate in Braddock's defeat, the French troop presence led to a string of French victories in the following years. Royal Navy Admiral Edward Boscawen led a small squadron of ships which attacked the French ship Alcide in a naval action on June 8, 1755, capturing her and two troopships transporting some of those soldiers to North America.[48] Military actions continued on soil and at sea in North America until France and Great Britain declared war on each other in spring 1756. That marked the formal start of the Seven Years' War.[49]
 Because of the inconsistent nature of the record of the action, contemporary and historical coverage of it has been easily colored by preferences for one account over another. Francis Parkman, for example, accepted Washington's account and was highly dismissive of the accounts by Monceau and the Indian.[50]
 French authorities assembled a dossier of documents to counter British accounts of the affair. Entitled ""Mémoire contenant le précis des faits, avec leurs pièces justificatives, pour servir de réponse aux 'Observations' envoyées par les Ministres d'Angleterre, dans les cours de l'Europe"", a copy was intercepted in 1756, translated, and published as ""A memorial containing a summary view of facts, with their authorities, in answer to observations sent by the English ministry to the courts of Europe"".[51]  It used Washington's capitulation statement and other documents, including extracts of Washington's journal taken at Fort Necessity, to suggest that Washington had actually ordered the assassination of Jumonville.[52] However, not all Frenchmen agreed with the story: the Chevalier de Lévis called it a ""so-called assassination"".[53] The French story contrasted with that of the British account. Based on Washington's report, the British suggested that Jumonville, rather than being engaged on a diplomatic mission, was spying on them.  Jumonville's orders included specific instructions to notify Contrecœur if the summons was read so that additional forces might be sent if needed.[30][54]
 Historian Fred Anderson theorizes about the reasons for Tanacharison's action in the killing and provides a possible explanation for one of Tanacharison's men reporting the event as a British killing of a Frenchman. Tanacharison had lost influence over some of the local tribes (specifically the Delawares) and may have thought that conflict between the British and French would bring them back under his influence as allies of the British.[55] According to Parkman, after the Indians scalped the French, they sent a scalp to the Delawares and in essence offered them the opportunity to ""take up the hatchet"" with the British and against the French.[56]
 A portion of the battlefield, along with the Great Meadows, where Fort Necessity was located, has been preserved as a part of Fort Necessity National Battlefield.[57] Jumonville's name has been given to a Christian retreat center near the site. The non-profit Braddock Road Preservation Association, named for the road General Braddock constructed to reach Fort Duquesne, sponsors research and promotes the French and Indian War history of the area.[58]
"
Joseph Coulon de Jumonville,https://en.wikipedia.org/wiki/Joseph_Coulon_de_Jumonville,"Joseph Coulon de Villiers, Sieur de Jumonville (September 8, 1718 – May 28, 1754) was a French Canadian military officer. His last rank was second ensign (enseigne en second). Jumonville's defeat and killing at the Battle of Jumonville Glen by forces led by George Washington was one of the sparks that ignited the Seven Years' War, also known as the French and Indian War in the United States.
 Jumonville was born in the seigneury of Verchères, New France (now part of Quebec), the son of Nicolas-Antoine Coulon de Villiers, a French military officer. He began service with the French military at age 15, in his father's unit.
 He served in the army during several conflicts with native groups in the western Great Lakes region where he was stationed with his father and several of his brothers. His father and one of his brothers were killed at Baie-des-Puants (present Green Bay, Wisconsin) in 1733 during a battle with the Fox tribe. In 1739, he served in Governor Bienville's abortive expedition against the Chickasaw nation.  He was later promoted to Second Ensign and stationed in Acadia during King George's War (as the North American theater of the War of the Austrian Succession is sometimes called).  In 1745 he married Marie-Anne-Marguerite Soumande of Montreal.
 In June 1754, Jumonville was posted to Fort Duquesne with his older half-brother, Louis Coulon de Villiers. The French were building up military strength, much of it Native American recruitment[a][1] in the disputed territory of the Ohio Country in response to an increasing presence by British American traders and settlers.[b]
 On May 23, 1754, Jumonville took command of a 35-man detachment from the fort and headed southeast. The exact nature of Jumonville's mission has been the subject of considerable debate, both at the time and up to the present day. Officially, his mission was to scout the area south of the fort. The French would later claim that he was a diplomat on a peaceful mission to deliver a message to the British. The British contended that he was sent to spy on their garrison at Fort Necessity and their road-building project. Tanacharison, known as the Half King and the leader of a band of new[c] Iroquoian peoples allied to the British, the Mingos, believed he was planning an ambush.
 On May 27, 1754, a group of Native American scouts discovered Jumonville's party camped in a small valley (later called Jumonville Glen) near what is now Uniontown, Pennsylvania. Half King went to Washington and pleaded with him to attack the French encampment, claiming it was a hostile party sent to ambush them.
 Washington took a detachment of about 40 men and marched all night in a driving rain, arriving at the encampment at dawn. What happened next, like so much about the incident, is a matter of controversy. The British claimed the French discovered their approach and opened fire on them. The French claimed the British ambushed their encampment. In either event, the battle lasted little more than 15 minutes and was a complete British victory. Ten French soldiers were killed and 21 captured, including the wounded Jumonville.
 Washington treated Jumonville as a prisoner of war and extended him the customary courtesies due to a captured military officer. Washington attempted to interrogate Jumonville but the language barrier made communication difficult. During their conversation, however, the Half King walked up to Jumonville and, without warning, struck him in the head with a tomahawk, killing him.
 Why the Half King did this has never been clear. He had been kidnapped by the French and sold into slavery as a child. He claimed that the French had boiled and eaten his father. He was also a representative of the Iroquois Confederacy, which stood to lose its authority over other Indian peoples in the Ohio River Valley if the French were able to assert their control.[3]
 Other accounts state that Jumonville was not, in fact, captured but was one of the first killed by Washington's expeditionary forces. Adam Stephen, a military officer who had accompanied Washington to the scene, stated that Jumonville ""was killed the first fire."" No reference was made to Jumonville's having been captured and unsuccessfully interrogated by Washington.[4] Also, it is unclear as to whether Jumonville was dispatched by bullet or tomahawk. In his footnotes added to Washington's journal in 1893, J.M. Toner stated that Half-King ""was credited in certain quarters with having slain that officer [Jumonville] with his hatchet; but this was without any foundation in fact.""[5]
 When word reached Fort Duquesne about the incident, Jumonville's half brother, Captain Coulon de Villiers, vowed revenge. He attacked Washington and the garrison at Fort Necessity and forced them to surrender on July 3, 1754. In the surrender document, written in French, Coulon de Villiers inserted a clause describing Jumonville's death as an ""assassination"".[6]
 Washington was heavily criticized in Britain for the incident. British statesman Horace Walpole referred to the controversy surrounding Jumonville's death as the ""Jumonville Affair"" and described it as ""a volley fired by a young Virginian in the backwoods of America that set the world on fire.""[7]
 Jumonville's legacy was to resonate significantly throughout the Seven Years' War in the French national consciousness. As noted above, within a month of Jumonville's death, his younger brother, Captain Coulon de Villiers, marched on Fort Necessity on 3 July and forced Washington to surrender.[8] The parley between Washington and de Villiers was to be conducted in French, given that they were the victors. However, Fowler's research of the accounts of the engagement from Washington and his men reveal that only two of Washington's company spoke French: William La Peyronie and Jacob Van Braam. As such, La Peyronie and Van Braam were instructed to negotiate with Villiers, but La Peyronie had been seriously wounded in the initial engagement. Consequently, the terms were left to Van Braam to resolve.[9]  Braam, a former lieutenant in the Dutch army and a teacher of French in Virginia, alongside a captain in the Virginia Regiment, was Washington's de facto French and Dutch translator.[10] That said, Van Braam's capability of translating French has been questioned in historiography given that it was not his first language.[11] Ultimately, more research is required on Van Bramm's own life to corroborate his capabilities as a translator. Regardless, Fowler has translated the terms Van Braam eventually agreed to after consulting Washington:
 Crucially, the terms Van Braam presented to Washington articulated that Jumonville had been an ambassador assassinated by Washington.[13] The use of ""assassinated"" created a political pejorative that placed Washington and his men as the guilty party in the affair. Washington was only able to avoid a political scandal surrounding the Jumonville ""assassination"" affair by insisting he had not comprehended the text Van Braam had given to him, and even going so far as to accuse Van Braam of incompetence or duplicity.[14]
 The terms agreed to at Fort Necessity provided a nascent notion of Jumonville as an innocent Frenchman murdered by Washington and his men. Early research by Marcel Trudel and Donald Kent in the 1950s has demonstrated how the notion of Jumonville's killing being a murder gained currency in France, with Bishop de Pontbriand in a pastoral letter (1756) declaring:
 Trudel and Kent go on to demonstrate how pamphleteer Francois-Antoine Chevrier's 1758 mock-heroic poem 'L'Acadiade; Ou, Prouesses Angloises En Acadie, Canada and Antoine-Leonard Thomas' epic 1759 poem Jumonville further lamented Jumonville's death at the hands of Washington's men.[16] These works were hyperbolic in nature and often stressed the innocence of Jumonville and played off nationalistic sentiment which incited nationalistic revenge, evident by the subject of Thomas' poem: ""the assassination of Monsieur de Jumonville, and the vengeance for this murder.""[17]  The underlining significance of these nationalistic sentiments has only recently been highlighted by David Bell's research in the early 2000s. Bell, in his analysis of Thomas' Jumonville, several engravings and illustrations of Jumonville's death, and Jesuit papers commenting on the affair, demonstrates how France seized the concept of international warfare to further nurture an embryonic sense of patriotism and nationalism among its subjects.[18]  It is in this sense how Jumonville's legacy is best understood: as a French martyr utilised by the French war literature to mobilise public opinion surrounding the nation. Indeed, the war-martyr as an emblematic symbol of the nation to promote national sentiment was a growing trend across Europe. 
 
"
Scalped,https://en.wikipedia.org/wiki/Scalped,"Scalping is the act of cutting or tearing a part of the human scalp, with hair attached, from the head, and generally occurred in warfare with the scalp being a trophy.[1] Scalp-taking is considered part of the broader cultural practice of the taking and display of human body parts as trophies, and may have developed as an alternative to the taking of human heads, for scalps were easier to take, transport, and preserve for subsequent display. Scalping independently developed in various cultures in both the Old and New Worlds.[2]
 One of the earliest examples of scalping dates back to the mesolithic period, found at a hunter-gatherer cemetery in Sweden.[3] Several human remains from the stone-age Ertebølle culture in Denmark show evidence of scalping.[4] A man found in a grave in the Alvastra pile-dwelling in Sweden had been scalped approximately 5,000 years ago.[5]
 Georg Friederici noted that “Herodotus provided the only clear and satisfactory portrayal of a scalping people in the old world” in his description of the Scythians, a nomadic people then located to the north and west of the Black Sea.[6] Herodotus related that Scythian warriors would behead the enemies they defeated in battle and present the heads to their king to claim their share of the plunder. Then, the warrior would skin the head “by making a circular cut round the ears and shaking out the skull; he then scrapes the flesh off the skin with the rib of an ox, and when it is clean works it with his fingers until it is supple, and fit to be used as a sort of handkerchief. He hangs these handkerchiefs on the bridle of his horse, and is very proud of them. The best man is the man who has the greatest number.”[7]
 Ammianus Marcellinus noted the taking of scalps by the Alani in terms quite similar to those used by Herodotus.[8] The Abbé Emmanuel H. D. Domenech referred to the decalvare of the ancient Germans and the capillos et cutem detrahere of the code of the Visigoths as examples of scalping in early medieval Europe,[9] though some more recent interpretations of these terms relate them to shaving off the hair of the head as a legal punishment rather than scalping.[10]
 In England in 1036,  Earl Godwin, father of Harold Godwinson, was reportedly responsible for scalping his enemies, among whom was Alfred Aetheling. According to the ancient Anglo-Saxon Chronicle, 'some of them were blinded, some maimed, some scalped. No more horrible deed was done in this country since the Danes came and made peace here'.[11]
 In 1845, mercenary John Duncan observed what he estimated to be 700 scalps taken in warfare and displayed as trophies by a contingent of female soldiers—Dahomey Amazons—employed by the King of Dahomey (present-day Republic of Benin). Duncan noted that these would have been taken and kept over a long period of time and would not have come from a single battle. Although Duncan travelled widely in Dahomey, and described customs such as the taking of heads and the retention of skulls as trophies, nowhere else does he mention scalping.[12][13]
 Occasional instances of scalping of dead Axis troops by Allied military personnel are known from World War II.  While many of these instances took place in the Pacific Theater, along with more extreme forms of trophy-hunting (see American mutilation of Japanese war dead), occasional instances are reported in the European Theater as well.  One particularly widely reported, although disputed, case involves that of German general Friedrich Kussin, the commandant of the town of Arnhem who was ambushed and killed by British paratroopers in the early stages of Operation Market Garden.[14]
 There is physical evidence that scalping was practiced during the Longshan and Erlitou periods in China's central plain.[15]
 A skull from an Iron Age cemetery in South Siberia shows evidence of scalping. It lends physical evidence to the practice of scalp taking by the Scythians living there.[16]
 Some evidence is also found in the Indian Subcontinent. Bhai Taru Singh (c. 1720
– 1 July 1745)[17] was a prominent Sikh martyr known for sacrificing his life, in the name of protecting Sikh values, by having had his head scalped rather than cutting his hair and converting to Islam.[18][19]
 Scalping in the Americas predominantly arose from the practices of Native American tribes, and was later copied by European colonists on the continent. [21]
 
Specific scalping techniques varied somewhat from place to place, depending on the cultural patterns of the scalper regarding the desired shape, size, and intended use of the severed scalp, and on how the victims wore their hair, but the general process of scalping was quite uniform: The scalp separated from the skull along the plane of the areolar connective tissue, the fourth (and least substantial) of the five layers of the human scalp. Scalping was not in itself fatal, though it was most commonly inflicted on the gravely wounded or the dead. The earliest instruments used in scalping were stone knives crafted of flint, chert, or obsidian, or other materials like reeds or oyster shells that could be worked to carry an edge equal to the task. Collectively, such tools were also used for a variety of everyday tasks like skinning and processing game, but were replaced by metal knives acquired in trade through European contact. The implement, often referred to as a ""scalping knife"" in popular American and European literature, was not known as such by Native Americans, a knife being for them just a simple and effective multi-purpose utility tool for which scalping was but one of many uses.[23][24]
 There is substantial archaeological evidence of scalping in North America in the pre-Columbian era.[25][26] Carbon dating of skulls show evidence of scalping as early as 600 AD; some skulls show evidence of healing from scalping injuries, suggesting at least some victims occasionally survived at least several months.[26] Among Plains Indians, it seems to have been practiced primarily as part of intertribal warfare, with scalps only taken of enemies killed in battle.[26] However, author and historian Mark van de Logt wrote, ""Although military historians tend to reserve the concept of 'total war'"", in which civilians are targeted, ""for conflicts between modern industrial nations,"" the term ""closely approaches the state of affairs between the Pawnees, the Sioux, and the Cheyennes. Noncombatants were legitimate targets. Indeed, the taking of a scalp of a woman or child was considered honorable because it signified that the scalp taker had dared to enter the very heart of the enemy's territory.""[27]
 Many tribes of Native Americans practiced scalping, in some instances up until the end of the 19th century. Of the approximately 500 bodies at the Crow Creek massacre site, 90 percent of the skulls show evidence of scalping. The event took place circa 1325 AD.[28] European colonisation of the Americas increased the incidence of intertribal conflict, and consequently an increase in the prevalence of scalping.[25]
 Officials in the English colonies of Connecticut and Massachusetts offered bounties for the heads of killed Indians, and later for just their scalps during the Pequot War.[29][30] Connecticut authorities specifically reimbursed Mohegans for killing Pequot tribespeople in 1637.[31] Four years later, the Dutch colony of New Amsterdam offered bounties for the heads of Raritans.[31] In 1643, the Iroquois attacked a group of Wyandot fur traders and French carpenters near Montreal, killing and scalping three Frenchmen.[32]
 Bounties for Indian captives or their scalps appeared in the legislation of several English colonies during the Susquehannock War (1675–77).[33] The New England Colonies offered bounties to white settlers and Narragansett people in 1675 during King Philip's War.[31] By 1692, New France also paid their native allies for scalps of their enemies.[31] In 1697, on the northern frontier of Massachusetts colony, white settler Hannah Duston killed ten of her Abenaki captors during her nighttime escape, presented their ten scalps to the Massachusetts General Court and was rewarded with bounties for two men, two women, and six children, even though colonial authorities had rescinded the law authorizing scalp bounties six months earlier.[29] There were six colonial wars with New England and the Iroquois Confederacy fighting New France and the Wabanaki Confederacy over a 75-year period, starting with King William's War in 1688. All sides scalped victims, including noncombatants, during this frontier warfare.[34] Bounty policies originally intended only for Native American scalps were extended to enemy colonists.[31]
 Massachusetts created a scalp bounty during King William's War in July 1689, and continued doing so during Queen Anne's War in 1703.[35][36] During Father Rale's War (1722–1725), on August 8, 1722, Massachusetts put a bounty on native families, paying 100 pounds sterling for the scalps of male Indians aged 12 and over, and 50 pounds sterling for women and children.[30][37] Ranger John Lovewell is known to have conducted scalp-hunting expeditions, the most famous being the Battle of Pequawket in New Hampshire.[citation needed]
 In the 1710s and 1720s, New France engaged in frontier warfare with the Natchez people and the Meskwaki people, during which both sides employed the practice.[citation needed] In response to repeated attacks on British settlers by the French and their native allies during King George's War, Massachusetts Governor William Shirley issued a bounty in 1746 to be paid to British-allied Indians for the scalps of French-allied Indian men, women, and children.[38] New York passed a scalp act in 1747.[39]
 During Father Le Loutre's War and the Seven Years' War in Nova Scotia and Acadia, French colonists offered payments to Indians for British scalps.[40] In 1749, governor of Nova Scotia Edward Cornwallis created an proclamation which included a bounty for male scalps or prisoners, though no scalps were turned in. During the Seven Years' War, governor of Nova Scotia Charles Lawrence offered a reward for male Mi'kmaq scalps in 1756.[41] In 2000, Mi'kmaq activists argued that this proclamation was still legal in Nova Scotia, though government officials pointed out that it was no longer legal because the bounty was superseded by the Halifax Treaties.[42]
 During the French and Indian War, as of June 12, 1755, Massachusetts governor William Shirley was offering a bounty of £40 for a male Indian scalp, and £20 for scalps of females or of children under 12 years old.[35][43]  In 1756, Pennsylvania Lieutenant Governor Robert Morris, in his declaration of war against the Lenni Lenape (Delaware) people, offered ""130 Pieces of Eight, for the Scalp of Every Male Indian Enemy, above the Age of Twelve Years,"" and ""50 Pieces of Eight for the Scalp of Every Indian Woman, produced as evidence of their being killed.""[35][44]
 Although much has been made of the existence of scalp bounties, generally because they have been easily accessible as statutes, little research exists on the numbers of bounties actually paid.  Early frontier warfare in forested areas in the era of flintlock muzzle-loading rifles favored tomahawks and knives over firearms because of the long loading time after a shot was fired.  Advantage was clearly held by bow, knife, and hatchet.  Some states had a history of escalating the payout of bounties offered per scalp, presumably because lower bounties were ineffective and were not worth risking one's life in exchange for the payoff.  Rising bounties were a measure of bounty system failure.[citation needed]
 During the American Revolutionary War, British Indian Department official Henry Hamilton was nicknamed the ""hair-buyer general"" by American Patriots as they believed he encouraged and paid British-allied Natives to scalp Americans. As a result, when Hamilton was captured by American troops, he was treated as a war criminal instead of a prisoner of war. However, American historians have noted that there was no proof that he had ever offered rewards for scalps,[45] and no British officer paid for scalps during the conflict.[46]
 However, both sides of the war scalped enemy corpses. The September 13, 1779 journal entry of American Lieutenant William Barton recounted how U.S. troops scalped Native dead during the Sullivan Expedition.[47] British-allied Iroquois also practiced scalping. The most famous case was that of Jane McCrea, whose fiancé was a Loyalist officer. She was abducted by two Iroquois warriors and ultimately scalped and shot. Her death inspired many American colonists to resist a British invasion from Canada, which ended in defeat at the battles of Saratoga.[48]
 During the Apache–Mexico Wars in 1835, the government of the Mexican state of Sonora put a bounty on the Apache which,[49] over time, evolved into a payment by the government of 100 pesos for each scalp of a male 14 or more years old.[50] In 1837, the Mexican state of Chihuahua also offered a bounty on Apache scalps, 100 pesos per warrior, 50 pesos per woman, and 25 pesos per child.[49] Harris Worcester wrote: ""The new policy attracted a diverse group of men, including Anglos, runaway slaves led by Seminole John Horse, and Indians — Kirker used Delawares and Shawnees; others, such as Terrazas, used Tarahumaras; and Seminole chief Coacoochee led a band of his own people who had fled from Indian Territory.""[51] Mexico's scalp bounties were infamously exploited by the Glanton gang: originally charged with fighting the Apache, the gang later began to take scalps from peaceful Natives and non-Native Mexicans.[52]
 Some scalping incidents occurred during the American Civil War of 1861-1865. For example, Confederate guerrillas led by ""Bloody Bill"" Anderson were well known for decorating their saddles with the scalps of Union soldiers they had killed.[53]  Archie Clement had the reputation of being Anderson's “chief scalper”.
 In 1851, the U.S. Army displayed Indian scalps in Stanislaus County, California. 
 In 1851, the Tehama Massacre occurred in Tehama County, California, wherein U.S. military and citizens razed villages and scalped hundreds of men, women, and children.[54] This attack targeted Native communities specifically, in the villages of Yana, Konkow, Nisenan, Wintu, Nomlaki, Patwin, Yuki, and Maidu.[55]
 Scalping also occurred during the Sand Creek Massacre on November 29, 1864, during the American Indian Wars, when a 700-man force of U.S. Army volunteers destroyed the village of Cheyenne and Arapaho in southeastern Colorado Territory, killing and mutilating[56][57] an estimated 70–163 Native American civilians.[58][59][60] An 1867 New York Times article reported that ""settlers in a small town in Colorado Territory had recently subscribed $5,000 to a fund ‘for the purpose of buying Indian scalps (with $25 each to be paid for scalps with the ears on)’ and that the market for Indian scalps ‘is not affected by age or sex’."" The article noted this behavior was ""sanctioned"" by the U.S. federal government, and was modeled on patterns the U.S. had begun a century earlier in the ""American East"".[61]: 206 
 From one writer's point of view, it was a ""uniquely American"" innovation that the use of scalp bounties in the wars against indigenous societies ""became an indiscriminate killing process that deliberately targeted Indian non-combatants (including women, children, and infants), as well as warriors.""[61]: 204  Some American states such as Arizona paid bounty for enemy Native American scalps.[62]
"
Fort Necessity,https://en.wikipedia.org/wiki/Fort_Necessity,"

 Fort Necessity National Battlefield is a National Battlefield in Fayette County, Pennsylvania, United States, which preserves the site of the Battle of Fort Necessity.  The battle, which took place on July 3, 1754, was an early battle of the French and Indian War, and resulted in the surrender of British colonial forces under Colonel George Washington, to the French and Indians, under Louis Coulon de Villiers.
 The site also includes the Mount Washington Tavern, once one of the inns along the National Road, and in two separate units the grave of British General Edward Braddock, killed in 1755, and the site of the Battle of Jumonville Glen.
 After returning to the Great Meadows in northwestern Virginia, and what is now Fayette County, Pennsylvania, George Washington decided it prudent to reinforce his position. Supposedly named by Washington as Fort Necessity or Fort of Necessity, the structure protected a storehouse for supplies such as gunpowder, rum, and flour. The crude palisade they erected was built more to defend supplies in the fort's storehouse from Washington's own men, whom he described as ""loose and idle"", than as a planned defense against a hostile enemy.  The sutler of Washington's force was John Fraser, who earlier had been second-in-command at Fort Prince George.  Later he served as chief scout to General Edward Braddock and then chief teamster to the Forbes Expedition.
 By June 13, 1754, Washington had under his command 295 colonials and the nominal command of 100 additional regular British army troops from South Carolina. Washington spent the remainder of June 1754 extending the wilderness road further west and down the western slopes of the Allegheny range into the valley of the Monongahela River. He wanted to create a river crossing point roughly 41 mi (66 km) away, near Redstone Creek and Redstone Old Fort.
 This was a prehistoric Native American earthwork mound on a bluff overlooking the river crossing.  The aboriginal mound structure may have once been part of a fortification. Five years later in the war, in 1759, Fort Burd was constructed at Redstone Old Fort. The area eventually became the site of Nemacolin Castle and Brownsville, Pennsylvania—an important western jumping-off point for travelers crossing the Alleghenies in the late 18th and early 19th centuries.
 To reach the Ohio River basins' navigable waters as soon as possible on the Monongahela River, Washington chose to follow Nemacolin's Trail, a Native American trail which had been somewhat improved by colonists, with Nemacolin's help.  He preferred this to following the ridge-hopping, high-altitude path traversed by the western part of the route that was later chosen for Braddock's Road.  It jogged to the north near the fort and passed over another notch near Confluence, Pennsylvania, into the valley and drainage basin of the Youghiogheny River. The Redstone destination at the terminus of Nemacolin's Trail was a natural choice for an advanced base.  The location was one of the few known good crossing points where both sides of the wide deep river had low accessible banks; steep sides were characteristic of the Monongahela River valley.
 Late in the day on July 3, Washington did not know the French situation. Believing his situation was impossible, he accepted surrender terms which allowed the peaceful withdrawal of his forces, which he completed on July 4, 1754.[4] The French subsequently occupied the fort and then burned it. Washington did not speak French, and stated later that if he had known that he was confessing to the ""assassination"" of Joseph Coulon de Jumonville, he would not have signed the surrender document.
 During the Great Depression of the 20th century, attempts to preserve the location of Fort Necessity were undertaken.  On March 4, 1931, Congress declared the location a National Battlefield Site under management of the War Department. Transferred to the National Park Service in 1933, the park was redesignated a National Battlefield on August 10, 1961. As with all historic sites administered by the National Park Service, the battlefield was listed on the National Register of Historic Places on October 15, 1966.
 Subsequent archaeological research helped to uncover the majority of the original fort position, shape and design. A replica of the fort was constructed on site in the 1970s. A new visitor center, which also is home to a National Road interpretive center, opened on October 8, 2005. The battlefield and fort are currently being improved; the battlefield itself has seen much vegetation growth, and the general public are asked to stay out of the battlefield grounds. As the remains of the casualties of the battle were never truly recovered, the battlefield is treated as hallowed ground.
 On a hillside adjacent to the battlefield and within the boundaries of the park is Mount Washington Tavern, a classic example of the many inns once lining the National Road, the United States' first federally funded highway.
 The land on which the tavern was built was originally owned by George Washington.  In 1770 he purchased the site on which he had commanded his first battle.  Around the 1830s, Judge Nathanial Ewing of Uniontown constructed the tavern. James Sampey acquired the tavern in 1840.  It was operated by his family until the railroad construction boom caused the National Road to decline in popularity, rendering the inn unprofitable.
 In 1855, it was sold to the Fazenbaker family.  They used it as a private home for the next 75 years, until the Commonwealth Of Pennsylvania  purchased the property in 1932.  In 1961 the National Park Service purchased the property from the state, making the building a part of Fort Necessity.  The Mount Washington Tavern demonstrates the standard features of an early American tavern, including a simple barroom that served as a gathering place, a more refined parlor that was used for relaxation, and bedrooms in which numerous people would crowd to catch up on sleep.
 In a separate unit of the park, lying about one mile (1.6 km) northwest of the battlefield, is the grave of General Edward Braddock. The British commander led a major expedition to the area in 1755 which included the construction of Braddock's Road, a useful but inadequate wilderness road through western Pennsylvania. Braddock was severely wounded in the Battle of the Monongahela as the British advanced toward Fort Duquesne.
 He and his forces fled along the wilderness road to a site near Great Meadows. Braddock died on July 13, 1755, and was buried in an elaborate ceremony officiated by George Washington. He was buried under the road in order to hide the location of his grave from the enemy French and Indians.[5] In 1804 Braddock's remains were discovered by men making repairs to the wilderness road.[citation needed] A marker was erected in 1913.
"
British Army Independent Companies in South Carolina,https://en.wikipedia.org/wiki/British_Army_Independent_Companies_in_South_Carolina,"British Army Independent Companies in South Carolina formed a major component of the Province of South Carolina's military security. Regular independent companies were first established in British North America in 1664. The first Independent Company in South Carolina was organized in 1721. With the raising of Oglethorpe's Regiment in 1737 it was disbanded. In 1746 three understrength independent companies were sent to South Carolina, but they were disbanded two years later. When Oglethorpe's Regiment was disbanded in 1748, three new independent companies were raised in South Carolina, partly recruited with soldiers from the disbanded regiment. These three companies participated in the French and Indian War and the Cherokee War, participating in the Battle of Fort Necessity, the Braddock Expedition, the battle of the Monongahela, and the siege of Fort Loudoun. They were disbanded in 1763, with the rest of the British army independent companies in North America.
 Independent companies are military units not belonging to a regimental organization. In England, independent garrison companies existed since the end of the 15th century. The first three English independent companies in North America arrived in Boston in 1664, and were used to conquer the Dutch colony of New Netherland. During the French and Indian Wars, independent garrison companies were stationed not only in the New York Colony, but also in Massachusetts Bay, Virginia, and South Carolina. In 1740, the four independent companies of New York were the only in the Thirteen Colonies, but after the disbandment of Oglethorpe's Regiment in 1748 three new independent companies were raised for service in South Carolina. The four companies in New York and the three companies in South Carolina were the independent companies that served during the French and Indian War.[1][2][3][4][5]
 The independent companies were recruited in Britain and the soldiers rarely, if ever, returned to the old country after having left the service. The British Army was largely recruited among the poor and the criminal classes; yet, the independent companies had lower status. Their ranks were often filled with people who had left the regular service; former soldiers mainly, but also deserters. The officers were often promoted non-commissioned officers. As the independent companies were virtually ignored by the military authorities in Britain they became dependent on the local American communities, often relying on them for food, clothing, and housing. Soon they became rooted in the local society; transforming the military service into a sideline of a civilian occupation.[5][6][7]
 Facing an expected Spanish threat, the province of South Carolina in 1719 requested military aid from the motherland. The British government drafted men from all garrison companies in Britain, thereby managing to create an independent company of 100 men, which was sent to South Carolina in 1721. The company was used to garrison Fort King George, previously built and garrisoned by provincial scouts, until a fire in 1727 destroyed the fort, when the company was moved to Port Royal. In 1730 part of the company was transferred to St. Simons Island, Georgia where they built and garrisoned Fort Delegal. When the raising of Oglethorpe's Regiment was authorized in 1737, the South Carolina independent company ceased to exist, forming the nucleus of the new regiment.[8][9][10]
 During King George's War, South Carolina was not content with being protected by troops based in Georgia, and asked the government in London for troops stationed in the colony. In 1746 three understrength independent companies were sent to South Carolina, their 60 officers and other ranks forming a core for enlargement; the missing men to be recruited in Charleston and Virginia. After the end of the war, the companies were disbanded together with Oglethorpe's Regiment. Three new independent companies would be recruited, however, to serve in South Carolina. The discharged soldiers could enlist in the new companies, return to England, or remain in Georgia.[11][12]
 In the preludes to the French and Indian War, Lieutenant Colonel George Washington had been ordered to remove the French from Fort Duquesne. In addition to the 300 men from his own provincial Virginia Regiment, an independent company from South Carolina was sent under the command of Captain James Mackay to his aid; ultimately suffering defeat and surrender with Washington at the Battle of Fort Necessity. Captain Mackay, being an officer with the King's commission, refused to obey Washington's orders, as coming from a provincial officer. Washington left Mackay and his company at Fort Necessity when initially moving forward towards Fort Duquesne, since the captain refused to let his men work on the road Washington was making through the woods, without extra pay. At the battle, however, they fought with fervor, suffering greater losses than the Virginians.[13][14][15]
 Later, the company, now under Captain Paul Demere, participated in Braddock's Expedition, again suffering a defeat, now at the battle of the Monongahela. When inspected, it was found to be in much better military order than the two independent companies from New York also joining the expedition. At the battle they formed the rearguard together with a provincial company of Virginia rangers. During the expedition's confused retreat, the steadfastness and fighting spirit of these two companies saved the remnants of the army from being surrounded and totally annihilated.[16]
 A second South Carolina independent company, under Captain Raymond Demere participated in the construction of Fort Loudon on the Tennessee River in 1756, built on the request of the Cherokees in the Overhill Cherokee country. The fort was then garrisoned by the company, with Captain Demere as its commandant. In 1757, the command was transferred to Captain Paul Demere.[17] The  beginning of the Cherokee War saw South Carolina Independent Companies in garrison at Charleston, Fort Prince George, and Fort Loudoun. Soon hostile Cherokees invested both forts. Fort Loudoun had to surrender in 1760; all the officers except one being killed after the surrender, the men becoming Cherokee hostages. After the war, the prisoners were ransomed and released. Fort Prince George held out until finally relieved in 1761.[18][19][20]
 In 1763, all the independent companies in British North America were disbanded as a matter of policy; being replaced by regular British army regiments permanently stationed in America.[21]
"
James Mackay (British Army officer),https://en.wikipedia.org/wiki/James_Mackay_(British_Army_officer),"
 James Mackay (1718–1785) was a captain in the British Army during the French and Indian War.  He was in command of an Independent Company of South Carolina when he was sent by the Governor of South Carolina to assist Virginia's defense of the Ohio Country from the French in the summer of 1754.  He was co-commander of Fort Necessity along with George Washington during the Battle of the Great Meadows on 3 July 1754.[1][2][3]
 
"
Battle of Fort Necessity,https://en.wikipedia.org/wiki/Battle_of_Fort_Necessity,"

 The Battle of Fort Necessity, also known as the Battle of the Great Meadows, took place on July 3, 1754, in present-day Farmington in Fayette County, Pennsylvania. The engagement, along with a May 28 skirmish known as the Battle of Jumonville Glen, was the first military combat experience for George Washington, who was later selected as commander of the Continental Army during the American Revolutionary War by the Second Continental Congress in Philadelphia.[6] 
 The Battle of Fort Necessity began the French and Indian War, which later spiraled into the global conflict known as the Seven Years' War. Washington built Fort Necessity on an alpine meadow west of the summit of a pass through the Laurel Highlands of the Allegheny Mountains.  Another pass nearby leads to Confluence, Pennsylvania; to the west, Nemacolin's Trail begins its descent to Uniontown, Pennsylvania, and other parts of Fayette County along the relatively low altitudes of the Allegheny Plateau.
 The French Empire, despite having colonized North America in the 16th century, had between only 75,000 and 90,000 colonists living in New France in the mid-1700s.[7]: 7 [8] However, France was able to control the large colonies of New France (modern-day Canada), Acadia, and the French Louisiana with relatively few people by controlling waterways (especially the Saint Lawrence River, the Great Lakes, the Ohio River, and the Mississippi River) and cultivating strong political and economic relationships with powerful Native American nations.[9] The Ohio Country, an area located roughly between Lake Erie and the Ohio River, became increasingly important to the French throughout the 18th century. As more settlers moved from Montreal, Quebec, and other established French settlements along the St. Lawrence to the newer Louisiana colony, the Ohio Country became an important connection between New France and Louisiana.[citation needed]
 British settlers were also expanding into the Ohio Country at this time. The British colonies were far more populated than the French (there were about 1.5 million British subjects living in North America in 1754, meaning that the British outnumbered the French almost twenty to one), and settlers were eager to move over the Appalachian Mountains and into the Ohio Country and other western lands.[7]: 7, 11  Most British traders declared that, despite the facts that the French had been trading in the Ohio Country for years and that more and more displaced Native Americans were moving west from the Atlantic coast every year, the Ohio Country was unsettled, uncharted, and therefore unclaimed land that should be open to all traders.[7]: 11  The French had no interest in trying to compete with the British for trade in the Ohio Country. Due to their high population and large colonial cities, British traders could offer Native Americans cheaper, higher quality goods than could their French counterparts.[10] The French therefore set about keeping the British as far away from the Ohio Country as possible.[citation needed]
 Authorities in New France became more aggressive in their efforts to expel British traders and colonists from this area, and in 1753 began construction of a series of fortifications in the area.[11] In previous wars, the Canadians had more than held their own against the English colonials.[12]
 The French action drew the attention of not just the British, but also the Indian tribes of the area. Despite good Franco-Indian relations, British traders became successful in convincing the Indians to trade with them in preference to the French Canadians, and the planned large-scale advance was not well received by all.[13] The reason  was that they had to provide them with the goods that the Anglo-American traders had previously supplied and at similar prices, which proved to be singularly difficult. With the exception of one or two Montreal merchant traders, the Canadians showed a great reluctance to venture into the Ohio country.[14]  In particular, Tanacharison, a Mingo chief also known as the ""Half King,"" became anti-French as a consequence. In a meeting with Paul Marin de la Malgue, commander of the Canadian construction force, de la Malgue reportedly lost his temper and shouted at the Indian chief, ""I tell you, down the river I will go. If the river is blocked up, I have the forces to burst it open and tread under my feet all that oppose me.  I despise all the stupid things you have said.""[15] He then threw down some wampum that Tanacharison had offered as a goodwill gesture.[15] Marin died not long after, when command of the operations was turned over to Jacques Legardeur de Saint-Pierre.[16]
 Virginians felt that their colonial charter, the oldest in the British colonies, gave them claim to the Ohio Country despite competing claims from Native Americans, the French, and other British colonies. In 1748, wealthy Virginians formed the Ohio Company with the aim of solidifying Virginia's claim and profiting off the speculation of western lands.[7]: 11  Governor Robert Dinwiddie, the royal governor of Virginia and founding investor in the Ohio Company, sent the 21-year-old Virginia Lieutenant Colonel George Washington to travel from Williamsburg to Fort Le Boeuf in the Ohio Territory (a territory claimed by several of the British colonies, including Virginia) as an emissary in December 1753, to deliver a letter. Washington's older brothers Lawrence and Augustine had been instrumental in organizing the Ohio Company, and George had become familiar with the Ohio Company by surveying for his brothers as a young man. After a long trek and several near-death experiences, Washington and his party (which included the Mingo sachem, Tanacharison, and the explorer Christopher Gist) arrived at Fort Le Boeuf and met with the regional commander, Jacques Legardeur de Saint-Pierre.[17] Saint-Pierre politely informed Washington that he was there pursuant to orders and that Washington's letter should have been addressed to his commanding officer in Canada.[18]
 Washington returned to Williamsburg and informed Dinwiddie that the French refused to leave.[19]  Dinwiddie ordered Washington to begin raising a militia regiment to hold the Forks of the Ohio in what is now Pittsburgh, a site Washington had identified as a fine location for a fortress.[20]  However, unlike the French, Washington and his Virginia regiment could not easily reach the Forks by river. The governor therefore also issued a captain's commission to an Ohio Company employee, William Trent, with instructions to raise a small force capable of moving quickly through the wilderness and virgin forest that lie between Williamsburg and the Forks. Once there, they were to immediately begin construction of a fortification on the Ohio. Dinwiddie issued these instructions on his own authority without even asking for funding from the Virginia House of Burgesses until after the fact.[21] Trent's company arrived on site in February 1754 and began construction of a storehouse and stockade with the assistance of Tanacharison and the Mingos.[21][22] In response, the French Canadians sent a force of about 500 men, Canadian, French, and Indians under Claude-Pierre Pécaudy de Contrecœur (rumors reaching Trent's men put its size at 1,000). On April 16, they arrived at the forks; the next day, Trent's force of 36 men, led by Ensign Edward Ward in Trent's absence, agreed to leave the site.[23] The Canadians tore down the British works and began construction of the fort that they called Fort Duquesne.[24]
 In March 1754, Governor Dinwiddie sent Washington back to the frontier with orders to ""act on the [defensive], but in Case any Attempts are made to obstruct the Works or interrupt our [settlements] by any Persons whatsoever, You are to restrain all such Offenders, & in Case of resistance to make Prisoners of or kill & destroy them"". Historian Fred Anderson describes Dinwiddie's instructions, which were issued without the knowledge or direction of the British government in London, as ""an invitation to start a war"".[25] Washington was ordered to gather as many supplies and paid volunteers as he could along the way. By the time he left for the frontier on April 2, he had gathered 186 men.[26]
 Contrecœur operated under orders that forbade attacks by his force unless they were provoked. On May 23, he sent Joseph Coulon de Villiers de Jumonville with 35 men to see if Washington had entered French territory, and with a summons to order Washington's troops to leave; this summons was similar in nature to the one Washington had delivered to them four months previous.[27] Sources disagree on the exact composition of Jumonville's force, which may have included French troupes de la marine, Canadian militia, and Indians.[28][29]
 During the march through the forests of the frontier, Washington received a few more men from another regiment that they met at Winchester.[26] At this point Captain Trent arrived with news of the advance of the French force under Jumonville. Trent was accompanied by Tanacharison, who promised warriors to assist the British.[26] To keep Tanacharison's support, Washington decided not to turn back, choosing instead to build a fortification 37 miles (60 km) south of the forks and await further instructions.[30] The men of the Virginia Regiment built a road through the wilderness as they went, ""broad enough to pass with all our Artillery and our Baggage.""[31] This road was essential, not just to allow Washington and his men to move quickly to Fort Duquesne, but to open up the Ohio country to Virginia troops and settlers in the future. Washington and the Ohio Company had originally hoped to use the Potomac River to travel between the tidewater and the Ohio country; however, the Great Falls made such a journey impossible until the completion of the Patowmack Canal in 1803.[32]
 Washington sent out Captain Hog with 75 men to pursue French troops who had threatened to destroy his house and property.[33] However, shortly after Hog left, Washington called together some young Indians and told them that the French had come to kill Tanacharison, and the Indians also left to pursue the French. That evening, Washington received a message from Tanacharison, who said he had found the French encampment.[34] Washington decided to attack himself and brought 40 soldiers with him towards Tanacharison's camp. That morning, they met with Tanacharison's 12 Indian warriors, and Washington and Tanacharison agreed to attack the encampment.[35] Washington ambushed the French, killing 10 to 12, wounding 2 and capturing 21.[36] Among the dead was Jumonville; the exact manner of his death is uncertain, but by several accounts Tanacharison executed Jumonville in cold blood, crushing his head with a tomahawk and washing his hands in Jumonville's brains.[36] One account, reported by an Indian to Contrecœur, claimed that Jumonville was killed by the Half King while the summons was being read.[37]
 After retiring from Jumonville, Washington expected to be attacked.[38] Tanacharison attempted to convince the Lenape, Shawnee and the Mingo Indians to join the Virginians at Great Meadows. With about 150 Virginians at Great Meadows, they began to construct a fort, which Washington named Fort Necessity. The fort was completed on June 3.[38]
By June 9, the rest of the Virginia Regiment arrived at Great Meadows, excluding Colonel Joshua Fry, who had fallen from his horse, broken his neck and died.[4] Washington took his place as colonel. A few days later, 100 British regulars under the command of James Mackay arrived, but, instead of making camp with the Virginians, they camped separately outside the fort.[4]
 Washington had heard that there were 500 poorly supplied French troops at Fort Duquesne, and thus he led the roughly 300 Virginians out of Great Meadows on June 16 to widen the road, for he had been unable to convince the other chiefs to assist. They had said that they would also be unable to help the Virginians. Although he had lost Indian support, which made his troops more vulnerable to attack, Washington continued to widen the road towards Red Stone Creek.[39]
 On June 28, after a council of war, Washington ordered the withdrawal to Great Meadows. That same day 600 French and 100 Indians left Fort Duquesne led by the slain Jumonville's older brother, Louis Coulon de Villiers. In order to keep ahead of the French/Canadian force, the Virginians had to abandon most of their supplies. On July 1, they reached Fort Necessity.[40]
 At Fort Necessity, the provision hut was depleted, and there was little shelter from the heavy rain that started to fall on the 2nd.[41] With the rain, the trenches that Washington had ordered to be dug had turned into streams. Washington realized that he would have to defend against a frontal assault and also realized that it would be difficult because the woods were less than 100 yards away, within musket range, making it possible for a besieging attacker to pick off the defenders.[2] To improve the defense, Washington ordered his men to cut trees down and to make them into makeshift breastworks.[2]
 As the British worked, Coulon approached Fort Necessity using the road the Virginians had built.[2] He arrived at Jumonville's Glen early on the morning of July 3. Horrified to find several scalped French bodies, he immediately ordered them to be buried.[2]
 By 11:00 am on the 3rd of July 1754, Louis Coulon de Villiers came within sight of Fort Necessity. At this time, the Virginians were digging a trench in the mud. The pickets fired their muskets and fell back to the fort, whereupon three columns of Canadian soldiers and Indians advanced downhill towards the fort. However, Coulon had miscalculated the location of the fort and had advanced with the fort at his right. As Coulon halted and then redeployed his troops, Washington began to prepare for an attack.[2]
 Coulon moved his troops into the woods, within easy musket range of the fort.[2] Washington knew he had to dislodge the Canadians and Indians from that position, so he ordered an assault with his entire force across the open field. Seeing the assault coming, Coulon ordered his soldiers, led by Indians, to charge directly at Washington's line. Washington ordered the men to hold their ground and fire a volley. Mackay's regulars obeyed Washington's command, and supported by two swivel cannons, they inflicted several casualties on the oncoming Indians. The Virginians, however, fled back to the fort, leaving Washington and the British regulars greatly outnumbered. Washington ordered a retreat back to the fort.[42]
 Coulon reformed his troops in the woods. The Canadians spread out around the clearing and kept up heavy fire on Fort Necessity. Washington ordered his troops to return fire, but they aimed too high, inflicting few casualties, and the swivel cannon fared no better. To add to the garrison's troubles, heavy rain began to fall that afternoon, and Washington's troops were unable to continue the firefight because their gunpowder was wet.[42]
 Louis Coulon de Villiers, with his men exhausted, powder and ball were running low, and reason to fear that American reinforcements were approaching, decided to negotiate, sending an officer under a white flag to Washington and Mackay.[43] Washington did not allow the Canadian officer into or near the fort, but sent two of his own men, including his translator Jacob Van Braam, to negotiate. The French had no desire to disturb the peace between the two kingdoms but wished only to “avenge the murder of one of our officers, bearer of a summons, and of his escort, and also to prevent any establishment being made on the lands of my King.”[44] As negotiations began, the Virginians, against Washington's orders, broke into the fort's liquor supply and got drunk. Coulon told Van Braam that all he wanted was the surrender of the garrison, and the Virginians could go back to Virginia. He warned, however, that if they did not surrender now, the Indians might storm the fort and scalp the entire garrison.[43]
 Van Braam brought this message to Washington, who agreed to these basic terms. One of Louis Coulon de Villiers' aides then wrote down Coulon's surrender terms and then gave them to Van Braam, who in turn gave them to Washington. Washington, who could not read French, had Van Braam translate it for him, and in the document it said that Jumonville had been ""assassinated"". Both Washington and Mackay signed the surrender document.[43]
 On July 4, Washington and his troops abandoned Fort Necessity.[46] The garrison marched away with drums beating and flags flying, but the Indians and the French began to loot the garrison's baggage on their way out. Washington, who feared a bloodbath, did not try to stop the looting.[46] The Indians continued to plunder the soldiers until July 5. Washington and his troops arrived back in eastern Virginia in mid-July.[47] On the 17th, Washington delivered his report of the battles to Governor Dinwiddie, expecting a rebuke, but Washington instead received a vote of thanks from the House of Burgesses and Dinwiddie blamed the defeat not on Washington but on poor supply and the refusal of aid by the other colonies.[47]
 When news of the two battles reached England in August, the government of the Duke of Newcastle, after several months of negotiations, decided to send an army the following year to dislodge the French.[48]  Major General Edward Braddock was chosen to lead the expedition.[49] His expedition ended in disaster, and the French remained in control of Fort Duquesne until November 1758, when an expedition under General John Forbes finally succeeded in taking the fort.[50]
 Word of the British military plans leaked to France well before Braddock's departure for North America, and King Louis XV dispatched a much larger body of troops to Canada in 1755.[51]  Although they arrived too late to participate in Braddock's defeat, the French troop presence led to a string of French victories in the following years.  In a second British act of aggression, Admiral Edward Boscawen fired on the French ship Alcide in a naval action on June 8, 1755, capturing her and two troop ships carrying some of those troops.[52]  Military matters escalated on both North American soil and sea until France and Britain declared war on each other in spring 1756, marking the formal start of the Seven Years' War.[53]
 The battlefield is preserved at Fort Necessity National Battlefield.
 
"
French and Indian War,https://en.wikipedia.org/wiki/French_and_Indian_War,"


 The French and Indian War (1754–1763) was a theater of the Seven Years' War, which pitted the North American colonies of the British Empire against those of the French, each side being supported by various Native American tribes. At the start of the war, the French colonies had a population of roughly 60,000 settlers, compared with 2 million in the British colonies.[5] The outnumbered French particularly depended on their native allies.[6]
 Two years into the war, in 1756, Great Britain declared war on France, beginning the worldwide Seven Years' War. Many view the French and Indian War as being merely the American theater of this conflict; however, in the United States the French and Indian War is viewed as a singular conflict which was not associated with any European war.[7] French Canadians call it the guerre de la Conquête ('War of the Conquest').[8][9]
 The British colonists were supported at various times by the Iroquois, Catawba, and Cherokee tribes, and the French colonists were supported by Wabanaki Confederacy members Abenaki and Mi'kmaq, and the Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot (Huron).[10] Fighting took place primarily along the frontiers between New France and the British colonies, from the Province of Virginia in the south to Newfoundland in the north. It began with a dispute over control of the confluence of the Allegheny River and Monongahela River called the Forks of the Ohio, and the site of the French Fort Duquesne at the location that later became Pittsburgh, Pennsylvania. The dispute erupted into violence in the Battle of Jumonville Glen in May 1754, during which Virginia militiamen under the command of 22-year-old George Washington ambushed a French patrol.[11]
 In 1755, six colonial governors met with General Edward Braddock, the newly arrived British Army commander, and planned a four-way attack on the French.  None succeeded, and the main effort by Braddock proved a disaster; he lost the Battle of the Monongahela on July 9, 1755, and died a few days later. British operations failed in the frontier areas of the Province of Pennsylvania and the Province of New York during 1755–57 due to a combination of poor management, internal divisions, effective Canadien scouts, French regular forces, and Native warrior allies. In 1755, the British captured Fort Beauséjour on the border separating Nova Scotia from Acadia, and they ordered the expulsion of the Acadians (1755–64) soon afterwards. Orders for the deportation were given by Commander-in-Chief William Shirley without direction from Great Britain. The Acadians were expelled, both those captured in arms and those who had sworn the loyalty oath to the king. Natives likewise were driven off the land to make way for settlers from New England.[12]
 The British Pitt government fell due to disastrous campaigns in 1757, including a failed expedition against Louisbourg and the Siege of Fort William Henry; this last was followed by the Natives torturing and massacring their colonial victims. William Pitt came to power and significantly increased British military resources in the colonies at a time when France was unwilling to risk large convoys to aid the limited forces that they had in New France, preferring to concentrate their forces against Prussia and its allies who were now engaged in the Seven Years' War in Europe. The conflict in Ohio ended in 1758 with the British–American victory in the Ohio Country. Between 1758 and 1760, the British military launched a campaign to capture French Canada. They succeeded in capturing territory in surrounding colonies and ultimately the city of Quebec (1759). The following year the British were victorious in the Montreal Campaign in which the French ceded Canada in accordance with the Treaty of Paris (1763).
 France also ceded its territory east of the Mississippi to Great Britain, as well as French Louisiana west of the Mississippi River to its ally Spain in compensation for Spain's loss to Great Britain of Spanish Florida (Spain had ceded Florida to Britain in exchange for the return of Havana, Cuba). France's colonial presence north of the Caribbean was reduced to the islands of Saint Pierre and Miquelon, confirming Great Britain's position as the dominant colonial power in northern America.
 In British America, wars were often named after the sitting British monarch, such as King William's War or Queen Anne's War. There had already been a King George's War in the 1740s during the reign of King George II, so British colonists named this conflict after their opponents, and it became known as the French and Indian War.[13] This continues as the standard name for the war in the United States, although indigenous peoples fought on both sides of the conflict. It also led into the Seven Years' War overseas, a much larger conflict between France and Great Britain that did not involve the American colonies; some historians make a connection between the French and Indian War and the Seven Years' War overseas, but most residents of the United States consider them as two separate conflicts—only one of which involved the American colonies,[14] and American historians generally use the traditional name. Less frequently used names for the war include the Fourth Intercolonial War and the Great War for the Empire.[13]
 In Europe, the French and Indian War is conflated into the Seven Years' War and not given a separate name. ""Seven Years"" refers to events in Europe, from the official declaration of war in 1756—two years after the French and Indian War had started—to the signing of the peace treaty in 1763. The French and Indian War in America, by contrast, was largely concluded in six years from the Battle of Jumonville Glen in 1754 to the capture of Montreal in 1760.[13]
 Canadians conflate both the European and American conflicts into the Seven Years' War (Guerre de Sept Ans).[8] French Canadians also use the term ""War of Conquest"" (Guerre de la Conquête), since it is the war in which New France was conquered by the British and became part of the British Empire. In Quebec, this term was promoted by popular historians Jacques Lacoursière and Denis Vaugeois, who borrowed from the ideas of Maurice Séguin in considering this war as a dramatic tipping point of French Canadian identity and nationhood.[15]
 At this time, North America east of the Mississippi River was largely claimed by either Great Britain or France. Large areas had no colonial settlements. The French population numbered about 75,000 and was heavily concentrated along the St. Lawrence River valley, with some also in Acadia (present-day New Brunswick and parts of Nova Scotia), including Île Royale (Cape Breton Island). Fewer lived in New Orleans; Biloxi, Mississippi; Mobile, Alabama; and small settlements in the Illinois Country, hugging the east side of the Mississippi River and its tributaries. French fur traders and trappers traveled throughout the St. Lawrence and Mississippi watersheds, did business with local Indian tribes, and often married Indian women.[16] Traders married daughters of chiefs, creating high-ranking unions.
 British settlers outnumbered the French 20 to 1[17] with a population of about 1.5 million ranged along the Atlantic coast of the continent from Nova Scotia and the Colony of Newfoundland in the north to the Province of Georgia in the south.[18] Many of the older colonies' land claims extended arbitrarily far to the west, as the extent of the continent was unknown at the time when their provincial charters were granted. Their population centers were along the coast, but the settlements were growing into the interior. The British captured Nova Scotia from France in 1713, which still had a significant French-speaking population. Britain also claimed Rupert's Land where the Hudson's Bay Company traded for furs with local Indian tribes.
 Between the French and British colonists, large areas were dominated by Indian tribes. To the north, the Mi'kmaq and the Abenakis were engaged in Father Le Loutre's War and still held sway in parts of Nova Scotia, Acadia, and the eastern portions of the province of Canada, as well as much of Maine.[19] The Iroquois Confederation dominated much of upstate New York and the Ohio Country, although Ohio also included Algonquian-speaking populations of Delaware and Shawnee, as well as Iroquoian-speaking Mingos. These tribes were formally under Iroquois rule and were limited by them in their authority to make agreements.[20] The Iroquois Confederation initially held a stance of neutrality to ensure continued trade with both French and British. Though maintaining this stance proved difficult as the Iroquois Confederation tribes sided and supported French or British causes depending on which side provided the most beneficial trade.[21]
 The Southeast interior was dominated by Siouan-speaking Catawbas, Muskogee-speaking Creeks and Choctaw, and the Iroquoian-speaking Cherokee tribes.[22] When war broke out, the French colonists used their trading connections to recruit fighters from tribes in western portions of the Great Lakes region, which was not directly subject to the conflict between the French and British; these included the Hurons, Mississaugas, Ojibwas, Winnebagos, and Potawatomi.
 The British colonists were supported in the war by the Iroquois Six Nations and also by the Cherokees, until differences sparked the Anglo-Cherokee War in 1758. In 1758, the Province of Pennsylvania successfully negotiated the Treaty of Easton in which a number of tribes in the Ohio Country promised neutrality in exchange for land concessions and other considerations. Most of the other northern tribes sided with the French, their primary trading partner and supplier of arms. The Creeks and Cherokees were subject to diplomatic efforts by both the French and British to gain either their support or neutrality in the conflict.[23][additional citation(s) needed]
 At this time, Spain claimed only the province of Florida in eastern America. It controlled Cuba and other territories in the West Indies that became military objectives in the Seven Years' War. Florida's European population was a few hundred, concentrated in St. Augustine.[24]
 There were no French regular army troops stationed in America at the onset of war. New France was defended by about 3,000 troupes de la marine, companies of colonial regulars (some of whom had significant woodland combat experience). The colonial government recruited militia support when needed. The British had few troops. Most of the British colonies mustered local militia companies to deal with Indian threats, generally ill trained and available only for short periods, but they did not have any standing forces. Virginia, by contrast, had a large frontier with several companies of British regulars.[citation needed]
 When hostilities began, the British colonial governments preferred operating independently of one another and of the government in London. This situation complicated negotiations with Indian tribes, whose territories often encompassed land claimed by multiple colonies. As the war progressed, the leaders of the British Army establishment tried to impose constraints and demands on the colonial administrations.[citation needed]
 New France's Governor-General Roland-Michel Barrin de La Galissonière was concerned about the incursion and expanding influence in the Ohio Country of British colonial traders such as George Croghan.  In June 1747, he ordered Pierre-Joseph Céloron to lead a military expedition through the area. Its objectives were:
 Céloron's expedition force consisted of about 200 Troupes de la marine and 30 Indians, and they covered about 3,000 miles (4,800 km) between June and November 1749. They went up the St. Lawrence, continued along the northern shore of Lake Ontario, crossed the portage at Niagara, and followed the southern shore of Lake Erie. At the Chautauqua Portage near Barcelona, New York, the expedition moved inland to the Allegheny River, which it followed to the site of Pittsburgh. There Céloron buried lead plates engraved with the French claim to the Ohio Country.[25] Whenever he encountered British colonial merchants or fur-traders, he informed them of the French claims on the territory and told them to leave.[25]
 Céloron's expedition arrived at Logstown where the Indians in the area informed him that they owned the Ohio Country and that they would trade with the British colonists regardless of the French.[26] He continued south until his expedition reached the confluence of the Ohio and the Miami rivers, which lay just south of the village of Pickawillany, the home of the Miami chief known as ""Old Briton"". Céloron threatened Old Briton with severe consequences if he continued to trade with British colonists, but Old Briton ignored the warning. Céloron returned disappointedly to Montreal in November 1749.[27]
 Céloron wrote an extensively detailed report. ""All I can say is that the Natives of these localities are very badly disposed towards the French,"" he wrote, ""and are entirely devoted to the English. I don't know in what way they could be brought back.""[26] Even before his return to Montreal, reports on the situation in the Ohio Country were making their way to London and Paris, each side proposing that action be taken. Massachusetts governor William Shirley was particularly forceful, stating that British colonists would not be safe as long as the French were present.[28]
 The War of the Austrian Succession ended in 1748 with the signing of the Treaty of Aix-la-Chapelle, which was primarily focused on resolving issues in Europe. The issues of conflicting territorial claims between British and French colonies were turned over to a commission, but it reached no decision. Frontier areas were claimed by both sides, from Nova Scotia and Acadia in the north to the Ohio Country in the south. The disputes also extended into the Atlantic Ocean, where both powers wanted access to the rich fisheries of the Grand Banks off Newfoundland.[citation needed]
 In 1749, the British government gave land to the Ohio Company of Virginia for the purpose of developing trade and settlements in the Ohio Country.[29] The grant required that it settle 100 families in the territory and construct a fort for their protection. But the territory was also claimed by Pennsylvania, and both colonies began pushing for action to improve their respective claims.[30] In 1750, Christopher Gist explored the Ohio territory, acting on behalf of both Virginia and the company, and he opened negotiations with the Indian tribes at Logstown.[31] He completed the 1752 Treaty of Logstown in which the local Indians agreed to terms through their ""Half-King"" Tanacharison and an Iroquois representative. These terms included permission to build a strong house at the mouth of the Monongahela River on the modern site of Pittsburgh, Pennsylvania.[32]
 Governor-General of New France Marquis de la Jonquière died on March 17, 1752, and he was temporarily replaced by Charles le Moyne de Longueuil. His permanent replacement was to be the Marquis Duquesne, but he did not arrive in New France until 1752 to take over the post.[33] The continuing British activity in the Ohio territories prompted Longueuil to dispatch another expedition to the area under the command of Charles Michel de Langlade, an officer in the Troupes de la Marine. Langlade was given 300 men, including French-Canadians and warriors of the Ottawa tribe. His objective was to punish the Miami people of Pickawillany for not following Céloron's orders to cease trading with the British. On June 21, the French war party attacked the trading center at Pickawillany, capturing three traders[27] and killing 14 Miami Indians, including Old Briton. He was reportedly ritually cannibalized by some Indians in the expedition party.
 In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier. Céloron, however, had limited the record of French claims to the burial of lead plates, whereas Marin constructed and garrisoned forts. He first constructed Fort Presque Isle on Lake Erie's south shore near Erie, Pennsylvania, and he had a road built to the headwaters of LeBoeuf Creek. He then constructed a second fort at Fort Le Boeuf in Waterford, Pennsylvania, designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson was a chief of the Mingo Indians, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French whom he accused of killing and eating his father. He traveled to Fort Le Boeuf and threatened the French with military action, which Marin contemptuously dismissed.[34]
 The Iroquois sent runners to the manor of William Johnson in upstate New York, who was the British Superintendent for Indian Affairs in the New York region and beyond. Johnson was known to the Iroquois as Warraghiggey, meaning ""he who does great things."" He spoke their languages and had become a respected honorary member of the Iroquois Confederacy in the area, and he was made a colonel of the Iroquois in 1746; he was later commissioned as a colonel of the Western New York Militia.
 The Indian representatives and Johnson met with Governor George Clinton and officials from some of the other American colonies at Albany, New York. Mohawk Chief Hendrick was the speaker of their tribal council, and he insisted that the British abide by their obligations[which?] and block French expansion. Clinton did not respond to his satisfaction, and Hendrick said that the ""Covenant Chain"" was broken, a long-standing friendly relationship between the Iroquois Confederacy and the British Crown.
 Governor Robert Dinwiddie of Virginia was an investor in the Ohio Company, which stood to lose money if the French held their claim.[35] He ordered 21-year-old Major George Washington (whose brother was another Ohio Company investor) of the Virginia Regiment to warn the French to leave Virginia territory in October 1753.[36] Washington left with a small party, picking up Jacob Van Braam as an interpreter, Christopher Gist (a company surveyor working in the area), and a few Mingos led by Tanaghrisson. On December 12, Washington and his men reached Fort Le Boeuf.[37][38]
 Jacques Legardeur de Saint-Pierre succeeded Marin as commander of the French forces after Marin died on October 29, and he invited Washington to dine with him. Over dinner, Washington presented Saint-Pierre with the letter from Dinwiddie demanding an immediate French withdrawal from the Ohio Country. Saint-Pierre said, ""As to the Summons you send me to retire, I do not think myself obliged to obey it.""[39] He told Washington that France's claim to the region was superior to that of the British, since René-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.[40]
 Washington's party left Fort Le Boeuf early on December 16 and arrived in Williamsburg on January 16, 1754. He stated in his report, ""The French had swept south"",[41] detailing the steps which they had taken to fortify the area, and their intention to fortify the confluence of the Allegheny and Monongahela rivers.[42]
 Even before Washington returned, Dinwiddie had sent a company of 40 men under William Trent to that point where they began construction of a small stockaded fort in the early months of 1754.[43] Governor Duquesne sent additional French forces under Claude-Pierre Pécaudy de Contrecœur to relieve Saint-Pierre during the same period, and Contrecœur led 500 men south from Fort Venango on April 5, 1754.[44] These forces arrived at the fort on April 16, but Contrecœur generously allowed Trent's small company to withdraw. He purchased their construction tools to continue building what became Fort Duquesne.[45]
 Dinwiddie had ordered Washington to lead a larger force to assist Trent in his work, and Washington learned of Trent's retreat while he was en route.[46] Mingo sachem Tanaghrisson had promised support to the British, so Washington continued toward Fort Duquesne and met with him. He then learned of a French scouting party in the area from a warrior sent by Tanaghrisson, so he added Tanaghrisson's dozen Mingo warriors to his own party. Washington's combined force of 52 ambushed 40 Canadiens (French colonists of New France) on the morning of May 28 in what became known as the Battle of Jumonville Glen.[47] They killed many of the Canadiens, including their commanding officer Joseph Coulon de Jumonville, whose head was reportedly split open by Tanaghrisson with a tomahawk. Historian Fred Anderson suggests that Tanaghrisson was acting to gain the support of the British and to regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships. One of Tanaghrisson's men told Contrecoeur that Jumonville had been killed by British musket fire.[48] Historians generally consider the Battle of Jumonville Glen as the opening battle of the French and Indian War in North America, and the start of hostilities in the Ohio valley.
 Following the battle, Washington pulled back several miles and established Fort Necessity, which the Canadians attacked under the command of Jumonville's brother at the Battle of Fort Necessity on July 3. Washington surrendered and negotiated a withdrawal under arms. One of his men reported that the Canadian force was accompanied by Shawnee, Delaware, and Mingo warriors—just those whom Tanaghrisson was seeking to influence.[49]
 News of the two battles reached England in August. After several months of negotiations, the government of the Duke of Newcastle decided to send an army expedition the following year to dislodge the French.[50] They chose Major General Edward Braddock to lead the expedition.[51] Word of the British military plans leaked to France well before Braddock's departure for North America. In response, King Louis XV dispatched six regiments to New France under the command of Baron Dieskau in 1755.[52] The British sent out their fleet in February 1755, intending to blockade French ports, but the French fleet had already sailed. Admiral Edward Hawke detached a fast squadron to North America in an attempt to intercept them.
 In a second British action, Admiral Edward Boscawen fired on the French ship Alcide on June 8, 1755, capturing her and two troop ships.[53] The British harassed French shipping throughout 1755, seizing ships and capturing seamen. These actions contributed to the eventual formal declarations of war in spring 1756.[54]
 An early important political response to the opening of hostilities was the convening of the Albany Congress in June and July, 1754.  The goal of the congress was to formalize a unified front in trade and negotiations with the Indians, since the allegiance of the various tribes and nations was seen to be pivotal in the war that was unfolding.  The plan that the delegates agreed to was neither ratified by the colonial legislatures nor approved by the Crown.  Nevertheless, the format of the congress and many specifics of the plan became the prototype for confederation during the War of Independence.
 The British formed an aggressive plan of operations for 1755. General Braddock was to lead the expedition to Fort Duquesne,[55] while Massachusetts governor William Shirley was given the task of fortifying Fort Oswego and attacking Fort Niagara. Sir William Johnson was to capture Fort St. Frédéric at Crown Point, New York,[56] and Lieutenant Colonel Robert Monckton was to capture Fort Beauséjour to the east on the frontier between Nova Scotia and Acadia.[57]
 Braddock led about 1,500 army troops and provincial militia on the Braddock expedition in June 1755 to take Fort Duquesne, with George Washington as one of his aides. The expedition was a disaster. It was attacked by French regulars, Canadian Militiamen, and Indian warriors ambushing them from hiding places up in trees and behind logs, and Braddock called for a retreat. He was killed and approximately 1,000 British soldiers were killed or injured.[55] The remaining 500 British troops retreated to Virginia, led by Washington. Washington and Thomas Gage played key roles in organizing the retreat—two future opponents in the American Revolutionary War.
 The British government initiated a plan to increase their military capability in preparation for war following news of Braddock's defeat and the start of parliament's session in November 1755. Among the early legislative measures were the Recruiting Act 1756,[58] the Commissions to Foreign Protestants Act 1756[59] for the Royal American Regiment, the Navigation Act 1756,[60] and the Continuance of Laws Act 1756.[61] England passed the Naval Prize Act 1756 following the proclamation of war on May 17 to allow the capture of ships and establish privateering.[62]
 The French acquired a copy of the British war plans, including the activities of Shirley and Johnson. Shirley's efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by his inexperience in managing large expeditions. In conjunction, he was made aware that the French were massing for an attack on Fort Oswego in his absence when he planned to attack Fort Niagara. As a response, he left garrisons at Oswego, Fort Bull, and Fort Williams, the last two located on the Oneida Carry between the Mohawk River and Wood Creek at Rome, New York. Supplies were cached at Fort Bull for use in the projected attack on Niagara.
 Johnson's expedition was better organized than Shirley's, which was noticed by New France's governor the Marquis de Vaudreuil. Vaudreuil had been concerned about the extended supply line to the forts on the Ohio, and he had sent Baron Dieskau to lead the defenses at Frontenac against Shirley's expected attack. Vaudreuil saw Johnson as the larger threat and sent Dieskau to Fort St. Frédéric to meet that threat. Dieskau planned to attack the British encampment at Fort Edward at the upper end of navigation on the Hudson River, but Johnson had strongly fortified it, and Dieskau's Indian support was reluctant to attack. The two forces finally met in the bloody Battle of Lake George between Fort Edward and Fort William Henry. The battle ended inconclusively, with both sides withdrawing from the field. Johnson's advance stopped at Fort William Henry, and the French withdrew to Ticonderoga Point, where they began the construction of Fort Carillon (later renamed Fort Ticonderoga after the British captured it in 1759).
 Colonel Monckton captured Fort Beauséjour in June 1755 in the sole British success that year, cutting off the French Fortress Louisbourg from land-based reinforcements. To cut vital supplies to Louisbourg, Nova Scotia's Governor Charles Lawrence ordered the deportation of the French-speaking Acadian population from the area. Monckton's forces, including companies of Rogers' Rangers, forcibly removed thousands of Acadians, chasing down many who resisted and sometimes committing atrocities. Cutting off supplies to Louisbourg led to its demise.[63] The Acadian resistance was sometimes quite stiff, in concert with Indian allies including the Mi'kmaq, with ongoing frontier raids against Dartmouth and Lunenburg, among others. The only clashes of any size were at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757, other than the campaigns to expel the Acadians ranging around the Bay of Fundy, on the Petitcodiac and St. John rivers, and Île Saint-Jean.
 Following the death of Braddock, William Shirley assumed command of British forces in North America, and he laid out his plans for 1756 at a meeting in Albany in December 1755. He proposed renewing the efforts to capture Niagara, Crown Point, and Duquesne, with attacks on Fort Frontenac on the north shore of Lake Ontario and an expedition through the wilderness of the Maine district and down the Chaudière River to attack the city of Quebec. His plan, however, got bogged down by disagreements and disputes with others, including William Johnson and New York's Governor Sir Charles Hardy, and consequently gained little support.
 Newcastle replaced him in January 1756 with Lord Loudoun, with Major General James Abercrombie as his second in command. Neither of these men had as much campaign experience as the trio of officers whom France sent to North America.[54] French regular army reinforcements arrived in New France in May 1756, led by Major General Louis-Joseph de Montcalm and seconded by the Chevalier de Lévis and Colonel François-Charles de Bourlamaque, all experienced veterans from the War of the Austrian Succession. On May 17, 1756, Britain formally declared war on France, which expanded the war into Europe and came to be known as the Seven Years' War.
 Governor Vaudreuil had ambitions to become the French commander in chief, in addition to his role as governor, and he acted during the winter of 1756 before those reinforcements arrived. Scouts had reported the weakness of the British supply chain, so he ordered an attack against the forts which Shirley had erected at the Oneida Carry. In the Battle of Fort Bull, French forces destroyed the fort and large quantities of supplies, including 45,000 pounds of gunpowder. They set back any British hopes for campaigns on Lake Ontario and endangered the Oswego garrison, already short on supplies. French forces in the Ohio valley also continued to intrigue with Indians throughout the area, encouraging them to raid frontier settlements. This led to ongoing alarms along the western frontiers, with streams of refugees returning east to get away from the action.
 The new British command was not in place until July. Abercrombie arrived in Albany but refused to take any significant actions until Loudoun approved them, and Montcalm took bold action against his inertia. He built on Vaudreuil's work harassing the Oswego garrison and executed a strategic feint by moving his headquarters to Ticonderoga, as if to presage another attack along Lake George. With Abercrombie pinned down at Albany, Montcalm slipped away and led the successful attack on Oswego in August. In the aftermath, Montcalm and the Indians under his command disagreed about the disposition of prisoners' personal effects. The Europeans did not consider them prizes and prevented the Indians from stripping the prisoners of their valuables, which angered the Indians.
 Loudoun was a capable administrator but a cautious field commander, and he planned one major operation for 1757: an attack on New France's capital of Quebec. He left a sizable force at Fort William Henry to distract Montcalm and began organizing for the expedition to Quebec. He was then ordered to attack Louisbourg first by William Pitt, the Secretary of State responsible for the colonies. The expedition was beset by delays of all kinds but was finally ready to sail from Halifax, Nova Scotia, in early August. In the meantime, French ships had escaped the British blockade of the French coast, and a fleet awaited Loudoun at Louisbourg which outnumbered the British fleet. Faced with this strength, Loudoun returned to New York amid news that a massacre had occurred at Fort William Henry.
 French irregular forces (Canadian scouts and Indians) harassed Fort William Henry throughout the first half of 1757. In January, they ambushed British rangers near Ticonderoga. In February, they launched a raid against the position across the frozen Lake George, destroying storehouses and buildings outside the main fortification. In early August, Montcalm and 7,000 troops besieged the fort, which capitulated with an agreement to withdraw under parole. When the withdrawal began, some of Montcalm's Indian allies attacked the British column because they were angry about the lost opportunity for loot, killing and capturing several hundred men, women, children, and slaves. The aftermath of the siege may have contributed to the transmission of smallpox into remote Indian populations, as some Indians were reported to have traveled from beyond the Mississippi to participate in the campaign and returned afterward. Modern writer William Nester believes that the Indians might have been exposed to European carriers, although no proof exists.[64]
 Vaudreuil and Montcalm were minimally resupplied in 1758, as the British blockade of the French coastline limited French shipping. The situation in New France was further exacerbated by a poor harvest in 1757, a difficult winter, and the allegedly corrupt machinations of François Bigot, the intendant of the territory. His schemes to supply the colony inflated prices and were believed by Montcalm to line his pockets and those of his associates. A massive outbreak of smallpox among western Indian tribes led many of them to stay away from trading in 1758. The disease probably spread through the crowded conditions at William Henry after the battle;[65] yet the Indians blamed the French for bringing ""bad medicine"" as well as denying them prizes at Fort William Henry.
 Montcalm focused his meager resources on the defense of the St. Lawrence, with primary defenses at Carillon, Quebec, and Louisbourg, while Vaudreuil argued unsuccessfully for a continuation of the raiding tactics that had worked quite effectively in previous years.[66] The British failures in North America combined with other failures in the European theater and led to Newcastle's fall from power along with the Duke of Cumberland, his principal military advisor.
 Newcastle and Pitt joined in an uneasy coalition in which Pitt dominated the military planning. He embarked on a plan for the 1758 campaign that was largely developed by Loudoun. He had been replaced by Abercrombie as commander in chief after the failures of 1757. Pitt's plan called for three major offensive actions involving large numbers of regular troops supported by the provincial militias, aimed at capturing the heartlands of New France. Two of the expeditions were successful, with Fort Duquesne and Louisbourg falling to sizable British forces.
 The Forbes Expedition was a British campaign in September–October 1758, with 6,000 troops led by General John Forbes sent to drive out the French from the contested Ohio Country. The French withdrew from Fort Duquesne and left the British in control of the Ohio River Valley.[67] The great French fortress at Louisbourg in Nova Scotia was captured after a siege.[68]
 The third invasion was stopped with the improbable French victory in the Battle of Carillon, in which 3,600 Frenchmen defeated Abercrombie's force of 18,000 regulars, militia, and Indian allies outside the fort which the French called Carillon and the British called Ticonderoga. Abercrombie saved something from the disaster when he sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac, including caches of supplies destined for New France's western forts and furs destined for Europe. Abercrombie was recalled and replaced by Jeffery Amherst, victor at Louisbourg.
 The French had generally poor results in 1758 in most theaters of the war. The new foreign minister was the duc de Choiseul, and he decided to focus on an invasion of Britain to draw British resources away from North America and the European mainland. The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France and sent funds to Britain's mainland ally of Prussia, while the French Navy failed in the 1759 naval battles at Lagos and Quiberon Bay. In one piece of good fortune, some French supply ships did manage to depart France and elude the British blockade of the French coast.
 The British proceeded to wage a campaign in the northwest frontier of Canada in an effort to cut off the French frontier forts to the west and south. They captured Ticonderoga and Fort Niagara, and they defeated the French at the Thousand Islands in the summer of 1759. In September 1759, James Wolfe defeated Montcalm in the Battle of the Plains of Abraham which claimed the lives of both commanders. After the battle, the French capitulated the city to the British.
 In April 1760, François Gaston de Lévis led French forces to launch an attack to retake Quebec. Although he won the Battle of Sainte-Foy, Lévis' subsequent siege of Quebec ended in defeat when British ships arrived to relieve the garrison. After Lévis had retreated he was given another blow when a British naval victory at Restigouche brought the loss of French ships meant to resupply his army. In July Jeffrey Amherst then led British forces numbering around 18,000 men in a three pronged attack on Montreal. After eliminating French positions along the way all three forces met up and surrounded Montreal in September. Many Canadians deserted or surrendered their arms to British forces while the Native allies of the French sought peace and neutrality. De Lévis and the Marquis de Vaudreuil reluctantly signed the Articles of Capitulation of Montreal on September 8 which effectively completed the British conquest of New France.
 Most of the fighting ended in America in 1760, although it continued in Europe between France and Britain. The notable exception was the French seizure of St. John's, Newfoundland. General Amherst heard of this surprise action and immediately dispatched troops under his nephew William Amherst, who regained control of Newfoundland after the Battle of Signal Hill in September 1762.[69] Many of the British troops who were stationed in America were reassigned to participate in further British actions in the West Indies, including the capture of Spanish Havana when Spain belatedly entered the conflict on the side of France, and a British expedition against French Martinique in 1762 led by Major General Robert Monckton.[70]
 Governor Vaudreuil in Montreal negotiated a capitulation with General Amherst in September 1760. Amherst granted his requests that any French residents who chose to remain in the colony would be given freedom to continue worshiping in their Roman Catholic tradition, to own property, and to remain undisturbed in their homes. The British provided medical treatment for the sick and wounded French soldiers, and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.[71]
 General Amherst also oversaw the transfer of French fortifications to British control on the western frontier. The policies which he introduced in those lands disturbed large numbers of Natives and contributed to the outbreak of Pontiac's War in 1763,[72] in which a series of Native attacks on frontier forts occurred, such as that on Fort Miami which effectively brought a nearly half-century long period of European garrisoning at Kekionga to an end. The frontier settlements required the continued deployment of British forces, and the conflict was not fully concluded until 1766.[73]
 Beginning from the 1750s and lasting until the 1760s, a smallpox outbreak devastated several Native communities throughout the American Midwest. The outbreak was brought on in part by victorious Native warriors who had fought on the side of the French bringing home prizes of war which had been infected with the disease; the Ojibwe, Odawa and Potawatomi peoples were most affected by the outbreak. An oral account from Odawa tribal leader and historian Andrew Blackbird claimed that the outbreak had ""entirely depopulated and laid waste"" to Waganagisi, a large Odawa settlement.[74][75]
 The war in North America, along with the global Seven Years' War, officially ended with the signing of the Treaty of Paris on 10 February 1763, by the kingdoms of Great Britain, France and Spain, with Portugal in agreement. The British offered France the choice of surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique, which had been occupied by the British. France chose to cede the former but was able to negotiate the retention of Saint Pierre and Miquelon, two small islands in the Gulf of St. Lawrence, along with fishing rights in the area. They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent. French philosopher Voltaire referred to Canada disparagingly as nothing more than a few acres of snow. The British, however, were happy to take New France, as defence of their North American colonies would no longer be an issue (though the absence of that threat caused many colonists to conclude they no longer needed British protection). Britain also had ample places from which to obtain sugar. Spain traded Florida to Britain in order to regain Cuba, but they also gained Louisiana from France, including New Orleans, in compensation for their losses. Great Britain and Spain also agreed that navigation on the Mississippi River was to be open to vessels of all nations.[76]
 The war changed economic, political, governmental, and social relations among the three European powers, their colonies, and the people who inhabited those territories. France and Britain both suffered financially because of the war, with significant long-term consequences.
 Britain gained control of French Canada and Acadia, colonies containing approximately 80,000 primarily French-speaking Roman Catholic residents. The deportation of Acadians beginning in 1755 made land available to immigrants from Europe and migrants from the colonies to the south. The British resettled many Acadians throughout its American provinces, but many went to France and some went to New Orleans, which they expected to remain French. Some were sent to colonize places as diverse as French Guiana and the Falkland Islands, but these efforts were unsuccessful. The Louisiana population contributed to founding the Cajun population. (The French word ""Acadien"" changed to ""Cadien"" then to ""Cajun"".)[77]
 King George III issued the Royal Proclamation of 1763 on October 7, 1763, which outlined the division and administration of the newly conquered territory, and it continues to govern relations to some extent between the government of Canada and the First Nations. Included in its provisions was the reservation of lands west of the Appalachian Mountains to its Indian population,[78] a demarcation that was only a temporary impediment to a rising tide of westward-bound settlers.[79] The proclamation also contained provisions that prevented civic participation by the Roman Catholic Canadians.[80]
 The Quebec Act 1774 addressed issues brought forth by Roman Catholic French Canadians from the 1763 proclamation, and it transferred the Indian Reserve into the Province of Quebec. The Act maintained French Civil law, including the seigneurial system, a medieval code removed from France within a generation by the French Revolution. The Quebec Act was a major concern for the largely Protestant Thirteen Colonies over the advance of ""popery"". It is typically associated with other Intolerable Acts, legislation that eventually led to the American Revolutionary War. The Quebec Act served as the constitutional document for the province of Quebec until it was superseded by the Constitutional Act 1791.
 The Seven Years' War nearly doubled Great Britain's national debt. The Crown sought sources of revenue to pay it off and attempted to impose new taxes on its colonies. These attempts were met with increasingly stiff resistance, until troops were called in to enforce the Crown's authority, and they ultimately led to the start of the American Revolutionary War.[81] France attached comparatively little value to its American possessions, apart from the highly profitable sugar-producing Antilles islands which it retained. Minister Choiseul considered that he had made a good deal at the Treaty of Paris, and Voltaire wrote that Louis XV had lost a few acres of snow.[82] However, the military defeat and the financial burden of the war weakened the French monarchy and contributed to the advent of the French Revolution in 1789.[83]
 The elimination of French power in America meant the disappearance of a strong ally for some Indian tribes.[83] The Ohio Country was now more available to colonial settlement due to the construction of military roads by Braddock and Forbes.[84] The Spanish takeover of the Louisiana territory was not completed until 1769, and it had modest repercussions. The British takeover of Spanish Florida resulted in the westward migration of Indian tribes who did not want to do business with them. This migration also caused a rise in tensions between the Choctaw and the Creek, historic enemies who were competing for land.[85] The change of control in Florida also prompted most of its Spanish Catholic population to leave. Most went to Cuba, although some Christianized Yamasee were resettled to the coast of Mexico.[86]
 France returned to America in 1778 with the establishment of a Franco-American alliance against Great Britain in the American Revolutionary War, in what historian Alfred A. Cave describes as French ""revenge for Montcalm's death"".[87]
"
Edward Braddock,https://en.wikipedia.org/wiki/Edward_Braddock,"
 Edward Braddock (January 1695 – 13 July 1755) was a British officer and commander-in-chief for the Thirteen Colonies during the start of the French and Indian War (1754–1763), the North American front of what is known in Europe and Canada as the Seven Years' War (1756–1763). He is remembered for his command of a disastrous expedition against the French-occupied Ohio River Valley in 1755 which led to his death.
 Born in 1695 as the son of Major-General Edward Braddock of the Coldstream Guards and his wife,[1] Braddock followed his father into the British army. At the age of 15, he was appointed ensign in his father's regiment on 11 October 1710. He was promoted to lieutenant of the grenadier company in 1716. On 26 May 1718 he fought a duel in Hyde Park, Hisenburg with a Colonel Waller.
 Braddock was promoted to captain in 1736, at the age of 41. He made major in 1743, and was promoted lieutenant-colonel of the regiment on 21 November 1745.
 He participated in the Siege of Bergen op Zoom in 1747. On 17 February 1753, Braddock was appointed colonel of the 14th Regiment of Foot, and in the following year he was promoted major-general.[2]
 Appointed shortly afterward to command against the French in America, Braddock landed with two regiments of British regulars on 20 February 1755 in Hampton, in the colony of Virginia.[1] He met with several of the colonial governors at the Congress of Alexandria on 14 April and was persuaded to undertake vigorous actions against the French.[1] The attack would proceed on four fronts: a general from Massachusetts would attack at Fort Niagara, General William Johnson would attack Fort Saint-Frédéric at Crown Point, Colonel Robert Monckton at Fort Beausejour on the Bay of Fundy, while Braddock himself would lead an expedition against Fort Duquesne (now Pittsburgh) at the Forks of the Ohio River.
 After some months of preparation, in which he was hampered by administrative confusion and want of resources previously promised by the colonials, the Braddock expedition took the field with a picked column, in which George Washington served as a volunteer officer.[3] Braddock took some of his men and marched forward, leaving most of his men behind. The column crossed the Monongahela River on 9 July 1755, and shortly afterward collided head-on with an Indian and French force which was rushing from Fort Duquesne to oppose the river crossing.[1] Although the initial exchange of musketry favored the British, felling the French commander and causing some Canadian militia to flee, the remaining Indian/French force reacted quickly. They ran down the flanks of the column and put it under a murderous crossfire.
 Braddock's troops reacted poorly and became disordered. The British attempted retreat, but ran into the rest of the British soldiers earlier left behind. Braddock rallied his men repeatedly, but fell at last, mortally wounded by a shot through the chest.[1] Although the exact causes of the defeat are debated to this day, a contributing factor was likely Braddock's underestimation of how effectively the French and Indians could react in a battle situation, and how rapidly the discipline and fighting effectiveness of his own men could evaporate.
 Braddock was borne off the field by Washington and Col. Nicholas Meriwether;[4] he died on 13 July from wounds suffered in the battle. Before he died, Braddock left Washington his ceremonial sash that he wore with his battle uniform, as well as his two pistols.[5] Some of his last words were, ""Who would have thought?"" and ""we shall know better another time"". Reportedly, Washington always took this sash with him for the rest of his life, both as the commander of the Continental Army and for his presidential duties. It is still on display today at Washington's home on the Potomac River, Mount Vernon.
 Braddock was buried just west of Great Meadows, where the remnants of the column halted on its retreat to reorganize.[1] He was buried in the middle of the road that his men had just cut through and wagons were rolled over top of the grave site to prevent his body from being discovered and desecrated by the Indians.[3] George Washington presided at the burial service,[3] as the chaplain had been severely wounded.
 Benjamin Franklin's Autobiography (1791) includes an account of helping General Braddock garner supplies and carriages for the general's troops.  He also describes a conversation with Braddock in which he explicitly warned the General that his plan to march troops to the fort through a narrow valley would be dangerous because of the possibility of an ambush.  This is sometimes cited as advice against the disastrous eventual outcome, but the fact remains that Braddock was not ambushed in that final action, and the battle site was not, in any case, a narrow valley. Braddock had in fact taken great precautions against ambuscade, and had crossed the Monongahela an additional time to avoid the narrow Turtle Creek defile.
 In 1804, human remains believed to be Braddock's were found buried in the roadway about 1.5 miles (2.4 km) west of Great Meadows by a crew of road workers.[6]  The remains were exhumed and moved to a nearby site for reburial.[7] A marble monument was erected over the new grave site in 1913 by the Coldstream Guards.
 General Braddock is the namesake of Braddock Borough, Mt. Braddock, Braddock Hills, and North Braddock in Pennsylvania;[8] the community of Braddock Heights or Braddock Mountain west of Frederick, Maryland; Braddock Middle School and Braddock Road in Cumberland, Maryland; and, in Virginia, Braddock Road, which runs from Alexandria to Aldie, a separate Braddock Road within the city of Alexandria – namesake of the Metrorail station at its eastern terminus – and Braddock Street in Winchester. Sections of the road cut by the British Army are known as the Braddock Road and form most of eastern U.S. Route 40 in Maryland and Pennsylvania.
 Braddock appears as an antagonist in the video game Assassin's Creed III, where George Washington is introduced as a young officer serving under Braddock in the French and Indian War.[9] The game portrays Braddock as a ruthless general, who indiscriminately kills his enemies, civilians, and even his own allies to achieve his goals. Additionally, he is a former member of the fictitious Templar Order, and a rival of Haytham Kenway, the playable character during the game's early missions. During his 1755 expedition, he is assassinated by Haytham with the help of several Native American tribes, who sought to see Braddock eliminated because his men ravaged their villages.[10][11] 
 Robert Matzen directed, wrote and produced the documentary When the Forest Ran Red: Washington, Braddock & a Doomed Army, which dramatizes the ambush of Braddock by 250 French soldiers and 600 Native Americans.[12]
"
Braddock Expedition,https://en.wikipedia.org/wiki/Braddock_Expedition,"The Braddock Expedition, also known as Braddock's Campaign or Braddock's Defeat, was a British military expedition which attempted to capture Fort Duquesne from the French in 1755 during the French and Indian War. The expedition, named after its commander General Edward Braddock, was defeated at the Battle of the Monongahela on July 9 and forced to retreat; Braddock was killed in action along with more than 500 of his troops. It ultimately proved to be a major setback for the British in the early stages of the war, one of the most disastrous defeats suffered by British forces in the 18th century.[6]
 Braddock's expedition was part of a massive British offensive against the French in North America that summer. As commander-in-chief of the British Army in America, General Edward Braddock led the main thrust against the Ohio Country with a column some 2,100 strong. His command consisted of two regular line regiments, the 44th and 48th, in all 1,400 regular soldiers and 700 provincial troops from several of the Thirteen Colonies, and artillery and other support troops. With these men, Braddock expected to seize Fort Duquesne easily, and then push on to capture a series of French forts, eventually reaching Fort Niagara. George Washington, promoted to Lieutenant-Colonel of the Virginia Regiment on June 4, 1754, by Governor Robert Dinwiddie,[7] was then just 23, knew the territory and served as a volunteer aide-de-camp to General Braddock.[8] Braddock's Chief of Scouts was Lieutenant John Fraser of the Virginia Regiment. Fraser owned land at Turtle Creek, had been at Fort Necessity, and had served as Second-in-Command at Fort Prince George (replaced by Fort Duquesne by the French), at the confluence of the Allegheny and Monongahela Rivers.
 Braddock mostly failed in his attempts to recruit Native American allies from those tribes not yet allied with the French; he had but eight Mingo Indians with him led by George Croghan, serving as scouts. A number of Native Americans in the area, notably Delaware leader Shingas, remained neutral. Caught between two powerful European empires at war, the local Native Americans could not afford to be on the side of the loser. They would decide based on Braddock's success or failure.
 Setting out from Fort Cumberland in Maryland on May 29, 1755, the expedition faced an enormous logistical challenge: moving a large body of men with equipment, provisions, and (most importantly, for attacking the forts) heavy cannons, across the densely wooded Allegheny Mountains and into western Pennsylvania, a journey of about 110 miles (180 km). Braddock had received important assistance from Benjamin Franklin, who helped procure wagons and supplies for the expedition. Among the wagoners were two young men who would later become legends of American history: Daniel Boone and Daniel Morgan. Other members of the expedition included Ensign William Crawford and Charles Scott. Among the officers of the expedition were Thomas Gage, Charles Lee, future American president George Washington, and Horatio Gates.
 The expedition progressed slowly because Braddock considered making a road to Fort Duquesne a priority in order to effectively supply the position he expected to capture and hold at the Forks of the Ohio, and because of a shortage of healthy draft animals. In some cases, the column was only able to progress at a rate of two miles (about 3 km) a day, creating Braddock's Road — an important legacy of the march — as they went. To speed up movement, Braddock split his men into a ""flying column"" of about 1,300 men which he commanded, and, lagging far behind, a supply column of 800 men with most of the baggage, commanded by Colonel Thomas Dunbar. They passed the ruins of Fort Necessity along the way, where the French and Canadians had defeated Washington the previous summer. Small French and Native American war bands skirmished with Braddock's men during the march.
 Meanwhile, at Fort Duquesne, the French garrison consisted of only about 250 French marines and Canadian militia, with about 640 Native American allies camped outside the fort. The Native Americans were from a variety of tribes long associated with the French, including Ottawas, Ojibwas, and Potawatomis. Claude-Pierre Pécaudy de Contrecœur, the Canadian commander, received reports from Native American scouting parties that the British were on their way to besiege the fort. He realised he could not withstand Braddock's cannon, and decided to launch a preemptive strike, an ambush of Braddock's army as he crossed the Monongahela River. The Native American allies were initially reluctant to attack such a large British force, but the French field commander Daniel Liénard de Beaujeu, who dressed himself in full war regalia complete with war paint, convinced them to follow his lead.
 By July 8, 1755, the Braddock force was on the land owned by the Chief Scout, Lieutenant John Fraser. That evening, the Native Americans sent delegates to the British to request a conference. Braddock chose Washington and Fraser as his emissaries. The Native Americans asked the British to halt their advance, claiming that the French could be persuaded to peacefully leave Fort Duquesne. Both Washington and Fraser recommended that Braddock approve the plan, but he demurred.
 On July 9, 1755, Braddock's men crossed the Monongahela without opposition, about 10 miles (16 km) south of Fort Duquesne. The advance guard of 300 grenadiers and colonials, accompanied by two cannon, and commanded by Lieutenant Colonel Thomas Gage began to move ahead. Washington tried to warn Braddock of the flaws in his plan — such as pointing out that the French and the Native Americans fought differently than the open-field style used by the British -- but his efforts were ignored: Braddock insisted that his troops fight as ""gentlemen"". Then, unexpectedly, Gage's advance guard came upon Beaujeu's party of French and Native Americans, who were hurrying to the river, behind schedule and too late to prepare an ambush.
 In the skirmish that followed between Gage's soldiers and the French, Beaujeu was among those killed by the first volley of musket fire by the grenadiers. Although some 100 French Canadians fled back to the fort and the noise of the cannon held the Native Americans off, Beaujeu's death did not have a negative effect on French morale. Jean-Daniel Dumas, a French officer, rallied the rest of the French and their Native American allies. The battle, known as the Battle of the Monongahela, or the Battle of the Wilderness, or just Braddock's Defeat, was officially begun. Braddock's force was approximately 1,400 men. The British faced a French and Native American force estimated to number between 300 and 900. The battle, frequently described as an ambush, was actually a meeting engagement, where two forces clash at an unexpected time and place. The quick and effective response of the French and Native Americans — despite the early loss of their commander — led many of Braddock's men to believe they had been ambushed. However, French battle reports state that while an ambush had been planned, the sudden arrival of the British forced a direct confrontation.
 After an exchange of fire, Gage's advance group fell back. In the narrow confines of the road, they collided with the main body of Braddock's force, which had advanced rapidly when the shots were heard. The entire column dissolved in disorder as the Canadian militiamen and Native Americans enveloped them and began firing from the dense woods on both sides. At this time, the French marines began advancing from the road and blocked any attempt by the British to move forward.
 Following Braddock's example, the officers kept trying to form their men into standard battle lines so they could fire in formation - a strategy that did little but make the soldiers easy targets. The artillery teams tried to provide covering fire, but there was no space to load the pieces properly and the artillerymen had no protection from enemy sharpshooters. The provincial troops accompanying the British eventually broke ranks and ran into the woods to engage the French; confused by what they thought were enemy reinforcements, panicking British regulars started mistakenly firing on the provincials. After several hours of intense combat, Braddock was fatally shot off his horse, and effective resistance collapsed. Washington, although he had no official position in the chain of command, was able to impose and maintain some order. He formed a rear guard, which allowed the remnants of the force to disengage. This earned him the sobriquet Hero of the Monongahela, by which he was toasted, and established his fame for some time to come.
 By sunset, the surviving British forces were retreating back down the road they had built. Braddock died of his wounds during the long retreat, on July 13, and is buried within the Fort Necessity parklands. Of the approximately 1,300 men Braddock had led into battle, 456 were killed and 422 wounded. Commissioned officers were prime targets and suffered greatly: out of 86 officers, 26 were killed and 37 wounded. Of the 50 or so women that accompanied the British column as maids and cooks, only 4 survived. The French and Canadians reported 8 killed and 4 wounded; their Native American allies lost 15 killed and 12 wounded.
 Colonel Dunbar, with the reserves and rear supply units, took command when the survivors reached his position. He ordered that excess supplies and cannons should be destroyed before withdrawing, burning about 150 wagons on the spot. Ironically, at this point the defeated, demoralized and disorganised British forces still outnumbered their opponents. The French and Native Americans did not pursue; they were far too busy looting dead bodies and collecting scalps. The French commander, Dumas, realized Braddock's army was utterly defeated. Yet, to avoid upsetting his men, he did not attempt any further pursuit.
 According to returns given June 8, 1755, at the encampment at Will's Creek.
 Detachement under Capt. Robert Hind
"
Ohio Country,https://en.wikipedia.org/wiki/Ohio_Country,"
 The Ohio Country (Ohio Territory,[a] Ohio Valley[b]) was a name used for a loosely defined region of colonial North America west of the Appalachian Mountains and south of Lake Erie.
 Control of the territory and the region's fur trade was disputed in the 17th century by the Iroquois, Huron, Algonquin, other Native American tribes, and France. New France claimed this area as part of the administrative district of La Louisiane. France and Britain fought the French and Indian War over this area in the mid-18th century as the North American front of their Seven Years' War (1756–1763). Following the British victory, France ceded its territory east of the Mississippi River to the British Empire in the 1763 Treaty of Paris.
 During the following decades, several minor frontier wars, including Pontiac's Rebellion and Lord Dunmore's War, were fought in the territory. In 1783, the Ohio Country became unorganized U.S. territory under the Treaty of Paris that officially ended the American Revolutionary War and became one of the first American frontier regions of the United States. Several of the original U.S. states had overlapping claims to portions of it, based on historical royal and colonial charters. The states' claims were largely extinguished after negotiations with the federal government by 1787, and it became part of the larger, organized Territory Northwest of the River Ohio. Most of the former areas north-west of the Ohio River were eventually organized as the state of Ohio, admitted to the Union in 1803.
 In the 17th century, the area north of the Ohio River was occupied by the Algonquian-speaking Shawnee and some Siouan language-speaking tribes, such as the Omaha and Ponca. Around 1660, during a conflict known as the Beaver Wars, the Iroquois and allied tribes seized control of the Ohio Country, driving out the Shawnee and Siouan peoples. Those tribes mostly moved further northwest and west, with several eventually settling west of the Mississippi River.[c] In the east, the Iroquois (or Haudenosaunee) conquered and absorbed the Erie (who also spoke an Iroquoian language) during this time. The Ohio Country, however, remained largely uninhabited for decades, used primarily as a hunting ground by the Iroquois peoples.
 In the 1720s, a number of Native American groups began to migrate into the Ohio Country from the east, driven by pressure from encroaching European colonists. By 1724, Delaware Indians had established the village of Kittanning on the Allegheny River in present-day western Pennsylvania. With them came those Shawnee who had historically expanded further to the east. Other eastern bands of the scattered Shawnee tribe began to return to the Ohio Country in the decades that followed. A number of Seneca and other Iroquois peoples also migrated to the Ohio Country, moving away from the Anglo-French rivalries and warfare south of Lake Ontario. The Seneca were the westernmost of the original Five Nations of the Iroquois centered in western New York. In 1722, the Tuscarora, an Iroquoian-speaking tribe from the Carolinas, completed a migration to the area and were allowed to settle near the lands of the Oneida. They were considered cousins to the Iroquois and became the sixth nation in the confederacy.
 In the late 1740s and the second half of the 18th century, the British and French angled for control of the territory.[1] The English intended to gain control of the area by sheer number of settlers on the ground. In 1749, The Crown through the government of the Colony of Virginia granted the Ohio Company a beneficial deal on this territory on the condition that it be settled by colonists from the Thirteen Colonies.[2]
 With the arrival of Europeans to America, both Great Britain and France had claimed the territory and sent fur traders into the area to do business with the Ohio Country Indians. The Iroquois League also claimed the region by right of conquest. The rivalry among the two European nations, the Iroquois nations, and the Ohio valley Indian tribes for control of the region played an important part in the French and Indian War that lasted from 1754 through 1760. Having initially remained neutral, eventually the Ohio Country Indians largely sided with the French who were more interested in hunting in the region and were not actively settling the area as was their British colonial rivals. Armed with supplies and guns from the French, the Indians launched raids against their enemies via the Kittanning Path east of the Alleghenies. After they destroyed Fort Granville in the summer of 1756, Pennsylvania's Proprietary Governor John Penn ordered Captain John Armstrong to destroy the Shawnee villages west of the Alleghenies, hoping to put an end to their raiding activities. Meanwhile, other British and colonial forces drove the French from Fort Duquesne. They built Fort Pitt at the confluence of the Allegheny and Monongahela rivers that form the Ohio River.[d] After being defeated by Britain, France ceded their claims to the entire Ohio Country in the 1763 Treaty of Paris. They had done so, however, without consulting their Native American allies who—in many cases—continued the fight against the colonial frontiersmen.
 Colonies such as Pennsylvania, Virginia, New York, and Connecticut claimed some of the westward lands as had been granted by their original charters. The area, however, was officially closed to European settlement by the Royal Proclamation of 1763, an attempt to preserve the western lands as territory exclusively set aside for use by Native American peoples. By enacting the treaty, the British Crown no longer recognized prior claims that the colonies made on this territory. On June 22, 1774, Parliament in England passed the Quebec Act, which annexed the region to the Province of Quebec. Colonists in the Thirteen Colonies considered this one of the Intolerable Acts that contributed to the call for American Revolution the following year, which began in earnest the following year, in 1775.
 Despite the Crown's actions limiting westward expansion, frontiersmen from the Virginia and Pennsylvania colonies had migrated across the Allegheny Mountains for over a decade since the Proclamation. This eventually led them into conflict with the Shawnee tribes that claimed the area as their hunting grounds. The Shawnee, who referred to the settlers as the 'long knives', viewed the colonists as competitors to their resources and a threat to their way of life. Because of this, the Shawnee and other Indian tribes of the Ohio Country, chose to side with the British against the American colonists during the American Revolutionary War. They hoped to expel the colonists permanently from their lands.
 In 1778, after several Patriot military victories in the region by an expeditionary force led by General George Rogers Clark, the Virginia legislature organized a nominal civil government over the area. They called this first official territory Illinois County, Virginia. It encompassed all of the lands lying west and north of the Ohio River to which Virginia had previously laid claim.
 The high water mark of the Native American struggle to retain control of the region was in 1782, when the Ohio Valley Indian Nations met with the British in a war council at Chalawgatha, a Shawnee village located along the Little Miami River, where they planned what was to become a successful rout of the Americans two weeks later at the Battle of Blue Licks. In 1783, following the Treaty of Paris in which America had gained its independence, Britain ceded its claims over the area to the new United States. The new federal government immediately opened this area to settlement by American pioneers, considering it unorganized territory. The Ohio Country quickly became one of the most desirable locations for Trans-Appalachian settlements, in particular among veterans of the Revolutionary War, who were often granted land in lieu of pay for their military service during the war.
 In the treaties of Fort Stanwix (late 1784) and Fort McIntosh (early 1785), the United States fixed boundaries between territory open to settlement and the native tribal lands. The Shawnee and other tribes, however, continued to resist American encroachment into their historic hunting grounds. This resistance eventually led to the Northwest Indian War.
 Considered highly desirable, the area was subject to the overlapping and conflicting territorial ambitions of several eastern states:
 After negotiation with the federal government, these states ceded their claims to the United States between 1780 and 1786. In July 1787, most of Ohio Country, the southern peninsula of what is today the state of Michigan, and eastern Illinois Country were incorporated as the Territory Northwest of the River Ohio. In 1803, most of what was formerly Ohio Country north and west of the Ohio River was admitted to the union as the state of Ohio.
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}40°30′N 82°30′W﻿ / ﻿40.5°N 82.5°W﻿ / 40.5; -82.5
"
Dysentery,https://en.wikipedia.org/wiki/Dysentery,"

 Dysentery (.mw-parser-output .IPA-label-small{font-size:85%}.mw-parser-output .references .IPA-label-small,.mw-parser-output .infobox .IPA-label-small,.mw-parser-output .navbox .IPA-label-small{font-size:100%}UK: /ˈdɪsəntəri/ DISS-ən-tər-ee,[7] US: /ˈdɪsəntɛri/ DISS-ən-terr-ee),[8] historically known as the bloody flux,[9] is a type of gastroenteritis that results in bloody diarrhea.[1][10] Other symptoms may include fever, abdominal pain, and a feeling of incomplete defecation.[2][6][11] Complications may include dehydration.[3]
 The cause of dysentery is usually the bacteria from genus Shigella, in which case it is known as shigellosis, or the amoeba Entamoeba histolytica; then it is called amoebiasis.[1] Other causes may include certain chemicals, other bacteria, other protozoa, or parasitic worms.[2] It may spread between people.[4] Risk factors include contamination of food and water with feces due to poor sanitation.[6] The underlying mechanism involves inflammation of the intestine, especially of the colon.[2]
 Efforts to prevent dysentery include hand washing and food safety measures while traveling in countries of high risk.[4] While the condition generally resolves on its own within a week, drinking sufficient fluids such as oral rehydration solution is important.[4] Antibiotics such as azithromycin may be used to treat cases associated with travelling in the developing world.[11] While medications used to decrease diarrhea such as loperamide are not recommended on their own, they may be used together with antibiotics.[11][4]
 Shigella results in about 165 million cases of diarrhea and 1.1 million deaths a year with nearly all cases in the developing world.[5] In areas with poor sanitation nearly half of cases of diarrhea are due to Entamoeba histolytica.[6] Entamoeba histolytica affects millions of people and results in more than 55,000 deaths a year.[12] It commonly occurs in less developed areas of Central and South America, Africa, and Asia.[12] Dysentery has been described at least since the time of Hippocrates.[13]
 The most common form of dysentery is bacillary dysentery, which is typically a mild sickness, causing symptoms normally consisting of mild abdominal pains and frequent passage of loose stools or diarrhea. Symptoms normally present themselves after 1–3 days, and are usually no longer present after a week. The frequency of urges to defecate, the large volume of liquid feces ejected, and the presence of blood, mucus, or pus depends on the pathogen causing the disease. Temporary lactose intolerance can occur, as well. In some occasions, severe abdominal cramps, fever, shock, and delirium can all be symptoms.[2][14][15][16]
 In extreme cases, people may pass more than one liter of fluid per hour. More often, individuals will complain of diarrhea with blood, accompanied by extreme abdominal pain, rectal pain and a low-grade fever. Rapid weight loss and muscle aches sometimes also accompany dysentery, while nausea and vomiting are rare.
 On rare occasions, the amoebic parasite will invade the body through the bloodstream and spread beyond the intestines. In such cases, it may more seriously infect other organs such as the brain, lungs, and most commonly the liver.[17]
 Dysentery results from bacterial or parasitic infections. Viruses do not generally cause the disease.[10] These pathogens typically reach the large intestine after entering orally, through ingestion of contaminated food or water, oral contact with contaminated objects or hands, and so on. Each specific pathogen has its own mechanism or pathogenesis, but in general, the result is damage to the intestinal linings, leading to the inflammatory immune responses. This can cause elevated physical temperature, painful spasms of the intestinal muscles (cramping), swelling due to fluid leaking from capillaries of the intestine (edema) and further tissue damage by the body's immune cells and the chemicals, called cytokines, which are released to fight the infection. The result can be impaired nutrient absorption, excessive water and mineral loss through the stools due to breakdown of the control mechanisms in the intestinal tissue that normally remove water from the stools, and in severe cases, the entry of pathogenic organisms into the bloodstream. Anemia may also arise due to the blood loss through diarrhea.[citation needed]
 Bacterial infections that cause bloody diarrhea are typically classified as being either invasive or toxogenic. Invasive species cause damage directly by invading into the mucosa. The toxogenic species do not invade, but cause cellular damage by secreting toxins, resulting in bloody diarrhea. This is also in contrast to toxins that cause watery diarrhea, which usually do not cause cellular damage, but rather they take over cellular machinery for a portion of life of the cell.[18]
 Definitions of dysentery can vary by region and by medical specialty. The U. S. Centers for Disease Control and Prevention (CDC) limits its definition to ""diarrhea with visible blood"".[19] Others define the term more broadly.[20] These differences in definition must be taken into account when defining mechanisms. For example, using the CDC definition requires that intestinal tissue be so severely damaged that blood vessels have ruptured, allowing visible quantities of blood to be lost with defecation. Other definitions require less specific damage.[citation needed]
 Amoebiasis, also known as amoebic dysentery, is caused by an infection from the amoeba Entamoeba histolytica,[21] which is found mainly in tropical areas.[22] Proper treatment of the underlying infection of amoebic dysentery is important; insufficiently treated amoebiasis can lie dormant for years and subsequently lead to severe, potentially fatal, complications.[citation needed]
 When amoebae inside the bowel of an infected person are ready to leave the body, they group together and form a shell that surrounds and protects them. This group of amoebae is known as a cyst, which is then passed out of the person's body in the feces and can survive outside the body. If hygiene standards are poor – for example, if the person does not dispose of the feces hygienically – then it can contaminate the surroundings, such as nearby food and water.
If another person then eats or drinks food or water that has been contaminated with feces containing the cyst, that person will also become infected with the amoebae. Amoebic dysentery is particularly common in parts of the world where human feces are used as fertilizer.
After entering the person's body through the mouth, the cyst travels down into the stomach. The amoebae inside the cyst are protected from the stomach's digestive acid. From the stomach, the cyst travels to the intestines, where it breaks open and releases the amoebae, causing the infection. The amoebae can burrow into the walls of the intestines and cause small abscesses and ulcers to form. The cycle then begins again.[citation needed]
 Dysentery may also be caused by shigellosis, an infection by bacteria of the genus Shigella, and is then known as bacillary dysentery (or Marlow syndrome). The term bacillary dysentery etymologically might seem to refer to any dysentery caused by any bacilliform bacteria, but its meaning is restricted by convention to Shigella dysentery.[citation needed]
 Some strains of Escherichia coli cause bloody diarrhea. The typical culprits are enterohemorrhagic Escherichia coli, of which O157:H7 is the best known. These types of E. coli also make Shiga toxin.[23]
 A diagnosis may be made by taking a history and doing a brief examination. Dysentery should not be confused with hematochezia, which is the passage of fresh blood through the anus, usually in or with stools.[24]
 The mouth, skin, and lips may appear dry due to dehydration. Lower abdominal tenderness may also be present.[17]
 Cultures of stool samples are examined to identify the organism causing dysentery. Usually, several samples must be obtained due to the number of amoebae, which changes daily.[17] Blood tests can be used to measure abnormalities in the levels of essential minerals and salts.[17]
 Efforts to prevent dysentery include hand washing and food safety measures while traveling in areas of high risk.[4]
 Although there is currently no vaccine that protects against Shigella infection, several are in development.[25][26] Vaccination may eventually become a part of the strategy to reduce the incidence and severity of diarrhea, particularly among children in low-resource settings. For example, Shigella is a longstanding World Health Organization (WHO) target for vaccine development, and sharp declines in age-specific diarrhea/dysentery attack rates for this pathogen indicate that natural immunity does develop following exposure; thus, vaccination to prevent this disease should be feasible. The development of vaccines against these types of infection has been hampered by technical constraints, insufficient support for coordination, and a lack of market forces for research and development. Most vaccine development efforts are taking place in the public sector or as research programs within biotechnology companies.[citation needed]
 Dysentery is managed by maintaining fluids using oral rehydration therapy.[4] If this treatment cannot be adequately maintained due to vomiting or the profuseness of diarrhea, hospital admission may be required for intravenous fluid replacement. In ideal situations, no antimicrobial therapy should be administered until microbiological microscopy and culture studies have established the specific infection involved. When laboratory services are not available, it may be necessary to administer a combination of drugs, including an amoebicidal drug to kill the parasite, and an antibiotic to treat any associated bacterial infection.[citation needed] Laudanum (Deodorized Tincture of Opium)] may be used for severe pain and to combat severe diarrhea.
 If shigellosis is suspected and it is not too severe, letting it run its course may be reasonable – usually less than a week. If the case is severe, antibiotics such as ciprofloxacin or TMP-SMX may be useful. However, many strains of Shigella are becoming resistant to common antibiotics, and effective medications are often in short supply in developing countries. If necessary, a doctor may have to reserve antibiotics for those at highest risk for death, including young children, people over 50, and anyone suffering from dehydration or malnutrition.[citation needed]
 Amoebic dysentery is often treated with two antimicrobial drugs such as metronidazole and paromomycin or iodoquinol.[27]
 With correct treatment, most cases of amoebic and bacterial dysentery subside within 10 days, and most individuals achieve a full recovery within two to four weeks after beginning proper treatment. If the disease is left untreated, the prognosis varies with the immune status of the individual patient and the severity of disease. Extreme dehydration can delay recovery and significantly raises the risk for serious complications including death.[28]
 Insufficient data exists, but Shigella is estimated to have caused the death of 34,000 children under the age of five in 2013, and 40,000 deaths in people over five years of age.[25] Amoebiasis infects over 50 million people each year, of whom 50,000 die (one per thousand).[29]
 Shigella evolved with the human expansion out of Africa 50,000 to 200,000 years ago.[30]
 The seed, leaves, and bark of the kapok tree have been used in traditional medicines by indigenous peoples of the rainforest regions in the Americas, west-central Africa, and Southeast Asia in the treatment of this disease.[31][32][33]
 In 1915, Australian bacteriologist Fannie Eleanor Williams was serving as a medic in Greece with the Australian Imperial Force, receiving casualties directly from Gallipoli. In Gallipoli, dysentery was severely affecting soldiers and causing significant loss of manpower. Williams carried out serological investigations into dysentery, co-authoring several groundbreaking papers with Sir Charles Martin, director of the Lister Institute.[34] The result of their work into dysentery was increased demand for specific diagnostics and curative sera.[35]
 Bacillus subtilis was marketed throughout America and Europe from 1946 as an immunostimulatory aid in the treatment of gut and urinary tract diseases such as rotavirus and Shigella,[36] but declined in popularity after the introduction of consumer antibiotics.
"
Battle of the Monongahela,https://en.wikipedia.org/wiki/Battle_of_the_Monongahela,"
 The Battle of the Monongahela (also known as the Battle of Braddock's Field and the Battle of the Wilderness) took place on July 9, 1755, at the beginning of the French and Indian War at Braddock's Field in present-day Braddock, Pennsylvania, 10 miles (16 km) east of Pittsburgh. A British force under General Edward Braddock, moving to take Fort Duquesne, was defeated by a force of French and Canadian troops under Captain Daniel Liénard de Beaujeu with its American Indian allies.
 The defeat marked the end of the Braddock Expedition, by which the British had hoped to capture Fort Duquesne and gain control of the strategic Ohio Country. Both Braddock and Beaujeu were killed in action during the battle. Braddock was mortally wounded in the fight and died during the retreat near present-day Uniontown, Pennsylvania. He specifically asked for George Washington, who accompanied him on the march, to oversee his burial. The remainder of the British column retreated south-eastwards. Fort Duquesne and the surrounding region remained in French hands until its capture in 1758.
 General Edward Braddock was dispatched to the Thirteen Colonies in the new position of Commander-in-Chief, North America, bringing with him two British Army regiments (the 44th and 48th) from Ireland.[7] He added to this by recruiting provincial troops in British America, swelling his forces to roughly 2,200 by the time he set out from Fort Cumberland, Maryland on 29 May.[8]  He was accompanied by Colonel George Washington, who had led the previous year's expedition to the area.[1]
 Braddock's expedition was part of a four-pronged attack on the French in North America. Braddock's orders were to launch an attack into the Ohio Country, disputed by Britain and France. Control of the area was dominated by Fort Duquesne on the forks of the Ohio River. Once it was in his possession, he was to proceed on to Fort Niagara, establishing British control over Ohio Country.
 Braddock soon encountered a number of difficulties. He was scornful of the need to recruit local Indians as scouts and left with only eight Mingo guides. He found that the road he was trying to use was slow and needed constant widening to move artillery and supply wagons along it, which only served to waste time and exhaust his supplies. Frustrated, he split his force in two, leading a flying column ahead, with a slower force following with the artillery and wagons.[8]
 The flying column of 1,300 crossed the Monongahela River on 9 July, within 10 miles (16 km) of their target, Fort Duquesne. Despite being very tired after weeks of crossing extremely hard terrain, many of the British regulars and provincial troops anticipated a relatively easy victory — or even for the French to abandon the fort upon their approach.[9] Fort Duquesne had been very lightly defended but had recently received significant reinforcements.[10] Claude-Pierre Pecaudy de Contrecœur, the commander of the fort, had around 1,600 French troupes de la Marine, Canadian Militia, and Indian allies. Concerned by the approach of the British, he dispatched Captain Daniel Liénard de Beaujeu with around 800 troops (108 Troupes de la Marine, 146 militia, and 600 Indians),[11] to check their advance.[12]
 The French and Indians arrived too late to set an ambush, as they had been delayed, and the British had made surprisingly speedy progress. They ran into the British advance guard, commanded by Lieutenant-Colonel Thomas Gage. Seeing the French and Indians in the trees, Gage ordered his men to open fire. Despite the limited range of their smooth-bore muskets, their opening volleys succeeded in killing Captain Beaujeu.
 The Indians took up positions to attack. They were fighting on traditional hunting grounds, with numerous trees and shrubbery separated by wide open spaces that enabled them to easily move about in concealment. The rolling platoon fire of the British initially caused roughly 100 French troops to flee back to the fort. Captain Dumas rallied the rest of the French troops. The Indians, which included warriors from the Ottawa, Ojibwa and Potawatomi tribes, used psychological warfare against the British by nailing the scalps of their dead comrades to trees. During the battle, Indians made a terrifying ""whoop"" sound that caused fear and panic to spread among British troops.[13]
 As they came under heavy fire, Gage's advance guard began taking casualties and withdrew. In the narrow confines of the road, they collided with the main body of Braddock's force, which had advanced rapidly when the shots were heard. Despite comfortably outnumbering their attackers, the British were immediately on the defensive. Most of the regulars were not accustomed to fighting in forest terrain; instead of scattering, they maintained tight formations that the Indians and French could easily target. Confusion reigned, and several British platoons fired at each other.[14] The entire column dissolved in disorder as the French and Indians enveloped them and continued to snipe at the British flanks from the woods on the sides of the road. At this time, French regulars began advancing along the road and began to push the British back. Braddock rode forward to try to rally his men, who had lost all sense of unit cohesion.
 Efforts were made to counterattack, but the inability of Braddock's troops to adapt the tactics of the French and Indians continued to interfere. The cannons were sent forth, but there was no space to effectively use them. Braddock had several horses shot out from under him, yet retained his composure, providing the only sign of order to his frightened troops.[14] Many of the provincial troops, lacking the training of British regulars to stand their ground, fled and sheltered behind trees, where they were mistaken for enemy fighters by the regulars and fired at.[14] The rearguard, made up of soldiers of the Virginia Regiment, managed to fight effectively from the trees — something they had learned in previous years of fighting Indians.[15] Despite the unfavorable conditions, the British began to stand firm and blast volleys at the French and Indians. Braddock believed that the French and Indians would eventually give way in the face of the discipline displayed by British troops. Despite lacking officers to command them, the often-makeshift platoons continued to hold their crude ranks.
 Finally, after three hours of intense combat, Braddock was shot in the lung, possibly by one of his own men, and effective resistance collapsed. He fell from his horse, badly wounded, and was carried back to safety by his men. As a result of Braddock's injuries, and without an order being given, the British began to withdraw. They did so largely with order, until they reached the Monongahela River, when they were attacked by Indians in close-quarter combat. Believing that they were trapped, the soldiers lost their discipline and ran in panic.[16]
 Washington, although he had no official position in the chain of command, was able to impose and maintain some order, and formed a rear guard, which allowed the remnants of the British force to disengage. By sunset, the surviving British forces were fleeing back down the road they had built, carrying their wounded. Behind them on the road, bodies were piled high. The Indians did not pursue the fleeing British, but instead set about scalping and looting the corpses of the wounded and dead, and drinking two hundred gallons of captured rum.[17] A number of British soldiers and female camp followers were captured in the battle. Some of the soldiers were spared, as were most of the women, but around a dozen soldiers were tortured and burned to death by the Indians that night, witnessed by American prisoner James Smith.[18]
 Daniel Boone, a famous American pioneer, explorer, woodsman, and frontiersman — and one of the first folk heroes of the United States — was among the soldiers involved in the battle. Boone served under Captain Hugh Waddell of North Carolina, whose militia unit was assigned in 1755 to serve under Braddock. Boone acted as a wagoner, along with his cousin Daniel Morgan, who would later be a key general in the American Revolution.[19] In the Battle of the Monongahela, Boone narrowly escaped death when the baggage wagons were attacked, by cutting his wagons and fleeing. Boone remained critical of Braddock's blunders for the rest of his life.[20][page needed] While on the campaign, Boone met John Finley, a packer who worked for George Croghan in the trans-Appalachian fur trade. Finley first interested Boone in the abundance of game and other natural wonders of the Ohio Valley. He would take Boone on his first hunting trip to Kentucky 12 years later.[21]
 Of the approximately 1,300 men Braddock led into battle,[1] 456 were killed outright and 422 were wounded. Commissioned officers were prime targets and suffered greatly: out of 86 officers, 26 were killed and 37 wounded. Of the 50 or so women that accompanied the British column as maids and cooks, only 4 returned with the British; about half were taken as captives. The French and Canadians reported only 23 killed, including the French commander, and 20 wounded.[22]: 235–236  Braddock died of his wounds on July 13, four days after the battle, and was buried on the road near Fort Necessity.
 Colonel Thomas Dunbar, with the reserves and rear supply units, took command when the survivors reached his position. Realizing there was no further likelihood of his force proceeding to capture Fort Duquesne, he decided to retreat.  He ordered the destruction of supplies and cannon before withdrawing, burning about 150 wagons on the spot. His forces retreated back toward Philadelphia. The French did not pursue, realizing that they did not have sufficient resources for an organized pursuit. Beaujeu was buried on July 12 at Fort Duquesne.[22] The battle was a devastating defeat, and has been characterized as one of the most disastrous in British colonial history.[23] It marked the end of the Braddock expedition, which many had believed contained overwhelming force, to seize the Ohio Country. It awakened many in London to the sheer scale of forces that would be needed to defeat the French and their Indian allies in North America.[24]
 The inability of the British to use skirmishers, and the vulnerability this caused for the main force, had a profound effect on British military thinking. Although Braddock had posted a company of flankers on each side, these troops were untrained to do anything but stand in line and fire platoon volleys, which were unsuited to such conditions. Learning from their mistakes the British made much better use of skirmishers, often equipped with rifles, who could protect the main body of troops from such devastating fire, both later in the French and Indian War and in the American Revolutionary War.
 Because of the speed with which the French and Indians launched their attack and enveloped the British column, the battle is often erroneously reported as an ambush by many who took part. In fact, the French had been unprepared for their contact with the British, whom they had blundered into. The speed of their response allowed them to quickly gain the upper hand, and brought about their victory. The French remained dominant in the Ohio Country for the next three years, and persuaded many previously neutral Indian tribes to enter the war on their side.[23] The French were eventually forced to abandon Fort Duquesne in 1758 by the approach of the Forbes Expedition.
 The debate on how Braddock, with professional soldiers, superior numbers, and artillery, could fail so miserably began soon after the battle and continues to this day. Some blamed Braddock, some blamed his officers, and some blamed the regular and provincial troops under their command. Washington, for his part, supported Braddock and found fault with the regulars.[25] Braddock's tactics are still debated. One school of thought holds that Braddock's reliance on time-honoured European methods, with men standing shoulder-to-shoulder in the open and firing mass volleys in unison, were not appropriate for frontier fighting and cost Braddock the battle. Skirmish tactics (""Indian style""), which American colonials had learned from frontier fighting, with men taking cover and firing individually, were superior in the American environment.[26]
 However, in some studies, the interpretation of ""Indian-style"" superiority has been argued to be a myth by several military historians. European regular armies already employed irregular forces of their own and had extensive theories of how to use and counter-guerilla warfare. Stephen Brumwell argues just the opposite by stating that contemporaries of Braddock, like John Forbes and Henry Bouquet, recognized that ""war in the forests of America was a very different business from war in Europe.""[27]
 Peter Russell argues it was Braddock's failure to rely on the time-honoured European methods that cost him the battle.[28] The British Army already had experience fighting against irregular forces in the Jacobite uprisings. Furthermore, Eastern European irregulars, such as Pandurs and hussars, had already made an impact on European warfare and theory by the 1740s. Braddock's failure, according to proponents of this theory, was caused by not adequately applying traditional military doctrine (particularly by not using distance), not his lack of use of frontier tactics.[29] Russell, in his study, shows that on several occasions before the battle, Braddock had successfully adhered to standard European tactics to counter ambushes and so had become nearly immune to earlier French and Canadian attacks.
 In 1930, on the 175th anniversary of the Battle of Braddock's Field, a statue of Colonel Washington was unveiled, and a commemorative postage stamp, modeled after that statue, was released for usage the same day.
"
Thomas Gage,https://en.wikipedia.org/wiki/Thomas_Gage,"


 General Thomas Gage (10 March 1718/19 – 2 April 1787) was a British Army officer and colonial administrator best known for his many years of service in North America, including serving as Commander-in-Chief, North America during the early days of the American Revolution.
 Being born into an aristocratic family in England, he entered the Army and saw action in the French and Indian War, where Gage served alongside his future opponent George Washington in the 1755 Battle of the Monongahela. After the successful Montreal campaign in 1760, he was named military governor of the region. During this time Gage did not distinguish himself militarily, but proved himself to be a competent administrator.
 From 1763 to 1775, he served as commander-in-chief of British forces in North America, overseeing Britain's response to the outbreak of Pontiac's War in 1763. In 1774, Gage was also appointed the military governor of the Province of Massachusetts Bay, with instructions to implement the Intolerable Acts, punishing Massachusetts for the Boston Tea Party. His attempts to seize the military stores of Patriot militias in April 1775 sparked the battles of Lexington and Concord, beginning the American War of Independence. After Britain's pyrrhic victory in the Battle of Bunker Hill in June, he was replaced by General William Howe in October 1775, and returned to England where he died in 1787.
 Thomas Gage was born on 10 March 1718/19 at Firle and christened 31 March 1719 at Westminster St James, Middlesex, England, son of Thomas Gage, 1st Viscount Gage, and Benedicta Maria Teresa Hall.[1] Firle Place, Firle, Sussex, is where the Gage family had been seated since the 15th century.[2] His father, Thomas Gage, 1st Viscount Gage, was a noted nobleman given titles in Ireland.[3] Thomas Gage (the elder) had three children, of whom Thomas was the second.[4] The first son, William Hall Gage, 2nd Viscount Gage, was born 6 January 1717/18 and christened 29 January 1717/18, also at Westminster St James.[5] In 1728 Gage began attending the prestigious Westminster School where he met such figures as John Burgoyne, Richard Howe, Francis Bernard, and George Germain.[6] Despite the family's long history of Catholicism, Viscount Gage had adopted the Anglican Church in 1715.[7] During his school years Thomas the younger became firmly attached to the latter church; he eventually developed a dislike for the Roman Catholic Church that became evident in later years.[8]
 After he left Westminster School in 1736, there are no records of Gage's activities[9] until he joined the British Army, eventually receiving a commission as ensign. His early duties consisted of recruiting in Yorkshire. In January 1741 he purchased a lieutenant's commission in the 1st Northampton Regiment, where he stayed until May 1742, when he transferred to Battereau's Regiment with the rank of captain-lieutenant. Gage received promotion to captain in 1743, and saw action in the War of the Austrian Succession with British forces in Flanders, where he served as aide-de-camp to the Earl of Albemarle in the Battle of Fontenoy.[10] He saw further service in the Second Jacobite Uprising, which culminated in the 1746 Battle of Culloden. From 1747 to 1748, Gage saw action under Albemarle in the Low Countries. In 1748 he purchased a major's commission and transferred to the 55th Foot Regiment (which was later renumbered to the 44th). The regiment was stationed in Ireland from 1748 to 1755; Gage was promoted to lieutenant colonel in March 1751.[11]
 During his early service years, he spent leisure time at White's Club, where he was a member, and occasionally travelled, going at least as far as Paris. He was a popular figure in the army and at the club, even though he neither liked alcohol nor gambled very much.[11] His friendships spanned class and ability. Charles Lee once wrote to Gage, ""I respected your understanding, lik'd your manners and perfectly ador'd the qualities of your heart.""[12] Gage also made some important political connections, forming relationships with important figures like Lord Barrington, the future Secretary at War, and Jeffery Amherst, a man roughly his age who rose to great heights in the French and Indian War.[13]
 In 1750, Gage became engaged to a ""lady of rank and fortune, whom he persuaded to yield her hand in an honourable way"".[14] The engagement was eventually broken, leaving Gage broken-hearted.[14] In 1753, both Gage and his father stood for seats in Parliament. Both lost in the April 1754 election, even though his father had been a Member of Parliament for some years prior. They both contested the results, but his father died soon after, and Gage withdrew his protest in early 1755, as his regiment was being sent to America following the outbreak of the French and Indian War.[15]
 In 1755 Gage's regiment was sent to North America as part of General Edward Braddock's expeditionary force, whose objective was the expulsion of French forces from the Ohio Country, territory disputed between French and British colonies where there had been military clashes in 1754. On this expedition Gage's regiment was in the vanguard of the troops when they came upon a company of French and First Nations people who were trying to set up an ambush. This skirmish began the Battle of the Monongahela, in which Braddock was mortally wounded, and George Washington distinguished himself for his courage under fire and his leadership in organising the retreat. The commander of the 44th, Colonel Sir Peter Halkett, was one of many officers killed in the battle and Gage, who temporarily took command of the regiment, was slightly wounded.[16] The regiment was decimated, and Captain Robert Orme (General Braddock's aide-de-camp) levelled charges that poor field tactics on the part of Gage had led to the defeat; as a result of his accusations Gage was denied permanent command of the 44th Regiment.[17] Gage and Washington maintained a somewhat friendly relationship for several years after the expedition, but distance and lack of frequent contact likely cooled the relationship.[18] By 1770, Washington was publicly condemning Gage's actions in asserting British authority in Massachusetts.[19]
 In the summer of 1756 Gage served as second-in-command of a failed expedition to resupply Fort Oswego, which fell to the French while the expedition was en route.[20] The following year, he was assigned to Captain-General John Campbell Loudoun in Halifax, Nova Scotia, where a planned expedition against Louisbourg turned back when confronted by a larger French fleet.[21]
 In December 1757, Gage proposed to Loudoun the creation of a regiment of light infantry that would be better suited to woodland warfare. Loudoun approved the plan before he was recalled that month, also recommending Gage to the king for promotion to full colonel. Gage spent the winter in New Jersey, recruiting for the newly raised 80th Regiment of Light-Armed Foot, the ""first definitely light-armed regiment in the British army.""[22] While it is uncertain exactly when he met the Kembles, his choice of the Brunswick area may well have been motivated by his interest in Margaret Kemble, a well-known beauty of the area, a descendant of the Schuyler family, and the granddaughter of New York Mayor Stephanus Van Cortlandt.[23][24] Recruiting and courtship were both successful. By February 1758 Gage was in Albany, preparing for that year's campaign, and he and Margaret were married on 8 December of that year.[25]
 The campaign for which Gage went to Albany culminated in the disastrous Battle of Carillon, in which 16,000 British forces were defeated by barely 4,000 French forces. Gage, whose regiment was in the British vanguard, was again wounded in that battle, in which the British suffered more than 2,000 casualties.[26][27] Gage, who had been brevetted a brigadier general for the 1758 campaign, received in 1759 a full promotion to the position, largely through the political manoeuvring of his brother, Lord Gage.[28][29]
 The new brigadier general was placed in command of the Albany post, serving under Major General Jeffery Amherst.[30] In 1759, shortly after capturing Ticonderoga without a fight, General Amherst learned of the death of General John Prideaux whose expedition had captured Fort Niagara. Amherst then ordered Gage to take Prideaux's place, and to take Fort de La Présentation (also known as Fort La Galette) at the mouth of the Oswegatchie River on Lake Ontario. When Amherst learned that the French had also abandoned Fort St. Frédéric, he sent a messenger after Gage with more explicit instructions to capture La Galette and then, if at all possible, to advance on Montreal.[31]
 When Gage arrived at Oswego, which had been captured in July by troops under Frederick Haldimand's command, he surveyed the situation, and decided that it was not prudent to move against La Galette. Expected reinforcements from Fort Duquesne had not arrived, the French military strength at La Galette was unknown, and its strength near Montreal was believed to be relatively high. Gage, believing an attack on La Galette would not gain any significant advantage, decided against action, and sent Amherst a message outlining his reasons.[32] Although there was no immediate censure from either Amherst or the government, Amherst was incensed at the failure, and Gage's troops were in the rear of Amherst's army in the 1760 expedition that resulted in Montreal's surrender.[33]
 After the French surrender, Amherst named Gage the military Governor of Montreal, a task Gage found somewhat thankless, because it involved the minute details of municipal governance along with the administration of the military occupation. He was also forced to deal with civil litigation, and manage trade with the First Nations in the Great Lakes region, where traders disputed territorial claims, and quarrelled with the First Nations.[34] Margaret came to stay with him in Montreal and that is where his first two children, Harry, the future 3rd Viscount Gage, and Maria Theresa, were born.[35] In 1761, he was promoted to major general, and in 1762, again with the assistance of his brother, was placed in command of the 22nd Regiment, which assured a command even in peacetime.[36]
 By all accounts, Gage appeared to be a fair administrator, respecting people's lives and property, although he had a healthy distrust of the landowning seigneurs and of the Roman Catholic clergy, who he viewed as intriguers for the French. When peace was announced following the 1763 Treaty of Paris, Gage began lobbying for another posting, as he was ""very much [tired] of this cursed Climate, and I must be bribed very high to stay here any longer"".[33] In October 1763 the good news arrived that he would act as commander-in-chief of North America while Amherst was on leave in Britain. He immediately left Montreal, and took over Amherst's command in New York on 17 November 1763. When he did so, he inherited the job of dealing with the outbreak of Pontiac's War.[37]
 Following the conquest of New France, Amherst, who had little respect for the First Nations, instituted policies that severely hampered Anglo-Indian relations, principally forbidding the sale of ammunition to them. Combined with widespread concern about British expansion into their territories, this prompted the tribes of the Ohio Country and the formerly French Pays d'en Haut to rise against the British.[38] In May 1763, under the leadership of the Odawa leader Pontiac, they launched a series of attacks on lightly garrisoned British frontier forts, successfully driving the British from some, threatening others, and also terrorising British settlers in those areas.[39]
 Hoping to end the conflict diplomatically, Gage ordered Colonel John Bradstreet and Colonel Henry Bouquet out on military expeditions and also ordered Sir William Johnson to engage in peace negotiations.[41] Johnson negotiated the Treaty of Fort Niagara in the summer of 1764 with some of the disaffected tribes, and Colonel Bouquet negotiated a cease-fire of sorts in October 1764, which resulted in another peace treaty finalised by Johnson in 1765. In 1765, the 42nd Regiment of Foot finally got through to Fort Cavendish, the last fort still in French hands. The conflict was not fully resolved until Pontiac himself travelled to Fort Ontario and signed a formal treaty with Johnson in July 1766.[42]
 When General Amherst left North America in 1763, it was on a leave of absence from his position as commander-in-chief. In 1764, Amherst announced that he had no intention of returning to North America, at which point Gage's appointment to that post was made permanent. (Amherst retained posts as governor of Virginia and colonel of the 60th Foot, positions he only gave up in 1768 when he was required to actually go to Virginia or give up the post.)[43] Intrigues of other high-ranking officers, especially Robert Monckton and his supporters, for his offices, continued throughout his tenure as commander-in-chief. Gage was promoted to lieutenant general in 1771.[44] In 1767 Gage ordered the arrest of Major Robert Rogers, the former leader of Rogers' Rangers who Gage had come to dislike and distrust during the war. The arrest was based on flimsy evidence that Rogers might have been engaging in a treasonous relationship with the French; he was acquitted in a 1768 court martial.[45]
 Gage spent most of his time as commander-in-chief, the most powerful office in British America, in and around New York City.[46] Although Gage was burdened by the administrative demands of managing a territory that spanned the entirety of North America east of the Mississippi River, the Gages clearly relished life in New York, actively participating in the social scene.[47] One way he did this was by joining the American Philosophical Society in 1768 through his election.[48] Although his position gave him the opportunity to make financial arrangements that might have lined the pockets of high-ranking officers at the expense of the military purse, there is little evidence that he engaged in any significant improper transactions. In addition to the handsome sum of £10 per day as commander-in-chief, he received a variety of other stipends, including his colonel's salary, given for leading his regiment. These funds made it possible to send all of the Gage children (at least six of whom survived to adulthood) to school in England.[49]
 If Gage did not dip his hand unnecessarily in the public till, he did engage in the relatively common practices of nepotism and political favouritism. In addition to securing advantageous positions for several people named Gage or Kemble, he also apparently assisted in the placement of some of his friends and political supporters, or their children.[50]
 During Gage's administration, political tensions rose throughout the American colonies. As a result, Gage began withdrawing troops from the frontier to fortify urban centres like New York City and Boston.[51] As the number of soldiers stationed in cities grew, the need to provide adequate food and housing for these troops became urgent. Parliament passed the Quartering Act of 1765, permitting British troops to be quartered in vacant houses, barns, and outbuildings, but not private residences.[52]
 Gage's thoughts on the reasons for colonial unrest played an important role in furthering the unrest. He at first believed that the popular unrest after the 1765 Stamp Act was primarily due to a small number of colonial elites, led by those in Boston. In 1768 he recommended the deployment of two regiments to occupy Boston, a move that further inflamed the city. Among the troops quartered in the city was the 29th Regiment of Foot, which had previously clashed with colonists in Quebec and New York, and had a reputation for poor discipline. This occupation eventually led to the Boston Massacre in 1770.[53] Later that year he wrote that ""America is a mere bully, from one end to the other, and the Bostonians by far the greatest bullies.""[54]
 Gage later came to change his opinion about the source of the unrest, believing that democracy was a significant threat. He saw the movement of colonists into the interior, beyond effective Crown control, and the development of the town meeting as a means of local governance as major elements of the threat, and wrote in 1772 that ""democracy is too prevalent in America"".[55] He believed that town meetings should be abolished and recommended that colonisation should be limited to the coastal areas where British rule could be enforced.[55]
 Gage returned to Britain in June 1773 with his family and thus missed the Boston Tea Party in December of that year.[56] The British Parliament reacted to the Tea Party with a series of punitive measures against Massachusetts known in the colonies as the Intolerable Acts.[57] Some of the terms of those acts, for example the option to remove political trials to England, originated with Gage,[55] and measures such as curbing the activities of town meetings and withholding representative government from the Ohio Country also show his influence.[58] With his military experience and relative youth (Massachusetts governor Thomas Hutchinson was then 62 years old and unpopular, and the equally unpopular lieutenant governor Andrew Oliver was 67 in 1773 and died in March 1774),[59] Gage, a popular figure on both sides of the Atlantic, was deemed the best man to handle the brewing crisis and enforce the Parliamentary acts.[60]
 In early 1774, he was appointed military governor of Massachusetts, replacing Hutchinson.[62] He arrived from Britain in early May, first stopping at Castle William on Castle Island in Boston Harbour. He then arrived in Boston on 13 May 1774, having been carried there by HMS Lively. His arrival was met with little pomp and circumstance, but was generally well received at first as Bostonians were happy to see Hutchinson go.[63] Local attitudes toward him rapidly deteriorated as he began implementing the various acts, including the Boston Port Act, which put many people out of work, and the Massachusetts Government Act, which formally rescinded the provincial assembly's right to nominate members of the Governor's Council, though it retained the elected General Court.[64][65] Gage dissolved the assembly in June 1774 after he discovered the Massachusetts representatives were sending delegates to the extralegal Continental Congress.[66] He called for new elections to be held as per the Massachusetts Government Act, but his authority was undermined by the representatives who refused to meet with the new, appointed Governor's Council.[66] He attempted to buy off political leaders in Massachusetts, notably Benjamin Church and Samuel Adams. With the former he was successful—Church secretly supplied him with intelligence on the activities of rebel leaders—but Adams and other rebel leaders were not moved.[53]
 In September 1774 Gage withdrew his garrisons from New York City, New Jersey, Philadelphia, Halifax and Newfoundland and brought all under his wing in Boston together with a large British naval presence under the control of Admiral Samuel Graves.[67] He also sought to strictly enforce army directives calling for the confiscation of war-making materials. In September 1774, he ordered a mission to remove provincial gunpowder from a magazine in what is now Somerville, Massachusetts.[68] This action, although successful, caused a huge popular reaction known as the Powder Alarm, resulting in the mobilization of thousands of provincial militiamen who marched towards Cambridge, Massachusetts.[69] Although the militia soon dispersed, the show of force on the part of the provincials had a lasting effect on Gage, and he subsequently grew more cautious in his actions.[70] The rapid response of the provincials was in large part due to Paul Revere and the Sons of Liberty. The Sons of Liberty kept careful watch over Gage's activities and successfully warned others of future actions before Gage could mobilise his British regulars to execute them.[71] A Committee of Safety was also tasked with sounding the alarm for local militias if Gage were spotted sending significant numbers of British troops outside of Boston.[72]
 Gage was criticised for allowing groups like the Sons of Liberty to exist. One of his officers, Lord Percy, remarked, ""The general's great lenity and moderation serve only to make them [the colonists] more daring and insolent.""[73] Gage himself wrote after the Powder Alarm, ""If force is to be used at length, it must be a considerable one, and foreign troops must be hired, for to begin with small numbers will encourage resistance, and not terrify; and will in the end cost more blood and treasure.""[74] Edmund Burke described Gage's conflicted relationship by saying in Parliament, ""An Englishman is the unfittest person on Earth to argue another Englishman into slavery.""[75]
 On 14 April 1775 Gage received orders from London to take decisive action against the Patriots.[76][72] Given intelligence that the militia had been stockpiling weapons at Concord, Massachusetts, he ordered detachments of regulars from the Boston garrison to march there on the night of 18 April to confiscate them.[77] A brief skirmish in Lexington scattered colonial militia forces gathered there, but in a later standoff in Concord, a portion of the British force was routed by a stronger colonial militia contingent. When the British left Concord following their search (which was largely unsuccessful, as the colonists, with advance warning of the action, had removed most of the supplies), arriving colonial militia engaged the British column in a running battle back to Charlestown. The Battles of Lexington and Concord resulted in 273 total casualties for the British[78] and 93 for the American rebels.[79][80]
 The British expedition to Lexington and Concord was supposed to have been a ""profound secret,"" but nevertheless Sons of Liberty leader Joseph Warren found out about it. He then dispatched Paul Revere and William Dawes to warn the colonists, which resulted in the Battle of Lexington and Concord, and starting the American War of Independence. Gage had told his plans to only his second-in-command and ""one other person."" There is evidence to suggest that the other person was his wife, Margaret Kemble Gage, who was an American, and that she may have passed on this information to Warren.[81][82]
 Following Lexington and Concord, thousands of colonial militia surrounded the city, beginning the Siege of Boston. At first, the rebels (led mainly by Massachusetts General Artemas Ward) faced some 4,000 British regulars, who were bottled up in the city.[83] British Admiral Samuel Graves commanded the fleet that continued to control the harbour. On 25 May, 4,500 reinforcements arrived in the city, along with three more generals: Major General William Howe and Brigadiers John Burgoyne and Henry Clinton.[84]
 On 14 June, Gage issued a proclamation, believed to have been written by Burgoyne but distributed in Gage's name, granting a general pardon to all who would demonstrate loyalty to the crown—with the notable exceptions of John Hancock and Samuel Adams.[85] Gage also worked with the newly arrived generals on a plan to break the grip of the besieging forces. They would use an amphibious assault to take control of the unoccupied Dorchester Heights, which would be followed up by an attack on the rebel camp at Roxbury. They would then seize the heights on the Charlestown peninsula, including Breed's Hill and Bunker Hill. This would allow the British to eventually take the colonial headquarters at Cambridge.[86] The colonists were warned of these plans, and seized the initiative. On the night of 16–17 June, they fortified Breed's Hill, threatening the British position in Boston. On 17 June 1775, British forces under General Howe seized the Charlestown Peninsula at the Battle of Bunker Hill.[87] It was a Pyrrhic victory; Britain won but suffered more than 1,000 casualties without significantly altering the state of the siege. Henry Clinton called it ""[a] dear bought victory, another such would have ruined us"",[88][89] while other officers noted that nothing had been gained in the victory.[90] Gage himself wrote the Secretary at War:
 On 25 June 1775, Gage wrote a dispatch to Great Britain, notifying Lord Dartmouth of the results of the battle on 17 June.[92] Three days after his report arrived in England, Dartmouth issued the order recalling Gage and replacing him with William Howe.[93] The rapidity of this action is likely attributable to the fact that people within the government were already arguing for Gage's removal, and the battle was just the final straw.[94] Gage received the order in Boston on 26 September, and set sail for England on 11 October.[95]
 The nature of Dartmouth's recall order did not actually strip Gage of his offices immediately. William Howe temporarily replaced him as commander of the forces in Boston, while General Guy Carleton was given command of the forces in Quebec.[96] Although King George wanted to reward his ""mild general"" for his service, Gage's sole reward after Lord George Germain (who succeeded Dartmouth as the Secretary of State for North America) formally gave his command to Howe in April 1776 was that he retained the governorship of Massachusetts.[97]
 On the Gages' return to England, the family eventually settled into a house on Portland Place in London. Although he was presumably given a friendly reception in his interview with a sympathetic King George,[96] the public and private writings about him and his fall from power were at times vicious. One correspondent wrote that Gage had ""run his race of glory ... let him alone to the hell of his own conscience and the infamy which must inevitably attend him!""[97] Others were kinder; New Hampshire Governor Benning Wentworth characterised him as ""a good and wise man ... surrounded by difficulties.""[97]
 Gage was briefly reactivated to duty in April 1781, when Amherst appointed him to mobilise troops for a possible French invasion.[98] The next year, Gage assumed command of the 17th Light Dragoons. He was promoted to full general on 20 November 1782,[33] and later transferred to command the 11th Dragoons.[99]
 As the war machinery was reduced in the mid-1780s, Gage's military activities declined. He supported the efforts of Loyalists to recover losses incurred when they were forced to leave the colonies, notably confirming the activities of Benjamin Church to further his widow's claims for compensation.[100] He received visitors at Portland Place and at Firle, including Frederick Haldimand and Thomas Hutchinson.[101] His health began to decline early in the 1780s.[100]
 Gage died at Portland Place on 2 April 1787, and was buried in the family plot at Firle.[102] His wife survived him by almost 37 years.[103] His son Henry inherited the family title upon the death of Gage's brother William, and became one of the wealthiest men in England.[104] His youngest son, William Hall Gage, became an admiral in the Royal Navy, and all three daughters married into well-known families.[105]
 Gagetown, New Brunswick was named in his honour; the Canadian Forces base CFB Gagetown consequently reflects his name.[citation needed]
 In 1792, the Lieutenant-Governor of Upper Canada, John Graves Simcoe, renamed the archipelago of islands in the mouth of the St. Lawrence River for the victorious generals of the Conquest of Canada: Wolfe Island, Amherst Island, Howe Island, Carleton Island, and Gage Island. The last is now known as Simcoe Island.[citation needed]
 In the 2015 miniseries Sons of Liberty, Gage is portrayed by Marton Csokas.[106]
"
Rear guard,https://en.wikipedia.org/wiki/Rear_guard,"A rearguard or rear security is a part of a military force that protects it from attack from the rear, either during an advance or withdrawal. The term can also be used to describe forces protecting lines, such as communication lines, behind an army.[1] Even more generally, a rearguard action may refer idiomatically to an attempt at preventing something though it is likely too late to be prevented; this idiomatic meaning may apply in either a military or non-military context.[2]
 The term rearguard (also rereward, rearward) comes from the Old French reregarde,[3] i.e. ""the guard which is behind"", originating with the medieval custom of dividing an army into three battles or wards; Van, Main (or Middle) and Rear.[4] The Rear Ward usually followed the other wards on the march and during a battle usually formed the rearmost of the three if deployed in column or the left-hand ward if deployed in line.
 The commonly accepted definition of a rearguard in military tactics was largely established in the battles of the late 19th century.  Before the mechanization of troop formations, most rearguard tactics originally contemplated the use of cavalry forces.[5]  This definition was later extended to highly mobile infantry as well as mechanized or armored forces.
 Narrowly defined, a rearguard is a covering detachment that protects the retreating main ground force element (main body), or column, and is charged with executing defensive or retrograde movements between the main body and the enemy to prevent the latter from attacking or interfering with the movement of the main body.[6][7]
 A more expansive definition of the rearguard arose during the large-scale struggles between nation-states during World War I and World War II. In this context, a rearguard can be a minor unit of regular or irregular troops that protect the withdrawal of larger numbers of personnel (military or civilian) during a retreat, by blocking, defending, delaying, or otherwise interfering with enemy forces in order to gain time for the remainder to regroup or reorganize. Rearguard actions may be undertaken in a number of ways: defensively, such as by defending strongpoints or tactically important terrain; or offensively, by pre-emptively assaulting with a spoiling attack an enemy that is preparing offensive operations.[8]
 Three examples of rearguard actions are:
 A World War I-era example is the rearguard action fought by small units of the Serbian Army to protect retreating Serbian troops, the royal family, and Serbian refugees from advancing forces of the Central Powers during their retreat through Albania and Montenegro in 1915–1916.[10][11][12] The nature of combat in rearguard actions involving combat between armies of nation-states is typically desperate and vicious, and rearguard troops may be called upon to incur heavy casualties or even to sacrifice all of their combat strength and personnel for the benefit of the withdrawing forces.[13][14]
 Fighting or mounting a rearguard action is also sometimes an idiomatic expression, outside any military context. That idiom refers to trying very hard to prevent a thing from happening even though it is probably too late.[2] An example of a famous rearguard action outside the military context is the effort by Roman emperor Julian around 362 A.D. to restore Paganism as the state religion instead of Christianity.[15] Sportswriters employ the idiom as well.[9][16]
"
John Dagworthy,https://en.wikipedia.org/wiki/John_Dagworthy,"John Dagworthy (1721–1784) was from Trenton, New Jersey, and had a military career that spanned three wars.  During King George's War  Dagworthy recruited a company of soldiers and was given command over them. During the French and Indian War he was a captain in command at Fort Cumberland. When George Washington of the Virginia militia, returned with the survivors of the Braddock Expedition Dagworthy became involved in a lengthy dispute with Washington, challenging him over matters of rank and seniority. During the American Revolutionary War he was a brigadier general commanding the Sussex County (Delaware) militia.[1]
 John Dagworthy, born in 1721, came from a prominent Royalist family in Trenton, New Jersey and was a devoted member of the Anglican Church of England.  He went on to serve in three different wars during his military career.[2]
 His first military service occurred when he served in King George's War against France. He  was commissioned a captain on August 23, 1746, and recruited a company of soldiers that later would join the regiment of Colonel Peter Schuyler.  His company participated in the proposed invasion of Canada up the Hudson valley, which never actually got underway. At this time the Council of New Jersey wrote to the Duke of Newcastle with the intention of getting a commission for Dagworthy.  To this end the young captain Dagworthy sailed to England in the interest of his own cause, where he finally got his wish and received a royal commission in His Majesty's service.[2]
 During the French and Indian War Captain John Dagworthy, was under the overall command of Colonel James Innes, the commander-in-chief of colonial forces at that time. Dagworthy was in command of the troops of the Maryland militia who built Fort Cumberland, a crude frontier fort constructed at the confluence of Wills Creek and the Potomac River in the autumn of 1754. The fort at this time marked the westernmost outpost of the British Empire in America. The fort was the starting point for General Braddock's failed expedition against the French at Fort Duquesne, located in present-day Pittsburgh, Pennsylvania. When Braddock was killed, George Washington, at the time a young officer of the Virginia militia, led the surviving troops back to Fort Cumberland.[3]
 In 1746, Dagworthy obtained  a royal commission for an intended expedition against the French in Canada that never materialized—he only functioned as a recruiter for the proposed expedition.[4] Dagworthy was under the notion that a royal commission, even though outdated, somehow rendered his rank superior to that of a colonial major. Dagworthy first encountered George Washington, who was then twenty-two, in 1755, on the Maryland frontier at Fort Cumberland. Although Washington possessed a superior rank, Dagworthy refused to recognize his command.[5]
 As a captain in the British Provincial Troops, Dagworthy disputed the authority of George Washington. At that time, Washington was a major in the Virginia militia, a rank that Dagworthy considered inferior to his own Royal commission as a captain.[6] The fort was built at the confluence of Wills Creek and the Potomac River, by troops of the Maryland militia under Dagworthy's command, in the fall of 1754.[7]
 Washington deeply resented the idea that a British captain considered himself to be of superior rank, especially since Dagworthy had obtained his royal commission for a price.[5] When Dagworthy challenged Washington's authority, Washington in a letter of 5 December 1755, to Virginia Lieutenant Governor Robert Dinwiddie,  his long-time friend, protested  Dagworthy's challenge and threatened to resign, and declared, ""I can never submit to the command of Captain Dagworthy.""[5] Dinwiddie sent a letter of appeal to Massachusetts Governor William Shirley. Washington wanted Shirley to have his regiment absorbed into the British army.[8] Dinwiddie then granted Washington permission to travel to Boston in February 1756 so he could discuss matters with Shirley in person. Shirley reasserted Washington's command over Dagworthy, but never granted Washington a royal commission.[5]
 Washington's attempts to obtain a royal commission continued to fail, and eventually distanced Washington from his superiors. The matter also increased the doubt in Washington's mind about American colonists' place in the British Empire.[5]
 When the American Revolutionary War broke out between the colonies and Great Britain, Dagworthy became a member of the Sussex County Committee of Safety and duly assigned a military post. In the Committee meeting records of September 16, 1775 he is referred to as a colonel in reference to his previous service, but in the January 1776 Committee records he is referred to as a brigadier general of Sussex County.[2]
 Dagworthy was not given a field command in the Continental Army, but was instead given command of the Sussex County militia.  Sussex county was at the southern end of Delaware and often considered of little regard by military and  Delaware officials, who were more concerned with British ships making their way through Delaware Bay and up the river to Philadelphia. The Delaware River and Bay also gave the Philadelphia access to the sea.[2]
 Dagworthy's sister, Mary Dagworthy, married Abraham Hunt, a Lieutenant Colonel in the New Jersey militia, famous for his role in befriending Hessian commander Johann Rall and keeping him preoccupied while George Washington mounted a successful surprise attack at the Battle of Trenton in New Jersey.[4]
 Dagworthy's remains are buried in the cemetery of Prince George's Chapel, located near Dagsboro.[9]
The town of Dagsboro, Delaware and the Dagsboro Hundred both take their names from General Dagworthy.[10][9]
"
Fort Cumberland (Maryland),https://en.wikipedia.org/wiki/Fort_Cumberland_(Maryland),"Fort Cumberland (built 1754) was an 18th-century frontier fort at the current site of Cumberland, Maryland, USA. It was an important military and economic center during the French and Indian War (1754–63) and figured significantly in the early career of George Washington.
 At the current location of the city of Cumberland, Maryland, a crude frontier fort was constructed at the confluence of Wills Creek and the Potomac River in fall 1754 by troops of the Maryland militia, under the command of Captain John Dagworthy, and under the overall command of Colonel James Innes, the commander-in-chief of colonial forces at that time.[1] A few years earlier, Thomas Cresap had established a trading post nearby, and hired Native Americans including the local chief Nemacolin to blaze a shorter path across the Allegheny Mountains to Redstone Creek on the Monongahela River, which became known as Nemacolin's Path.  Initially named Fort Mount Pleasant, it was renamed Fort Cumberland in 1755.[2] Ft Cumberland figured prominently in the French & Indian War in 1755, when it became a rally point for British forces under command of 
General Braddock. The wood palisade fort is now gone, and occupying the site is the existing Emmanuel Episcopal Church, but the old fort tunnels still remain underneath.
 This fort once marked the westernmost outpost of the British Empire in America, and was the jumping-off point for General Braddock's disastrous expedition against the French at Fort Duquesne in present-day Pittsburgh, Pennsylvania. When Braddock was killed, a young officer of Virginia militia, George Washington, led the troops back to Fort Cumberland. At the fort, Washington clashed with Captain Dagworthy over the issue of military rank and which colonial officer should be in command: Washington was a major in the Virginia militia, outranking the Maryland captain, but Dagworthy countered that because he also held a royal commission as a captain in the Provincial Troops, he automatically outranked any colonial militia officer.[3][4]
 Fort Cumberland was a supply center of basic provisions such as flour, salt, and lead for the Western Department of the Continental Army during the American Revolutionary War.[5] The fort also served as a prisoner-of-war camp for British prisoners from about 1778 until the end of the war.[5] 
 In May 1755, one of the British officers with General Braddock described the newly christened Fort Cumberland: ""[It] is situated within 200 yards of Will's Creek, on a hill and about 400 from the Potomack; its length from east to west is about 200 yards, and breadth 46 yards, and is built by logs driven into the ground, and about 12 feet above it."" Eleven days later, he reported that 100 carpenters were at work building a magazine and constructing a bridge over Will's Creek.[6]
 Eventually Fort Cumberland evolved into an earthen fort with twenty-foot-thick earthworks.[5]
 Diagrams and drawings of the fort exist in the British Museum. A scale model of the fort resides in the aforementioned church [1].[7]
 .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}39°39′04″N 78°45′55″W﻿ / ﻿39.6511°N 78.7653°W﻿ / 39.6511; -78.7653
"
"Commander-in-Chief, North America","https://en.wikipedia.org/wiki/Commander-in-Chief,_North_America","
 The office of Commander-in-Chief, North America was a military position of the British Army.  Established in 1755 in the early years of the Seven Years' War, holders of the post were generally responsible for land-based military personnel and activities in and around those parts of North America that Great Britain either controlled or contested.  The post continued to exist until 1775, when Lieutenant-General Thomas Gage, the last holder of the post, was replaced early in the American War of Independence. The post's responsibilities were then divided: Major-General William Howe became Commander-in-Chief, America, responsible for British troops from West Florida to Newfoundland, and General Guy Carleton became Commander-in-Chief, Quebec, responsible for the defence of the Province of Quebec.
 This division of responsibility persisted after American independence and the loss of East and West Florida in the Treaty of Paris (1783).  One officer was given the posting for Quebec, which later became the Commander-in-Chief of The Canadas when Quebec was divided into Upper and Lower Canada, while another officer was posted to Halifax with responsibility for military matters in the maritime provinces.
 Prior to 1784, the Bermuda Garrison (an independent company, detached from the 2nd Regiment of Foot, from 1701 to 1763; replaced by a company of the 9th Regiment of Foot detached from Florida along with a detachment from the Bahamas Independent Company until 1768; leaving only the militia until the American War of Independence, when part of the Royal Garrison Battalion had been stationed in Bermuda between 1778 and its disbandment there in 1784; the garrison was permanently re-established by the 47th Regiment of Foot and an invalid company of the Royal Artillery during the French Revolution, along with the establishment of what was to become the Royal Naval Dockyard, Bermuda) had been placed under the military Commander-in-Chief America, but was subsequently to become part of the Nova Scotia Command until the 1860s.
 During the American War of 1812, Lieutenant-General Sir George Prevost was Captain-General and Governor-in-Chief in and over the Provinces of Upper-Canada, Lower-Canada, Nova-Scotia, and New~Brunswick, and their several Dependencies, Vice-Admiral of the same, Lieutenant-General and Commander of all His Majesty’s Forces in the said Provinces of Lower Canada and Upper-Canada, Nova-Scotia and New-Brunswick, and their several Dependencies, and in the islands of Newfoundland, Prince Edward, Cape Breton and the Bermudas, &c. &c. &c.
 Beneath Prevost, the staff of the British Army in the Provinces of Nova-Scotia, New-Brunswick, and their Dependencies, including the Islands of Newfoundland, Cape Breton, Prince Edward and Bermuda were under the Command of Lieutenant-General Sir John Coape Sherbrooke. Below Sherbrooke, the Bermuda Garrison was under the immediate control of the Governor of Bermuda, Major-General George Horsford), New Brunswick was under Major-General George Stracey Smyth, Newfoundland was under Major-General Charles Campbell, and Cape Breton was under Major-General Hugh Swayne.[1]
 Following Canadian Confederation in 1867, these commanders were replaced in 1875 by the General Officer Commanding the Forces (Canada), whose post was succeeded in 1904 by the Chief of the General Staff Canada, a position which was established for a Canadian Army commander.
"
William Shirley,https://en.wikipedia.org/wiki/William_Shirley,"
 
 
 William Shirley (2 December 1694 – 24 March 1771) was a British Army officer and colonial administrator who served as the governor of the British American colonies of Massachusetts Bay and the Bahamas. He is best known for his role in organizing the successful capture of Louisbourg during King George's War, and for his role in managing military affairs during the French and Indian War. He spent most of his years in the colonial administration of British North America working to defeat New France, but his lack of formal military training led to political difficulties and his eventual downfall.
 Politically well connected, Shirley began his career in Massachusetts as advocate general in the admiralty court, and quickly became an opponent of Governor Jonathan Belcher. He joined with Belcher's other political enemies to bring about Belcher's recall, and was appointed Governor of Massachusetts Bay in Belcher's place. He successfully quieted political divisions within the province, and was able to bring about united action against New France when King George's War began in 1744. The successful capture of Louisbourg, which Shirley had a major role in organizing, was one of the high points of his administration.
 After King George's War Shirley became mired in disputes over funding and accounting for the war effort, and returned to England in 1749 to deal with political and legal matters arising from those disputes. He was then assigned to a commission established by Great Britain and France to determine the colonial borders in North America. His hard-line approach to these negotiations contributed to their failure, and he returned to Massachusetts in 1753.
 Military matters again dominated Shirley's remaining years in Massachusetts, with the French and Indian War beginning in 1754. Shirley led a military expedition to reinforce Fort Oswego in 1755, and became Commander-in-Chief, North America upon the death of General Edward Braddock.
 Notably, as commander-in-chief, Shirley knew George Washington, who served under his command. In 1756, Shirley wrote a letter advocating in favor of a promotion for George Washington, writing, ""I do therefore give it as my Opinion that Capt. Dagworthy who now acts under a Commission from the Governor of the Province of Maryland, and where there are no regular Troops join'd, can only take Rank as Provincial Captain and of Course is under the Command of all Provincial Field Officers, and in case it shall happen, that Colonel Washington and Capt. Dagworthy should join at Fort Cumberland. It is my Orders that Colonel Washington should take the Command.""[1]
 His difficulties in organizing expeditions in 1755 and 1756 were compounded by political disputes with New York politicians, and over military matters with Indian agent Sir William Johnson. These disagreements led to his recall in 1757 as both commander-in-chief and as governor. In his later years he served as governor of the Bahamas, before returning to Massachusetts, where he died.
 William Shirley, the son of William and Elizabeth Godman Shirley, was born on 2 December 1694 at Preston Manor in East Sussex, England.[2] He was educated at Pembroke College, Cambridge, and then read law at the Inner Temple in London.[3][4] In 1717 his grandfather died, leaving him Ote Hall in Wivelsfield and some funds, which he used to purchase a clerkship in London. About the same time, he married Frances Barker, with whom he had a large number of children.[5] He was called to the bar in 1720.[6] Although his inheritance had been substantial (about £10,000), he cultivated an expensive lifestyle, and suffered significant financial reverses in the depression of 1721. The financial demands of his large family (he and Frances had eight children by 1731) prompted him to seek an appointment in the North American colonies.[5] His family was connected by marriage to the Duke of Newcastle, who became an important patron and sponsor of Shirley's advancement, and to that of Arthur Onslow, the Speaker of the House of Commons.[7] Armed with letters of introduction from Newcastle and others (but no appointment), Shirley arrived in Boston, Massachusetts, in 1731.[8]
 Shirley was initially received with indifference by Massachusetts governor Jonathan Belcher, who refused him patronage positions that became available.[9]  In 1733 Shirley sought to secure from David Dunbar the commission as the crown surveyor general, but Dunbar eventually decided to retain the office.[10]  Influence from Newcastle eventually yielded Shirley a position as advocate general in the admiralty court. Belcher resisted further entreaties from Newcastle to promote Shirley, and Shirley began using his position to actively prosecute Belcher supporters whose illegal logging activities came under his jurisdiction.[9]
 Shirley also made common cause with Samuel Waldo, a wealthy merchant and major landowner in the province eastern district (present-day Maine) where Belcher's lax enforcement of timber-cutting laws was harming his business with the Royal Navy.[9] In 1736 Shirley sent his wife to London to lobby on his behalf against Belcher.[11]  Waldo also eventually went to London; the combination of Shirley's connection to Newcastle and Waldo's money soon made inroads in the colonial administration.[12][13] When these were joined by discontented New Hampshire interests (Belcher was also governor of New Hampshire), a full-scale offensive was launched in the late 1730s to unseat Belcher.[14] This included at least one forged letter on the part of Belcher opponents in an attempt to discredit the governor, which Shirley denounced.[15] By 1738 Newcastle was in a dominant position in not just the colonial administration, but also in the British government as an opponent of Prime Minister Sir Robert Walpole, and he actively encouraged Belcher's opponents.[16]
 In 1739 the Privy Council reprimanded Belcher, voted to separate the Massachusetts and New Hampshire governorships, and began debating the idea of replacing the governor.[17] The exact reasons for Belcher's dismissal have been a recurring subject of scholarly interest, due to the many colonial, imperial, and political factors at play.[18] Two principal themes within these analyses are Belcher's acquisition of many local enemies, and the idea that good imperial governance in London eventually required his replacement.[19] Before the issues of 1739 most of the efforts to unseat Belcher had failed: Belcher himself noted in that year that ""the warr I am ingag'd in is carrying on in much the same manner as for 9 years past.""[20] Historian Stephen Foster further notes that someone as powerful as Newcastle was at the time generally had much weightier issues to deal with than arbitrating colonial politics. In this instance, however, imperial and colonial considerations coincided over the need for Massachusetts to provide a significant number of troops for Newcastle's proposed West Indies expedition in the War of Jenkins' Ear.[21] In April 1740 Newcastle in effect offered Shirley the opportunity to prove, in the light of Belcher's political difficulties, that he could more effectively raise troops than the governor could.[22] Shirley consequently engaged in recruiting, principally outside Massachusetts (where Belcher refused his offers of assistance, understanding what was going on), and deluged Newcastle with documentation of his successes while Belcher was preoccupied with a banking crisis.[23][24] Newcastle handed the issue off to Martin Bladen, secretary to the Board of Trade and a known Belcher opponent. The Board of Trade then apparently decided, based on the weight of the extant evidence, that Belcher needed to be replaced.[25] In April 1741 the Privy Council approved William Shirley's commission as governor of Massachusetts, and Benning Wentworth's commission as governor of New Hampshire was issued the following June.[26][27]
 When Shirley assumed the governorship of Massachusetts in August 1741, he was immediately confronted with a currency crisis. The province had been suffering for many years with inflation caused by issuance of increasing quantities of paper currency. Late in Belcher's tenure, competing banking proposals had been made in a bid to address the issue, and a popular proposal for a bank secured by real estate had been enacted.[28] This bank (the controversy over it having contributed to Belcher's recall) had been dissolved by an act of parliament, and Shirley had to negotiate the dissolution of the bank's assets and reclamation of the notes it had issued. In this process, which occupied the rest of 1741, Shirley deftly navigated legislation through the provincial assembly that provided a schedule for redeeming the bank's currency without causing the bank's principal owners to collapse under a deluge of redemptions.[29]
 With rising tensions Shirley acted to strengthen the military defenses of the colony. He created a series of provincial companies along the frontier. These included Burke's Rangers and Gorham's Rangers which became the model for Shirley's more famous creation Roger's Rangers.
 Britain captured Acadia from France in Queen Anne's War (1702–1713), but the Treaty of Utrecht left Cape Breton Island in French hands, and did not clearly demarcate a boundary between New France and the British colonies on the Atlantic coast.[30] To protect the crucial passageway of the Saint Lawrence River into the heart of New France, France built a strong fortress at Louisbourg on the Atlantic coast of Cape Breton Island.[31]
 When Shirley took office, relations between France and Britain were strained, and there was a possibility that Britain would be drawn into the War of the Austrian Succession, which had started on the European mainland in 1740.[32] Shirley was able to finesse his restrictions on the production of paper currency to achieve an updating of the province's defences, and in 1742 requested permission from the Board of Trade for the printing of additional currency should war break out.[33] This permission was granted in 1743, along with a warning that war with France was likely.[34] France declared war against Britain in March 1744, and forces from Louisbourg raided the British fishing port of Canso on the northern end of mainland Nova Scotia before its residents were aware they were at war.[35] French privateers also began preying on British and colonial vessels. British colonial governors along the coast, including Shirley, sent colonial guard ships and authorized their own privateers in response, neutralizing the French activity.[36]
 Canso was used by New England fishermen, and as such its fall was of interest to Massachusetts. Shirley had, prior to its capture, received a request for assistance from the lieutenant governor of Nova Scotia, Paul Mascarene, for support in the defence of Annapolis Royal.  In response to the fall of Canso and a second, urgent request from Mascarene, Shirley promptly despatched two companies of volunteers to Annapolis Royal.[37]  The timely arrival of these troops in early July broke up a siege.[38]
 John Bradstreet, who had been captured at Canso and held prisoner at Louisbourg, returned to New England in a prisoner exchange, and gave a detailed report to Shirley that emphasised the weaknesses of the French fort.[39] William Vaughn, who owned several businesses in Maine that were vulnerable to raids from New France, toured New England advocating an expedition to capture Louisbourg.[40] Shirley and other leaders in New England and New York sent letters to colonial authorities in London seeking support for such an expedition, citing the vulnerable conditions at Louisbourg.[41] Vaughn and Bradstreet wanted to attack Louisbourg that winter with an all-colonial force. Shirley doubted the practicality of that plan, but in January 1745 submitted it to the provincial assembly (General Court), which declined to support the plan, but did request that Britain undertake an attack on Louisbourg.[42]
 Vaughn continued to advocate for a quick all-American expedition, enlisting the support of fishing captains, merchants and 200 ""principal gentlemen"" of Boston.[43] Shirley called the General Court into session to discuss the matter once more, and the proposal was submitted to a committee chaired by William Pepperrell. The committee reported favourably on the plan, and it was approved by a single vote when several opponents were absent from the chamber.[44]
 Shirley appointed a reluctant William Pepperrell to command the expedition, William Vaughn was appointed colonel, but without a command position, and John Bradstreet was appointed as a military advisor to Pepperrell.[45] Shirley requested support for the expedition from Peter Warren, commodore of the Royal Navy squadron in the West Indies, but Warren declined due to the strenuous objections of his captains. This news arrived in Boston just as the expedition was preparing to leave.[46]
 Despite the absence of support from the Royal Navy, the New England expedition set out in March 1745 for Louisbourg.[47] More than 4,000 men on more than 90 transports (mainly fishing boats and coastal traders), escorted by six colonial guard ships, descended on Canso, where the expedition waited for the ice to clear from Gabarus Bay, the site just south of Louisbourg that had been chosen for the troop landing.[48] Starting on 22 April the expedition was joined by four Royal Navy warships under the command of Commodore Warren,[49] who received orders (issued in January, but not received until after his previous refusal) to assist the expedition.[50]
 The provincial forces began landing at Gabarus Bay on 30 April, and laid siege to the fortress while the British ships blockaded the harbour.[51] The Americans began suffering battle losses, while the British naval officers, who had a low opinion of American soldiers, grew increasingly critical of the American efforts. Warren tried to exert control over the provincial troops, but Pepperrell resisted him.[52] Louisbourg surrendered on 17 June. The Americans lost 180 men in combat, to disease or at sea during the siege, while the Royal Navy ships did not fire on the fortress, and lost just one sailor.[53] As the victors settled into occupation of Louisbourg, friction grew between the Americans and the British. The terms of surrender guaranteed the French in all of their possessions; there was no plunder for the American troops.[54] On the other hand, the Royal Navy had captured several rich French prizes, and British sailors on shore leave bragged to the Americans about how rich they were going to be from their shares.[55]
 The American troops had signed up to capture Louisbourg, and expected to go home after siege ended.[56] The British government, who had believed that the provincial troops were incapable of capturing Louisbourg on their own, had made no plans to send British troops to take over occupation of the fortress.[57] When it became evident that British troops would not be relieving the provincials until after winter had passed, Governor Shirley travelled to Louisbourg to raise the morale of the troops.[58] His first speech to the troops had little effect, and some troops were close to mutiny.[59] In a second speech Shirley promised to send home more troops immediately, and provide higher pay and better supplies for those who stayed until spring.[60] Honors from the British government were sparse; Pepperrell was made a baronet, he and Shirley were made colonels in the British Army with the right to raise their own regiments, and Warren was promoted to rear admiral.[61]
 Shirley had engaged in the Louisbourg campaign primarily as a way to ensure British interests in the Atlantic fisheries. The victory, however, made him expand his vision to encompass the possibility of capturing all of New France. After capturing the French fort he wrote to Newcastle, proposing a series of expeditions to gain control of all of North America as far west as the Mississippi River, starting with one that would go up the Saint Lawrence from Louisbourg.[62] Upon his return to Boston, Shirley began making preparations for such an expedition.[63] In May 1746 he received plans for London outlining an attempt on Quebec using Royal Navy and provincial forces, while a second expedition was to attack Fort Saint-Frédéric on Lake Champlain.[64] Shirley stepped up recruiting in Massachusetts and asked neighboring governors to contribute men and resources to the effort.[65] Expected support from Britain never arrived, however, and the 1746 expeditions were called off.[66]
 While waiting for definite word from London of plans for 1747 Shirley beefed up the province's western defenses, and in the spring of 1747 he began sending supplies to the Hudson River valley in anticipation of a move toward Fort Saint-Frédéric.[67] Word then arrived from Newcastle that the British establishment would not support any expeditions against New France. The drop in military spending that resulted had negative consequences on the Massachusetts economy, harming Shirley's popularity.[68]
 Shirley personally profited from the supply activities surrounding the Louisbourg expedition. In 1746 he used the funds to purchase an estate in Roxbury, on which he built an elaborate mansion, now known as the Shirley-Eustis House. Before the building was complete his wife died of a fever in August 1746; she was interred in King's Chapel.[69]
 While Governor Shirley was at Louisbourg trouble had been brewing between the Royal Navy and the people of Boston.[70] The Navy had long sought to press Americans into service on its ships.[71] Impressment was a long-standing practice in Britain, but its application in America was resisted by the colonists. In 1702 Fort William on Castle Island had fired on a Royal Navy ship as it tried to leave Boston Harbour with six recently impressed men aboard.[72] As a result of American complaints (reinforced by British merchants), Parliament in 1708 banned impressment in the American colonies.[73] Navy leaders argued that the American exemption from impressment had been in force only during Queen Anne's War, which ended in 1713. In practice, Royal Navy captains had to apply to colonial governors for a license to press men.[74] In late November 1745 a fight between a press gang and some sailors staying in a boarding house in Boston left two of the sailors with fatal injuries. Two members of the press gang were charged with murder and convicted, but were released when the indictment was found invalid.[75]
 Two years later Commodore Charles Knowles, who served as Governor of Louisbourg after its capture, had a large number of seamen from Boston harbour impressed for service in his squadron. A mob of more than 300 men seized three naval officers and a deputy sheriff and beat the sheriff. The mob then went to Governor Shirley's house, demanding the release of the men impressed by Knowles. Shirley tried to call out the militia, but they did not respond. Shirley did succeed in getting the naval officers into his house, and the mob eventually left. Later in the day Shirley went to the Town House to meet the people. The mob, now consisting of several thousand people, attacked the Town House, breaking many windows in the building. Shirley spoke to the mob and promised to present their demands to Commodore Knowles. The mob left, intending to find a Royal Navy ship to burn.[76]
 After Shirley had returned home that afternoon, the mob, which had seized another naval officer and several petty officers, returned to his house. Shirley ordered a number of armed men who were protecting his house to fire at the mob, but William Pepperrell was able to stop Shirley's men from firing and to persuade the mob to leave. In the meantime, Commodore Knowles threatened to bombard Boston with his squadron. It was only after the Massachusetts Council adopted resolutions in support of the demands of the mob that the situation became quieter in Boston. The mob eventually released its hostages and Knowles released the impressed seamen.[77]
 Another issue of contention was compensation to the American colonies by Britain for the costs of the expedition against Louisbourg and the long occupation by American troops until the British Army finally took over.[78] This presented Shirley with a problem, because the expedition's leaders, including his former ally Samuel Waldo, grossly inflated their claimed costs. Waldo used Shirley's unwillingness to openly act against him to begin his own efforts to topple the governor.[79] Shirley was only able to forestall this effort by promising the colonial administration that he would achieve financial stability in the province by retiring its paper currency.[80]
 The British government was also slow in responding to requests for compensation.[78] While waiting for a response, the question of how to use any compensation was debated in provincial newspapers and pamphlets. Some, such as Samuel Adams (father of the famous American Revolution leader), advocated placing the money in London banks to serve as backing for the paper currency issued by the colonies. Others, including William Douglass and Thomas Hutchinson, speaker of the General Court, favoured using the compensation to redeem the paper currency and give Massachusetts a hard currency.[81] In 1748 the Treaty of Aix-la-Chapelle returned Louibourg to France, with Massachusetts still awaiting compensation for its seizure.[82]
 In the meantime, Governor Shirley had been trying to finance a campaign to capture Fort St. Frédéric (at present-day Crown Point, New York), for which he issued more paper money. The campaign was abandoned when the colonies failed to support it, but the resulting inflation helped turn supporters of Shirley against him.[83] The loss of Louisbourg increase public dissatisfaction with Shirley, who was seen as complicit in British scheming against the American colonies. Even William Pepperrell joined the large number of citizens calling for Shirley's removal.[84] Samuel Adams edited and Gamaliel Rogers and Daniel Fowle published The Independent Advertiser, which regularly criticised the British government and Shirley's administration. The paper published several of Shirley's letters to officials in Britain that were critical of Americans, and regularly called for the governor's removal.[85] William Douglass, a prominent physician in Boston, wrote a series of pamphlets (published by Rogers and Fowle) attacking Shirley, Commodore Knowles, and the whole conduct of the campaign for Louisbourg and its occupation. Both Shirley and Knowles sued Douglass for libel, but lost their cases in court.[86]
 Shirley's conflict with Samuel Waldo over expenses eventually reached a high pitch: Shirley had successfully attached some of Waldo's assets in legal action, which Waldo had countered with further legal action.  Shirley appealed these actions to London, and was granted permission (received in August 1749) to travel to London to deal with the matter.[87] He sailed for Britain in September 1749, just before the long promised compensation reached Boston.[84] Under legislation shepherded by Thomas Hutchinson, the specie delivered was used to retire the paper currency.[88] While Shirley was abroad, Hutchinson, Andrew Oliver, and others served as his surrogates,[89] and he carefully instructed Lieutenant Governor Spencer Phips to not give his enemies opportunities to manoeuvre in his absence.[90]
 In London Shirley met with Newcastle and the colonial secretary, the Duke of Bedford to discuss colonial matters and his situation. Newcastle ordered the military books of Waldo and Pepperrell to be scrutinized; the analysis was found to confirm Shirley's position.  Shirley's accounts were also examined, and were found to be ""made up with great exaction"", ""more conformable to his Majesty's orders ... than any other of the colonies.""[91]
 Shirley also communicated political concerns over which he and New York governor George Clinton had commiserated.  While he was in London, word arrived that Clinton wanted to leave his post. Shirley applied to Newcastle for the job, but was turned down.[92] Newcastle may have been upset with Shirley, who had accepted an unexpected offer from Bedford to participate in a commission established to delineate the boundaries between the British and French territories in North America. The commission was set to meet in Paris, and Shirley saw it as an opportunity to advance his expansionist views. Newcastle and Bedford were at the time involved in a political struggle, and Newcastle was unhappy that Shirley had accepted Bedford's offer. Shirley was able to convince Newcastle that his experience and position would be of use in the negotiations.[93]
 The commission met in Paris, and Shirley was accompanied by William Mildmay, a somewhat mild-mannered merchant, as cocommissioner. Shirley adopted a hard line in the negotiations, arguing in a technical and lawyerly fashion for an expansive reading of British territory; he claimed all territory east of a line from the Kennebec River north to the Saint Lawrence River, while the French claimed all of that area except peninsular Nova Scotia. Shirley's approach served to harden negotiating positions and bogged the commission's work down in minutiae. When Mildmay complained of this to London, Bedford rebuked Shirley for spending too much effort on trivialities.[94] While the negotiations dragged on, both French and British operatives were actively expanding their interests in the Ohio River valley, raising tensions.[95]
 In 1751 Shirley incited a minor scandal when he married Julie, the young daughter of his Paris landlord.[96] He was recalled to London after Mildmay complained that Shirley was taking actions without consulting him. Shirley returned to London convinced that the French needed to be driven from North America.[97] Mildmay attempted to continue the negotiations, believing that he could overcome Shirley's previous obstructionism, but the negotiations ended in failure.[98]
 Shirley renewed his application for the New York governorship, but was snubbed by Newcastle, who was upset over Shirley's marriage.[98] He was instead ordered to return to Massachusetts. This he did, leaving his wife in London. It is unclear if they ever saw each other again: biographer John Schutz believes they did not, but family lore is that they were reunited after Shirley left the Massachusetts governorship.[99]
 The opposition in Massachusetts to Shirley had died down while he was in England and Paris.[100] Shirley soon had to deal with the increasing conflict on the frontier with French Canada. Tensions had been increasing, particularly in the Ohio Country, where British and French traders were coming into conflict. When (false) rumors reached Boston in 1754 of French military activity on the province's northern frontier (Maine), Shirley was quick to organize an expedition to the Kennebec River to bolster the area's defenses. This expedition erected Fort Halifax in what is now Winslow, Maine. News of hostilities in the Ohio Country brought further urgency to that matter, as well as attendance at a planned conference of colonies at Albany, New York.[101]  Because of the urgency, and the support of politically powerful Maine landowners, Shirley's relationship with the provincial assembly was relatively good.[102] Shirley instructed the provincial representatives to the Albany Conference to seek a colonial union,[103] but the provincial assembly (along with those of other provinces) rejected the conference's proposals.[104]
 Shirley was approached by Nova Scotia Governor Charles Lawrence for assistance in dealing with the French threat on that province's frontiers, suggesting that they collaborate on military actions there.[105] Shirley and Lawrence believed their proposed expedition would also require assistance from Britain, and sent letters requesting the same. At the same time they ramped up preparations in anticipation of the request being approved.[106] Shirley was also ordered to activate and recruit for his regiment, which was to serve in Braddock's force. Because he could not leave the province he sent one of his sons to New York to recruit troops there; Massachusetts men were being drafted for the Nova Scotia expedition.[107] He furthermore revived the idea of an expedition against Fort St. Frédéric, although he limited the first year's action to the establishment of a fort at the southern end of Lake George, and sought to draw the leaders of neighboring colonies to assist in the operation.[108] He mollified New York's acting governor James DeLancey, who was generally hostile to Massachusetts interests, by proposing that the expedition be led by New York's Indian commissioner, Colonel William Johnson. Johnson was at first reluctant, but Shirley was able to convince him to take the command.[109]
 Since the French and Indian War had become a matter of imperial concern, two British Army regiments under General Edward Braddock were sent to America. In written exchanges, Braddock announced his intention to use this force against Fort Duquesne in the Ohio Country, while Shirley unsuccessfully lobbied him to instead target Fort Niagara.[110] At a conference of governors and military leaders in April 1755 Shirley favorably impressed Braddock. Braddock declared that Duquesne would be his target, but he authorized Shirley to take his regiment and that of Sir William Pepperrell to Fort Niagara, and confirmed Johnson's command of the Lake George campaign. Braddock's instructions only gave Shirley the vaguest command over Johnson, which was to later become a source of trouble. The two northern expeditions were to be made without logistical assistance from the regular army.[111]
 From the conference Shirley traveled to New York City, where he negotiated with merchants for supplying his expedition. The frosty relationship he had with Governor DeLancey continued; the DeLanceys objected to what they saw as Massachusetts interference in their provincial affairs.[112] When Shirley moved to prevent New York agent Oliver DeLancey from recruiting in Connecticut, it caused a stink and threatened to derail planning for the New York expeditions. Shirley then created a breach with Johnson by attempting to siphon troops from Johnson's command to increase his own force for the Fort Niagara expedition. The antagonism was furthered by the fact that the two expeditions were competing for supplies from the same sources, and was also exacerbated by ongoing border disputes between the provinces.[113]
 When Shirley and Johnson met in July 1755 before their respective expeditions set off, tension between the two men continued, and Johnson delayed decisions on assigning Indian auxiliaries to Shirley's campaign, observing that much of the expedition was traveling through friendly Iroquois territory, where they would not yet be needed. Shirley took offense at this as an act of insubordination.[114] Believing he outranked Johnson, Shirley next sought to bypass the Indian agent and negotiate directly with the tribes for recruits, but Johnson and his subordinates actively opposed the move.[115] The Iroquois also objected to the presence of Shirley's recruiting agent, Colonel John Lydius, with whom they had outstanding issues over past land transactions.[116] The situation was not made easier by the fact that neither Johnson nor Shirley had ever commanded expeditions of the size and scope proposed.[117]
 Shirley's expedition reached Fort Oswego in mid-August. The trek up the Mohawk River had been slowed by low water, and it was being incompetently supplied, resulting in a shortage of provisions.[118] Shirley learned en route that General Braddock had died in the aftermath of 13 July Battle of the Monongahela, which also claimed the life of Shirley's son William.[119] As a result, he became temporary commander-in-chief of North American forces.[120] His expedition then became bogged down at Fort Oswego by the need to improve its defenses, and the ongoing provisioning crisis. In a council on 18 September it was decided to proceed with plans to reach Fort Niagara, but one week later the decision was reversed. Shirley returned to Albany, preoccupied with the need to manage the entire British war effort on the continent.[121]
 William Johnson's expedition fared little better than Shirley's. He reached the southern end of Lake George, where his forces had an inconclusive encounter with French forces on 8 September,[122] and began work on Fort William Henry.[123] Rumors of French movements brought a flurry of activity in November, but when the opposition failed to materialize, much of Johnson's force abandoned the camp to return home. Shirley had to pressure New England's governors to assign militia to the new posting for the winter.[124]
 In Nova Scotia, Governor Lawrence had easily captured Fort Beauséjour,[125] and had then embarked on what has since become known as the Great Expulsion, the forcible removal of more than 12,000 Acadians from Nova Scotia. When some of the ships carrying the Acadians entered Boston Harbor in early December 1755, Shirley ordered that they not disembark. For three winter months, until March 1756, the Acadians remained on the ships, where half died from the cold weather and malnutrition.[126]
 During the winter of 1755–56 Shirley's feud with Johnson continued. Johnson, who was being advised by Thomas Pownall, continued to assert his exclusive authority over interactions with Indians, and renewed complaints about Shirley's interference in recruiting for the 1755 campaign. In one letter Johnson wrote that Shirley had ""become my inveterate enemy"" who would do everything he could ""to blast if he can my character.""[127] Johnson made common cause with the DeLanceys (to whom he was related by marriage) in their dislike of Shirley.[128][129] They all fed unflattering reports to the new New York governor, Sir Charles Hardy, who forwarded them on to London. Shirley was unaware of this looming threat to his authority.[130]
 As commander-in-chief, Shirley made a grandiose proposal for the 1756 campaign season in November 1755, continuing the routes of attack begun in 1755 and adding an expedition to Quebec via the Kennebec River.[131] However, the complaints against him had reached the Duke of Newcastle, who felt he needed someone less embroiled in controversies with other leaders in charge of military matters in North America.[132] British leaders had also received intercepted letters destined for France that some believed might have been written by Shirley, in part because he married a Frenchwoman. Thomas Pownall traveled to London in early 1756 and further denounced Shirley to the colonial administration. Shirley did not learn of these matters until April 1756, by which time the British leadership had already decided to replace him as commander-in-chief.[133]
 While waiting for his replacement (Lord Loudoun) Shirley made every effort to advance supplies and reinforcements to the Fort Oswego garrison, which had been on short rations for the winter, and whose supply line had been interrupted by the Battle of Fort Bull in March 1756.[134] He continued to mobilize resources and personnel for at least the Oswego and Lake George efforts, but his authority was waning due to widespread knowledge of his replacement. Military affairs continued to deteriorate on the New York frontier before Loudoun finally arrived in July 1756;[135] Fort Oswego fell to the French on 10 August.[136]
 Although Shirley had been removed as commander-in-chief, he retained the Massachusetts governorship. He expected to lose even that post not long after his return to Boston in August.[136] However, no replacement had yet been named, and Loudoun saw either Shirley's interference or ineffectiveness in all that was wrong on the New York frontier.  He also raised detailed questions about Shirley's war-related expenditures, which he (and later historians) concluded was poorly disguised patronage spending.[137][138] Loudoun and Shirley argued over many issues, including Shirley's continuance of military preparations after January 1756, when Loudoun's commission was issued. Shirley pointed out that British leadership could hardly expect preparations to cease in the interval between Loudoun's commission and his arrival to take command.  While he waited for a replacement to be announced, Shirley took depositions, gathered evidence to support his version of affairs, and worked to close his financial affairs down.[139] (Loudoun was of the opinion that Shirley delayed his departure intentionally as a political maneuver.)[140] He sailed for England in October 1756.[141] Shirley would be formally replaced by Thomas Pownall in 1757.[142]
 Upon his arrival in London, Shirley was received by Newcastle and other sympathetic figures, but Newcastle had been forced from office by the poor showing in the war, and Shirley's ongoing disagreements with Loudoun meant he was unlikely to receive another North American posting.  Newcastle then withdrew his support from Shirley over a hearing into matters disputed between Loudoun and Shirley. Shirley was not granted formal hearings on other aspects of his conduct, and managed to convince Newcastle to overlook the matter of his ""muddled"" accounts.[140][143] His prospects brightened when Loudoun and Pownall were both damaged by the continued poor military performance in North America (notably the debacle of the Siege of Fort William Henry in August 1757, which resulted in Loudoun's recall). These failures served to rehabilitate Shirley and bring him back into Newcastle's good graces.[144]
 In late 1758 Shirley was commissioned as Governor of the Bahamas.[145] This was followed in early 1759 with a promotion to lieutenant general. After a lengthy passage, Shirley arrived in the Bahamas on 31 December, when his ship was wrecked on a reef in the islands. He eventually arrived without incident or injury at Nassau and assumed the reins of power.[146][Notes 1]  His rule was quiet; dealing with smugglers in the islands was the major issue demanding the governor's attention. In part to combat illicit trade he lobbied the London government that Nassau be established as a free port. Although he was influential in this regard, Nassau did not receive this status until after he left office.[147] He also oversaw renovations to the governor's mansion, and promoted the construction of churches with funding from the Society for the Propagation of the Gospel.[148] In 1765, after his wife's death, he took his children to England so that they could be properly cared for.[149] He returned to the islands, where he had to deal with protests of the recently enacted Stamp Act. When he proposed the use of the stamps on official documents to the local assembly, the reaction in opposition was so visceral that Shirley dissolved the body.[150] By the time the next assembly met, the Stamp Act had been repealed.[151]
 His health failing, Shirley was eventually replaced as governor by his son Thomas, who was appointed in November 1767 and arrived to assume office the following year. Shirley sailed for Boston, where he took up residence in his old house in Roxbury with his daughter and son-in-law. There he died on 24 March 1771. After a state funeral, he was interred in King's Chapel.[152]
 Shirley married twice and had two sons and three daughters.
 His elder son Thomas (later Sir Thomas) became a major general in the British army, was created a baronet in 1786 as ""Shirley baronets, of Oat Hall (1786)"" Sussex, and served, after his posting to the Bahamas, as Governor of Dominica and Governor of the Leeward Islands. Sir Thomas died in 1800.[153] The Baronetcy became extinct after the death of Sir Thomas Shirley's son Sir William Warden Shirley, 2nd Baronet (1772–1815).
 Shirley's other son, William Jr., was killed in 1755 at the Battle of the Monongahela whilst serving with Edward Braddock.[154] Shirley's eldest daughter Anne married John Erving, a member of the Massachusetts Governor's Council; their daughter Anne married Duncan Stewart of Ardsheal, Chief of the Clan Stewart of Appin. Shirley's youngest daughter Maria Catherina married John Erving Jnr.
 Shirley built a family home in Roxbury between 1747 and 1751. He sold it to his daughter and son-in-law, Eliakim Hutchinson, in 1763. It later came into the hands of William Eustis, Governor of Massachusetts in the 19th century. Now known as the Shirley-Eustis House, it still stands at 33 Shirley Street. It has largely been restored and is a museum open to the public.[155]
 The town of Shirley, Massachusetts, was founded during his term as Massachusetts governor. The Winthrop, Massachusetts geographical feature Shirley Point and the former feature Shirley Gut are named for him. Shirley helped to establish a cod fishery in Winthrop in 1753.[156] Shirley is also the namesake of Shirley Street in Halifax, Nova Scotia (which is parallel to Pepperell Street, named after William Pepperell).[157]
"
"John Campbell, 4th Earl of Loudoun","https://en.wikipedia.org/wiki/John_Campbell,_4th_Earl_of_Loudoun","

 General John Campbell, 4th Earl of Loudoun (5 May 1705 – 27 April 1782) was a British Army officer and peer.
 Born in Scotland two years before the creation of Great Britain in which his father, Hugh Campbell, 3rd Earl of Loudoun, was a significant figure, Campbell inherited his father's estates and peerages in 1731 and became Lord Loudoun. 
 He raised a Highland regiment of infantry, Loudon's Highlanders, which took part in the Jacobite Rising of 1745 on the side of the Hanoverian government. The regiment consisted of twelve companies, with Loudoun as colonel and John Campbell (later 5th Duke of Argyll) as lieutenant-colonel. The regiment served in several different parts of Scotland. Three of the twelve companies, raised in the south, were captured at the Battle of Prestonpans. 
 Eight companies, under the personal command of Lord Loudoun, were stationed in Inverness. Loudoun set out in February 1746 with that portion of his regiment and several of the Independent Companies in an attempt to capture the Jacobite pretender, Charles Edward Stuart. The expedition was met by a ruse de guerre by only four Jacobites, which suggested a large force was protecting Stuart, and it returned without engagement.  
 That was later publicised as the Rout of Moy. Loudoun then fell back to join the Duke of Cumberland's army up and gave up the town of Inverness to the rebels. After the Battle of Culloden, Loudoun led his mixed force of regulars, militia and Highlanders in mopping-up operations against the remaining rebels.
 In 1756, Loudoun was sent to North America as Commander-in-Chief and Governor General of Virginia, where he was unpopular with many of the colonial leaders. When he learned that some merchants were still trading with the French while he was trying to fight a war against them, he temporarily closed all American ports. Despite his unpopularity the County of Loudoun, formed from Fairfax in 1757, was named in his honour.[1] 
 As Commander-in-Chief during the Seven Years' War, called the French and Indian War in the Thirteen Colonies, he planned an expedition to seize Louisbourg from the French in 1757 but he called it off when intelligence, possibly including a French deception, indicated that the French forces there were too strong for him to defeat. While Loudoun was thus engaged in Canada, French forces captured Fort William Henry from the British, and he was replaced by James Abercrombie and returned to London. Francis Parkman, a 19th-century historian of the Seven Years' War, rates Loudun's martial conduct of the affair poorly.
 Many historians debate whether he played a fundamental part in the Seven Years' War. Arguably, he was an influential figure as he embarked on reforms for the army such as replacing the ordinary musket with the flintlock musket for greater accuracy. He made improvements by embarking on a road improvement programme and recognised the need to supply the army as he replaced the traditional supply line with army wagons. His focus was centralising the system of supplies and had built storehouses in Halifax and Albany and recognised the importance of waterways as a means of transport. Most notably, he integrated regular troops with local militias, and the irregulars were to fight a different kind of war from the linear European style of warfare in which the British had previously been trained.[citation needed]
 Benjamin Franklin provides several first-hand anecdotes of Loudon's North American days in his Autobiography, none of which is complimentary.[2] The following are excerpts:
 In 1762, he was sent to Portugal to counter the Spanish invasion of Portugal as second in command, and he became overall commander in 1763. Despite being unable to prevent the loss of Almeida, the British forces soon launched a counter-attack that drove the invaders back across the border.
 Back in Scotland, Loudon in 1763 was made Governor of Edinburgh Castle,[3] a post that he held for the rest of his life.
 In 1770, he was promoted to full general.[4]
 Loudoun's interest in horticulture led to his estate being renowned for its landscaping. He collected willow species in particular from around the globe.
 On 23 January 1773, the town of Loudon, New Hampshire, was incorporated and named in his honor.[5] Loudonville, New York, was also named after him as well as the unincorporated town of Loudon, Massachusetts, which was renamed to Otis upon its incorporation.
 Campbell remained a bachelor and on his death in 1782 was succeeded as earl by his cousin, James Mure-Campbell.
"
Forbes Expedition,https://en.wikipedia.org/wiki/Forbes_Expedition,"The Forbes Expedition was a British military campaign to capture Fort Duquesne, led by Brigadier-General John Forbes in 1758, during the French and Indian War. While advancing to the fort, the expedition built the Forbes Road. The Treaty of Easton served to cause a loss of Native American support for the French, resulting in the French destroying the fort before the expedition could arrive on November 24.
 Similar to the unsuccessful Braddock Expedition early in the war, the strategic objective was the capture of Fort Duquesne, a French fort that had been constructed at the confluence of the Allegheny River and the Monongahela River in 1754. The site is now located in Pittsburgh's Golden Triangle in the downtown area (Or The Point)
 Forbes commanded about 6,000 men, including a contingent of Virginians led by George Washington. Forbes, very ill, did not keep up with the advance of his army, but entrusted it to his second in command, Lieutenant Colonel Henry Bouquet, a Swiss mercenary officer commanding a battalion of the Royal American Regiment.
 The expedition methodically constructed Forbes Road across what is now the southern part of Pennsylvania's Appalachian Plateau region, staging from Carlisle and exploiting the climb up via one of the few southern gaps of the Allegheny through the Allegheny Front, into the disputed territory of the Ohio Country, which was then a largely-depopulated Amerindian tributary territory of the Iroquois Confederation.[a] The well-organized expedition was in contrast to a similar expedition led by Edward Braddock in 1755, which ended in the disastrous Battle of the Monongahela.
 Working for most of the summer on the construction of the road and on periodic fortified supply depots, the expedition did not come within striking distance of Fort Duquesne until September 1758. In mid-September, a reconnaissance force was soundly defeated in the Battle of Fort Duquesne when its leader, Major James Grant, attempted to capture the fort instead of gathering information alone. The French had their supply line from Montreal cut by other British actions and so attacked one of the expedition's forward outposts, Fort Ligonier, in an attempt to drive off the British or to acquire further supplies, but they were repulsed during the Battle of Fort Ligonier.
 The Treaty of Easton concluded on October 26, 1758, caused the remnants[b] of the Lenape (Delaware), Mingo, and Shawnee tribes in the Ohio Valley to abandon the French and set up the conditions that ultimately forced them to move westward once again. The collapse of Native American support made it impossible for the French to hold Fort Duquesne and the Ohio Valley. When the expedition neared to within a few miles of Fort Duquesne in mid-November, the French abandoned and blew up the fort. Three units of scouts led by Captain Hugh Waddell entered the smoking remnants of the fort under the orders of Colonel George Washington on November 24. General Forbes, who was ill with dysentery for much of the expedition, only briefly visited the ruins. He was returned to Philadelphia in a litter and died not long afterward. The collapse of Indian support and subsequent withdrawal of the French from the Ohio Country helped contribute to the ""year of wonders"" the string of British ""miraculous"" victories also known by the Latin phrase Annus Mirabilis.
"
John Forbes (British Army officer),https://en.wikipedia.org/wiki/John_Forbes_(British_Army_officer),"

 Brigadier-General John Forbes (5 September 1707 – 11 March 1759) was a British Army officer. During the French and Indian War, he commanded the 1758 Forbes Expedition which occupied the French outpost of Fort Duquesne. This required the construction of a military trail known as the Forbes Road, which became an important route for settlement of the Western United States. Forbes died in Philadelphia and was buried in the chancel of Christ Church, Philadelphia.
 John Forbes was born in Dunfermline on 5 September 1707, youngest child of Colonel John Forbes, 1658–1707, who died several months before his birth, and Elizabeth Graham, daughter of an Edinburgh merchant. His uncle, Duncan Forbes (1644-1704), was a prominent supporter of William of Orange and obtained his brother John an army commission.[1]
 In 1701, Colonel Forbes purchased Pittencrieff Park, near Dunfermline, and it was here John grew up. He had five elder sisters, of whom little is known, and two older brothers; Arthur (1703-1757), who inherited the estate, and Hugh (1704-1760), who became a lawyer.[2] All three of the Forbes brothers had problems with money; John borrowed large sums to pursue his military career, while Arthur ruined himself expanding Pittencrief, which was sold after his death.[3]
 Forbes married Anna Donald and had a daughter Anna.
 The Forbes family were prominent civic leaders in Inverness, who supported the succession of George I in 1714 and were political allies of the Campbell Dukes of Argyll. John's cousin Duncan Forbes of Culloden, (1685-1747), became senior Scottish legal officer in 1737 and played a key role in suppressing the 1745 Jacobite Rising. These personal connections were essential; like many contemporaries, John was also a Freemason, another of the informal networks needed for a successful public career in this period.[4]
 Educated locally in Dunfermline, Forbes is thought to have studied medicine at Edinburgh University.[5] In September 1729, he was appointed surgeon in the Royal Scots Greys, [a] then based in Scotland. He remained with the regiment for the next 28 years but gave up his medical post in 1735, when he was commissioned as a cornet.[6]
 The long period of peace from 1713 to 1739 meant limited opportunities for promotion, while the commission purchase system worked against those like Forbes with little money.[b] It was not until April 1742 he was promoted lieutenant, shortly before the regiment was posted to the Austrian Netherlands to fight in the War of the Austrian Succession.[7] Forbes became aide-de-camp to James Campbell of Lawers, (1690-1745), colonel of the Scots Greys and commander of the Allied cavalry. He fought at Dettingen in June 1743 and in September 1744, purchased a commission as captain.[8]
 At Fontenoy in May 1745, Campbell sent Forbes with instructions to Brigadier General Ingoldsby on the Allied right, ordering him to attack a French redoubt whose fire was impeding their advance. One shot badly wounded Sir James, who was carried from the field; Forbes stayed with him after the Allies retreated and was taken prisoner. He was soon exchanged but Campbell died a few days later and Forbes' letters display genuine grief at his death.[9]
 Many viewed Fontenoy as a 'defeat snatched from the jaws of victory', and in the recriminations that followed, Ingoldsby was court-martialled. The grounds were his failure to comply with three separate orders to attack the French position, given by Campbell, Cumberland, the Allied commander, and Ligonier. Ingoldsby claimed he received conflicting instructions and attempted to blame Forbes, who testified at his trial. While he had some justification, any confusion was caused by Cumberland, not Forbes; in any case, this was not considered an adequate excuse and Ingoldsby was forced to resign.[10]
 For reasons that are unclear, unlike many who fought at Fontenoy Forbes did not benefit from Cumberland's patronage, although he was appointed aide to the elderly Earl of Stair, (1673-1747), Campbell's successor as colonel of the Scots Greys. Some units were sent to Britain in October to put down the 1745 Rising, but not the cavalry, since transporting horses by sea was considered impractical during the winter months. Stair was military commander of Southern Britain and Forbes may have served there for a short period but contrary to legend, he was not present at Culloden.[11] Instead, he returned to Flanders in December 1745 as Deputy Quartermaster-General and was a major when the war ended in 1748.[12]
 Forbes spent the next few years on garrison duty in different parts of Britain and in November 1750, purchased a commission as Lieutenant-Colonel of the Scots Greys.[13] To do so, he borrowed £5,000 but with limited opportunities for further advancement, his debts became an increasingly large problem.[14]
 The Treaty of Aix-la-Chapelle set up a commission to resolve territorial disputes between British and French colonies in North America, including the Ohio Country, French Acadia and Nova Scotia. Neither side was willing to make concessions, which led to the 1754-1763 French and Indian War; in 1755, an expedition under General Braddock to capture Fort Duquesne ended in a disastrous defeat.[15]
 When the global conflict known as the Seven Years' War began in 1756, James Campbell's nephew, the Earl of Loudoun, was appointed Commander-in-Chief, North America and Governor General of Virginia. In early 1757, Forbes was in Southern England, training a 'light company' of the Scots Greys for attacks on the French coast.[16] In March, he was promoted colonel of the 17th Foot, part of a force of 5,400 sent to Nova Scotia for an attempt on Louisbourg.[17]
 Following the failure of the 1757 Louisbourg Expedition, Forbes was promoted Brigadier general in December 1757 and given command of another attack on Fort Duquesne. His force contained 1,400 regulars, 400 from the Royal American Regiment, commanded by the experienced Swiss mercenary, Lt-Colonel Henry Bouquet, along with 1,000 Scots who made up Montgomerie's Highlanders. There were also 5,000 provincial militia from Virginia and Pennsylvania, commanded by George Washington, who had carried messages to Fort Le Boeuf in 1753, and accompanied Braddock in 1755.[18] 
 Forbes decided to build a new road from the Pennsylvania frontier, since it required fewer river crossings than that used by Braddock, which followed a trail cut in 1752 by the Virginia-based Ohio Company. The decision led to protests from his Virginian officers, many of whom were investors in the company, including two of Washington's brothers.[19]
 As a compromise, Forbes agreed to improve Braddock's original road, but use the route through Pennsylvania. A base was established at Carlisle, Pennsylvania, and a trail cut through the Allegheny Mountains, which became the Forbes Road. Already severely ill, Forbes had to be carried in a litter and relied heavily on Bouquet, who commanded the advance guard. Construction of the road and bases such as Fort Ligonier was supervised by Lt-Colonel John St Clair, who proved to be incompetent and required Forbes to do much of the work, despite his poor health.[20]
 A less appreciated aspect of Forbes' leadership was in building relationships with local Native Americans, who previously refused to co-operate with the British. These efforts were bolstered by the capture of Fort Frontenac in August, increasing British prestige, while the loss of French traders severely impacted the local economy.[21] This methodical approach was jeopardised by the Battle of Fort Duquesne, on 15 September 1758, when a column under Major James Grant advanced too far ahead of the main body and suffered over 300 casualties. Forbes decided to suspend operations but on 26 October, 13 Ohio Valley tribes signed the Treaty of Easton with Pennsylvania and New Jersey.[22]
 After the loss of their local allies, the French abandoned Fort Duquesne and the British took possession on 25 November. Forbes ordered the construction of Fort Pitt, named after British Secretary of State Pitt the Elder. He also established a settlement between the rivers, the site of modern Pittsburgh.[23] His health rapidly declined during the campaign; described as a 'wasting disease', this is thought to have been stomach cancer, combined with severe dysentery.
 On 3 December 1758, he left Colonel Hugh Mercer in command and returned to Philadelphia, where he died on 11 March 1759 and was buried with full military honours. His final correspondence with Lord Amherst, the new commander in North America, included the recommendation he make his relationship with Native Americans a priority and 'not to think lightly of them or their friendship.'[24] [c]
 Forbes Field, now demolished but which formerly served as the home field for the Pittsburgh Pirates, Pittsburgh Steelers and the Pitt Panthers football team, was named after John Forbes.  Forbes Field was America's first all-steel and concrete baseball stadium. Forbes Avenue, one of the city of Pittsburgh's principal boulevards, runs from the Monongahela River in Downtown Pittsburgh to Frick Park and the start of the eastern suburbs.  It is named in his honor and roughly follows his colonial road.[25] John Forbes Lane in Kanpur running from British India Corporation to Huddard High School is named in his memory.
 
"
Brevet (military),https://en.wikipedia.org/wiki/Brevet_(military),"In the military, a brevet (/brəˈvɛt/ or /ˈbrɛvɪt/ ⓘ) is a warrant that gives a commissioned officer a higher rank title as a reward, but which may not confer the authority and privileges of real rank.[1] 
 The promotion would be noted in the officer's title (for example, ""Bvt. Maj. Gen. Joshua L. Chamberlain"" or ""Bvt. Col. Arthur MacArthur"").
 It is not to be confused with a Brevet d'état-major in Francophone European military circles, where it is an award, nor should it be confused with temporary commissions.
 In France, brevet is a word with a very broad meaning, which includes every document giving a capacity to a person. For instance, the various military speciality courses, such as military parachutism, are ended by the award of a brevet.
 The more important brevet in the French military is the one of the École de guerre (lit. ""school of war""), the French Staff College. Between  1870 and 1940, an officier breveté was a graduate of the École supérieure de guerre.[2] Nowadays, while many officers still attend the école de guerre, they do not use the term officier breveté.
 The French military does not use brevets to give officers a higher standing, employing temporary commissions instead.[3] As an example, Charles de Gaulle was promoted ""provisional brigadier general"" (général de brigade à titre provisoire) in 1940 when he was commander of an armoured division.
 In the Prussian and German army and navy, it was possible to bestow a Charakter rank on officers that was in many respects similar to a brevet rank. For example, an Oberst could receive the Charakter als Generalmajor. Very often, German officers would be promoted to the next higher Charakter rank on the day of their retirement.
 It was not uncommon during the 19th century to distinguish between empleo (""employed""), the permanent rank and graduación (""grade"") the honorary, brevet rank. In the 1884 rank regulations (which with minor modifications were in force during the Spanish–American War) stars marked the rank whilst the actual post was reflected in gold lace on the cuffs.
 As in practice both situations coincided the system was dropped in 1908 leaving only the starred system of denoting rank. Nevertheless, during the Spanish Civil War the system was revived in the Nationalist side due to the lack of trained officers because of the enlargement of the army. The breveted officers (known as habilitados or estampillados) wore their actual rank on the cuffs but their brevetted one in a rectangular black patch on the left breast of their coats or shirts.
 In the United Kingdom the brevet commission was only by courtesy. Officially, both titles were used, as: ""Major and Brevet Lieutenant Colonel Cornwallis"". Originally the term designated a promotion given on such occasions as a coronation, or the termination of a great war, and had its origin during the reign of King James II (1685–1688); but it was abused so frequently and used to such an extent by the general award of brevet commissions that from 1854, during the Crimean War of 1853–1856 and subsequently, its bestowal was limited by the government strictly to cases of very distinguished service in the field and on the principle of seniority. The British Army confined brevet commissions to ranks from captain to lieutenant-colonel.
 The brevet conferred rank in the British Army overall, but importantly, not in the regiment. Advancement in the regiment could take place generally only by purchase until 1871 or by seniority, with the exception of the Royal Regiment of Artillery and the Royal Engineers where it has never been possible to buy commissions and promotion was based on merit, and when there was a suitable vacancy caused by the death, retirement or promotion of a more senior officer. For an officer on duty with his regiment, only regimental rank counted. If the regiment formed part of a larger formation then brevet rank could be used to determine command of temporary units formed for special purposes.[4]
 In particular brigadier did not become a permanent rank until 1947, so command of brigades was determined by seniority, including by the date of promotion to any brevet rank.  Thus it was possible for a regimental major to hold a brevet lieutenant-colonelcy with seniority over the commission of his own commanding officer as lieutenant-colonel and be given command of a brigade, potentially including his own regiment. Similarly, while the officer served in a staff position or as an aide-de-camp, then he could use his brevet rank.  Appointment to a brevet also counted towards the requirement to have served for a sufficient time in a lower rank to be eligible for promotion (by purchase) to a more senior one.[4]
 The Articles of War adopted by the United States Army in 1776 and slightly revised in 1806 established the use and significance of brevet ranks or awards in the U.S. Army. When first used, a brevet commission in the U.S. Army entitled the officer to be identified by a higher rank, but the award had limited effect on the right to higher command or pay. A brevet rank had no effect within the officer's current unit. When assigned duty at the brevet rank by the U.S. President, such an officer would command with the brevet rank and be paid at the higher rank.[5]
 This higher command and pay would last only for the duration of that assignment. The brevet promotion would not affect the officer's seniority and actual permanent rank in the army.[5] Beginning on April 16, 1818, brevet commissions also required confirmation by the United States Senate, just as all other varieties of officer commissions did.[6]
 Brevets were first used in the U.S. Army during the American Revolutionary War. Often, the nation's Continental Congress could not find suitable positions for foreign officers—mostly from France—who sought commissions. The first U.S. brevet was given to Jacques Antoine de Franchessin on July 20, 1776, allowing him to hold the rank of lieutenant colonel within the Continental Army. Franchessin and another 35 men of foreign birth would hold brevet commissions in the Army by the end of the war. By 1784, an additional 50 officers would receive brevets for ""meritorious services"" during the conflict.[5]
 In the 19th-century U.S. Army, brevet promotions were quite common because the Army had many frontier forts to garrison and other missions to perform but could not always appoint appropriately ranked officers to command these forts or missions. The U.S. Congress permitted only a limited number of officers of each rank. Thus, an officer of lower rank might receive a brevet commission to a rank more appropriate for his assignment. Also, newly commissioned officers often received brevet rank until authorized positions became available.[7]
 For example, an officer might graduate from West Point and be appointed a brevet second lieutenant until a permanent second lieutenant posting opened up. In early 1861, some recent graduates of West Point temporarily were named brevet second lieutenants because not enough Regular Army officer vacancies were available to give them commissions as regular second lieutenants.[7] In addition to officers being appointed to a brevet rank to temporarily serve in positions designated for higher-ranked officers (i.e., in lieu of promotion to permanent rank), officers might be awarded brevet rank as recognition for gallantry or meritorious service.
 During the American Civil War, almost all senior Union officers received some form of brevet award, mainly during the final months of the war. But these awards were made for gallantry or meritorious service, rather than for command. In addition to the authorization in a previous law for awards of brevet ranks to Regular Army officers, an act of Congress of March 3, 1863, authorized the award of brevet rank to officers of the United States Volunteers.[8] Thus, brevet awards became increasingly common later in the war.[9]
 Some officers even received more than one award. Because of the existence of both Regular Army and United States Volunteers ranks, and the possibility that an officer could hold actual and brevet ranks in both services, some general and other officers could hold as many as four different ranks simultaneously. For example, by the end of the war, Ranald S. Mackenzie was a brevet major general of volunteers, an actual, full-rank brigadier general of volunteers, a brevet brigadier general in the United States Regular Army, and an actual Regular Army captain.[9]
 Brevet rank in the Union Army, whether in the Regular Army or the United States Volunteers, during and at the conclusion of the American Civil War, may be regarded as an honorary title which conferred none of the authority, precedence, nor pay of real or full rank.[10] The vast majority of the Union Army brevet ranks were awarded posthumously or on or as of March 13, 1865, as the war was coming to a close.[10] U.S. Army regulations concerning brevet rank provided that brevet rank could be claimed ""in courts-martial and on detachments, when composed of different corps"" and when the officer served with provisional formations made up of different regiments or companies, or ""on other occasions"".[10] These regulations were vague enough to support the positions of some brevet generals who caused controversies by claiming supposed priorities or privileges of brevet ranks that had been awarded to them at earlier dates during the war.[10]
 Some full-rank brigadier generals in the United States Volunteers (USV) in the American Civil War had been awarded brevet brigadier general rank in the USV before receiving full-rank promotions to brigadier general of United States Volunteers. Some full-rank brigadier generals in the USV were awarded the rank of brevet major general in the USV, but were not promoted to full-rank major generals in the USV. Some United States Regular Army officers who served with the USV in ranks below general officer were awarded brevet general officer rank in the USV, but were not promoted to full-rank general officers in the USV.
 On the other hand, at least a few USV general officers also were awarded brevet general officer rank in the Regular Army in addition to their full-rank appointments or brevet general officer awards in the United States Volunteers. Many of the Regular Army officers of lower rank who became full-rank USV generals, however, received neither actual promotions to a general officer rank nor brevet general officer awards in the Regular Army in addition to their USV ranks or awards. Some of them who stayed in the United States Regular Army after the war did achieve general officer rank in later years.
 In addition to the brevet awards to current (or future) full-rank United States Volunteers (USV) generals during the American Civil War, 1,367 other USV officers of lower ranks were awarded the rank of brevet brigadier general, brevet major general, or both, in the United States Volunteers, but not promoted to full-rank USV generals.[11] At least one enlisted man, Private Frederick W. Stowe, was brevetted as a second lieutenant in the Union Army during the Civil War.[12]
 The Confederate States of America had legislation and regulations for the use of brevets in their armed forces, provided by Article 61 of the nation's Articles of War, and by their 1861 Army Regulations, which were based on the U.S. Army's 1857 version of their regulations. Although Article 61 was revised in 1862, it ultimately had no practical effect since the Confederate States Army did not use any brevet commissions or awards during its existence.[13]
 The United States Marine Corps also issued brevets.  After officers became eligible for the Medal of Honor, a rare Marine Corps Brevet Medal was issued to living officers who had been brevetted between 1861 and 1915.[14]
 The practice of brevetting disappeared from the (regular) U.S. military at the end of the 19th century; honors were bestowed instead with a series of medals. Brevetting was declared obsolete in 1922.[15] However, the similar practice of frocking continues in four of the six branches of the U.S. armed forces. The U.S. Air Force does not allow the regular practice of frocking before a promotion date, except in rare circumstances, such as when an officer selected for promotion is assigned to a billet (typically a senior joint duty assignment), that requires them to hold/wear the higher rank to which they are expected to be promoted.  Frocking typically requires special approval to be obtained from the service headquarters.[16]
 The services differ in how they deal with officers who have been selected for promotion, but not yet promoted, as happens with a promotion list. An Army lieutenant colonel who has been selected for promotion to colonel uses lieutenant colonel (promotable), while in the Air Force, that officer would use colonel (select).
 The promotion of an enlisted person or non-commissioned officer to commissioned officer rank as a reward for displaying leadership and bravery is referred to as a direct appointment rather than a brevet. It temporarily grants up to the rank of first lieutenant. The holder must then attend Officer Candidate School in order to keep the commission. They must also have or acquire a four-year college degree if they wish to be promoted to the rank of captain or above.
 Prior to the suspension of the draft in 1973, the US Army also utilized the administrative distinction between the Regular Army and the Army of the United States as a mechanism for rapid, temporary promotion of officers.  An officer's rank in the Regular Army was their ""permanent rank"", with a ""theater rank"" in the much larger, conscripted Army of the United States.  
 The U.S. National Guard, which depends on the governor of a state to concede its commissions in the Army National Guard and Air National Guard, may still confer brevets.  Many states maintain a clause permitting the governor to confer any rank in its defense forces, including the militia and National Guards.  Some states provide that the sitting governor may confer any rank, but this appointment is considered valid only for the duration of the governor's own term in office.
 Some states also confer brevets as part of their regular honors system. Georgia confers honorary ranks into its state police force. Kentucky is famous for its colonels, and so too is Tennessee, both of which make the appointment as an honorary member of the governor's staff. Alabama, Kentucky, Texas and Nebraska also confer flag officer ranks within a symbolic navy.  Similar honors have been issued for the Georgia Naval Militia, which has existed only on paper since 1908.  In all cases these honorary titles may be considered effective brevets, equal to that of the National Guard, by being conferred by a sitting governor. 
 The 2019 John S. McCain National Defense Authorization Act (NDAA) provided the Army with 770 brevet positions. The U.S. Army Brevet promotion program selects officers for temporary promotion to serve at the next higher rank in a critical billet. A brevet promotion entitles an officer to be temporarily promoted to the next grade and to avail the pay and benefits of the higher rank. This program is one of the nine new authorities that provide the Army flexibility to determine the characteristics of a talent management system.[17]
"
Friendly fire,https://en.wikipedia.org/wiki/Friendly_fire,"
 In military terminology, friendly fire or fratricide[a] is an attack by belligerent or neutral forces on friendly troops while attempting to attack enemy or hostile targets. Examples include misidentifying the target as hostile, cross-fire while engaging an enemy, long range ranging errors or inaccuracy. Accidental fire not intended to attack enemy or hostile targets, and deliberate firing on one's own troops for disciplinary reasons is not called friendly fire,[1] and neither is unintentional harm to civilian or neutral targets, which is sometimes referred to as collateral damage.[2] Training accidents and bloodless incidents also do not qualify as friendly fire in terms of casualty reporting.[3]
 Use of the term friendly in a military context for allied personnel started during the First World War, often when shells fell short of the targeted enemy.[4] The term friendly fire was originally adopted by the United States military; S.L.A. Marshall used the term in Men Against Fire in 1947.[5] Many North Atlantic Treaty Organization (NATO) militaries refer to these incidents as blue on blue, which derives from military exercises where NATO forces were identified by blue pennants and units representing Warsaw Pact forces by red pennants. In classical forms of warfare where hand-to-hand combat dominated, death from a ""friendly"" was rare, but in industrialized warfare, deaths from friendly fire are more common.[6]
 Friendly fire should not be confused with fragging, which is the uncondoned intentional (or attempted) killing of servicemen by fellow personnel serving on the same side.
 Paul R. Syms argues that friendly fire is an ancient phenomenon.[7] He notes recorded events in Ancient Greece and other early accounts of battles. He and other historians also note that weapons such as guns, artillery, and aircraft dramatically increased friendly-fire casualties.
 By the 20th and 21st centuries, friendly-fire casualties have likely become a significant percentage of combat injuries and fatalities. Jon Krakauer provides an overview of American casualties during and since the Second World War:
 In the annals of warfare, deaths at the hand of the enemy are often valorized, while those at the hand of friendly forces may be cast in shame. Moreover, because public relations and morale are important, especially in modern warfare, the military may be inclined to under-report incidents of friendly-fire, especially when in charge of both investigations and press releases:
 Although there may well be a longstanding history of such bias,[9][10] Krakauer claims ""the scale and sophistication of these recent propaganda efforts, and the unabashedness of their executors"" in Iraq and Afghanistan is new.[11]
 Friendly fire can arise from the ""fog of war"" – the confusion inherent in warfare. Friendly fire that is the result of apparent recklessness or incompetence may be improperly lumped into this category. The concept of a fog of war has come under considerable criticism, as it can be used as an excuse for poor planning, weak or compromised intelligence and incompetent command.[1]
 Errors of position occur when fire aimed at enemy forces may accidentally end up hitting one's own. Such incidents are exacerbated by close proximity of combatants and were relatively common during the First and Second World Wars, where troops fought in close combat and targeting was relatively inaccurate. As the accuracy of weapons improved, this class of incident has become less common but still occurs.
 Errors of identification happen when friendly troops are mistakenly attacked in the belief that they are the enemy. Highly mobile battles, and battles involving troops from many nations are more likely to cause this kind of incident as evidenced by incidents in the 1991 Gulf War, or the shooting down of a British aircraft by a U.S. Patriot battery during the 2003 invasion of Iraq.[12] In the Tarnak Farm incident, four Canadian soldiers were killed and eight others injured when a U.S. Air National Guard major dropped a 500 lb (230 kg) bomb from his F-16 onto the Princess Patricia's Canadian Light Infantry regiment which was conducting a night firing exercise near Kandahar.[13][14] Another case of such an accident was the death of Pat Tillman in Afghanistan, although the exact circumstances of that incident are yet to be definitively determined.[15]
 During World War II, ""invasion stripes"" were painted on Allied aircraft to assist identification in preparation for the invasion of Normandy. Similar markings had been used when the Hawker Typhoon was first introduced into use as it was otherwise very similar in profile to a German aircraft. Late in the war the ""protection squadron"" that covered the elite German jet fighter squadron as it landed or took off were brightly painted to distinguish them from raiding Allied fighters.
 Errors of response inhibition have recently been proposed as another potential cause of some friendly fire accidents.[16][17] These types of errors are different from visual misidentification, and instead appear to be caused by a failure to inhibit a shooting response.
 A number of situations can lead to or exacerbate the risk of friendly fire. Difficult terrain and visibility are major factors. Soldiers fighting on unfamiliar ground can become disoriented more easily than on familiar terrain. The direction from which enemy fire comes may not be easy to identify, and poor weather conditions and combat stress may add to the confusion, especially if fire is exchanged. Accurate navigation and fire discipline are vital. In high-risk situations, leaders need to ensure units are properly informed of the location of friendly units and must issue clear, unambiguous orders, but they must also react correctly to responses from soldiers who are capable of using their own judgement. Miscommunication can be deadly. Radios, field telephones, and signalling systems can be used to address the problem, but when these systems are used to co-ordinate multiple forces such as ground troops and aircraft, their breakdown can dramatically increase the risk of friendly fire. When allied troops are operating, the situation is even more complex, especially with language barriers to overcome.[18]
 Some analyses dismiss the material impact of friendly fire, by concluding friendly-fire casualties are usually too few to affect the outcome of a battle.[19][20] The effects of friendly fire, however, are not just material. Troops expect to be targeted by the enemy, but being hit by their own forces has a huge negative impact on morale. Forces doubt the competence of their command, and its prevalence makes commanders more cautious in the field.[21]
 Attempts to reduce this effect by military leaders involve identifying the causes of friendly fire and overcoming repetition of the incident through training, tactics and technology.[18]
 Most militaries use extensive training to ensure troop safety as part of normal coordination and planning, but are not always exposed to possible friendly-fire situations to ensure they are aware of situations where the risk is high. Difficult terrain and bad weather cannot be controlled, but soldiers must be trained to operate effectively in these conditions, as well as being trained to fight at night. Such simulated training is now commonplace for soldiers worldwide. Avoiding friendly fire can be as straightforward as ensuring fire discipline is instilled in troops, so that they fire and cease firing when they are told to. Firing ranges now also include ""don't fire"" targets.[21]
 The increasing sophistication of weaponry, and the tactics employed against American forces to deliberately confuse them has meant that while overall casualties have fallen for American soldiers in the late 20th and 21st centuries, the overall percentage of deaths due to friendly fire in American actions has risen dramatically. In the 1991 Gulf War, most of the Americans killed by their own forces were crew members of armored vehicles hit by anti-tank rounds. The response in training includes recognition training for Apache helicopter crews to help them distinguish American tanks and armored vehicles at night and in bad weather from those of the enemy. In addition, tank gunners must watch for ""friendly"" robotic tanks that pop out on training courses in California's Mojave Desert. They also study video footage to help them recognize American forces in battle more quickly.[22]
 Improved technology to assist in identifying friendly forces is also an ongoing response to friendly fire problems.
From the earliest days of warfare, identification systems were visual and developed into extremely elaborate suits of armour with distinctive heraldic patterns. During the Napoleonic Wars, Admiral Nelson ordered that ships under his command adopt a common paint scheme to reduce friendly fire incidents; this pattern became known as the Nelson Chequer. Invasion stripes served a similar function during the Allied invasion of Normandy in World War II.  When radar was developed during World War II, IFF (""Identification friend or foe"") systems to identify aircraft developed into a multitude of radio beacons.
 Correct navigation is vital to ensuring units know where they are in relation to their own force and the enemy. Efforts to provide accurate compasses inside metal boxes in tanks and trucks has proven difficult, with GPS a major breakthrough.
 Other technological changes include hand-held navigational devices that use satellite signals, giving ground forces the exact location of enemy forces as well as their own. The use of infrared lights and thermal tape that are invisible to observers without night-goggles, or fibres and dyes that reflect only specific wavelengths are developing into key identifiers for friendly infantry units at night.
 There is also some development of remote sensors to detect enemy vehicles – the Remotely Monitored Battlefield Sensor System (REMBASS) uses a combination of acoustic, seismic vibration, and infrared to not just detect, but identify vehicles.[21]
 Some tactics make friendly fire virtually inevitable, such as the practice of dropping barrages of mortars on enemy machine gun posts in the final moments before capture. This practice continued throughout the 20th century since machine guns were first used in World War I. The high friendly fire risk has generally been accepted by troops since machine gun emplacements are tactically so valuable, and at the same time so dangerous that the attackers wanted them to be shelled, considering the shells far less deadly than the machine guns.[21]
Tactical adjustments include the use of ""kill boxes"", or zones that are placed off-limits to ground forces while allied aircraft attack targets, which goes back to the beginning of military aircraft in World War I.[22]
 The shock and awe battle tactics adopted by the American military – overwhelming power, battlefield awareness, dominant maneuvers, and spectacular displays of force – are employed because they are believed to be the best way to win a war quickly and decisively, reducing casualties on both sides. However, if the only people doing the shooting are American, then a high percentage of total casualties are bound to be the result of friendly fire, blunting the effectiveness of the shock and awe tactic. It is probably the fact that friendly fire has proven to be the only fundamental weakness of the tactics that has caused the American military to take significant steps to overturn a blasé attitude to friendly fire and assess ways to eliminate it.[21]
 During Operation Husky, codename for the Allied invasion of Sicily, on the night of 11 July 1943, American C-47 transport planes were mistakenly fired upon by American ground and naval forces and 23 planes were shot down and 37 damaged, resulting in 318 casualties, with 60 airmen and 81 paratroopers killed.[23]
 This led to the use of Invasion stripes that were used during D-Day as a visible way to prevent friendly fire.[24] During the Russian invasion of Ukraine the Z (military symbol) has been used on Russian vehicles as a form of marking. There are various explanations as to its meaning, however, one is that both sides are using the same equipment. Ukrainian forces have responded by using visible Ukrainian flags on their vehicles.[25] The picture has become more confused as both sides are using captured or abandoned equipment with Ukraine using captured Russian tanks.[26][27]
 Incidents include: the killing of Royalist commander, the Earl of Kingston, by Royalist cannon fire during the English Civil War;[28] the bombing of American troops by Eighth Air Force bombers during Operation Cobra in World War II;[29] the attack on the Royal Navy 1st Minesweeping Flotilla off Cap d'Antifer, Le Havre by 263 Squadron and 266 Squadron RAF on 27 August 1944, sinking HMS Britomart and Hussar, and irreparably damaging HMS Salamander, killing 117 sailors and wounding 153 more;[30] the eight-hour firefight between British units during the Cyprus Emergency;[31] the sinking of the German destroyers Leberecht Maass and Max Schultz by the Luftwaffe in the North Sea during World War II; the downing of a British Army Gazelle helicopter by a British warship during the Falklands War;[32] the downing of two U.S. Army Black Hawk helicopters by USAF fighters in 1994 during the Iraqi no-fly zones;[33] the shooting down and killing of Italo Balbo, the Italian governor of Libya over Tobruk by Italian anti aircraft fire in 1940; the accidental shooting of Stonewall Jackson during the American Civil War; the killing of a Royal Military Policeman by a British sniper during the war in Afghanistan;[34] and the Tarnak Farm incident when US Air National Guard pilots in 2002 bombed 12 Canadian soldiers, four of whom were killed;[35] these were the first Canadian casualties of the war in Afghanistan.
  Media related to Friendly fire at Wikimedia Commons
"
Martha Washington,https://en.wikipedia.org/wiki/Martha_Washington,"


 Martha Dandridge Custis Washington (June 2, 1731 O.S. – May 22, 1802) was the wife of George Washington, who was the first president of the United States. Although the title was not coined until after her death, she served as the inaugural first lady of the United States, defining the role of the president's wife and setting many precedents that future first ladies observed. During her tenure, she was referred to as ""Lady Washington"". Washington is consistently ranked in the upper half of first ladies by historians.
 Martha Dandridge married Daniel Parke Custis on May 15, 1750, and the couple had four children, only one of whom survived to adulthood.[1] She was widowed in 1757 at the age of 26, inheriting a large estate. She was remarried to George Washington in 1759, moving to his plantation, Mount Vernon. Her youngest daughter died of epilepsy in 1773, and the Washingtons were unable to conceive any children of their own. Washington became a symbol of the American Revolution after her husband was appointed commander-in-chief of the Continental Army. During the war, she played a maternal role, visiting encampments when fighting stalled each winter. Her only surviving child, John Parke Custis, died from a camp illness during the war. After the war ended in 1783, she sought retirement at Mount Vernon, but returned to public life when her husband became president of the United States in 1789.
 Washington took on the social role of the president's wife reluctantly, becoming a national celebrity in the process. She found this life unpleasant, feeling that she was restricted and wishing for retirement. In addition to hosting weekly social events, Washington understood that how she composed herself would reflect on the nation, both domestically and abroad. As such, she struck a careful balance between the dignity associated with a head of state's wife and the humility associated with republican government. The Washingtons returned to Mount Vernon in 1797, and she spent her retirement years greeting admirers and advising her successors. She was widowed for a second time in 1799, and she died two-and-a-half years later in 1802.
 Martha Dandridge was born on June 2, 1731, on her parents' tobacco plantation,[2]: 9  Chestnut Grove Plantation in New Kent County the Colony of Virginia. She was the oldest daughter of Frances Jones, the granddaughter of an Anglican rector,[3]: 2  and John Dandridge, a Virginia planter[3]: 2  and county clerk[2]: 9  who emigrated from England. She had three brothers and four sisters: John (1733–1749), William (1734–1776), Bartholomew (1737–1785), Anna Maria ""Fanny"" Bassett (1739–1777), Frances Dandridge (1744–1757), Elizabeth Aylett Henley (1749–1800), and Mary Dandridge (1756–1763).[4] As the oldest of eight, including one sister that was 25 years her junior, Dandridge played a maternal and domestic role beginning early in life.[2]: 10  Dandridge may have also had an illegitimate half-sister born into slavery, Ann Dandridge Costin,[5] and an illegitimate white half-brother, Ralph Dandridge.[6]: 26–27  
 Dandridge's father was well connected with the Virginia aristocracy despite his relative lack of wealth, and she was taught to behave as a woman of the upper class.[7] She received a relatively high quality education for the daughter of a planter, though it was still inferior to that of her brothers.[8] She took to equestrianism, at one point riding her horse up and down the stairs of her uncle's home and escaping chastisement because her father was so impressed by her skill.[9]: 8 
 In 1749, Dandridge met Daniel Parke Custis, the son of a wealthy planter in Virginia.[7] They wished to marry, but the father of Dandridge's prospective groom, John Custis, was highly selective of what woman would marry into the family's fortune. She eventually won his approval, and Dandridge married Custis, who was two decades her senior, on May 15, 1750.[3]: 2  After they were married, Custis moved with her husband to his residence at White House Plantation on the Pamunkey River. Here they had four children: Daniel, born 1751; Frances, born 1753; John, born 1754; and Martha, born 1756. Daniel died in 1754 and Frances died in 1757.[10]: 4  Daniel Parke Custis was one of the wealthiest men in the Virginia colony as well as one of the largest slaveowners, owning nearly 300 slaves.[11]
 Custis became a widow at the age of 26 when her husband died (possibly from a severe infection of the throat).[12] Upon his death, she inherited the large estate that he had previously inherited from his father.[7] After his death in 1757, she received one third of his estate outright, and the remaining two thirds were granted to their two young children. The total inheritance amounted to approximately $33,000 (equivalent to $1,104,439 in 2023), 17,000 acres of land, and hundreds of slaves.[3]: 2  The legal and financial matters of the inheritance presented a considerable burden on Custis while she was raising her two surviving children and grieving the losses of her husband, two children, and her father.[10]: 4  She was also left with the responsibility of managing the farmland and overseeing the well-being of the slaves.[3]: 2  According to her biographer, ""she capably ran the five plantations left to her when her first husband died, bargaining with London merchants for the best tobacco prices"".[13]
 By one account Custis met George Washington during the Williamsburg social season, and they courted over the following months during his leaves from the military.[10]: 4  By another, they were introduced by Colonel Chamberlayne, a mutual acquaintance, when they both stayed the night at his home in May 1758.[9]: 8–9  They married on January 6, 1759, at the White House plantation.[10]: 5 
 The couple honeymooned at the Custis family's White House plantation,[6]: 124  followed by a stay in Williamsburg where her husband was a representative in the House of Burgesses before setting up house at his Mount Vernon estate.[3]: 3  At the time of their wedding, she was one of the wealthiest widows in the Thirteen Colonies.[14]: 27  Their marriage remained happy over the following 40 years, in part because of their similar worldviews.[10]: 4  It was a marriage based in mutual respect and shared habits, with both maintaining similar schedules in day-to-day life and both prioritizing family and image over excitement and vice.[2]: 11 
 From 1759 to 1775, the Washingtons lived at Mount Vernon where they tended to their plantation.[7] Washington ran the household and regularly entertained visitors. She knitted and oversaw the making of clothes, and she became talented in curing meat in their smokehouse.[3]: 3  Washington entertained almost daily, having visitors for dinner or for longer stays as the family became more prominent in the political and social life of Virginia.[10]: 5  Washington's husband used her wealth to expand their home at Mount Vernon and turn it into a profitable estate.[3]: 3 
 The Washingtons had no children together, but they raised Martha's two surviving children. She was highly protective of them, especially after her two previous children had died and Patsy was found to have epilepsy.[3]: 3  In 1773, Patsy died when she was 17 during an epileptic seizure.[15][16] Washington's last surviving child, John, left King's College that fall and married Eleanor Calvert in February 1774.[16] The Washingtons hoped for more children throughout their marriage, but they were unable to conceive.[3]: 3–4 
 Life for the Washingtons was interrupted as the American Revolution escalated in the 1770s.[3]: 4  Though rumors were spread that she was a Loyalist, Washington consistently shared her husband's political beliefs.[9]: 3–4  She strongly supported his role in the Patriot movement and his work to advance his beliefs in the cause. She stayed at Mount Vernon when he was appointed commander-in-chief of the Continental Army in 1775, overseeing the construction of new wings to their home. She then moved to the home of her brother-in-law so as not to be so conspicuous a target during the American Revolutionary War.[3]: 4 
 The revolution was the first time in their marriage that they were apart for an extended period. In the fall of 1775, Washington traveled to Massachusetts to meet with her husband.[10]: 6  On the journey north, she experienced her newfound celebrity status for the first time as the wife of a famed general.[10]: 6  She joined him in Cambridge, from where he and the other Continental Army officers were operating. While staying in Cambridge, she served as a hostess for guests of the officers.[3]: 4  She would also sew clothes for the soldiers while at camp, encouraging other officers' wives to do the same, leading to the creation of a sewing circle that contributed to the war effort.[9]: 5  Though she hid it from those around her, Washington was frightened by the gunfire that could be heard from the nearby Siege of Boston.[3]: 4  She accompanied her husband when operations were relocated to New York, but she was sent to Philadelphia as British forces came closer.[3]: 5  Each spring, when conflict resumed, she returned to Mount Vernon.[10]: 7 
 The American Revolution became increasingly stressful for Martha after the signing of the Declaration of Independence, as George faced increased risks on the battlefield.[3]: 5  Each winter, Washington would join her husband at his encampment while fighting was stalled. The quality of her housing varied during these visits, both in comfort and in safety.[3]: 5  General Lafayette observed that she loved ""her husband madly"".[17] Washington was kept informed of the war's developments by her husband, sometimes performing clerical work for him, and she was even permitted to know military secrets.[2]: 14  She became a symbol of the war effort, alongside George Washington, as a grandmotherly figure that cared for the soldiers.[10]: 7 
 The Continental Army settled in Valley Forge, the third of the eight winter encampments of the Revolution, on December 19, 1777. Washington traveled 10 days and hundreds of miles to join her husband in Pennsylvania.[18] On April 6, Elizabeth Drinker and three friends arrived at Valley Forge to plead with the General to release their husbands from jail; the men, all Quakers, had refused to swear a loyalty oath to the American revolutionaries. Because the commander was not available at first, the women visited with Martha.[19] Drinker described her later in her diary as ""a sociable pretty kind of Woman"".[20]
 Washington's son John was serving as a civilian aide to his father during the siege of Yorktown in 1781 when he died of ""camp fever"", a diagnosis for epidemic typhus.[16] After his death, she and George took in the youngest two of John's four children, Eleanor (Nelly) Parke Custis and George Washington Parke (Washy) Custis.[10]: 7  The Washingtons also provided personal and financial support to the children of many of their relatives and friends.[21]
 The Washingtons returned to Mount Vernon in 1783.[10]: 7  They stayed at Mount Vernon for much of the Confederation period, living in retirement with their nephew, nieces, and grandchildren.[3]: 6  Washington, now in poorer health, believed that her husband was finished with public service.[3]: 6  She spent her time raising their grandchildren, constantly worried for their health after having all four of her children and many other relatives die of illness. She also resumed hosting company at Mount Vernon, recruiting several of her nieces and other young women to assist her, as the house was overwhelmed with visitors.[10]: 7–8  Their life at Mount Vernon was interrupted again when he was asked to participate at the Constitutional Convention in 1787 and again when he was chosen as the first president of the United States in 1789.[3]: 7 
 After the war, Washington was not fully supportive of her husband's agreeing to be president of the newly formed United States.[22] She did not immediately join him at the capital in New York City, only arriving in May 1789.[7] The journey was followed by the press, which was unprecedented in the attention that it paid to a woman's actions, and the entourage was met with admirers and fanfare in each town that it passed through. It was during this journey that she gave her only public speech as first lady, thanking those that came to see her.[2]: 15  She arrived on the presidential barge, escorted by her husband, immediately establishing the president's wife as a public figure.[23]: 3  After arriving at the capital, Washington became the inaugural first lady of the United States, though the term would not be used until later. Instead, she was referred to as ""Lady Washington"".[24]: 13 
 As the inaugural first lady, many of Washington's practices in the White House became traditions for future first ladies, including the opening of the White House to the public on New Year's Day, a practice that would continue until the Hoover administration.[7] She hosted many affairs of state at New York City and Philadelphia during their years as temporary capitals.[9]: 6  Taking her responsibility as the lady of the house seriously, Washington returned the official calls of every lady that left her card at the heavily-trafficked presidential home to ensure that everyone could reach the president, always doing so within three days.[23]: 6–7 
 Washington was also tasked by her husband with the responsibility of hosting drawing room events on Fridays in which ladies were permitted to attend.[23]: 5  She would remain seated during such events while the president greeted their guests.[10]: 9  The guests were at first uncertain as to whether they should follow the royal custom of waiting for the hostess to leave before they do, and she resolved the issue by announcing her husband always retired at nine.[23]: 6  She was careful during these events to avoid political talk, encouraging a change of subject when it came up.[9]: 6  The social circles that developed among those in American politics at this time became known as the Republican Court.[25]
 The first presidential residence was a house on Cherry Street, followed by a house on Broadway. The capital was moved to Philadelphia in 1790, and the presidential residence again moved,[23]: 7  this time to a house on High Street (now Market Street).[10]: 10  Washington much preferred the Philadelphia residence, as it had a greater social life and was closer to Mount Vernon.[3]: 8  Early in her husband's presidency, she had little opportunity to go out, as any action she took would have political implications.[10]: 9–10  After their move to Philadelphia, the Washingtons loosened their self-imposed limits on personal activity.[10]: 11  While serving as first lady, Washington became close to Polly Lear, the wife of her husband's secretary Tobias Lear.[3]: 8  She also associated with Lucy Flucker Knox, wife of war secretary Henry Knox, and Abigail Adams, the second lady.[10]: 10  The time she spent with her grandchildren was another high point for Washington, who would sometimes take them to shows and museums.[24]: 60  She also made a point of frequently attending church, owing to her firm Episcopalian beliefs.[2]: 12 
 Washington was forced to take control of the presidential residence at one point shortly after her husband's presidency began, forbidding guests from entering, as he was undergoing the removal of a tumor.[24]: 67–68  In July 1790, artist John Trumbull gave Washington a full-length portrait painting of her husband as a gift. It was displayed in their home at Mount Vernon in the New Room.[26] When Washington learned that her husband might take on a second term as president, she uncharacteristically protested against the decision. Despite her opposition, he was reelected in 1793, and she reluctantly accepted four more years as the wife of the president.[3]: 8  The young Georges Washington de La Fayette joined the Washington family in 1795 while his father, Marquis de Lafayette, was held as a political prisoner in France. He would live with the Washingtons until fall of 1797.[10]: 12  In 1796, Washington's slave and personal maid Oney Judge escaped and fled to New Hampshire. Despite Washington's insistence to her husband that Judge should be returned and again should be Washington's slave, the president did not attempt to pursue Judge.[8] Washington's tenure as first lady ended in 1797.[3]: 8 
 As the wife of both the head of government and the head of state, Washington was immediately faced with the pressure of representing the United States. She had to present the United States as a dignified nation to establish credibility among the countries of Europe, but she also had to respect the spirit of democracy by refusing to present herself as a queen.[3]: 7  She was also aware that the precedent she set would be inherited by future presidential wives.[24]: 19  Washington balanced these responsibilities by playing the role of a social hostess at presidential events, a role that would become the primary function of the first lady. In turn, this made the position of first lady an important point of contact between the president and the people.[14]: 27–29 
 Washington presented an image of herself as an amiable wife, but privately she complained about the restrictions placed on her life.[27] She found the pageantry of the presidency to be boring and artificial.[2]: 12  Washington was not exempt from the political attacks often levied at her husband's administration by opposition-owned newspapers. While her social role was celebrated by her husband's supporters, the anti-Federalists criticized her as emulating royalty and encouraging aristocracy.[14]: 29–30  At the same time, other critics accused her social activities of being too informal.[24]: 19  To her displeasure, she found that she was constantly the subject of public attention, and she was forced to pay increased attention to her hair and clothes each day.[10]: 9–10  Despite this, she still opted to dress simply in homespun clothes, feeling that it was more appropriate in a republic.[24]: 37 
 The Washingtons left the capital immediately after the inauguration of John Adams, making the return journey to Mount Vernon, which by then had begun to decay.[10]: 12  Again they went into retirement, and they saw to several renovations for their home.[3]: 9  In the years after the presidency, the Washingtons received more visitors than ever, from friends and strangers alike. They eventually took in one of the former president's nephews, Lawrence Lewis, to serve as secretary, and he would eventually marry Washington's granddaughter Nelly.[10]: 13 
 Washington feared that her husband would again be called away to lead a provisional army against France, but no such conflict took place.
Her husband died of a severe throat infection on December 14, 1799, at the age of 67.[28] As a widow, Washington spent her final years living in a garret where she knitted, sewed, and responded to letters. Though she was the legal owner of her husband's property, she gave control of its business affairs to her relatives.[3]: 9  She also inherited her husband's slaves on the condition that they be freed upon her death. Fearing that these slaves might hurt her, she freed them. She did not have the authority to free her dower slaves, and she chose not to free the one slave, Elish, whom she personally owned.[11]
 Washington retained an interest in the presidency after her tenure as first lady, beginning the tradition of advising her successors.[24]: 124  The Washington family long disliked Thomas Jefferson and Jeffersonian politics, in part because of the central role he played in criticizing the Washington administration.[10]: 11  Washington took offense when Jefferson became president, as she felt that he did not give adequate respect to the office.[9]: 8  
 Washington's health, always somewhat precarious, declined after her husband's death.[29] She had anticipated her death since that of her husband. When she developed a fever in 1802, she burned all of her husband's letters to her, summoned a clergyman to administer last communion, and chose her funeral dress.[9]: 8  Two and a half years after the death of her husband, Washington died on May 22, 1802, at the age of 70.[29] Following her death, Washington's body was interred in the original Washington family tomb vault at Mount Vernon.[30] In 1831, the surviving executors of George's estate removed the bodies of the Washingtons from the old vault to a similar structure within the present enclosure at Mount Vernon.[30]
 Just as her husband had set the precedent for the presidency, Washington established what would eventually become the role of first lady. She was prominent in the ceremonial aspects of the presidency, assisting her husband in his role as head of state, but she had very little public involvement in his administrative role as head of government. This would be the standard of presidential wives for the next century.[23]: 7–8  Washington was recognized for her humility and her mild-mannered nature, to the point that her contemporaries were often taken by surprise when meeting her.[9]: 3  No personal records of Washington exist from before the death of her first husband, and she destroyed many letters that she had written since then. Many recipients of her letters kept them, however, and those letters have been preserved in archives such as at Mount Vernon and the Virginia Historical Society. Several collections of these letters have been published.[10]: 14 
 During the Revolutionary War, one of the regiments at Valley Forge named themselves ""Lady Washington's Dragoon"" in her honor.[2]: 14 The Martha Washington College for Women was founded in Abingdon, Virginia in 1860.[31] It was merged with Emory & Henry College in 1918,[32] and the main original building of Martha Washington College was converted to the Martha Washington Inn.[33] Martha Washington Seminary, a finishing school for young women in Washington, DC, was opened in 1905,[34] and it ceased operations in 1949.[35]
 A postage stamp featuring Martha Washington, the first stamp to honor an American woman, was issued as part of the 1902 stamp series. An 8-cent stamp, it was printed in violet-black ink.[36] The second stamp issued in her honor,[37] a 4-cent definitive stamp printed in yellow-brown ink, was released in 1923.[38] A .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1+1⁄2-cent stamp was issued in 1938 to honor Washington as part of the Presidential Issue series.[39] Washington's image was featured on the one dollar silver certificate banknote beginning in 1886, making her the second woman to appear on an American banknote after Pocahontas.[40] To prevent confusion with existing coinage, pattern coins testing new metals have been produced by the U.S. mint, or a company contracted to it, with Martha Washington on the obverse.[41]
 Since 1982 Siena College Research Institute has periodically conducted surveys asking historians to assess American first ladies according to a cumulative score on the independent criteria of their background, value to the country, intelligence, courage, accomplishments, integrity, leadership, being their own women, public image, and value to the president.[42] Consistently, Washington has been ranked in the upper-half of first ladies by historians in these surveys. In terms of cumulative assessment, Washington has been ranked:
 In the 2008 Siena Research Institute survey, Washington was ranked 3rd-highest in the criteria of public image.[43] In the 2014 survey, Washington and her husband were ranked the 2nd-highest out of 39 first couples in terms of being a ""power couple"".[45]
"
Daniel Parke Custis,https://en.wikipedia.org/wiki/Daniel_Parke_Custis,"
 Daniel Parke Custis (October 15, 1711[1] – July 8, 1757) was an American planter and politician who was the first husband of Martha Dandridge. After his death, his widow, Martha Dandridge Custis married George Washington, who later became the first president of the United States.
 Custis was born in York County, Virginia, on October 15, 1711. He was one of two children of John Custis IV (1678–1749), a powerful member of Virginia's Governor's Council, and Frances Parke Custis. The Custis family was one of the wealthiest and most socially prominent of Virginia.[2] Custis' mother, Frances, was the daughter of Daniel Parke, a political enemy of the Custises.[3]
 As Daniel Custis was the sole male heir in the Custis family, he inherited the Southern plantations owned by his father.[4] However, Custis did not choose to take a leading role in colonial Virginia politics.
 At the age of 37, Custis met 16-year-old Martha Dandridge at the St. Peter's Church where Martha attended and Custis was a vestryman.[5][6] His father, John Custis, disapproved of the relationship, but eventually relented. After a two-year courtship, Custis and Dandridge were married on May 15, 1750.[7] The couple lived at Custis's plantation called the White House in New Kent County, Virginia.[4]
 They had four children:[8]
 Custis died on July 8, 1757, in New Kent County, Virginia, with some historians stating the cause of death as a heart attack,[9][10] but others stating that he died from a severe throat infection.[11]
 Custis is buried in the graveyard of the Bruton Parish Church in Williamsburg, Virginia beside two of his children, Daniel Parke Custis, Jr., and Frances Parke Custis.[12] Eighteen months after Custis died, his widow Martha married George Washington on January 6, 1759.[5]
 As Custis died intestate, or ""without a will"", his widow Martha received the lifetime use of one third of his property (known as a ""dower share""),[13] and the other two thirds were held in trust for their children. The January 1759 Custis estate also included at least 85 slaves.[14] According to the Mount Vernon slave census, by 1799 the dower share included 153 slaves. The October 1759 Custis estate inventory listed 17,779 acres (71.95 km2), or 27.78 square miles of land, spread over five counties.[15]
 Upon Martha Custis' marriage to George Washington in 1759, her dower share came under his control, pursuant to the common law doctrine of seisin jure uxoris. He also became guardian of her two minor children, and administrator of the Custis estate. John Parke Custis was the only child to reach his majority, upon which he inherited the non-dower two-thirds of his father's estate.
 Upon George Washington's death on December 14, 1799, the dower share and slaves reverted to Martha. Through a provision in his will, Washington directed that his 124 slaves be freed following his wife's death.[16] As Washington stated in his will, he ""earnestly wished"" to free his own slaves at the time of his death but acknowledged that doing so would create ""insuperable difficulties"" because they had intermarried with Martha's ""dower negroes,"" over whom he had no authority. He also believed that it would ""excite the most painful sensations"" and ""disagreeable consequences"" to attempt to separate them.[17]
 Washington's slaves were not part of the Custis estate, and Martha had no legal power to free them or the dower slaves, but they were freed at her request on January 1, 1801. The principal reason that Martha gave for requesting that her husband's slaves be set free is that she was concerned about her personal safety. Washington's slaves, having found out that they would be free upon her death, were suspected of wanting to hasten her death. They were also perceived as being restive and were believed to have been the cause of several suspicious fires on the Mount Vernon estate.[18]
 When Martha died on May 22, 1802, her dower share reverted to the Custis estate. Because of Martha Washington's dower share, the estate could not be liquidated for more than 45 years. Martha's dower share was eventually divided between John Parke Custis's widow, Eleanor Calvert Custis Stuart, and their four children. Martha also bequeathed Elisha, the one slave she owned herself, to her grandson George Washington Parke Custis.
"
Dower,https://en.wikipedia.org/wiki/Dower,"Dower is a provision accorded traditionally by a husband or his family, to a wife for her support should she become widowed. It was settled on the bride (being given into trust) by agreement at the time of the wedding, or as provided by law.
 The dower grew out of the practice of bride price, which was given over to a bride's family well in advance for arranging the marriage, but during the early Middle Ages, was given directly to the bride instead. However, in popular parlance, the term may be used for a life interest in property settled by a husband on his wife at any time, not just at the wedding. The verb to dower is sometimes used.
 In popular usage, the term dower may be confused with:
 Being for the widow and being accorded by law, dower differs essentially from a conventional marriage portion such as the English dowry (cf.  Roman dos, Byzantine proíx, Italian dote, French dot, Dutch bruidsschat, German Mitgift).
 The bride received a right to certain property from the bridegroom or his family. It was intended to ensure her livelihood in widowhood, and it was to be kept separate and in the wife's possession.
 Dower is the gift given by the groom to the bride, customarily on the morning after the wedding, though all dowerings from the man to his fiancée, either during the betrothal period, or wedding, or afterwards, even as late as in the testamentary dowering, are understood as dowers if specifically intended for the maintenance of the widow.
 Dower was a property arrangement for marriage first used in early medieval German cultures, and the Catholic Church drove its adoption into other countries, in order to improve the wife's security by this additional benefit. The practice of dower was prevalent in those parts of Europe influenced by Germanic Scandinavian culture, such as Sweden, Germany, Normandy and successor states of the Langobardian kingdom.
 The husband was legally prevented from using the wife's dower — as contrasted with her dowry, which was brought to the marriage by the bride and used by both spouses. This often meant that the woman's legal representative, usually a male relative, became guardian or executor of the dower, to ensure that it was not squandered.
 Usually, the wife was free from kin limitations to use (and bequeath) her dower to whatever and whomever she pleased. It may have become the property of her next marriage, been given to an ecclesiastical institution, or been inherited by her children from other relationships than that from which she received it.
 In English legal history, there were originally five kinds of dower:[1][2]
 Dower is thought to have been suggested by the bride price which Tacitus found to be usual among the Germans. This bride price he terms dos, but contrasts it with the dos (dowry) of the Roman law, which was a gift on the part of the wife to the husband, while in Germany the gift was made by the husband to the wife.[3] There was indeed in the Roman law what was termed donatio propter nuptias, a gift from the family of the husband, but this was only required if the dos were brought on the part of the wife. So too in the special instance of a widow (herself poor and undowried) of a husband rich at the time of his death, an ordinance of the Christian Emperor Justinian secured her the right to a part of her husband's property, of which no disposition of his could deprive her.
 Dower payments evolved from the Germanic custom of paying a bride price, which over centuries morphed into the bride gift. After the introduction of Christianity, the custom of dower persisted as a method of exacting from the husband at marriage a promise to endow his wife, a promise retained in form even now in the marriage ritual of the Established Church in England.[4] Dower is mentioned in an ordinance of King Philip Augustus of France (1214), and in the almost contemporaneous Magna Carta (1215); but it seems to have already become customary law in Normandy, Sicily, and Naples, as well as in England. The object of both ordinance and charter was to regulate the amount of the dower where this was not the subject of voluntary arrangement, dower by English law consisting of a wife's life estate in one-third of the lands of the husband ""of which any issue which she might have had might by possibility have been heir"".[5]
 There is judicial authority of the year 1310 for the proposition that dower was favoured by law,[6] and at a less remote period it was said to be with life and liberty one of three things which ""the law favoreth"". In England in the late 18th century, it became common for men to hold land with a trust that prevented their wives' acquiring dower. Accordingly, the English statute, the Fines and Recoveries Act 1833 was passed to impair the inviolability of dower by empowering husbands to cut off by deed or will their wives from dower. Wives married before the Act still had (in certain cases) to acknowledge the deed before a commissioner to bar their right to dower in property which their husband sold.  This was simpler than the previous procedure, which had required a fine to be levied in the Court of Common Pleas, a fictitious proceeding, by which she and her husband formally remitted their right to the property to the purchaser.
 In English law, dower was one third of the lands seised in fee by the husband during the marriage. However, in the early modern period, it was common for a wife to bar her right to dower in advance under a marriage settlement, under which she agreed to take instead a jointure, that is a particular interest in her husband's property, either a particular share, or a life interest in a particular part of the land, or an annuity.  This was often part of an arrangement by which she gave up her property to her husband in exchange for her jointure, which would accordingly be greater than a third.  Strictly dower was only available from land that her husband owned, but a life tenant under a settlement was often given power to appoint a jointure for his wife.  The wife would retain her right to dower (if not barred by a settlement) even if her husband sold the property; however this right could also be barred by a fictitious court proceeding known as levying a fine.  The widow of a copyholder was usually provided for by the custom of the manor with freebench, an equivalent right to dower, but often (but not necessarily) a half, rather than a third.
 
 As of 1913, the law in Quebec recognized a customary dower for widows, a holdover from old French law. This dower could be forfeited by women who took perpetual vows in certain religious orders.[7]
 Under Scots law, the part of the estate that cannot be denied to a surviving wife is referred to as jus relictae.
 It was the law of dower unimpaired by statute which, according to the American commentator Chancellor Kent, has been ""with some modifications everywhere adopted as part of the municipal jurisprudence of the United States"".[8]   In American law, a widow's dower estate has phases: inchoate dower while the husband is still alive (wives co-sign their husbands' deeds for land in order to release their inchoate dower rights),[9] unassigned dower after his death and before a dower lot is assigned to her, assigned (and if necessary admeasured) dower once the lot is determined.  Then she can live on the dower lot or get its usufruct (""fruits"" like actual fruit or animals grown there, and any rental income from her share), during her life.  She can sell her unassigned or assigned dower rights, but could not sell them while they were still inchoate before her husband's death.  Her dower lot is assigned to her by the husband's heirs who inherit the land, and it should be one-third of the husband's real property (by value, not by land area).  If the widow disputes it, she or the heirs may file an action in court for admeasurment of dower and the court will determine and assign a dower lot to the widow.  See Scribner on Dower.[10]  A widow's dower and widower's curtesy rights have been abolished by statute in most American states and territories, most recently in Michigan in 2016.[11]  Dower was never ""received"" into Louisianan law, its civil code being based mainly on French law.  In Arkansas,[12] Kentucky,[13] Ohio[14] and the Territory of Palmyra Island,[15] a widow's dower remains a valid estate in land—modified and augmented in Arkansas and Kentucky with other protections for surviving spouses like elective share and community property.
 During the pre-Reformation period, a man who became a monk and made his religious profession in England was deemed civilly dead, ""dead in law"";[16] consequently his heirs inherited his land forthwith as though he had died a natural death. Assignment of dower in his hand would nevertheless be postponed until the natural death of such a man, for only by his wife's consent could a married man be legally professed in religion, and she was not allowed by her consent to exchange her husband for dower. After the Reformation and the enactment of the English statute of 11 and 12 William III, prohibiting ""papists"" from inheriting or purchasing lands, a Roman Catholic widow was not held to be debarred of dower, for dower accruing by operation of law was deemed to be not within the prohibitions of the statute. By a curious disability of old English law a Jewish widow born in England would be debarred of dower in land which her husband, he having been an Englishman of the same faith and becoming converted after marriage, should purchase, if she herself remained unconverted.
 Some high-born persons have been prone to marry an ineligible spouse. Particularly in European countries where the equal birth of spouses (Ebenbürtigkeit) was an important condition to marriages of dynasts of reigning houses and high nobility, the old matrimonial and contractual law provision of dowering was taken into a new use by institutionalizing the morganatic marriage. Marriage being morganatical prevents the passage of the husband's titles and privileges to the wife and any children born of the marriage.
 Morganatic, from the Latin phrase matrimonium ad morganaticam, refers to the dower (Latin: morganaticum, German: Morgengabe, Swedish: morgongåva ). When a marriage contract is made that the bride and the children of the marriage will not receive anything else (than the dower) from the bridegroom or from his inheritance or patrimony or from his clan, that sort of marriage was dubbed as ""marriage with only the dower and no other inheritance"", i.e. matrimonium ad morganaticum.
 Neither the bride nor any children of the marriage has any right on the groom's titles, rights, or entailed property. The children are considered legitimate on other counts and the prohibition of bigamy applies.
 The practice of ""only-doweried"" is close to pre-nuptial contracts excluding the spouse from property, though children are usually not affected by prenuptials, whereas they certainly were by morganatical marriage.
 Morganatic marriage contained an agreement that the wife and the children born of the marriage will not receive anything further than what was agreed in pre-nuptials, and in some cases may have been zero, or something nominal. Separate nobility titles were given to morganatic wives of dynasts of reigning houses, but it sometimes included no true property. This sort of dower was far from the original purpose of the bride receiving a settled property from the bridegroom's clan, in order to ensure her livelihood in widowhood.
 The practice of morganatic marriage was most common in historical German states, where equality of birth between the spouses was considered an important principle among the reigning houses and high nobility. Morganatic marriage has not been and is not possible in jurisdictions that do not allow sufficient freedom of contracting, as it is an agreement containing that pre-emptive limitation to the inheritance and property rights of the wife and the children. Marriages have never been considered morganatic in any part of the United Kingdom.
 The payment from the groom to the bride is a mandatory condition for all valid Muslim marriages: a man must pay mahr to his bride. It is the duty of the husband to pay as stated in the Qu'ran (Sura Al-Nisaa’ verses 4 and 20–24), although often his family may assist, and by agreement can be in promissory form, i.e. in the event the husband pronounces talaq.[17] It is considered a gift which the bride has to agree on. The mahr can be any value as long as it is agreed upon by both parties. When the groom gives his bride the mahr, it becomes her property. While the mahr is usually in the form of cash, it may also be real estate or a business.
 The mahr is of assistance to a wife in times of financial need, such as a divorce or desertion by the husband.  If the mahr is in promissory form then it becomes payable if the husband initiates a divorce. If it was previously paid, the wife is entitled to keep her mahr. However, if the woman initiates the divorce (in the procedure called khula), the circumstances of the breakup become relevant. If the divorce is sought for cause (such as abuse, illness, impotence, or infidelity), the woman is generally considered to have the right to keep the mahr; however, if the divorce is not sought for a generally accepted cause, the husband may request its return. [citation needed]
 According to the Kitáb-i-Aqdas, the Baháʼí Faith's most holy book, the dower is paid from the groom to the bride. The dower, if the husband lives in a city, is nineteen mithqáls (approx. 2.2 troy ounces) of pure gold, or, if the husband lives outside a city, the same amount in silver.
"
Daniel Parke Custis,https://en.wikipedia.org/wiki/Daniel_Parke_Custis#Estate,"
 Daniel Parke Custis (October 15, 1711[1] – July 8, 1757) was an American planter and politician who was the first husband of Martha Dandridge. After his death, his widow, Martha Dandridge Custis married George Washington, who later became the first president of the United States.
 Custis was born in York County, Virginia, on October 15, 1711. He was one of two children of John Custis IV (1678–1749), a powerful member of Virginia's Governor's Council, and Frances Parke Custis. The Custis family was one of the wealthiest and most socially prominent of Virginia.[2] Custis' mother, Frances, was the daughter of Daniel Parke, a political enemy of the Custises.[3]
 As Daniel Custis was the sole male heir in the Custis family, he inherited the Southern plantations owned by his father.[4] However, Custis did not choose to take a leading role in colonial Virginia politics.
 At the age of 37, Custis met 16-year-old Martha Dandridge at the St. Peter's Church where Martha attended and Custis was a vestryman.[5][6] His father, John Custis, disapproved of the relationship, but eventually relented. After a two-year courtship, Custis and Dandridge were married on May 15, 1750.[7] The couple lived at Custis's plantation called the White House in New Kent County, Virginia.[4]
 They had four children:[8]
 Custis died on July 8, 1757, in New Kent County, Virginia, with some historians stating the cause of death as a heart attack,[9][10] but others stating that he died from a severe throat infection.[11]
 Custis is buried in the graveyard of the Bruton Parish Church in Williamsburg, Virginia beside two of his children, Daniel Parke Custis, Jr., and Frances Parke Custis.[12] Eighteen months after Custis died, his widow Martha married George Washington on January 6, 1759.[5]
 As Custis died intestate, or ""without a will"", his widow Martha received the lifetime use of one third of his property (known as a ""dower share""),[13] and the other two thirds were held in trust for their children. The January 1759 Custis estate also included at least 85 slaves.[14] According to the Mount Vernon slave census, by 1799 the dower share included 153 slaves. The October 1759 Custis estate inventory listed 17,779 acres (71.95 km2), or 27.78 square miles of land, spread over five counties.[15]
 Upon Martha Custis' marriage to George Washington in 1759, her dower share came under his control, pursuant to the common law doctrine of seisin jure uxoris. He also became guardian of her two minor children, and administrator of the Custis estate. John Parke Custis was the only child to reach his majority, upon which he inherited the non-dower two-thirds of his father's estate.
 Upon George Washington's death on December 14, 1799, the dower share and slaves reverted to Martha. Through a provision in his will, Washington directed that his 124 slaves be freed following his wife's death.[16] As Washington stated in his will, he ""earnestly wished"" to free his own slaves at the time of his death but acknowledged that doing so would create ""insuperable difficulties"" because they had intermarried with Martha's ""dower negroes,"" over whom he had no authority. He also believed that it would ""excite the most painful sensations"" and ""disagreeable consequences"" to attempt to separate them.[17]
 Washington's slaves were not part of the Custis estate, and Martha had no legal power to free them or the dower slaves, but they were freed at her request on January 1, 1801. The principal reason that Martha gave for requesting that her husband's slaves be set free is that she was concerned about her personal safety. Washington's slaves, having found out that they would be free upon her death, were suspected of wanting to hasten her death. They were also perceived as being restive and were believed to have been the cause of several suspicious fires on the Mount Vernon estate.[18]
 When Martha died on May 22, 1802, her dower share reverted to the Custis estate. Because of Martha Washington's dower share, the estate could not be liquidated for more than 45 years. Martha's dower share was eventually divided between John Parke Custis's widow, Eleanor Calvert Custis Stuart, and their four children. Martha also bequeathed Elisha, the one slave she owned herself, to her grandson George Washington Parke Custis.
"
"Norborne Berkeley, 4th Baron Botetourt","https://en.wikipedia.org/wiki/Norborne_Berkeley,_4th_Baron_Botetourt","

 Norborne Berkeley, 4th Baron Botetourt (c. 1717 – 15 October 1770) was a British Tory politician and colonial administrator who served as the governor of Virginia from 1768 to 1770, when he died in office.[1][2]
 While serving as rector at the College of William & Mary, Berkeley endowed the creation of the Botetourt Medal, an award to incentivize student scholarship.[3]: 147–148  After his death, the Virginia General Assembly commissioned Richard Hayward to produce Lord Botetourt, a marble statue depicting Berkeley that stood in the Capitol in Williamsburg. The original survives on the campus of the college, while a replica stands in front of the college's Wren Building.
 Norborne Berkeley was born about 1717, the only son of John Symes Berkeley of Stoke Gifford, Gloucestershire by his second wife Elizabeth Norborne, a daughter and co-heiress of Walter Norborne of Calne, Wiltshire and the widow of Edward Devereux, 8th Viscount Hereford. The Berkeleys of Stoke Gifford were descended from Maurice de Berkeley (d.1347), who died at the Siege of Calais, who had acquired the manor of Stoke Gifford in 1337, the second son of Maurice de Berkeley, 2nd Baron Berkeley, 7th feudal baron of Berkeley (1271–1326), Maurice the Magnanimous, of Berkeley Castle. His descendant Sir Thomas Berkeley (d.1361) of Uley, Gloucestershire married Katherine Botetourt (d.1388), a daughter and co-heiress of John Botetourt, 2nd Baron Botetourt. His son and heir was Sir Maurice Berkeley (1358-1400), of Uley and Stoke Gifford, MP for Gloucestershire in 1391.[4]
 In 1726, Berkeley was admitted to Westminster School. He succeeded his father to Stoke Park in Stoke Gifford in 1736 and remodelled both the house (now known as the Dower House) and the gardens in the 1740s and 1750s with the help of the designer Thomas Wright of Durham.
 He was appointed Colonel of the newly raised South Gloucestershire Militia and commanded it from 1758 to 1766.[5]
 His political career began in 1741 when he was elected to the House of Commons as a knight of the shire for Gloucestershire, a seat he held until 1763.[i] Considered a staunch Tory, Berkeley's fortunes were boosted considerably on the accession of George III in 1760, when he was appointed a Groom of the Bedchamber and in 1762 (until 1766) Lord Lieutenant of Gloucestershire. In 1764, almost 400 years after the title went into abeyance through lack of direct heirs, he successfully claimed the title of Baron Botetourt as the lineal descendant of Maurice de Berkeley (d. 1361) and his wife Catherine de Botetourt. He thus took a seat in the House of Lords as the 4th Baron de Botetourt, and in 1767 was appointed a Lord of the Bedchamber to George III.
 The Berkeley family owned liberties in the Kingswood coalfield. When William Champion expanded his copper-smelting works at Warmley in 1761, he proposed to local coal owners, also including Charles Whittuck of Hanham Hall  and Charles Bragge later Lord Bathurst, that they would supply his works with coal as a monopoly, excluding competition from the other local copper and brass makers, in exchange for partnerships in his new Warmley Company.[6] The large coal owners took this opportunity,[7] and construction began on the new furnaces. However the competing Brass Wire Company, the 'Old Bristol Company' was still able to obtain enough coal locally from small collieries who leased from the larger coal lords. The coal prices paid by the Old Bristol company,  including advance payments, even encouraged development of these small pits, with new horse-driven winding engines and even talk of the new steam engines for mine drainage. The monopoly plan did not succeed and the market for both coal and copper was saturated. By 1765 the new company had grown in capacity, but was encountering financial difficulties.[7] The major shareholders were Champion, the new Baron Botetourt, Bragge and Whittuck. Other local landowners and bankers, including Botetourt's coal viewer Charles Arthur, held smaller holdings but the company was under-capitalised; a planned share capital of £50,000 had only been subscribed to £29,000.[8] Efforts were made to re-organise the company in order to bring in more funds by making the existing shares transferable and so saleable through the stock market, but these were complicated, long-winded and had to be carried out in secrecy from the competitors.[ii][7] In 1768, the Company began to collapse. Champion, fearing a collapse, was discovered having tried to secretly withdraw some of his capital and was then dismissed from the company that he had founded.[7] Bragge wrote to Botetourt that he had been ""completely ruined by the consequence of my former infatuation"". Botetourt was himself in debt, his holdings in the Warmley Company were finally tradeable but now almost worthless and he was in no position to subscribe further money to shore up the company. He fled to America.[9]
 Despite having fled in 1768 to avoid his debts in England, Botetourt was still in political favour and prospered in America, being appointed Governor of Virginia. In Virginia, he acquired ownership over several slaves, including an enslaved woman named Hannah.
 The final Treaty was signed on 5 November 1768 which established a Line of Property following the Ohio River that gave the Kentucky portion of the Virginia Colony to the British Crown, as well as most of what is now West Virginia. The treaty also settled land claims between the Iroquois and the Penn family; the lands thereby acquired by American colonists in Pennsylvania were known as the New Purchase. This new Treaty sparked requests for additional surveys to be completed in the region.
 In a letter addressed to Berkeley dated 23 December 1768, Berkeley received a petition from forty signatories requesting for leave to take up and survey forty-five thousand acres of land lying on the eastern side of the Ohio River on the lower side of the Little Kanawha River having lately been recognized by the Six Nations of Indians. The names of the requestees were: 
George Rogers, John Winston, Phillip Pendleton, John Hawkins, William Plumer Thurston, John Todd, John Rice, Nathaniel Pendleton, Bernard Moore, William Overton, Winston Joseph Rogers, John Rogers, William Smith, Augustine Moore, John Pendleton, James Winston, Lewis Webb, Benjamin Lewis, Henry Pendleton, John Page Jr., Warner Lewis Jr., Thomas Jefferson, Thomas Strachan, John Walker, Alexander Donald, John Johnson, Patrick Morton, Richard Surls, Joseph Coleman, Ambrose Powell, James Boyd, Edward Green, Edward Brown, Thomas Dowel, John McColley, Peter Ferguson, John Sutton, Joseph Hail, Edward Baber, William Shinall, Thomas White, William Dandridge Jr., Isaac Davis, Mordecai Hord, and William Carr.[10]
 He died in Williamsburg on 15 October 1770, after an illness lasting several weeks. Botetourt never married and left no legitimate heirs.[11][12][13][14] Stoke Park passed to his sister Elizabeth, who continued his improvements.
 A statue of Botetourt was placed in the Capitol in Williamsburg in 1773. The Capital of Colonial Virginia was located in Williamsburg from 1699 until 1780, but at the urging of Governor Thomas Jefferson was moved to Richmond for security reasons during the American Revolution. In 1801 the statue of Botetourt was acquired by the College of William and Mary and moved to the campus from the former Capitol building. Barring a brief period during the Civil War when it was moved to the Public Asylum for safety, it stood in the College Yard until 1958 when it was removed for protection from the elements, and then in 1966 was installed in the new Earl Gregg Swem Library, in the new Botetourt Gallery. In 1993, as the college celebrated its tercentenary, a new bronze statue of Botetourt by William and Mary alumnus Gordon Kray was installed in the College Yard in front of the Wren Building, in the place occupied for generations by the original.[15]
 Botetourt County, Virginia, was named in Botetourt's honour. Historians also believe that Berkeley County, West Virginia, and the town of Berkeley Springs, both now in West Virginia, were also named in his honour, or possibly that of another popular colonial governor, Sir William Berkeley.[16]
 Lord Botetourt High School in the town of Daleville in Botetourt County, Virginia, is also named for him, as is the Botetourt Dorm Complex at The College of William and Mary. Two statues also adorn the campus of The College of William and Mary. Gloucester County, Virginia has an elementary school named for the governor.  Both Richmond, Virginia and Norfolk, Virginia have streets named in his honour.
"
Kanawha River,https://en.wikipedia.org/wiki/Kanawha_River,"
 The Kanawha River (/kəˈnɔːə/ kə-NAW-ə) is a tributary of the Ohio River, approximately 97 mi (156 km) long, in the U.S. state of West Virginia. The largest inland waterway in West Virginia, its watershed has been a significant industrial region of the state since early in the 19th century.
 It is formed at the town of Gauley Bridge in northwestern Fayette County, approximately 35 mi (56 km) SE of Charleston, by the confluence of the New and Gauley rivers 2 mi upstream from Kanawha Falls. The waterfall is 24 ft high and has been a barrier to fish movement for more than 1 million years.[4]: 13  The river flows generally northwest, in a winding course on the unglaciated Allegheny Plateau, through Fayette, Kanawha, Putnam, and Mason counties, past the cities of Charleston and St. Albans, and numerous smaller communities. It joins the Ohio at Point Pleasant. An environmental overview and summary of natural and human factors affecting water quality in the watershed was published in 2000.[5]
 Paleo-Indians, the earliest indigenous peoples, lived in the valley and the heights by 10,000 BC as evidenced by archaeological artifacts such as Clovis points. A succession of prehistoric cultures developed, with the Adena culture beginning the construction of numerous skilled earthwork mounds and enclosures more than 2000 years ago. Some of the villages of the Fort Ancient culture survived into the times of European contact.
 The area was a place of competition among historical American Indian nations. Invading from their base in present-day New York, the Iroquois drove out or conquered Fort Ancient culture peoples, as well as such tribes as the Huron and Conoy. By right of conquest, the Iroquois, Lenape (Delaware), and Shawnee reserved the area as a hunting ground. They resisted European-American settlement during the colonial years. Eventually the settlers took over by right of conquest.
 The watershed contains significant deposits of coal and natural gas. As of 1998, about 7 percent of the coal mined in the United States came from the Kanawha River watershed.[4]: 3 
A thriving chemical industry along the river banks provides a significant part of the local economy.
 In colonial times, the wildly fluctuating level of the river prevented its use for transportation. The removal of boulders and snags on the lower river in the 1840s allowed navigation, which was extended upriver by the construction of locks and dams by the U.S. Army Corps of Engineers starting in 1875. By 1898 the project made the Kanawha the first fully controlled river navigation system in America.[6] The river is now navigable for barge traffic[7] to Deepwater, an unincorporated community about 20 miles (32 km) upriver from Charleston.
 Multiple West Virginia state record fish were caught along the Bluestone Lake.[8][9][10]
 In addition to the New and Gauley River headwaters, the Kanawha is joined at Charleston by the Elk River, at St. Albans by the Coal River, and at Poca by the Pocatalico River.
 Major impoundments in the Kanawha River watershed include Claytor Lake and Bluestone Lake on the New River, Summersville Lake on the Gauley River, and Sutton Lake on the Elk River. Collectively, they have a capacity of 14 percent of the average annual flow at Charleston.[5]: 29  The reservoirs on New and Gauley Rivers reduced the estimated 100-year flood discharge under 20th-century climate by about half at Kanawha Falls.[11]
 Kanawha Falls is the upstream limit of several fish species that broadly inhabit the Ohio River watershed.[4]: 12–14 [12] 
Above the waterfall, the watershed has fewer fish species overall, a relatively high number of species found nowhere else in the world, and nearly as many non-native species as natives.
 The Kanawha River lends its name to a major geological section of the Appalachian Highlands physiographic region of the United States   called a physiographic region.  A physiographic region is a large portion of land that is grouped by several factors. Each region has similar geology, topography, and groups of plants and animals. There are eight physiographic divisions in the 48 contiguous United States. Each division is divided into provinces, there are 25 provinces in the United States. Each province is then divided into sections, creating 85 different physiographic sections in the United States. The Kanawha section is one of those 85 sections.
 The Kanawha section encompasses areas of Virginia, West Virginia, Pennsylvania, New York and Ohio. The two major cities in the section are Pittsburgh, PA and Charleston, WV.
 ""Ka(ih)nawha"" derives from the region's Iroquoian dialects meaning ""water way"" or ""canoe way"" implying the metaphor, ""transport way"", in the local language. The glottal consonant of the ""ih"" (stream or river, local Iroquois) dropped out as settlers and homesteaders arrived. Some say that it comes from a Shawnee word that means ""new water"" or that its a Catawba word meaning ""friendly brother"".
 The river has also had historical alternate names, alternate spellings and misspellings including Wood's River for the tributary known today as the New River, for Colonel Abraham Wood, an English explorer from Virginia, the first European known to have explored the river in the mid-17th century.
 Archaeological artifacts, such as Clovis points and later projectiles, indicate prehistoric indigenous peoples living in the area from the 12,500 BC era. Peoples of later cultures continued to live along the valley and heights. Those of the Adena culture built at least 50 earthwork mounds and 10 enclosures in the area between Charleston and Dunbar, as identified by an 1882 to 1884 survey by the Bureau of Ethnology (later part of the Smithsonian Institution). Three of their mounds survive in the valley, including Criel Mound at present-day South Charleston, West Virginia. Evidence has been found of the Fort Ancient culture peoples, who had villages that survived to the time of European contact, such as Buffalo and Marmet. They were driven out by Iroquois from present-day New York.
 According to French missionary reports, by the late 16th century, several thousand Huron, originally of the Great Lakes region, lived in central West Virginia. They were partially exterminated and their remnant driven out in the 17th century by the Iroquois' invading from western present-day New York. Other accounts note that the tribe known as Conois, Conoy, Canawesee, or Kanawha were conquered or driven out by the large Seneca tribe, one of the Iroquois Confederacy, as the Seneca boasted to Virginia colonial officials in 1744. The Iroquois and other tribes, such as the Shawnee and Delaware, maintained central West Virginia as a hunting ground. It was essentially unpopulated when the English and Europeans began to move into the area.[14]
 The first white person to travel through Virginia all the way to the Ohio River (other than as a prisoner of the Indians) was Matthew Arbuckle, Sr., who traversed the length of the Kanawha River valley arriving at (what would later be called) Point Pleasant around 1764. In April 1774, Captain Hanson was one of an expedition: ""18th. We surveyed 2,000 acres (8.1 km2) of Land for Col. Washington, bordered by Coal River & the Canawagh...""[15] This area is the lower area of today's St. Albans, West Virginia. After the Treaty of Fort Stanwix, ""The Kanawhas had gone from the upper tributaries of the river which bears their name, to join their kinsmen, the Iroquois in New York; the Shawnee had abandoned the Indian Old Fields of the valley of the South Branch of the Potomac; the Delaware were gone from the Monongahela; the Cherokee who claimed all the region between the Great Kanawha and Big Sandy, had never occupied it."" quoting Virgil A. Lewis (1887), corresponding member of the Virginia Historical Society.[16] The river's name changes to the Kanawha River at the Kanawha Falls. The Treaty of Big Tree between the Seneca nation and the United States established ten reservations. This formal treaty was signed on September 15, 1797.[17] Lewis was granted a large tract of land near the mouth of the Great Kanawha River in the late 18th century.
 The Little Kanawha and the Great Kanawha rivers, the two largest in the state, were named for the American Indian tribe that lived in the area prior to European settlement in the 18th century. Under pressure from the Iroquois, most of the Conoy/Kanawha had migrated to present-day Virginia by 1634, where they had settled on the west side of the Chesapeake Bay and below the Potomac River. They were also known to the colonists there as the Piscataway. They later migrated north to Pennsylvania, to submit and seek protection with the Susquehannock and Iroquois. The spelling of the Indian tribe varied at the time, from Conoys to Conois to Kanawha. The latter spelling was used and has gained acceptance over time.[14]
"
William Crawford (soldier),https://en.wikipedia.org/wiki/William_Crawford_(soldier),"
 William Crawford (September 2, 1722 – June 11, 1782) was an American military officer and surveyor who worked as a land agent alongside George Washington while Washington was a teenager. Crawford fought in the French and Indian War, Lord Dunmore's War and the American Revolutionary War arising to the rank of Colonel. In 1782, his unit was attacked, and while he and his surgeon escaped for less than one day, Crawford was eventually captured where he was tortured and burned at the stake by Crawford's former soldier turned British agent, Simon Girty, and Captain Pipe, a Chief of the Delaware Nation.[1] 
 Crawford was born on September 2, 1722, in Westmoreland County, Virginia.[2] Before a 1995 genealogical study by Allen W. Scholl, his birth year was erroneously estimated to be 1732.[3] He was a son of William Crawford Sr and his wife Honora Grimes,[4] who were Scots-Irish farmers.  William Crawford Sr was a Presbyterian of Scottish descent from Coleraine, Ireland in what is today Northern Ireland and Honora Grimes was a Presbyterian of Scottish descent from Ballymoney, Ireland in what is today Northern Ireland. After his father's death in 1736, Crawford's mother married Richard Stephenson. Crawford had a younger brother, Valentine Crawford, plus five half-brothers and one half-sister from his mother's second marriage.[5] Crawford's first cousin, Benjamin Lewis, led a campaign along with Thomas Jefferson in December 1768 on the need for expanded surveys in the Kanawha and Ohio region after the ratification of the Treaty of Fort Stanwix in November. Crawford under the direction of Washington eventually led subsequent surveys of the Kanawha, Ohio and Kentucky region in the years afterwards through 1773.
 In 1742 Crawford married one Ann Stewart, with whom he had one child, a daughter also named Ann, in 1743.[citation needed]  Apparently she died in childbirth or soon after, and on January 5, 1744, he married Hannah Vance, said to have been born in Pennsylvania in 1723. Their son John married one Effie Grimes; Ophelia married William McCormick, who served as a Captain during the Northwest Indian Wars,[6] and Sarah who married  Major William Harrison who was captured and killed by Captain Pipe. Simon Girty, before he tortured and killed Crawford, lied and told Crawford his son-in-law was alive and being held prisoner by a separate Shawnee, who had already been killed. 
 In 1749, Crawford became acquainted with George Washington, who at the time was still a teenager and just recently appointed as the surveyor of Culpeper County. Crawford, more seasoned, accompanied Washington on several surveying trips. In 1755, Crawford commissioned in the Virginia militia and served in the Braddock expedition holding the rank of ensign. Like Washington, he survived the disastrous Battle of the Monongahela. During the French and Indian War, he served in Washington's Virginia Regiment, guarding the Virginia frontier against Native American raiding parties. In 1758, Crawford was a member of General John Forbes's army which captured Fort Duquesne, where Pittsburgh, Pennsylvania, now stands. He continued to serve in the military, taking part in Pontiac's War in 1763.
 In 1765 Crawford built a cabin on the Braddock Road along the Youghiogheny River in what is now Connellsville, Fayette County, Pennsylvania. His wife and three children joined him there the following year. Crawford supported himself as a farmer and fur trader. When the 1768 Treaty of Fort Stanwix with the Iroquois opened up additional land for settlement, Crawford worked again as a surveyor, locating lands for settlers and speculators. Governor Robert Dinwiddie had promised bounty land to the men of the Washington's Virginia Regiment for their service in the French and Indian War. In 1770 Crawford and Washington travelled down the Ohio River to choose the land to be given to the regiment's veterans. The area selected was near what is now Point Pleasant, West Virginia. Crawford also made a western scouting trip in 1773 under the approval of Lord Dunmore, Governor of Virginia which was led by Thomas Bullitt. The expedition of over 40 men included Joshua Morris, Capt. Matthew Arbuckle, John Alderson, and Colonel John Field. Washington could not accompany them because of the sudden death of his stepdaughter. During the expedition, Arkbuckle and Alderson found the Kanawha Burning Springs,[7] who Thomas Hanson in 1774 described as ""one of the wonders of the world. Put a blaze of pine within 3 or 4 inches of the water, and immediately the water will be in a flame, & Continue so until it is put out by the Force of wind. The Springs are small and boil continually like a Pot on the Fire; the water is black & has a Taste of Nitre."" 
 At the outbreak of Dunmore's War in 1774, Crawford received a promotion to major from Lord Dunmore. He built Fort Fincastle at present Wheeling, West Virginia.[8] He also led an expedition which destroyed two Mingo villages (near present Steubenville, Ohio) in retaliation for Chief Logan's raids into Virginia.
 Crawford's service to Virginia in Dunmore's War was controversial in Pennsylvania, since the colonies were engaged in a bitter dispute over their borders near Fort Pitt. Crawford had been a justice of the peace in Pennsylvania since 1771, first for Bedford County, then for Westmoreland County when it was established in 1773. Beginning in 1776, Crawford served as a surveyor and justice for Virginia's short-lived Yohogania County.[9]
 When the American Revolutionary War began, Crawford initially was commissioned a lieutenant colonel in the 5th Virginia Regiment on February 13, 1776.[10] The 5th Virginia was raised in the counties around Richmond and originally based in Williamsburg,[11] where Crawford joined the regiment to participate in training of the recruits. Later that year, Crawford was promoted to Colonel of the 7th Virginia Regiment to fill a vacancy when Colonel William Daingerfield resigned his command of that unit.[12]
 A number of histories incorrectly state that Crawford raised the 7th Virginia Regiment near Fort Pitt at the beginning of the revolution.  The 7th Virginia initially was raised in southeastern Virginia near Gloucester Court House.[13]   The confusion may be due to Crawford's role in raising another regiment near Fort Pitt, the 13th Virginia, which was redesignated the 9th Virginia in 1778 and later renumbered to the 7th Virginia in 1781 while it was stationed at Fort Pitt.[14]  Crawford commanded the 13th Virginia for a time in 1777.
 Many histories also inaccurately state that Crawford led the 7th Virginia at the Battle of Long Island and the following retreat across New Jersey.[15]  Crawford's own words contradict this viewpoint in a letter written to George Washington from Williamsburg, VA on September 20, 1776: ""I Should have com to new York with those Reget ordred their but the Regt I belong to is Ordred to this place.""[16][17]  Regimental histories of the 7th Virginia[18] along with other historical references[19] also reveal that the 7th Virginia did not participate in the battle of Long Island.  Similar uncertainty surrounds narratives that state Crawford was with Washington at the crossing of the Delaware and the Battles of Trenton and Princeton. Recent historians have sought to correct these inaccuracies, such as H. Ward in his biography of Revolutionary soldiers from Virginia: ""It is disputed whether Crawford served in any part of the New York-New Jersey campaigns of 1776-1777…""[20] 
 Crawford apparently left the command of the 7th Virginia in November 1776.  A farewell letter to Crawford from the officers of the 7th Virginia was published in the Virginia Gazette newspaper on November 22, 1776. He responded with a letter of his own in the same edition of the Gazette, bidding farewell to the 7th Virginia.[21]  
 He returned to his home on the frontier late in 1776 and was actively engaged in raising the 13th Virginia Regiment,[22][23] which was authorized by Congress in September 1776 with recruiting beginning in December 1776 in the District of West Augusta of Virginia[24]   (this region was claimed by Virginia and encompassed parts of present day western Pennsylvania and West Virginia). Crawford wrote to Washington from Fredricktown Maryland on February 12, 1777, to inform him he was coming from the frontier, where the officers of the regiment already had recruited about 500 men. He was on his way to Congress to seek funding for arms and supplies and then planned to immediately return home.[25][26]   The Continental Congress resolved on February 17, 1777: ""That 20,000 dollars be paid to Colonel William Crawford for raising and equipping the regiment under his command, part of the Virginia new levies.""[27]  
 The 13th Virginia, or West Augusta regiment, was raised on the condition that it remain in the West in the event of an Indian War.[28]  However, with Washington's need for reinforcements in the East, the Continental Congress on January 8, 1777, requested the governor of Virginia to order the West Augusta regiment to join Washington in New Jersey.[29]  But with increased attacks on frontier settlements by Native Americans allied with the British in early 1777, a Council of War was held at Fort Pitt on March 24, 1777, that decided the 13th Virginia should not be deployed to the East at that time.[30][31]  Crawford wrote to Congress on April 22, 1777: ""Honorable Sir—Having received orders to join his Excellency General Washington in the Jerseys with the battalion now under my command, which orders I would willingly have obeyed, had not a council of war held at this place (proceedings of which were transmitted to Congress by express) resolve that I should remain here until further orders.""[32]  However, Colonel William Russell was now commander of the 13th Virginia and on June 9, 1777 a detachment of the regiment under Major Charles Simms marched eastward to Philadelphia.[33][34]  Colonel Russell apparently led another detachment of the 13th Virginia eastward in July after he was inoculated for smallpox.[35][36] Likewise, Crawford may have brought a number of recruits with him when he joined Washington in July or August 1777.[37]
 During the Philadelphia campaign, Crawford was placed in command of a scouting detachment as part of the light infantry corps for Washington's army.[38] [39] General William Maxwell commanded the light infantry[40] and Crawford was selected to serve as one of the field officers under Maxwell.  William Walker, a member of the light infantry, described Crawford in the typical attire of a frontier rifleman: ""…Colonel Crawford with his leather hunting shirt, pantaloons and Rifle…""[41] The British forces, after landing near Head of Elk, Maryland, approached Philadelphia from the south through Delaware. Washington sent Maxwell's Light Infantry to delay the British march along the main road to Wilmington at a crossing of the Christiana Creek known as Cooch's Bridge. On September 3, 1777, the fighting was intense between Maxwell's light infantry and the British vanguard, as John Chilton of the 3rd Virginia Regiment recorded in his Diary: ""3d Septr. - The enemy advanced as high as the red Lion, they were met with by our advanced party under Colo Crawford – the engagement was pretty hot. several on each side was wounded and some slain.""[42] Being outnumbered and outgunned, the light infantry was driven from Cooch's Bridge and fell back to the main American lines near Wilmington, Delaware.
 Crawford continued to serve in the light infantry corps at the battles of Brandywine and Germantown.  On October 11, 1777, militia units from the Virginia counties of Prince William, Culpepper, Loudoun, and Berkley were formed into a brigade and placed under Crawford's command.[43]   However, as the war on the western frontier intensified late in 1777, Crawford was transferred to the Western Department of the Continental Army. On November 20, 1777, Congress requested that Washington ""send Col. Wm. Crawford to Pittsburg to take command, under Brig. Gen. Hand, of the Continental troops and militia in the Western Department.""[44]  He served at Fort Pitt under Generals Edward Hand and Lachlan McIntosh. Crawford was present at the Treaty of Fort Pitt in 1778, and helped to build Fort Laurens and Fort McIntosh that year. Resources were scarce on the frontier, however, and Fort Laurens was abandoned in 1779. In 1780, Crawford visited Congress to appeal for more funds for the western frontier. In 1781, he retired from military service.
 In 1782, General William Irvine persuaded Crawford to lead an expedition against enemy Native American villages along the Sandusky River. Before leaving, on May 16 he made out his will and testament.[45] His son John Crawford, his son-in-law William Harrison, and his nephew and namesake William Crawford also joined the expedition.
 Crawford led about 500 volunteers deep into American Indian territory with the hope of surprising them. However, the Indians and their British allies at Detroit had learned about the expedition in advance, and brought about 440 men to the Sandusky to oppose the Americans. After a day of indecisive fighting, the Americans found themselves surrounded. During a confused retreat, Crawford and dozens of his men were captured. The Delaware Nation tribes tortured and executed many of the men in retaliation for the Gnadenhutten massacre earlier in the year, in which 96 peaceful Christian Indian men, women, and children had been murdered by Pennsylvanian militiamen. Crawford's execution was brutal, and was overseen by Simon Girty, a former soldier under Crawford's command who deserted his post after being wanted for high treason in 1778 for acting as an enemy of the state while serving in the militia[46] (the order was signed by Timothy Matlack, and George Bryan).  Crawford was tortured for at least two hours before he was burned at the stake. His nephew and son-in-law were also captured and executed. The war ended shortly thereafter, but Crawford's horrific execution was widely publicized in the United States, worsening the already strained relationship between Native Americans and European Americans.
 In 1982, the site of Colonel Crawford's execution was added to the National Register of Historic Places.  In 1877, the Pioneer Association of Wyandot County erected an 8.5 ft (2.6 m) Berea sandstone monument near the site.  The Ohio Historical Society also has an historical marker nearby.
 Crawford County, Ohio, Crawford County, Pennsylvania, Crawford County, Michigan, and Crawford County, Indiana, are named for William Crawford. So too is Colonel Crawford High School in North Robinson, Ohio.
 There is a replica of Crawford's cabin in Connellsville, Pennsylvania.
 Crawford's half-brother, James Stevenson, was a member of the Pennsylvania State Senate.[47]
 
"
"Frederick County, Virginia","https://en.wikipedia.org/wiki/Frederick_County,_Virginia","
.mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct,.mw-parser-output .geo-inline-hidden{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}39°13′N 78°16′W﻿ / ﻿39.21°N 78.26°W﻿ / 39.21; -78.26
 Frederick County is located in the Commonwealth of Virginia. As of the 2020 census, the population was 91,419.[1] Its county seat is Winchester.[2] The county was formed in 1743 by the splitting of Orange County. It is Virginia's northernmost county. Frederick County is included in the Winchester, VA-WV Metropolitan Statistical Area, which is also included in the Washington-Baltimore-Northern Virginia, DC-MD-VA-WV-PA Combined Statistical Area.
 The area that would become Frederick County, Virginia, was inhabited and transited by various indigenous peoples for thousands of years before European colonization.
 Colonization efforts began with the Virginia Company of London, but European settlement did not flourish until after the company lost its charter and Virginia became a royal colony in 1624. In order to stimulate migration to the colony, the headright system was used. Under this system, those who funded an emigrant's transportation costs (not the actual colonizers) were compensated with land.[3] In 1649 the exiled King Charles II granted several acres of colonial Virginia lands to ""seven loyal supporters"", including Lord Fairfax. The Fairfax lands passed to Thomas Fairfax, 5th Lord Fairfax of Cameron (1657-1710), who married the daughter of Thomas Colepeper, who also owned several acres of land. After their son, Lord Thomas Fairfax, inherited the combined grants, he controlled over 5,000,000 acres of land in Virginia, including much of the land that became Frederick County.[4]
 Frederick County was created from Orange County in 1738, and was officially organized in 1743.[5] The Virginia Assembly named the new county for Frederick Louis, Prince of Wales[6] (1707–1751), the eldest son of King George II of Great Britain. At that time, ""Old Frederick County"" encompassed all or part of four counties in present-day Virginia and five in present-day West Virginia:
 As commanding officer of the new Colonial Virginia regiment in 1754, Colonel George Washington located his headquarters in Winchester before and during the French and Indian War. He resigned from military service in 1758. He represented Frederick County in his first elective office, having been elected to the House of Burgesses in 1758 and 1761.
 Seventeen years later, on June 15, 1775, the Continental Congress ""elected"" George Washington as commander-in-chief of the yet-to-be-created Continental Army. He accepted the appointment the next day.[7] This preceded the Congress's declaration of independence and the outbreak of the American Revolutionary War.
 Winchester was a site of volatile conditions during the Civil War of 1861–1865, with control shifting between the Confederate and Union armies on average once every three weeks during the war. Many battles were fought in Frederick County. Some of those battles included:
 The first constitution of West Virginia provided for Frederick County to be added to the new state if approved by a local election.[8]
Unlike neighboring Berkeley and Jefferson counties, Frederick County remained in Virginia; as it was occupied by the Confederate army, no vote was permitted to ascertain the residents' wishes.[9]
 Four (alkaline, saline, chalybeate, and sulphured) types of mineral water springs naturally occur on the land that would later be named Rock Enon Springs.[10]: 868  The area was once called Capper Springs, named for area settler John Capper.[11]: 57  William Marker bought the 942 acres (381 ha) in 1856 and built a hotel, the first building of the Rock Enon Springs Resort. It survived the American Civil War.[12] On March 24, 1899, the Shenandoah Valley National Bank purchased the property for $3,500.[13]: 9  During the summer of 1914 botanists found a variety of ferns on the property: polypodium vulgare, phegopteris hexagonoptera, adiantum pedatum, pteris aquilina, and cheilanthes lanosa.[14]
 The idea that soaking in the natural spring water had medical value made this and other springs popular tourist destinations through the early 20th century.[15]
 In 1944, people no longer had as much faith in the springs, and there was much more competition for tourists at other sites. Due to declining business, the Glaize family sold the property to the Shenandoah Area Council. They adapted the resort to operate as a Boy Scout site, Camp Rock Enon.[12] In 1944 the 5 acres (0.020 km2) Miller Lake was created by adding a 200 feet (61 m) earth dam across Laurel Run using equipment, owned by the Federal fish hatchery in Leestown.[16]: M4  In 1958 ""walnut, chestnut and persimmon trees"" were planted on the property.[17]: 50 
 According to the U.S. Census Bureau, the county has a total area of 416 square miles (1,080 km2), of which 414 square miles (1,070 km2) is land and 2 square miles (5.2 km2) (0.5%) is water.[18] This is the northernmost county in the Commonwealth of Virginia.
 As of the census[25] of 2000, there were 59,209 people, 22,097 households, and 16,727 families residing in the county. The population density was 143 inhabitants per square mile (55/km2). There were 23,319 housing units at an average density of 56 units per square mile (22 units/km2). The racial makeup of the county was 94.99% White, 2.62% Black or African American, 0.16% Native American, 0.66% Asian, 0.02% Pacific Islander, 0.56% from other races, and 1.01% from two or more races. 1.70% of the population were Hispanic or Latino of any race.
 There were 22,097 households, out of which 36.60% had children under the age of 18 living with them, 62.50% were married couples living together, 8.80% had a female householder with no husband present, and 24.30% were non-families. 19.20% of all households were made up of individuals, and 6.80% had someone living alone who was 65 years of age or older. The average household size was 2.64 and the average family size was 3.02.
 In the county, the population was spread out, with 26.40% under the age of 18, 7.00% from 18 to 24, 31.90% from 25 to 44, 24.10% from 45 to 64, and 10.60% who were 65 years of age or older. The median age was 37 years. For every 100 females, there were 100.10 males. For every 100 females aged 18 and over, there were 96.70 males.
 The median income for a household in the county was $46,941, and the median income for a family was $52,281. Males had a median income of $35,705 versus $25,046 for females. The per capita income for the county was $21,080. About 4.00% of families and 6.40% of the population were below the poverty line, including 7.30% of those under age 18 and 6.90% of those age 65 or over.
 Frederick is represented by Republican Jill Holtzman Vogel (R), in the Virginia Senate, Wendy Gooditis (D), Chris Collins (R), and Dave LaRock (R), in the Virginia House of Delegates, and Ben Cline (R) in the U.S. House of Representatives.
 Frederick County is served by Frederick County Public Schools, which includes several elementary, middle, and high schools. Frederick County is also part of the region served by the Mountain Vista Governor's School, which offers upper-level classes to intellectually gifted high school students.
 Although designated as the county seat, Winchester, like all cities under Virginia law, is an independent city, politically independent of any county.
"
Virginia House of Burgesses,https://en.wikipedia.org/wiki/Virginia_House_of_Burgesses,"The House of Burgesses (/ˈbɜːrdʒəsɪz/) was the lower house of the Virginia General Assembly from 1619 to 1776. It existed during the colonial history of the United States when Virginia was a British colony. From 1642 to 1776, the House of Burgesses was an important feature of Virginian politics, alongside the Crown-appointed colonial governor and the Virginia Governor's Council, the upper house of the General Assembly.[1]
 When Virginia declared its independence from the Kingdom of Great Britain at the Fifth Virginia Convention in 1776 and became the independent Commonwealth of Virginia, the House of Burgesses was transformed into the House of Delegates, which continues to serve as the lower house of the General Assembly.[2]
 Burgess originally referred to a freeman of a borough, a self-governing town or settlement in England.
 The Colony of Virginia was founded by a joint-stock company, the Virginia Company, as a private venture, though under a royal charter. Early governors provided the stern leadership and harsh judgments required for the colony to survive its early difficulties.[citation needed]
 Early crises with famine, disease, Native American raids, the need to establish cash crops, and lack of skilled or committed labor, meant the colony needed to attract enough new and responsible settlers if it were to grow and prosper.[citation needed]
 To encourage settlers to come to Virginia, in November 1618 the Virginia Company's leaders gave instructions to the new governor, Sir George Yeardley, which became known as ""the great charter.""[3]
 It established that immigrants who paid their own way to Virginia would receive fifty acres of land and not be mere tenants. The civil authority would control the military. In 1619, based on the instructions, Governor Yeardley initiated the election of 22 burgesses by the settlements and Jamestown. They, together with the royally appointed Governor and six-member Council of State, would form the first General Assembly as a unicameral body.[4]
 The governor could veto its actions and the Company still maintained overall control of the venture, but the settlers would have a limited say in the management of their own affairs, including their finances.[4]
 A House of Assembly was created at the same time in Bermuda (which had also been settled by the Virginia Company, and was by then managed by its offshoot, the Somers Isles Company) and held its first session in 1620.[citation needed]
 A handful of Polish craftsmen, brought to the colony to supply skill in the manufacture of pitch, tar, potash, and soap ash, were initially denied full political rights. They downed their tools in protest but returned to work after being declared free and enfranchised, apparently by agreement with the Virginia Company.[5]
 On July 30, 1619, Governor Yeardley convened the Virginia General Assembly as the first representative legislature in the Americas for a six-day meeting at the new timber church on Jamestown Island, Virginia. The unicameral Assembly was composed of the Governor, a Council of State appointed by the Virginia Company, and the 22 locally elected representatives.[6][7]
 The Assembly's first session of July 30, 1619, was cut short by an outbreak of malaria and adjourned after five days.[8] On the third day of the assembly, the assembly's Journal noted ""Mr. Shelley, one of the Burgesses, deceased.""[9] Twenty-two (22) members were sent to the assembly from the following constituencies:[10] 
 The latter two burgesses were excluded from the assembly because John Martin refused to give up a clause in his land patent that exempted his borough ""from any command of the colony except it be aiding and assisting the same against any foreign or domestic enemy.""[11][12]
 Especially after the massacre of almost 400 colonists on March 22, 1622, by Native Americans, and epidemics in the winters before and after the massacre, the governor and council ruled arbitrarily, showing great contempt for the assembly and allowing no dissent.[13]
 By 1624, the royal government in London had heard enough about the problems of the colony and revoked the charter of the Virginia Company. Virginia became a crown colony and the governor and council would be appointed by the Crown. Nonetheless, the Assembly maintained management of local affairs with some informal royal assent, although it was not royally confirmed until 1639.[4]
 In 1634, the General Assembly divided the colony into eight shires (later renamed counties) for purposes of government, administration, and the judicial system. By 1643, the expanding colony had 15 counties. All of the county offices, including a board of commissioners, judges, sheriff, constable, and clerks, were appointed positions. Only the burgesses were elected by a vote of the people. Women had no right to vote. Only free and white men originally were given the right to vote, by 1670 only property owners were allowed to vote.[4]
 In 1642, Governor William Berkeley urged the creation of a bicameral legislature which the Assembly promptly implemented; the House of Burgesses was thus formed and met separately from the Council of State.[14][page needed]
 In 1652, the parliamentary forces of Oliver Cromwell forced the colony to submit to being taken over by the English government. Again, the colonists were able to retain the General Assembly as their governing body. Only taxes agreed to by the assembly were to be levied. Still, most Virginia colonists were loyal to Prince Charles and were pleased with his restoration as King Charles II in 1660. He went on to directly or indirectly restrict some of the liberties of the colonists, such as requiring tobacco to be shipped only to England, only on English ships, with the price set by the English merchant buyers;[15] but the General Assembly remained.[4]
 A majority of the members of the General Assembly of 1676 were supporters of Nathaniel Bacon. They enacted legislation designed to further popular sovereignty and representative government and to equalize opportunities.[16] Bacon took little part in the deliberations since he was busy fighting the Native Americans.[17]
 In 1691, the House of Burgesses abolished the enslavement of Native peoples; however, many Powhatans were held in servitude well into the 18th century.[18]
 The statehouse in Jamestown burned down for the fourth time on October 20, 1698. The General Assembly met temporarily in Middle Plantation, 11 miles (18 km) inland from Jamestown, and then in 1699 permanently moved the capital of the colony to Middle Plantation, which they renamed Williamsburg.[19]
 The French and Indian War in North America from 1754 to 1763 resulted in local colonial losses and economic disruption. Higher taxes were to follow, and adverse local reactions to these and how they were determined would drive events well into the next decade.[20]
 In 1764, desiring revenue from its North American colonies, Parliament passed the first law specifically aimed at raising colonial money for the Crown. The Sugar Act increased duties on non-British goods shipped to the colonies.[21] The same year, the Currency Act prohibited American colonies from issuing their own currency.[22] These angered many American colonists and began colonial opposition with protests. By the end of the year, many colonies were practicing non-importation, a refusal to use imported British goods.[21]
 In 1765, the British Quartering Act, which required the colonies to provide barracks and supplies to British troops, further angered American colonists; and to raise more money for Britain, Parliament enacted the Stamp Act on the American colonies, to tax newspapers, almanacs, pamphlets, broadsides, legal documents, dice, and playing cards.[23] American colonists responded to Parliament's acts with organized protest throughout the colonies. A network of secret organizations known as the Sons of Liberty was created to intimidate the stamp agents collecting the taxes, and before the Stamp Act could take effect, all the appointed stamp agents in the colonies had resigned.[24]  The Massachusetts Assembly suggested a meeting of all colonies to work for the repeal of the Stamp Act, and all but four colonies were represented.[25] The colonists also increased their non-importation efforts,[26][better source needed] and sought to increase in local production.
 In May 1765, Patrick Henry presented a series of resolves that became known as the Virginia Resolves, denouncing the Stamp Act and denying the authority of the British parliament to tax the colonies, since they were not represented by elected members of parliament. Newspapers around the colonies published all his resolves, even the most radical ones which had not been passed by the assembly.[27] The assembly also sent a 1768 Petition, Memorial, and Remonstrance to Parliament.[citation needed]
 From 1769–1775 Thomas Jefferson represented Albemarle County as a delegate in the Virginia House of Burgesses.[28] He pursued reforms to slavery and introduced legislation allowing masters to take control over the emancipation of slaves in 1769, taking discretion away from the royal Governor and General Court. Jefferson persuaded his cousin Richard Bland to spearhead the legislation's passage, but the reaction was strongly negative.[29]
 In 1769 the Virginia House of Burgesses passed several resolutions condemning Britain's stationing troops in Boston following the Massachusetts Circular Letter of the previous year; these resolutions stated that only Virginia's governor and legislature could tax its citizens.[30][page needed] The members also drafted a formal letter to the King, completing it just before the legislature was dissolved by Virginia's royal governor.[31]
 In 1774, after Parliament passed the Boston Port Act to close Boston Harbor, the House of Burgesses adopted resolutions in support of the Boston colonists which resulted in Virginia's royal governor, John Murray, 4th Earl of Dunmore, dissolving the assembly. The burgesses then reassembled on their own and issued calls for the first of five Virginia Conventions. These conventions were essentially meetings of the House of Burgesses without the governor and Council, Peyton Randolph the Speaker of the House would serve as the President of the convention, and they would elect delegates to the Continental Congress.[2] The First Continental Congress passed their Declaration and Resolves, which inter alia claimed that American colonists were equal to all other British citizens, protested against taxation without representation, and stated that Britain could not tax the colonists since they were not represented in Parliament.[32][page needed]
 In 1775 the burgesses, meeting in conventions, listened to Patrick Henry deliver his ""give me liberty or give me death!"" speech and raised regiments. The House of Burgesses was called back by Lord Dunmore one last time in June 1775 to address British Prime Minister Lord North's Conciliatory Resolution. Randolph, who was a delegate to the Continental Congress, returned to Williamsburg to take his place as Speaker. Randolph indicated that the resolution had not been sent to the Congress (it had instead been sent to each colony individually in an attempt to divide them and bypass the Continental Congress). The House of Burgesses rejected the proposal, which was also later rejected by the Continental Congress.[33] The burgesses formed a Committee of Safety to take over governance in the absence of the royal governor, Dunmore, who had organized loyalists forces but after defeats, he took refuge on a British warship.[34]
 In 1776 the House of Burgesses ended. The final entry in the Journals of the House of Burgesses is ""6th of May. 16 Geo. III. 1776 … FINIS.""[35] Edmund Pendleton, a member of the House of Burgesses (and President of the Committee of Safety) who was present at the final meeting, wrote in a letter to Richard Henry Lee on the following day, ""We met in an assembly yesterday and determined not to adjourn, but let that body die."" Later on the same morning, the members of the fifth and final Virginia Revolutionary Convention met in the chamber of the House of Burgesses in Williamsburg and elected Pendleton its president. The convention voted for independence from Britain.[36] The former colony had become the independent Commonwealth of Virginia and the convention created the Constitution of Virginia with a new General Assembly, composed of an elected Senate and an elected House of Delegates. The House of Delegates acceded to the role of the former House of Burgesses.[2]
 In 1619, the General Assembly first met in the church in Jamestown. Subsequent meetings continued to take place in Jamestown.[37]
 In 1700, the seat of the House of Burgesses was moved from Jamestown to Middle Plantation, near what was soon renamed Williamsburg.[38] The Burgesses met there, first (1700 to 1704) in the Great Hall of what is now called the Wren Building at the College of William and Mary, while the Capitol was under construction. When the Capitol burned in 1747, the legislature moved back into the college until the second Capitol was completed in 1754. The present Capitol building at Colonial Williamsburg is a reconstruction of the earlier of the two lost buildings.[citation needed]
 In 1779, and effective in April 1780, the House of Delegates moved the capital city to Richmond during the American Revolutionary War for safety reasons.[39]
 The House of Burgesses became the House of Delegates in 1776, retaining its status as the lower house of the General Assembly, the legislative branch of the Commonwealth of Virginia. Through the General Assembly and House of Burgesses, the Virginia House of Delegates is considered the oldest continuous legislative body in the New World.[40]
 In honor of the original House of Burgesses, every four years, the Virginia General Assembly traditionally leaves the current Capitol in Richmond and meets for one day in the restored Capitol building at Colonial Williamsburg. The most recent commemorative session (the 26th) was held in January 2016.[41][42]
 In January 2007, the Assembly held a special session at Jamestown to mark the 400th anniversary of its founding as part of the Jamestown 2007 celebration, including an address by then-Vice-President Dick Cheney.[43]
 In January 2019, to mark the 400th anniversary of the House of Burgesses, the Virginia House of Representatives Clerk's Office announced a new Database of House Members called ""DOME"" that ""[chronicles] the 9,700-plus men and women who served as burgesses or delegates in the Virginia General Assembly over the past four centuries.""[44][45][46]
"
Mercantilist,https://en.wikipedia.org/wiki/Mercantilist,"Mercantilism is a nationalist economic policy that is designed to maximize the exports and minimize the imports of an economy. In other words, it seeks to maximize the accumulation of resources within the country and use those resources for one-sided trade. 
 The concept aims to reduce a possible current account deficit or reach a current account surplus, and it includes measures aimed at accumulating monetary reserves by a positive balance of trade, especially of finished goods. Historically, such policies may have contributed to war and motivated colonial expansion.[1] Mercantilist theory varies in sophistication from one writer to another and has evolved over time.
 Mercantilism promotes government regulation of a nation's economy for the purpose of augmenting and bolstering state power at the expense of rival national powers. High tariffs, especially on manufactured goods, were almost universally a feature of mercantilist policy.[2] Before it fell into decline, mercantilism was dominant in modernized parts of Europe and some areas in Africa from the 16th to the 19th centuries, a period of proto-industrialization.[3] Some commentators argue that it is still practised in the economies of industrializing countries[4] in the form of economic interventionism.[5][6][7][8][9]
 With the efforts of supranational organizations such as the World Trade Organization to reduce tariffs globally, non-tariff barriers to trade have assumed a greater importance in neomercantilism.
 Mercantilism became the dominant school of economic thought in Europe throughout the late Renaissance and the early modern period (from the 15th to the 18th centuries). Evidence of mercantilistic practices appeared in early modern Venice, Genoa, and Pisa regarding control of the Mediterranean trade in bullion. However, the empiricism of the Renaissance, which first began to quantify large-scale trade accurately, marked the beginning of mercantilism as a codified school of economic theories.[2]  The Italian economist and mercantilist Antonio Serra is considered to have written one of the first treatises on political economy in his 1613 work, A Short Treatise on the Wealth and Poverty of Nations.[10]
 Mercantilism in its simplest form is bullionism, yet mercantilist writers emphasize the circulation of money and reject hoarding. Their emphasis on monetary metals accords with current[when?] ideas regarding the money supply, such as the stimulating effect of a growing money supply. Fiat money and floating exchange rates have since rendered specie concerns irrelevant. In time, industrial policy supplanted the heavy emphasis on money, accompanied by a shift in focus from the capacity to carry on wars to promoting general prosperity.
 England began the first large-scale and integrative approach to mercantilism during the Elizabethan Era (1558–1603). An early statement on national balance of trade appeared in Discourse of the Common Wealth of this Realm of England, 1549: ""We must always take heed that we buy no more from strangers than we sell them, for so should we impoverish ourselves and enrich them.""[11] The period featured various but often disjointed efforts by the court of Queen Elizabeth (r. 1558–1603) to develop a naval and merchant fleet capable of challenging the Spanish stranglehold on trade and of expanding the growth of bullion at home. Queen Elizabeth promoted the Trade and Navigation Acts in Parliament and issued orders to her navy for the protection and promotion of English shipping.
 Authors noted most for establishing the English mercantilist system include Gerard de Malynes (fl. 1585–1641) and Thomas Mun (1571–1641), who first articulated the Elizabethan system (England's Treasure by Foreign Trade or the Balance of Foreign Trade is the Rule of Our Treasure), which Josiah Child (c. 1630/31–1699) then developed further.
 Numerous French authors helped cement French policy around statist mercantilism in the 17th century, as King Louis XIV (reigned 1643–1715) followed the guidance of Jean Baptiste Colbert, his Controller-General of Finances from 1665 to 1683 who revised the tariff system and expanded industrial policy. Colbertism was based on the principle that the state should rule in the economic realm as it did in the diplomatic, and that the interests of the state as identified by the king were superior to those of merchants and of everyone else. Mercantilist economic policies aimed to build up the state, especially in an age of incessant warfare, and theorists charged the state with looking for ways to strengthen the economy and to weaken foreign adversaries.[12][need quotation to verify]
 In Europe, academic belief in mercantilism began to fade in the late-18th century after the East India Company annexed the Mughal Bengal,[13][14] a major trading nation, and the establishment of British India through the activities of the East India Company,[15] in light of the arguments of Adam Smith (1723–1790) and of the classical economists.[16] French economic policy liberalized greatly under Napoleon (in power from 1799 to 1814/1815). The British Parliament's repeal of the Corn Laws under Robert Peel in 1846 symbolized the emergence of free trade as an alternative system.
 Most of the European economists who wrote between 1500 and 1750 are today generally described as mercantilists; this term was initially used solely by critics, such as Mirabeau and Smith, but historians proved quick to adopt it. Originally the standard English term was ""mercantile system"". The word ""mercantilism"" came into English from German in the early-19th century.
 The bulk of what is commonly called ""mercantilist literature"" appeared in the 1620s in Great Britain.[17] Smith saw the English merchant Thomas Mun (1571–1641) as a major creator of the mercantile system, especially in his posthumously published Treasure by Foreign Trade (1664), which Smith considered the archetype or manifesto of the movement.[18] Perhaps the last major mercantilist work was James Steuart's Principles of Political Economy, published in 1767.[17]
 Mercantilist literature also extended beyond England. Italy and France produced noted writers of mercantilist themes, including Italy's Giovanni Botero (1544–1617) and Antonio Serra (1580–?) and, in France, Jean Bodin and Colbert. Themes also existed in writers from the German historical school from List, as well as followers of the American and British systems of free-trade, thus stretching the system into the 19th century. However, many British writers, including Mun and Misselden, were merchants, while many of the writers from other countries were public officials. Beyond mercantilism as a way of understanding the wealth and power of nations, Mun and Misselden are noted for their viewpoints on a wide range of economic matters.[19]
 The Austrian lawyer and scholar Philipp Wilhelm von Hornick, one of the pioneers of Cameralism, detailed a nine-point program of what he deemed effective national economy in his Austria Over All, If She Only Will of 1684, which comprehensively sums up the tenets of mercantilism:[20]
 Other than Von Hornick, there were no mercantilist writers presenting an overarching scheme for the ideal economy, as Adam Smith would later do for classical economics. Rather, each mercantilist writer tended to focus on a single area of the economy.[21] Only later did non-mercantilist scholars integrate these ""diverse"" ideas into what they called mercantilism. Some scholars thus reject the idea of mercantilism completely, arguing that it gives ""a false unity to disparate events"". Smith saw the mercantile system as an enormous conspiracy by manufacturers and merchants against consumers, a view that has led some authors, especially Robert E. Ekelund and Robert D. Tollison, to call mercantilism ""a rent-seeking society"". To a certain extent, mercantilist doctrine itself made a general theory of economics impossible.[22] Mercantilists viewed the economic system as a zero-sum game, in which any gain by one party required a loss by another.[23] Thus, any system of policies that benefited one group would by definition harm the other, and there was no possibility of economics being used to maximize the commonwealth, or common good.[24] Mercantilists' writings were also generally created to rationalize particular practices rather than as investigations into the best policies.[25]
 Mercantilist domestic policy was more fragmented than its trade policy. While Adam Smith portrayed mercantilism as supportive of strict controls over the economy, many mercantilists disagreed. The early modern era was one of letters patent and government-imposed monopolies; some mercantilists supported these, but others acknowledged the corruption and inefficiency of such systems. Many mercantilists also realized that the inevitable results of quotas and price ceilings were black markets. One notion that mercantilists widely agreed upon was the need for economic oppression of the working population; laborers and farmers were to live at the ""margins of subsistence"". The goal was to maximize production, with no concern for consumption. Extra money, free time, and education for the lower classes were seen to inevitably lead to vice and laziness, and would result in harm to the economy.[26]
 The mercantilists saw a large population as a form of wealth that made possible the development of bigger markets and armies. Opposite to mercantilism was the doctrine of physiocracy, which predicted that mankind would outgrow its resources. The idea of mercantilism was to protect the markets as well as maintain agriculture and those who were dependent upon it.
 Mercantilist ideas were the dominant economic ideology of all of Europe in the early modern period, and most states embraced it to a certain degree. Mercantilism was centred on England and France, and it was in these states that mercantilist policies were most often enacted.
 The policies have included:
 Mercantilism arose in France in the early 16th century soon after the monarchy had become the dominant force in French politics. In 1539, an important decree banned the import of woolen goods from Spain and some parts of Flanders. The next year, a number of restrictions were imposed on the export of bullion.[27]
 Over the rest of the 16th century, further protectionist measures were introduced. The height of French mercantilism is closely associated with Jean-Baptiste Colbert, finance minister for 22 years in the 17th century, to the extent that French mercantilism is sometimes called Colbertism. Under Colbert, the French government became deeply involved in the economy in order to increase exports. Protectionist policies were enacted that limited imports and favored exports. Industries were organized into guilds and monopolies, and production was regulated by the state through a series of more than one thousand directives outlining how different products should be produced.[28]
 To encourage industry, foreign artisans and craftsmen were imported. Colbert also worked to decrease internal barriers to trade, reducing internal tariffs and building an extensive network of roads and canals. Colbert's policies were quite successful, and France's industrial output and the economy grew considerably during this period, as France became the dominant European power. He was less successful in turning France into a major trading power, and Britain and the Dutch Republic remained supreme in this field.[28]
 France imposed its mercantilist philosophy on its colonies in North America, especially New France. It sought to derive the maximum material benefit from the colony, for the homeland, with a minimum of colonial investment in the colony itself. The ideology was embodied in New France through the establishment under Royal Charter of a number of corporate trading monopolies including La Compagnie des Marchands, which operated from 1613 to 1621, and the Compagnie de Montmorency, from that date until 1627. It was in turn replaced by La Compagnie des Cent-Associés, created in 1627 by King Louis XIII, and the Communauté des habitants in 1643. These were the first corporations to operate in what is now Canada.
 In England, mercantilism reached its peak during the Long Parliament government (1640–60). Mercantilist policies were also embraced throughout much of the Tudor and Stuart periods, with Robert Walpole being another major proponent. In Britain, government control over the domestic economy was far less extensive than on the Continent, limited by common law and the steadily increasing power of Parliament.[29] Government-controlled monopolies were common, especially before the English Civil War, but were often controversial.[30]
 With respect to its colonies,  British mercantilism meant that the government and the merchants became partners with the goal of increasing political power and private wealth, to the exclusion of other European powers. The government protected its merchants—and kept foreign ones out—through trade barriers, regulations, and subsidies to domestic industries in order to maximize exports from and minimize imports to the realm. The government had to fight smuggling, which became a favourite American technique in the 18th century to circumvent the restrictions on trading with the French, Spanish, or Dutch. The goal of mercantilism was to run trade surpluses to benefit the government. The government took its share through duties and taxes, with the remainder going to merchants in Britain. The government spent much of its revenue on the Royal Navy, which both protected the colonies of Britain but was vital in capturing the colonies of other European powers.[31][32]
 British mercantilist writers were themselves divided on whether domestic controls were necessary. British mercantilism thus mainly took the form of efforts to control trade. A wide array of regulations were put in place to encourage exports and discourage imports. Tariffs were placed on imports and bounties given for exports, and the export of some raw materials was banned completely. The Navigation Acts removed foreign merchants from being involved England's domestic trade. British policies in their American colonies led to friction with the inhabitants of the Thirteen Colonies, and mercantilist policies (such as forbidding trade with other European powers and enforcing bans on smuggling) were a major irritant leading to the American Revolution.[32][33]
 Mercantilism taught that trade was a zero-sum game, with one country's gain equivalent to a loss sustained by the trading partner. Some have argued that mercantilist policies had a positive impact on Britain, helping to transform the nation into the world's dominant trading power and a global hegemon.[33] One domestic policy that had a lasting impact was the conversion of ""wastelands"" to agricultural use. Mercantilists believed that to maximize a nation's power, all land and resources had to be used to their highest and best use, and this era thus saw projects like the draining of The Fens.[34]
 The American School of economics dominated United States national policies from the time of the American Civil War until the mid-20th century.[35][36][37][38][39][40] It is closely related to mercantilism, and it can be seen as contrary to classical economics. It consisted of these three core policies:
 The other nations of Europe also embraced mercantilism to varying degrees. The Netherlands, which had become the financial centre of Europe by being its most efficient trader, had little interest in seeing trade restricted and adopted few mercantilist policies. Mercantilism became prominent in Central Europe and Scandinavia after the Thirty Years' War (1618–48), with Christina of Sweden, Jacob Kettler of Courland, and Christian IV of Denmark being notable proponents.
 The Habsburg Holy Roman Emperors had long been interested in mercantilist policies, but the vast and decentralized nature of their empire made implementing such notions difficult. Some constituent states of the empire did embrace mercantilism, most notably Prussia, which under Frederick the Great had perhaps the most rigidly controlled economy in Europe.
 Spain benefited from mercantilism early on as it brought a large amount of precious metals such as gold and silver into their treasury by way of the new world. In the long run, Spain's economy collapsed as it was unable to adjust to the inflation that came with the large influx of bullion. Heavy intervention from the crown put crippling laws for the protection of Spanish goods and services. Mercantilist protectionist policy in Spain caused the long-run failure of the Castilian textile industry as the efficiency severely dropped off with each passing year due to the production being held at a specific level. Spain's heavily protected industries led to famines as much of its agricultural land was required to be used for sheep instead of grain. Much of their grain was imported from the Baltic region of Europe which caused a shortage of food in the inner regions of Spain. Spain limiting the trade of their colonies is one of the causes that led to the separation of the Dutch from the Spanish Empire. The culmination of all of these policies led to Spain defaulting in 1557, 1575, and 1596.[45]
 During the economic collapse of the 17th century, Spain had little coherent economic policy, but French mercantilist policies were imported by Philip V with some success. Ottoman Grand Vizier Kemankeş Kara Mustafa Pasha also followed some mercantilist financial policies during the reign of Ibrahim I. Russia under Peter I (Peter the Great) attempted to pursue mercantilism, but had little success because of Russia's lack of a large merchant class or an industrial base.
 Mercantilism was the economic version of warfare backed up by the state apparatus, and was well suited to an era of military warfare.[46] Since the level of world trade was viewed as fixed, it followed that the only way to increase a nation's trade was to take it from another. A number of wars, most notably the Anglo-Dutch Wars and the Franco-Dutch Wars, can be linked directly to mercantilist theories. Most wars had other causes but they reinforced mercantilism by clearly defining the enemy, and justified damage to the enemy's economy.
 Mercantilism fueled the imperialism of this era, as many nations expended significant effort to conquer new colonies that would be sources of gold (as in Mexico) or sugar (as in the West Indies), as well as becoming exclusive markets. European power spread around the globe, often under the aegis of companies with government-guaranteed monopolies in certain defined geographical regions, such as the Dutch East India Company or the Hudson's Bay Company (operating in present-day Canada).
 With the establishment of overseas colonies by European powers early in the 17th century, mercantile theory gained a new and wider significance, in which its aim and ideal became both national and imperialistic.[47][need quotation to verify]
 The connection between communism and mercantilism has been explored by Marxist economist and sociologist Giovanni Arrighi, who analyzed mercantilism as having three components: ""settler colonialism, capitalist slavery, and economic nationalism,"" and further noted that slavery was ""partly a condition and partly a result of the success of settler colonialism.""[48]
 In France, the triangular trade method was integral in the continuation of mercantilism throughout the 17th and 18th centuries.[49] In order to maximize exports and minimize imports, France worked on a strict Atlantic route: France, to Africa, to the Americas and then back to France.[48] By bringing African slaves to labor in the New World, their labor value increased, and France capitalized upon the market resources produced by slave labor.[49]
 Mercantilism as a weapon has continued to be used by nations through the 21st century by way of modern tariffs as it puts smaller economies in a position to conform to the larger economies goals or risk economic ruin due to an imbalance in trade. Trade wars are often dependent on such tariffs and restrictions hurting the opposing economy.
 The term ""mercantile system"" was used by its foremost critic, Adam Smith,[50] but Mirabeau (1715–1789) had used ""mercantilism"" earlier. Mercantilism functioned as the economic counterpart of the older version of political power: divine right of kings and absolute monarchy.[51]
 Scholars debate why mercantilism dominated economic ideology for 250 years.[52] One group, represented by Jacob Viner, sees mercantilism as simply a straightforward, common-sense system whose logical fallacies remained opaque to people at the time, as they simply lacked the required analytical tools.
 The second school, supported by scholars such as Robert B. Ekelund, portrays mercantilism not as a mistake, but rather as the best possible system for those who developed it. This school argues that rent-seeking merchants and governments developed and enforced mercantilist policies. Merchants benefited greatly from the enforced monopolies, bans on foreign competition, and poverty of the workers. Governments benefited from the high tariffs and payments from the merchants. Whereas later economic ideas were often developed by academics and philosophers, almost all mercantilist writers were merchants or government officials.[53]
 Monetarism offers a third explanation for mercantilism. European trade exported bullion to pay for goods from Asia, thus reducing the money supply and putting downward pressure on prices and economic activity. The evidence for this hypothesis is the lack of inflation in the British economy until the Revolutionary and Napoleonic Wars, when paper money came into vogue.
 A fourth explanation lies in the increasing professionalisation and technification of the wars of the era, which turned the maintenance of adequate reserve funds (in the prospect of war) into a more and more expensive and eventually competitive business.
 Mercantilism developed at a time of transition for the European economy. Isolated feudal estates were being replaced by centralized nation-states as the focus of power. Technological changes in shipping and the growth of urban centers led to a rapid increase in international trade.[54] Mercantilism focused on how this trade could best aid the states. Another important change was the introduction of double-entry bookkeeping and modern accounting. This accounting made extremely clear the inflow and outflow of trade, contributing to the close scrutiny given to the balance of trade.[55] New markets and new mines propelled foreign trade to previously inconceivable volumes, resulting in ""the great upward movement in prices"" and an increase in ""the volume of merchant activity itself"".[56]
 Before mercantilism, the most important work in economics in Europe was that of the medieval scholastic theorists. The goal of these thinkers was to find an economic system compatible with Christian doctrines of piety and justice. They focused mainly on microeconomics and on local exchanges between individuals. Mercantilism was closely aligned with the other theories and ideas that began to replace the medieval worldview. This period saw the adoption of Machiavellian realpolitik and the primacy of the raison d'état in international relations. The mercantilist idea of all trade as a zero-sum game, in which each side was trying to best the other in a ruthless competition, was integral to the works of Thomas Hobbes. This dark view of human nature also fit well with the Puritan view of the world, and some of the most stridently mercantilist legislation, such as the Navigation Ordinance of 1651, was enacted by the government of Oliver Cromwell.[57]
 Jean-Baptiste Colbert's work in 17th-century France came to exemplify classical mercantilism. In the English-speaking world, its ideas were criticized by Adam Smith with the publication of The Wealth of Nations in 1776 and later by David Ricardo with his explanation of comparative advantage. Mercantilism was rejected by Britain and France by the mid-19th century. The British Empire embraced free trade and used its power as the financial center of the world to promote the same. The Guyanese historian Walter Rodney describes mercantilism as the period of the worldwide development of European commerce which began in the 15th century with the voyages of Portuguese and Spanish explorers to Africa, Asia, and the New World.
 Adam Smith, David Hume, Edward Gibbon, Voltaire and Jean-Jacques Rousseau were the founding fathers of anti-mercantilist thought. A number of scholars found important flaws in mercantilism long before Smith developed an ideology that could fully replace it. Critics such as Hume, Dudley North and John Locke undermined much of mercantilism and it steadily lost favor during the 18th century.
 In 1690, Locke argued that prices vary in proportion to the quantity of money. Locke's Second Treatise also points towards the heart of the anti-mercantilist critique: that the wealth of the world is not fixed, but is created by human labor (represented embryonically by Locke's labor theory of value). Mercantilists failed to understand the notions of absolute advantage and comparative advantage (this idea was only fully fleshed out in 1817 by David Ricardo) and the benefits of trade.[58][note 1]
 Hume famously noted the impossibility of the mercantilists' goal of a constant positive balance of trade.[59] As bullion flowed into one country, the supply would increase, and the value of bullion in that state would steadily decline relative to other goods. Conversely, in the state exporting bullion, its value would slowly rise. Eventually, it would no longer be cost-effective to export goods from the high-price country to the low-price country, and the balance of trade would reverse. Mercantilists fundamentally misunderstood this, long arguing that an increase in the money supply simply meant that everyone gets richer.[60]
 The importance placed on bullion was also a central target, even if many mercantilists had themselves begun to de-emphasize the importance of gold and silver. Adam Smith noted that at the core of the mercantile system was the ""popular folly of confusing wealth with money"", that bullion was just the same as any other commodity, and that there was no reason to give it special treatment.[17] More recently, scholars have discounted the accuracy of this critique. They believe Mun and Misselden were not making this mistake in the 1620s, and point to their followers Josiah Child and Charles Davenant, who in 1699 wrote, ""Gold and Silver are indeed the Measures of Trade, but that the Spring and Original of it, in all nations is the Natural or Artificial Product of the Country; that is to say, what this Land or what this Labour and Industry Produces.""[61] The critique that mercantilism was a form of rent seeking has also seen criticism, as scholars such as Jacob Viner in the 1930s pointed out that merchant mercantilists such as Mun understood that they would not gain by higher prices for English wares abroad.[62]
 The first school to completely reject mercantilism was the physiocrats, who developed their theories in France. Their theories also had several important problems, and the replacement of mercantilism did not come until Adam Smith published The Wealth of Nations in 1776. This book outlines the basics of what is today known as classical economics. Smith spent a considerable portion of the book rebutting the arguments of the mercantilists, though often these are simplified or exaggerated versions of mercantilist thought.[53]
 Scholars are also divided over the cause of mercantilism's end. Those who believe the theory was simply an error hold that its replacement was inevitable as soon as Smith's more accurate ideas were unveiled. Those who feel that mercantilism amounted to rent-seeking hold that it ended only when major power shifts occurred. In Britain, mercantilism faded as the Parliament gained the monarch's power to grant monopolies. While the wealthy capitalists who controlled the House of Commons benefited from these monopolies, Parliament found it difficult to implement them because of the high cost of group decision making.[63]
 Mercantilist regulations were steadily removed over the course of the 18th century in Britain, and during the 19th century, the British government fully embraced free trade and Smith's laissez-faire economics. On the continent, the process was somewhat different. In France, economic control remained in the hands of the royal family, and mercantilism continued until the French Revolution. In Germany, mercantilism remained an important ideology in the 19th and early 20th centuries, when the historical school of economics was paramount.[64]
 Adam Smith criticized the mercantile doctrine that prioritized production in the economy; he maintained that consumption was of prime significance. Additionally, the mercantile system was well liked by the traders as it was what is now referred to as rent seeking.[65] John Maynard Keynes affirmed that motivating the production process was as significant as encouraging consumption, which benefited the new mercantilism. Keynes also affirmed that in the post-classical period the primary focus on gold and silver supplies (bullion) was rational. During the era before paper money, an increase in gold and silver was one of the ways of mercantilism increasing an economy's reserve or the supply of money. Keynes reiterated that the doctrines advocated for by mercantilism aided the improvement of both the domestic and foreign outlay—domestic because the policies lowered the domestic rate of interest, and investment by foreigners by tending to create a favorable balance of trade.[66] Keynes and other economists of the 20th century also realized that the balance of payments is an important concern. Keynes also supported government intervention in the economy as necessary, as did mercantilism.[67]
 As of 2010[update], the word ""mercantilism"" remains a pejorative term, often used to attack various forms of protectionism.[68] The similarities between Keynesianism (and its successor ideas) and mercantilism have sometimes led critics[who?] to call them neomercantilism.
 Paul Samuelson, writing within a Keynesian framework, wrote of mercantilism, ""With employment less than full and Net National Product suboptimal, all the debunked mercantilist arguments turn out to be valid.""[69]
 Some other systems that copy several mercantilist policies, such as Japan's economic system, are also sometimes called neo-mercantilist.[70] In an essay appearing in the May 14, 2007 issue of Newsweek, business columnist Robert J. Samuelson wrote that China was pursuing an essentially neo-mercantilist trade policy that threatened to undermine the post–World War II international economic structure.[4]
 Murray Rothbard, representing the Austrian School of economics, describes it this way:
 Rothbard viewed mercantilism not as a coherent economic theory but rather a series of post-hoc rationalizations for various economic policies by interested parties. 
 In specific instances, protectionist mercantilist policies also had an important and positive impact on the state that enacted them. Adam Smith, for instance, praised the Navigation Acts, as they greatly expanded the British merchant fleet and played a central role in turning Britain into the world's naval and economic superpower from the 18th century onward.[72] Some economists thus feel that protecting infant industries, while causing short-term harm, can be beneficial in the long term.
"
George Washington's Gristmill,https://en.wikipedia.org/wiki/George_Washington%27s_Gristmill,"George Washington's Gristmill was part of the original Mount Vernon plantation, constructed during the lifetime of the United States' first president. The original structure was destroyed about 1850. The Commonwealth of Virginia and the Mount Vernon Ladies’ Association have reconstructed the gristmill and the adjacent distillery.  The reconstructed buildings are located at their original site three miles (4.8 km) west of the Mount Vernon mansion near Woodlawn Plantation in the Mont Vernon area of Fairfax County.  Because the reconstructed buildings embody the distinctive characteristics of late eighteenth century methods of production and are of importance to the history of Virginia, the site is listed on the National Register of Historic Places despite the fact that the buildings are not original.
 George Washington inherited Mount Vernon in 1754. In 1771, he erected a large stone gristmill on the plantation to replace a mill his father had built in the 1730s. The new mill was located three miles (5 km) west of Mount Vernon on Dogue Run Creek. It was used to produce flour and cornmeal for the plantation as well as high-quality flour for export to the West Indies, England, and continental Europe. Washington also built a house for the miller and a cooperage to supply barrels for the mill, and later, the distillery operation.  The mill was powered by a large water wheel. To ensure a steady power supply, water was diverted from Piney Branch into Dogue Run Creek above the mill's headrace. The additional waterflow significantly increased the mill's production capacity. In 1791, Washington automated his mill using technology developed and patented by Oliver Evans of Delaware.[3][4] Evans was personally acquainted with the mill and had repaired some of its works.[5]
 Once the gristmill was well established, Washington's farm manager, James Anderson, suggested building a whiskey distillery adjacent to the mill.  When it was completed in 1797, the distillery was the largest in America. By 1799 it had become one of Washington's most successful enterprises, producing 11,000 gallons of whiskey per year.[3][6] A variety of whiskeys were produced at the site along with brandy and vinegar. The most common whiskey recipe used 60% rye, 35% corn, and 5% malted barley. Smaller amounts of rye whiskey were distilled up to four times and were more expensive. Some whiskey was also flavored with cinnamon. When rye was scarce the distillery used wheat. Apple, peach and persimmon brandies were also produced.[7] The whiskey was marketed in Alexandria, Virginia, or shipped directly from Mount Vernon's dock on the Potomac River. The distillery process produced a significant waste stream, which was fed to 150 cattle and 30 hogs that were kept at the site.[8]
 After Washington's death in December 1799, the gristmill and distillery passed to his nephew, Lawrence Lewis.  In 1808, he rented the site to Alexandria merchant James Douglass.  The last known reference to the distillery business is an 1808 whiskey advertisement.[4] The distillery building burned in 1814; this is documented by a small insurance payment made to Lewis that year.[7] In 1848, Lewis’ grandson sold the gristmill property along with Woodlawn Plantation. That is the last record of the original buildings. Local oral history suggests that the mill was quite run-down by 1848, and it was razed around 1850.[4]
 In 1932, the Commonwealth of Virginia purchased 6.65 acres (26,900 m2) around the old mill site.  The state initiated an archaeological field survey of the site with the goal of reconstructing the gristmill, distillery and other Washington-era buildings.  The gristmill and miller's house were reconstructed in 1933.  Shortly after their completion, the site was opened as a state park.  However, by 1936 the state had stopped maintaining the park.  Sometime around 1940 a local chapter of the Future Farmers of America began caring for the grounds, but public use of the park was sparse.  The state resumed responsibility for the park in 1962.  Over the next two decades, several additional structures were built on the site.[4]
 In 1997, Virginia conveyed the property to the Mount Vernon Ladies' Association which owns and operates the Mount Vernon estate. From 1997 to 2002 the main structures underwent major renovation, including rebuilding the internal millworkings, renovation of the miller's house, restoration of the millraces, and construction of new brick pathways throughout the site.[4] Because the property embodies the distinctive characteristics of late eighteenth-century production methods, the site was listed on the National Register of Historic Places in 2003.[9] In 1999, archaeologists began to investigate the distillery site; after five years of study the distillery reconstruction began. It was completed and opened to the public in 2007. This $2.1 million project was funded by the Distilled Spirits Council of the United States and the Wine and Spirits Wholesalers of America.[7]
 George Washington's Gristmill is located on 6.65 acres (2.69 ha) approximately three miles west of the Mount Vernon estate. It is situated on an eastward sloping lot, bounded by Dogue Run Creek to the south, pasture land belonging to the National Trust for Historic Preservation to the west, the park's paved parking lot and a housing subdivision to the north, and a wooded lot to the east. The property is bisected by Virginia Route 235. All the site's historic elements (except one archeological site) are located on the east side of the highway.  This includes the gristmill, distillery, miller's house and several archeological sites. The west side of the property contains one archaeological site and three non-contributing structures.[4]
 The gristmill is a Colonial Revival-style stone building that was constructed in 1933 based on archaeological and documentary evidence. Its rectangular footprint measures 37 feet (11 m) by 50 feet (15 m). Its foundation is built into a hillside, with two and one half stories above ground on the north side and three and one half stories on the south side. It is a masonry structure built with sandstone arranged in a random pattern with stone lintels and sills. The mill's roof is covered with wooden shingles. On the south side of the building is a vertical-board door on the first floor; there is an identical door on the ground floor on the north side as well. Both have stone landings. The millrace enters the mill on the north side and exits at ground level on the south side.[4]
 The internal millworkings and structural members installed during the 1933 reconstruction were taken from an 1818 gristmill located near Front Royal, Virginia.  Some of the structural members from Front Royal mill are still in the building; however, most of the millworkings were replaced between 1997 and 2002.  The interior has exposed masonry walls with heavy timber framing. The flooring throughout the building is random-width pine. A masonry fireplace is in the southwest corner of the first floor. The grinding platform is accessed from the second floor.  The third level is one large room with rolling screens and other processing equipment. A staircase in the western half of the structure runs from the first floor to attic.  The mill's two grinding stones are powered by a pitch-back water wheel. The mill machinery is enclosed in a hurst frame,[10] built from heavy oak and pine beams. Its frame is built directly on the mill's foundation, and is not connected to the walls.  This protects the structure from the machinery's potentially damaging vibrations. The hurst frame occupies the eastern half of the first two levels.[4]
 The miller's house was built at the same time as the gristmill; the reconstructed house was sited on its original site. The design for the building is based on archaeological evidence and a mid-nineteenth-century drawing. Its wood-frame structure rests on a stone foundation.  It is clad with beaded weatherboard.  In 1970, a .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1+1⁄2-story addition doubled the size of the building. The interior of the original section is laid out in a hall-parlor configuration with two major rooms and a small bathroom. This area is used as a gift shop. The new section of the house includes a kitchen, pantry, and additional retail space for the gift ship.[4]
 Because there is no surviving example of eighteenth century distillery, the reconstruction of Washington's distillery required extensive archeological and documentary investigation before an authentic structure could be built. The archeological study began in 1997 and lasted until 2006. During the excavation archaeologists uncovered the distillery's stone foundation which is some thirty-six inches (900 mm) thick. Some of the original foundation stones are over twenty-four inches (600 mm) in diameter. The initial course of the sandstone superstructure was intact along the southern foundation, and was two feet (600 mm) thick.  They discovered the location of five stills and boilers, and found many objects used in the distilling process along with fragments of domestic items such as teacups, drinking glasses and buttons.[4][7][11]
 The distillery was reconstructed in 2007.  To ensure an authentic reconstruction the wood was finished by hand and the construction used hand-made nails and hardware. There were some compromises necessary to comply with modern building codes and safety requirements.  For example, the original structure's 30 by 75 foot (9 m by 23 m) footprint was extended by 15 feet (4.6 m) in order to house an elevator and modern staircase for public use.[7]
 The distillery has two floors with five large[12] copper stills, mash tubs and a boiler that demonstrate the eighteenth-century distilling process. The building includes a storage cellar for whiskey barrels, an office, and two bedrooms where the site manager and his assistant would have lived.[6] The building's floors are made of three different materials. A stone floor is used in the mashing area to reduce vibrations that can disturb the fermentation process.  Around the boilers and under the staircase is a brick floor, and an elevated wooden floor around the stills.  Wooden planks are used for flooring in the rest of the building.[7]
 The original distillery was built to specifications provided by James Anderson, originally from Inverkeithing in Scotland and much of the correspondence between Washington and Anderson[13] is recorded in the Library of Congress.  The concept of building a distillery and entering into whisky production was suggested directly by Anderson and Washington's skepticism was notable in many letters.  In the end, on advice from friends, he agreed to proceed with the plans as outlined by Anderson.  The process is based on the Scottish distillation of Scotch Whisky but differs in that the spirit was not aged as Scotch is today.  As a result, the Whisky tastes quite different.  
 The gristmill, distillery, and gift shop are open to the public from April through October.  They are located approximately three miles west of Mount Vernon's main gate on Virginia Route 235.  Tickets to tour the gristmill and distillery are available at Mount Vernon and the gristmill's gift shop.  The tickets can be combined admission to Mount Vernon or purchased separately.  Public transportation is available between Mount Vernon and the gristmill.[6]
"
Hemp farming,https://en.wikipedia.org/wiki/Hemp_farming,"


 Hemp, or industrial hemp, is a plant in the botanical class of Cannabis sativa cultivars grown specifically for industrial and consumable use. It can be used to make a wide range of products.[1] Along with bamboo, hemp is among the fastest growing plants[2] on Earth. It was also one of the first plants to be spun into usable fiber 50,000 years ago.[3] It can be refined into a variety of commercial items, including paper, rope, textiles, clothing, biodegradable plastics, paint, insulation, biofuel, food, and animal feed.[4][5]
 Although chemotype I cannabis and hemp (types II, III, IV, V) are both Cannabis sativa and contain the psychoactive component tetrahydrocannabinol (THC), they represent distinct cultivar groups, typically with unique phytochemical compositions and uses.[6] Hemp typically has lower concentrations of total THC and may have higher concentrations of cannabidiol (CBD), which potentially mitigates the psychoactive effects of THC.[7] The legality of hemp varies widely among countries. Some governments regulate the concentration of THC and permit only hemp that is bred with an especially low THC content into commercial production.[8][9]
 The etymology is uncertain but there appears to be no common Proto-Indo-European source for the various forms of the word; the Greek term κάνναβις (kánnabis) is the oldest attested form, which may have been borrowed from an earlier Scythian or Thracian word.[10][11] Then it appears to have been borrowed into Latin, and separately into Slavic and from there into Baltic, Finnish, and Germanic languages.[12]
 In the Germanic languages, following Grimm's law, the ""k"" would have changed to ""h"" with the first Germanic sound shift,[10][13] giving Proto-Germanic *hanapiz, after which it may have been adapted into the Old English form, hænep, henep.[10] Barber (1991) however, argued that the spread of the name ""kannabis"" was due to its historically more recent plant use, starting from the south, around Iran, whereas non-THC varieties of hemp are older and prehistoric.[12] Another possible source of origin is Assyrian qunnabu, which was the name for a source of oil, fiber, and medicine in the 1st millennium BC.[12]
 Cognates of hemp in other Germanic languages include Dutch hennep, Danish and Norwegian hamp, Saterland Frisian Hoamp, German Hanf, Icelandic hampur and Swedish hampa. In those languages ""hemp"" can refer to either industrial fiber hemp or narcotic cannabis strains.[10]
 Hemp is used to make a variety of commercial and industrial products, including rope, textiles, clothing, shoes, food, paper, bioplastics, insulation, and biofuel.[4] The bast fibers can be used to make textiles that are 100% hemp, but they are commonly blended with other fibers, such as flax, cotton or silk, as well as virgin and recycled polyester, to make woven fabrics for apparel and furnishings. The inner two fibers of the plant are woodier and typically have industrial applications, such as mulch, animal bedding, and litter. When oxidized (often erroneously referred to as ""drying""), hemp oil from the seeds becomes solid and can be used in the manufacture of oil-based paints, in creams as a moisturizing agent, for cooking, and in plastics. Hemp seeds have been used in bird feed mix as well. A survey in 2003 showed that more than 95% of hemp seed sold in the European Union was used in animal and bird feed.[14]
 Hemp seeds can be eaten raw, ground into hemp meal, sprouted or made into dried sprout powder. Hemp seeds can also be made into a slurry used for baking or for beverages, such as hemp milk and tisanes.[17] Hemp oil is cold-pressed from the seed and is high in unsaturated fatty acids.[18]
 In the UK, the Department for Environment, Food and Rural Affairs treats hemp as a purely non-food crop, but with proper licensing and proof of less than 0.3% THC concentration, hemp seeds can be imported for sowing or for sale as a food or food ingredient.[19] In the US, hemp can be used legally in food products and, as of 2000[update], was typically sold in health food stores or through mail order.[18]
 A .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}100-gram (3+1⁄2-ounce) portion of hulled hemp seeds supplies 2,451 kilojoules (586 kilocalories) of food energy. They contain 5% water, 5% carbohydrates, 49% total fat, and 31% protein.[20]
 The share of protein obtained from the hemp seeds can be increased in by processing the seeds, such as by dehulling the seeds, or by using the meal or cake (also called hemp seed flour),[21] that is, the remaining fraction of hemp seed obtained after expelling its oil fraction.[22][23] The proteins are mostly located in the inner layer of the seed, whereas the hull is poor in proteins, as it mostly contains the fiber.[23][24]
 Hemp seeds are notable in providing 64% of the Daily Value (DV) of protein per 100-gram serving.[20] The three main proteins in hemp seeds are edestin (83% of total protein content), albumin (13%) and ß-conglycinin (up to 5%).[25][23] Hemp seed proteins are highly digestible compared to soy proteins when untreated (unheated).[26][23] The amino acid profile of hemp seeds is comparable to the profiles of other protein-rich foods, such as meat, milk, eggs, and soy.[27][24][28] Protein digestibility-corrected amino acid scores were 0.49–0.53 for whole hemp seed, 0.46–0.51 for hemp seed meal, and 0.63–0.66 for hulled hemp seed.[21][29] The most abundant amino acid in hemp seed is glutamic acid (3.74–4.58% of whole seed) followed by arginine (2.28–3.10% of whole seed).[22][23][24] The whole hemp seed can be considered a rich-protein source containing a protein amount higher or similar than other protein-rich products, such as quinoa (13.0%), chia seeds (18.2–19.7%), buckwheat seeds (27.8%) and linseeds (20.9%). Nutritionally, the protein fraction of hemp seed is highly digestible comparing to other plant-based proteins such as soy protein. Hemp seed protein has a good profile of essential amino acids, still, this profile of amino acids is inferior to that of soy or casein.[23][24][30]
 Hemp seeds are a rich source of dietary fiber (20% DV), B vitamins, and the dietary minerals manganese (362% DV), phosphorus (236% DV), magnesium (197% DV), zinc (104% DV), and iron (61% DV). About 73% of the energy in hemp seeds is in the form of fats and essential fatty acids,[20] mainly polyunsaturated fatty acids, linoleic, oleic, and alpha-linolenic acids.[24][28] The ratio of the 38.100 grams of polyunsaturated fats per 100 grams is 9.301 grams of omega-3 to 28.698 grams of omega-6.[31] Typically, the portion suggested on packages for an adult is 30 grams, approximately three tablespoons.[31]
 With its gluten content as low as 4.78 ppm, hemp is attracting attention as a gluten-free (<20 ppm) food material.[25]
 Despite the rich nutrient content of hemp seeds, the seeds contain antinutritional compounds, including phytic acid,[32] trypsin inhibitors, and tannins, in statistically significant concentrations.[26][33]
 Hemp oil oxidizes and turns rancid within a short period of time if not stored properly;[18] its shelf life is extended when it is stored in a dark airtight container and refrigerated. Both light and heat can degrade hemp oil.
 Hemp fiber has been used extensively throughout history, with production climaxing soon after being introduced to the New World. For centuries, items ranging from rope, to fabrics, to industrial materials were made from hemp fiber. Hemp was also commonly used to make sail canvas. The word ""canvas"" is derived from the word cannabis.[34][35] Pure hemp has a texture similar to linen.[36] Because of its versatility for use in a variety of products, today hemp is used in a number of consumer goods, including clothing, shoes, accessories, dog collars, and home wares. For clothing, in some instances, hemp is mixed with lyocell.[37] Its benefits in terms for sustainability also increase its appeal in industries, such as the clothing industry.[38][39]
 Hemp as a building construction material provides solutions to a variety of issues facing current building standards. Its light weight, mold resistance, breathability, etc. makes hemp products versatile in a multitude of uses.[40] Following the co-heating tests of NNFCC Renewable House at the Building Research Establishment (BRE), hemp is reported to be a more sustainable material of construction in comparison to most building methods used today.[41] In addition, its practical use in building construction could result in the reduction of both energy consumption costs and the creation of secondary pollutants.[41]
 In 2022, hemp-lime, also known as hempcrete, was accepted as a building material, along with methodologies for its use, by the International Code Council,[42] and was included in the 2024 edition of the International Residential Code as an appendix: ""Appendix BL Hemp-Lime (Hempcrete) Construction"".[43] This inclusion in the IRC model code is expected to promote expansion of the use and legitimacy of hemp-lime in construction in the United States.[44]
 The hemp market was at its largest during the 17th century. In the 19th century and onward, the market saw a decline during its rapid illegalization in many countries.[45] Hemp has resurfaced in green building construction, primarily in Europe.[46] The modern-day disputes regarding the legality of hemp lead to its main disadvantages: importing and regulating costs. Final Report on the Construction of the Hemp Houses at Haverhill, UK conducts that hemp construction exceeds the cost of traditional building materials by £48per square meter.[46]
 Currently, the University of Bath researches the use of hemp-lime panel systems for construction. Funded by the European Union, the research tests panel design within their use in high-quality construction, on site assembly, humidity and moisture penetration, temperature change, daily performance and energy saving documentations.[47] The program, focusing on Britain, France, and Spain markets aims to perfect protocols of use and application, manufacturing, data gathering, certification for market use, as well as warranty and insurance.[47]
 The most common use of hemp-lime in building is by casting the hemp-hurd and lime mix while wet around a timber frame with temporary shuttering and tamping the mix to form a firm mass. After the removal of the temporary shuttering, the solidified hemp mix is then ready to be plastered with lime plaster.[48]
 Hemp is classified under the green category of building design, primarily due to its positive effects on the environment.[49] A few of its benefits include but are not limited to the suppression of weed growth, anti-erosion, reclamation properties, and the ability to remove poisonous substances and heavy metals from soil.[49]
 The use of hemp is beginning to gain popularity alongside other natural materials. This is because cannabis processing is done mechanically with minimal harmful effects on the environment. A part of what makes hemp sustainable is its minimal water usage and non-reliance on pesticides for proper growth. It is recyclable, non-toxic, and biodegradable, making hemp a popular choice in green building construction.[49]
 Hemp fiber is known to have high strength and durability, and has been known to be a good protector against vermin. The fiber has the capability to reinforce structures by embossing threads and cannabis shavers. Hemp has been involved more recently in the building industry, producing building construction materials including insulation, hempcrete, and varnishes.[40][50][51][52][53][54]
 Hemp made materials have low embodied energy. The plant has the ability to absorb large amounts of CO2, providing air quality, thermal balance, creating a positive environmental impact.[50]
 Hemp's properties allow mold resistance, and its porous materiality makes the building materials made of it breathable. In addition hemp possesses the ability to absorb and release moisture without deteriorating. Hemp can be non-flammable if mixed with lime and could be applied on numerous aspects of the building (wall, roofs, etc.) due to its lightweight properties.[49][50]
 Hemp is commonly used as an insulation material. Its flexibility and toughness during compression allows for easier implementation within structural framing systems. The insulation material could also be easily adjusted to different sizes and shapes by being cut during the installation process. The ability to not settle and therefore avoiding cavity developments lowers its need for maintenance.[54]
 Hemp insulation is naturally lightweight and non-toxic, allowing for an exposed installation in a variety of spaces, including flooring, walling, and roofing. Compared to mineral insulation, hemp absorbs roughly double the amount of heat and could be compared to wood, in some cases even overpassing some of its types.[54]
 Hemp insulation's porous materiality allows for air and moisture penetration, with a bulk density going up to 20% without losing any thermal properties. In contrast, the commonly used mineral insulation starts to fail after 2%. The insulation evenly distributes vapor and allows for air circulation, constantly carrying out used air and replacing with fresh. Its use on the exterior of the structure, overlaid with breathable water-resistive barriers, eases the withdrawal of moisture from within the wall structure.[54]
 In addition, the insulation doubles as a sound barrier, weakening airborne sound waves passing through it.[54]
 In addition to the CO2 absorbed during its growth period, hemp-lime, also known as hempcrete, continues absorption during the curing process. The mixture hardens when the silica contained in hemp shives mixes with hydraulic lime, resulting in the mineralization process called ""carbonation"".[dubious – discuss].[53][55]
 Though not a load-bearing material,[56] hempcrete is most commonly used as infill in building construction due to its light weight (roughly seven times lighter than common concrete) and vapor permeability.[57] The building material is made of hemp hurds (shiv or shives), hydraulic lime, and water mixed in varying ratios.[52] The mix depends on the use of the material within the structure and could differ in physical properties. Surfaces such as flooring interact with a multitude of loads and would have to be more resistive, while walls and roofs are required to be more lightweight.[52] The application of this material in construction requires minimal skill.[52]
 Hempcrete can be formed in-situ or formed into blocks. Such blocks are not strong enough to be used for structural elements and must be supported by brick, wood, or steel framing.[41] In the end of the twentieth century, during his renovation of Maison de la Turquie in Nogent-sur-Seine, France, Charles Rasetti first invented and applied the use of hempcrete in construction.[58] Shortly after, in the 2000s, Modece Architects used hemp-lime for test designs in Haverhill.[59] The dwellings were studied and monitored for comparison with other building performances by BRE. Completed in 2009, the Center for the Built Environment's Renewable House was found to be among the most technologically advanced structures made of hemp-based material. A year later the first home made of hemp-based materials was completed in Asheville, North Carolina, US.[60]
 Cannabis seeds have high-fat content and contain 30-35% of fatty acids. The extracted oil is suited for a variety of construction applications.[50] The biodegradable hemp oil acts as a wood varnish, protecting flooring from mold, pests, and wear. Its use prevents the water from penetrating the wood while still allowing air and vapor to pass through.[40] Its most common use can be seen in wood framing construction, one of the most common construction methods in the world. Because of its low UV-resistant rating, the finish is most often used indoors, on surfaces such as flooring and wood paneling.[54][40]
 Hemp-based insulating plaster is created by combining hemp fibers with calcium lime and sand. This material, when applied on internal walls, ceilings, and flooring, can be layered up to ten centimeters in thickness. Its porous materiality allows the created plaster to regulate air humidity and evenly distribute it.[40] The gradual absorption and release of water prevent the material from cracking and breaking apart.[61][40] Similar to high-density fiber cement, hemp plaster can naturally vary in color and be manually pigmented.[62]
 Hemp ropes can be woven in various diameters, possessing high amounts of strength making them suitable for a variety of uses for building construction purposes.[53] Some of these uses include installation of frames in building openings and connection of joints. The ropes also used in bridge construction, tunnels, traditional homes, etc.[53] One of the earliest examples of hemp rope and other textile use can be traced back to 1500 BC Egypt.[63]
 Cannabis geotextiles could be put in both wet and dry conditions. Hemp-based bioplastic is a biodegradable alternative to regular plastic and can potentially replace polyvinyl chloride (PVC), a material used for plumbing pipes.[53]
 Hemp growth lasts roughly 100 days, a much faster time period than an average tree used for construction purposes. While dry, the fibers could be pressed into tight wood alternatives to wood-frame construction, wall/ceiling paneling, and flooring. As an addition, hemp is flexible and versatile allowing it to be used in a greater number of ways than wood.[40] Similarly, hemp wood could also be made of recycled hemp-based paper.[64]
 A mixture of fiberglass, hemp fiber, kenaf, and flax has been used since 2002 to make composite panels for automobiles.[65] The choice of which bast fiber to use is primarily based on cost and availability.
Various car makers are beginning to use hemp in their cars, including Audi, BMW, Ford, GM, Chrysler, Honda, Iveco, Lotus, Mercedes, Mitsubishi, Porsche, Saturn, Volkswagen[66] and Volvo. For example, the Lotus Eco Elise[67]
and the Mercedes C-Class both contain hemp (up to 20 kg in each car in the case of the latter).[68]
 Hemp paper are paper varieties consisting exclusively or to a large extent from pulp obtained from fibers of industrial hemp. The products are mainly specialty papers such as cigarette paper,[69] banknotes and technical filter papers.[70] Compared to wood pulp, hemp pulp offers a four to five times longer fiber, a significantly lower lignin fraction as well as a higher tear resistance and tensile strength. However, production costs are about four times higher than for paper from wood,[71] since the infrastructure for using hemp is underdeveloped. If the paper industry were to switch from wood to hemp for sourcing its cellulose fibers, the following benefits could be utilized:
 However, hemp has had a hard time competing with paper from trees or recycled newsprint. Only the outer part of the stem consists mainly of fibers which are suitable for the production of paper. Numerous attempts have been made to develop machines that efficiently and inexpensively separate useful fibers from less useful fibers, but none have been completely successful. This has meant that paper from hemp is still expensive compared to paper from trees.
 Hemp jewelry is the product of knotting hemp twine through the practice of macramé. Hemp jewelry includes bracelets, necklaces, anklets, rings, watches, and other adornments. Some jewelry features beads made from crystals, glass, stone, wood and bones. The hemp twine varies in thickness and comes in a variety of colors. There are many different stitches used to create hemp jewelry, however, the half knot and full knot stitches are most common.
 Hemp rope was used in the age of sailing ships, though the rope had to be protected by tarring, since hemp rope has a propensity for breaking from rot, as the capillary effect of the rope-woven fibers tended to hold liquid at the interior, while seeming dry from the outside.[77] Tarring was a labor-intensive process, and earned sailors the nickname ""Jack Tar"". Hemp rope was phased out when manila rope, which does not require tarring, became widely available. Manila is sometimes referred to as Manila hemp, but is not related to hemp; it is abacá, a species of banana.
 Hemp shives are the core of the stem, hemp hurds are broken parts of the core. In the EU, they are used for animal bedding (horses, for instance), or for horticultural mulch.[78] Industrial hemp is much more profitable if both fibers and shives (or even seeds) can be used.
 Hemp can be used as a ""mop crop"" to clear impurities out of wastewater, such as sewage effluent, excessive phosphorus from chicken litter, or other unwanted substances or chemicals. Additionally, hemp is being used to clean contaminants at the Chernobyl nuclear disaster site, by way of a process which is known as phytoremediation – the process of clearing radioisotopes and a variety of other toxins from the soil, water, and air.[79]
 Hemp crops are tall, have thick foliage, and can be planted densely, and thus can be grown as a smother crop to kill tough weeds.[80] Using hemp this way can help farmers avoid the use of herbicides, gain organic certification, and gain the benefits of crop rotation. However, due to the plant's rapid and dense growth characteristics, some jurisdictions consider hemp a prohibited and noxious weed, much like Scotch Broom.[81]
 Biodiesel can be made from the oils in hemp seeds and stalks; this product is sometimes called ""hempoline"".[82] Alcohol fuel (ethanol or, less commonly, methanol) can be made by fermenting the whole plant.
 Filtered hemp oil can be used directly to power diesel engines. In 1892, Rudolf Diesel invented the diesel engine, which he intended to power ""by a variety of fuels, especially vegetable and seed oils, which earlier were used for oil lamps, i.e. the Argand lamp"".[83][84][85]
 Production of vehicle fuel from hemp is very small. Commercial biodiesel and biogas is typically produced from cereals, coconuts, palm seeds, and cheaper raw materials like garbage, wastewater, dead plant and animal material, animal feces and kitchen waste.[86]
 Separation of hurd and bast fiber is known as decortication. Traditionally, hemp stalks would be water-retted first before the fibers were beaten off the inner hurd by hand, a process known as scutching. As mechanical technology evolved, separating the fiber from the core was accomplished by crushing rollers and brush rollers, or by hammer-milling, wherein a mechanical hammer mechanism beats the hemp against a screen until hurd, smaller bast fibers, and dust fall through the screen. After the Marijuana Tax Act was implemented in 1938, the technology for separating the fibers from the core remained ""frozen in time"". Recently, new high-speed kinematic decortication has come about, capable of separating hemp into three streams; bast fiber, hurd, and green microfiber.
 Only in 1997, did Ireland, parts of the Commonwealth and other countries begin to legally grow industrial hemp again. Iterations of the 1930s decorticator have been met with limited success, along with steam explosion and chemical processing known as thermomechanical pulping.[citation needed]
 Hemp is usually planted between March and May in the northern hemisphere, between September and November in the southern hemisphere.[87] It matures in about three to four months, depending on various conditions.
 Millennia of selective breeding have resulted in varieties that display a wide range of traits; e.g. suited for particular environments/latitudes, producing different ratios and compositions of terpenoids and cannabinoids (CBD, THC, CBG, CBC, CBN...etc.), fiber quality, oil/seed yield, etc. Hemp grown for fiber is planted closely, resulting in tall, slender plants with long fibers.[88]
 The use of industrial hemp plant and its cultivation was commonplace until the 1900s when it was associated with its genetic sibling a.k.a. Drug-Type Cannabis species (which contain higher levels of psychoactive THC). Influential groups misconstrued hemp as a dangerous ""drug"",[89] even though hemp is not a recreational drug and has the potential to be a sustainable and profitable crop for many farmers due to hemp's medical, structural and dietary uses.[90][91] In the United States, the public's perception of hemp as marijuana has blocked hemp from becoming a useful crop and product,""[90] in spite of its vital importance prior to World War II.[91] 
 Ideally, according to Britain's Department for Environment, Food and Rural Affairs, the herb should be desiccated and harvested toward the end of flowering. This early cropping reduces the seed yield but improves the fiber yield and quality.[92]
 The seeds are sown with grain drills or other conventional seeding equipment to a depth of 13 to 25 mm (1⁄2 to 1 in). Greater seeding depths result in increased weed competition. Nitrogen should not be placed with the seed, but phosphate may be tolerated. The soil should have available 89 to 135 kg/ha of nitrogen, 46 kg/ha phosphorus, 67 kg/ha potassium, and 17 kg/ha sulfur. Organic fertilizers such as manure are one of the best methods of weed control.[93]
 In contrast to cannabis for medical use, varieties grown for fiber and seed have less than 0.3% THC and are unsuitable for producing hashish and marijuana.[94] Present in industrial hemp, cannabidiol is a major constituent among some 560 compounds found in hemp.[95]
 Cannabis sativa L. subsp. sativa var. sativa is the variety grown for industrial use, while C. sativa subsp. indica generally has poor fiber quality and female buds from this variety are primarily used for recreational and medicinal purposes. The major differences between the two types of plants are the appearance, and the amount of Δ9-tetrahydrocannabinol (THC) secreted in a resinous mixture by epidermal hairs called glandular trichomes, although they can also be distinguished genetically.[94][96] Oilseed and fiber varieties of Cannabis approved for industrial hemp production produce only minute amounts of this psychoactive drug, not enough for any physical or psychological effects. Typically, hemp contains below 0.3% THC, while cultivars of Cannabis grown for medicinal or recreational use can contain anywhere from 2% to over 20%.[97]
 Smallholder plots are usually harvested by hand. The plants are cut at 2 to 3 cm above the soil and left on the ground to dry. Mechanical harvesting is now common, using specially adapted cutter-binders or simpler cutters.
 The cut hemp is laid in swathes to dry for up to four days. This was traditionally followed by retting, either water retting (the bundled hemp floats in water) or dew retting (the hemp remains on the ground and is affected by the moisture in dew and by molds and bacterial action).
 Several arthropods can cause damage or injury to hemp plants, but the most serious species are associated with the Insecta class. The most problematic for outdoor crops are the voracious stem-boring caterpillars, which include the European corn borer, Ostrinia nubilalis, and the Eurasian hemp borer, Grapholita delineana.[98] As the names imply, they target the stems reducing the structural integrity of the plant.[98] Another lepidopteran, the corn earworm, Helicoverpa zea, is known to damage flowering parts and can be challenging to control.[99] Other foliar pests, found in both indoor and outdoor crops, include the hemp russet mite, Aculops cannibicola, and cannabis aphid, Phorodon cannabis.[99] They cause injury by reducing plant vigor because they feed on the phloem of the plant. Root feeders can be difficult to detect and control because of their below surface habitat. A number of beetle grubs and chafers are known to cause damage to hemp roots, including the flea beetle and Japanese beetle, Popillia Japonica.[98] The rice root aphid, Rhopalosiphum rufiabdominale, has also been reported but primarily affects indoor growing facilities.[99] Integrated pest management strategies should be employed to manage these pests with prevention and early detection being the foundation of a resilient program. Cultural and physical controls should be employed in conjunction with biological pest controls, chemical applications should only be used as a last resort.
 Hemp plants can be vulnerable to various pathogens, including bacteria, fungi, nematodes, viruses and other miscellaneous pathogens. Such diseases often lead to reduced fiber quality, stunted growth, and death of the plant. These diseases rarely affect the yield of a hemp field, so hemp production is not traditionally dependent on the use of pesticides.
 Hemp is considered by a 1998 study in Environmental Economics to be environmentally friendly due to a decrease of land use and other environmental impacts, indicating a possible decrease of ecological footprint in a US context compared to typical benchmarks.[100] A 2010 study, however, that compared the production of paper specifically from hemp and eucalyptus concluded that ""industrial hemp presents higher environmental impacts than eucalyptus paper""; however, the article also highlights that ""there is scope for improving industrial hemp paper production"".[101] Hemp is also claimed to require few pesticides and no herbicides, and it has been called a carbon negative raw material.[102][103]
Results indicate that high yield of hemp may require high total nutrient levels (field plus fertilizer nutrients) similar to a high yielding wheat crop.[104]
A United Nations report endorses the versatility and sustainability of hemp and its productive potential in developing countries. Hemp uses a quarter of the water required by cotton, and absorbs more carbon dioxide than other crops and most trees.[105]
 The world-leading producer of hemp is China, which produces more than 70% of the world output. France ranks second with about a quarter of the world production. Smaller production occurs in the rest of Europe, Chile, and North Korea. Over 30 countries produce industrial hemp, including Australia, Austria, Canada, Chile, China, Denmark, Egypt, Finland, Germany, Greece,[106] Hungary, India, Italy, Japan, Korea, Netherlands, New Zealand, Poland, Portugal, Romania, Russia, Slovenia, Spain, Sweden, Switzerland, Thailand, Turkey, the United Kingdom and Ukraine.[107][108]
 The United Kingdom and Germany resumed commercial production in the 1990s. British production is mostly used as bedding for horses; other uses are under development. Companies in Canada, the UK, the United States, and Germany, among many others, process hemp seed into a growing range of food products and cosmetics; many traditional growing countries continue to produce textile-grade fiber.
 Air-dried stem yields in Ontario have from 1998 and onward ranged from 2.6 to 14.0 tons of dry, retted stalks per hectare (1–5.5 t/ac) at 12% moisture. Yields in Kent County, have averaged 8.75 t/ha (3.5 t/ac). Northern Ontario crops averaged 6.1 t/ha (2.5 t/ac) in 1998. Statistic for the European Union for 2008 to 2010 say that the average yield of hemp straw has varied between 6.3 and 7.3 ton per ha.[109][110] Only a part of that is bast fiber. Around one ton of bast fiber and 2–3 tons of core material can be decorticated from 3–4 tons of good-quality, dry-retted straw. For an annual yield of this level is it in Ontario recommended to add nitrogen (N):70–110 kg/ha, phosphate (P2O5): up to 80 kg/ha and potash (K2O): 40–90 kg/ha.[111]
The average yield of dry hemp stalks in Europe was 6 ton/ha (2.4 ton/ac) in 2001 and 2002.[14]
 FAO argue that an optimum yield of hemp fiber is more than 2 tons per ha, while average yields are around 650 kg/ha.[112]
 In the Australian states of Tasmania, Victoria, Queensland, Western Australia, New South Wales, and most recently, South Australia, the state governments have issued licenses to grow hemp for industrial use. The first to initiate modern research into the potential of cannabis was the state of Tasmania, which pioneered the licensing of hemp during the early 1990s. The state of Victoria was an early adopter in 1998, and has reissued the regulation in 2008.[113]
 Queensland has allowed industrial production under license since 2002,[114] where the issuance is controlled under the Drugs Misuse Act 1986.[115]
Western Australia enabled the cultivation, harvest and processing of hemp under its Industrial Hemp Act 2004,[116] New South Wales now issues licenses[117] under a law, the Hemp Industry Regulations Act 2008 (No 58), that came into effect as of 6 November 2008.[118]
Most recently, South Australia legalized industrial hemp under South Australia's Industrial Hemp Act 2017, which commenced on 12 November 2017.[119]
 Commercial production (including cultivation) of industrial hemp has been permitted in Canada since 1998 under licenses and authorization issued by Health Canada.[120]
 In the early 1990s, industrial hemp agriculture in North America began with the Hemp Awareness Committee at the University of Manitoba. The Committee worked with the provincial government to get research and development assistance and was able to obtain test plot permits from the Canadian government. Their efforts led to the legalization of industrial hemp (hemp with only minute amounts of tetrahydrocannabinol) in Canada and the first harvest in 1998.[121][122]
 In 2017, the cultivated area for hemp in the Prairie provinces include Saskatchewan with more than 56,000 acres (23,000 ha), Alberta with 45,000 acres (18,000 ha), and Manitoba with 30,000 acres (12,000 ha).[123] Canadian hemp is cultivated mostly for its food value as hulled hemp seeds, hemp oils, and hemp protein powders, with only a small fraction devoted to production of hemp fiber used for construction and insulation.[123]
 France is Europe's biggest producer (and the world's second largest producer) with 8,000 hectares (20,000 acres) cultivated.[124] 70–80% of the hemp fiber produced in 2003 was used for specialty pulp for cigarette papers and technical applications. About 15% was used in the automotive sector, and 5–6% was used for insulation mats. About 95% of hurds were used as animal bedding, while almost 5% was used in the building sector.[14] In 2010–2011, a total of 11,000 hectares (27,000 acres) was cultivated with hemp in the EU, a decline compared with previous year.[110][125]
 From the 1950s to the 1980s, the Soviet Union was the world's largest producer of hemp (3,000 square kilometres (1,200 sq mi) in 1970). The main production areas were in Ukraine,[126] the Kursk and Orel regions of Russia, and near the Polish border. Since its inception in 1931, the Hemp Breeding Department at the Institute of Bast Crops in Hlukhiv (Glukhov), Ukraine, has been one of the world's largest centers for developing new hemp varieties, focusing on improving fiber quality, per-hectare yields, and low THC content.[127]
 After the collapse of the Soviet Union, the commercial cultivation of hemp declined sharply. However, at least an estimated 2.5 million acres of hemp grow wild in the Russian Far East and the Black Sea regions.[128]
 In the United Kingdom, cultivation licenses are issued by the Home Office under the Misuse of Drugs Act 1971. When grown for nondrug purposes, hemp is referred to as industrial hemp, and a common product is fiber for use in a wide variety of products, as well as the seed for nutritional aspects and the oil. Feral hemp or ditch weed is usually a naturalized fiber or oilseed strain of Cannabis that has escaped from cultivation and is self-seeding.[129]
 In October 2019, hemp became legal to grow in 46 U.S. states under federal law. As of 2019, 47 states have enacted legislation to make hemp legal to grow at the state level, with several states implementing medical provisions regarding the growing of plants specifically for non-psychoactive CBD.[130]
 The 2018 Farm Bill, which incorporated the Hemp Farming Act of 2018, removed hemp as a Schedule I drug and instead made it an agricultural commodity. This legalized hemp at the federal level, which made it easier for hemp farmers to get production licenses, acquire loans, and receive federal crop insurance.[131]
 The process to legalize hemp cultivation began in 2009, when Oregon began approving licenses for industrial hemp.[133] Then, in 2013, after the legalization of marijuana, several farmers in Colorado planted and harvested several acres of hemp, bringing in the first hemp crop in the United States in over half a century.[134] After that, the federal government created a Hemp Farming Pilot Program as a part of the Agricultural Act of 2014.[135] This program allowed institutions of higher education and state agricultural departments to begin growing hemp without the consent of the Drug Enforcement Administration (DEA). Hemp production in Kentucky, formerly the United States' leading producer, resumed in 2014.[136] Hemp production in North Carolina resumed in 2017,[137] and in Washington State the same year.[138] By the end of 2017, at least 34 U.S. states had industrial hemp programs. In 2018, New York began taking strides in industrial hemp production, along with hemp research pilot programs at Cornell University, Binghamton University and SUNY Morrisville.[139]
 As of 2017, the hemp industry estimated that annual sales of hemp products were around $820 million annually; hemp-derived CBD have been the major force driving this growth.[140]
 Despite this progress, hemp businesses in the US have had difficulties expanding as they have faced challenges in traditional marketing and sales approaches. According to a case study done by Forbes, hemp businesses and startups have had difficulty marketing and selling non-psychoactive hemp products, as majority of online advertising platforms and financial institutions do not distinguish between hemp and marijuana.[141]
 Gathered hemp fiber was used to make cloth long before agriculture, nine to fifty thousand years ago.[3] It may also be one of the earliest plants to have been cultivated.[143][144] An archeological site in the Oki Islands of Japan contained cannabis achenes from about 8000 BC, probably signifying use of the plant.[145] Hemp use archaeologically dates back to the Neolithic Age in China, with hemp fiber imprints found on Yangshao culture pottery dating from the 5th millennium BC.[142][146] The Chinese later used hemp to make clothes, shoes, ropes, and an early form of paper.[142] The classical Greek historian Herodotus (ca. 480 BC) reported that the inhabitants of Scythia would often inhale the vapors of hemp-seed smoke, both as ritual and for their own pleasurable recreation.[147]
 Textile expert Elizabeth Wayland Barber summarizes the historical evidence that Cannabis sativa, ""grew and was known in the Neolithic period all across the northern latitudes, from Europe (Germany, Switzerland, Austria, Romania, Ukraine) to East Asia (Tibet and China),"" but, ""textile use of Cannabis sativa does not surface for certain in the West until relatively late, namely the Iron Age.""[148]
""I strongly suspect, however, that what catapulted hemp to sudden fame and fortune as a cultigen and caused it to spread rapidly westwards in the first millennium B.C. was the spread of the habit of pot-smoking from somewhere in south-central Asia, where the drug-bearing variety of the plant originally occurred. The linguistic evidence strongly supports this theory, both as to time and direction of spread and as to cause.""[149]
 Jews living in Palestine in the 2nd century were familiar with the cultivation of hemp, as witnessed by a reference to it in the Mishna (Kil'ayim 2:5) as a variety of plant, along with arum, that sometimes takes as many as three years to grow from a seedling. In late medieval Holy Roman Empire (Germany) and Italy, hemp was employed in cooked dishes, as filling in pies and tortes, or boiled in a soup.[150] Hemp in later Europe was mainly cultivated for its fibers and was used for ropes on many ships, including those of Christopher Columbus. The use of hemp as a cloth was centered largely in the countryside, with higher quality textiles being available in the towns.
 The Spaniards brought hemp to the Americas and cultivated it in Chile starting about 1545.[151] Similar attempts were made in Peru, Colombia, and Mexico, but only in Chile did the crop find success.[152] In July 1605, Samuel Champlain reported the use of grass and hemp clothing by the (Wampanoag) people of Cape Cod and the (Nauset) people of Plymouth Bay told him they harvested hemp in their region where it grew wild to a height of 4 to 5 ft.
[153] In May 1607, ""hempe"" was among the crops Gabriel Archer observed being cultivated by the natives at the main Powhatan village, where Richmond, Virginia, is now situated;[154] and in 1613, Samuell Argall reported wild hemp ""better than that in England"" growing along the shores of the upper Potomac. As early as 1619, the first Virginia House of Burgesses passed an Act requiring all planters in Virginia to sow ""both English and Indian"" hemp on their plantations.[155] The Puritans are first known to have cultivated hemp in New England in 1645.[151]
 George Washington pushed for the growth of hemp as it was a cash crop commonly used to make rope and fabric. In May 1765 he noted in his diary about the sowing of seeds each day until mid-April. Then he recounts the harvest in October which he grew 27 bushels that year.
 It is sometimes supposed that an excerpt from Washington's diary, which reads ""Began to seperate [sic] the Male from the Female hemp at Do.&—rather too late"" is evidence that he was trying to grow female plants for the THC found in the flowers. However, the editorial remark accompanying the diary states that ""This may arise from their [the male] being coarser, and the stalks larger""[156] In subsequent days, he describes soaking the hemp[157] (to make the fibers usable) and harvesting the seeds,[158] suggesting that he was growing hemp for industrial purposes, not recreational.
 George Washington also imported the Indian hemp plant from Asia, which was used for fiber and, by some growers, for intoxicating resin production. In a 1796 letter to William Pearce who managed the plants for him, Washington says, ""What was done with the Indian Hemp plant from last summer? It ought, all of it, to be sown again; that not only a stock of seed sufficient for my own purposes might have been raised, but to have disseminated seed to others; as it is more valuable than common hemp.""[159][160]
 Other presidents known to have farmed hemp for alternative purposes include Thomas Jefferson,[161] James Madison, James Monroe, Andrew Jackson, Zachary Taylor, and Franklin Pierce.[162]
 Historically, hemp production had made up a significant portion of antebellum Kentucky's economy. Before the American Civil War, many slaves worked on plantations producing hemp.[163]
 In 1937, the Marihuana Tax Act of 1937 was passed in the United States, levying a tax on anyone who dealt commercially in cannabis, hemp, or marijuana. The passing of the Act to destroy the U.S. hemp industry has been reputed to involve businessmen Andrew Mellon, Randolph Hearst and the Du Pont family.[164][165][166]
 One claim is that Hearst believed[dubious – discuss] that his extensive timber holdings were threatened by the invention of the decorticator that he feared would allow hemp to become a cheap substitute for the paper pulp used for newspaper.[164][167] Historical research indicates this fear was unfounded because improvements of the decorticators in the 1930s – machines that separated the fibers from the hemp stem – could not make hemp fiber a cheaper substitute for fibers from other sources. Further, decorticators did not perform satisfactorily in commercial production.[168][164]
 Another claim is that Mellon, Secretary of the Treasury and the wealthiest man in America at that time, had invested heavily in DuPont's new synthetic fiber, nylon, and believed[dubious – discuss] that the replacement of the traditional resource, hemp, was integral to the new product's success.[164][169][170][171][172][173][174][175] DuPont and many industrial historians dispute a link between nylon and hemp, nylon became immediately a scarce commodity.[clarification needed] Nylon had characteristics that could be used for toothbrushes (sold from 1938) and very thin nylon fiber could compete with silk and rayon in various textiles normally not produced from hemp fiber, such as very thin stockings for women.[168][176][177][178][179]
 While the Marijuana Tax Act of 1937 had just been signed into law, the United States Department of Agriculture lifted the tax on hemp cultivation during WWII.[180] Before WWII, the U.S. Navy used Jute and Manila Hemp from the Philippines and Indonesia for the cordage on their ships. During the war, Japan cut off those supply lines.[181] America was forced to turn inward and revitalize the cultivation of Hemp on U.S. soils.
 Hemp was used extensively by the United States during World War II to make uniforms, canvas, and rope.[182] Much of the hemp used was cultivated in Kentucky and the Midwest. During World War II, the U.S. produced a short 1942 film, Hemp for Victory, promoting hemp as a necessary crop to win the war.[181] By the 1980s the film was largely forgotten, and the U.S. government even denied its existence.[183] The film, and the important historical role of hemp in U.S. agriculture and commerce was brought to light by hemp activist Jack Herer in the book The Emperor Wears No Clothes.
 U.S. farmers participated in the campaign to increase U.S. hemp production to 36,000 acres in 1942.[184] This increase amounted to more than 20 times the production in 1941 before the war effort.[184]
 In the United States, Executive Order 12919 (1994) identified hemp as a strategic national product that should be stockpiled.[185]
 Hemp has been grown for millennia in Asia and the Middle East for its fiber. Commercial production of hemp in the West took off in the eighteenth century, but was grown in the sixteenth century in eastern England.[188] Because of colonial and naval expansion of the era, economies needed large quantities of hemp for rope and oakum. In the early 1940s, world production of hemp fiber ranged from 250,000 to 350,000 metric tons, Russia was the biggest producer.[168]
 In Western Europe, the cultivation of hemp was not legally banned by the 1930s, but the commercial cultivation stopped by then, due to decreased demand compared to increasingly popular artificial fibers.[189] Speculation about the potential for commercial cultivation of hemp in large quantities has been criticized due to successful competition from other fibers for many products. The world production of hemp fiber fell from over 300,000 metric tons 1961 to about 75,000 metric tons in the early 1990s and has after that been stable at that level.[190]
 In Japan, hemp was historically used as paper and a fiber crop. There is archaeological evidence cannabis was used for clothing and the seeds were eaten in Japan back to the Jōmon period (10,000 to 300 BC). Many Kimono designs portray hemp, or asa (Japanese: 麻), as a beautiful plant. In 1948, marijuana was restricted as a narcotic drug. The ban on marijuana imposed by the United States authorities was alien to Japanese culture, as the drug had never been widely used in Japan before. Though these laws against marijuana are some of the world's strictest, allowing five years imprisonment for possession of the drug, they exempt hemp growers, whose crop is used to make robes for Buddhist monks and loincloths for Sumo wrestlers. Because marijuana use in Japan has doubled in the past decade, these exemptions have recently been called into question.[191]
 The cultivation of hemp in Portuguese lands began around the fourteenth century.[citation needed] The raw material was used for the preparation of rope and plugs for the Portuguese ships. Portugal also utilized its colonies to support its hemp supply, including in certain parts of Brazil.[192]
 In order to recover the ailing Portuguese naval fleet after the Restoration of Independence in 1640, King John IV put a renewed emphasis on the growing of hemp. He ordered the creation of the Royal Linen and Hemp Factory in the town of Torre de Moncorvo to increase production and support the effort.[193]
 In 1971, the cultivation of hemp became illegal, and the production was substantially reduced. Because of EU regulations 1308–70, 619/71 and 1164–89, this law was revoked (for some certified seed varieties).[194]
"
Epilepsy,https://en.wikipedia.org/wiki/Epilepsy,"

 Epilepsy is a group of non-communicable neurological disorders characterized by recurrent epileptic seizures.[10] An epileptic seizure is the clinical manifestation of an abnormal, excessive, and synchronized electrical discharge in the neurons.[1] The occurrence of two or more unprovoked seizures defines epilepsy.[11] The occurrence of just one seizure may warrant the definition (set out by the International League Against Epilepsy) in a more clinical usage where recurrence may be able to be prejudged.[10] Epileptic seizures can vary from brief and nearly undetectable periods to long periods of vigorous shaking due to abnormal electrical activity in the brain.[1] These episodes can result in physical injuries, either directly, such as broken bones, or through causing accidents.[1] In epilepsy, seizures tend to recur and may have no detectable underlying cause.[11] Isolated seizures that are provoked by a specific cause such as poisoning are not deemed to represent epilepsy.[12] People with epilepsy may be treated differently in various areas of the world and experience varying degrees of social stigma due to the alarming nature of their symptoms.[11]
 The underlying mechanism of an epileptic seizure is excessive and abnormal neuronal activity in the cortex of the brain,[12] which can be observed in the electroencephalogram (EEG) of an individual. The reason this occurs in most cases of epilepsy is unknown (cryptogenic);[1] some cases occur as the result of brain injury, stroke, brain tumors, infections of the brain, or birth defects through a process known as epileptogenesis.[1][2][3] Known genetic mutations are directly linked to a small proportion of cases.[4][13] The diagnosis involves ruling out other conditions that might cause similar symptoms, such as fainting, and determining if another cause of seizures is present, such as alcohol withdrawal or electrolyte problems.[4] This may be partly done by imaging the brain and performing blood tests.[4] Epilepsy can often be confirmed with an EEG, but a normal reading does not rule out the condition.[4]
 Epilepsy that occurs as a result of other issues may be preventable.[1] Seizures are controllable with medication in about 69% of cases;[7] inexpensive anti-seizure medications are often available.[1] In those whose seizures do not respond to medication; surgery, neurostimulation or dietary changes may be considered.[5][6] Not all cases of epilepsy are lifelong, and many people improve to the point that treatment is no longer needed.[1]
 As of 2021[update], about 51 million people have epilepsy. Nearly 80% of cases occur in the developing world.[1][8] In 2021, it resulted in 140,000 deaths, an increase from 125,000 in 1990.[9][14][15] Epilepsy is more common in children and older people.[16][17] In the developed world, onset of new cases occurs most frequently in babies and the elderly.[18] In the developing world, onset is more common at the extremes of age – in younger children and in older children and young adults due to differences in the frequency of the underlying causes.[19] About 5–10% of people will have an unprovoked seizure by the age of 80.[20] The chance of experiencing a second seizure within two years after the first is around 40%.[21][22] In many areas of the world, those with epilepsy either have restrictions placed on their ability to drive or are not permitted to drive until they are free of seizures for a specific length of time.[23] The word epilepsy is from Ancient Greek ἐπιλαμβάνειν, 'to seize, possess, or afflict'.[24]
 Epilepsy is characterized by a long-term risk of recurrent epileptic seizures.[25] These seizures may present in several ways depending on the parts of the brain involved and the person's age.[25][26]
 The most common type (60%) of seizures are convulsive which involve involuntary muscle contractions.[26] Of these, one-third begin as generalized seizures from the start, affecting both hemispheres of the brain and impairing consciousness.[26] Two-thirds begin as focal seizures (which affect one hemisphere of the brain) which may progress to generalized seizures.[26] The remaining 40% of seizures are non-convulsive. An example of this type is the absence seizure, which presents as a decreased level of consciousness and usually lasts about 10 seconds.[2][27]
 Certain experiences, known as auras often precede focal seizures.[28] The seizures can include sensory (visual, hearing, or smell), psychic, autonomic, and motor phenomena depending on which part of the brain is involved.[2] Muscle jerks may start in a specific muscle group and spread to surrounding muscle groups in which case it is known as a Jacksonian march.[29] Automatisms may occur, which are non-consciously generated activities and mostly simple repetitive movements like smacking the lips or more complex activities such as attempts to pick up something.[29]
 There are six main types of generalized seizures:
 They all involve loss of consciousness and typically happen without warning.
 Tonic-clonic seizures occur with a contraction of the limbs followed by their extension and arching of the back which lasts 10–30 seconds (the tonic phase). A cry may be heard due to contraction of the chest muscles, followed by a shaking of the limbs in unison (clonic phase). Tonic seizures produce constant contractions of the muscles. A person often turns blue as breathing is stopped. In clonic seizures there is shaking of the limbs in unison. After the shaking has stopped it may take 10–30 minutes for the person to return to normal; this period is called the ""postictal state"" or ""postictal phase."" Loss of bowel or bladder control may occur during a seizure.[31] People experiencing a seizure may bite their tongue, either the tip or on the sides;[32] in tonic-clonic seizure, bites to the sides are more common.[32] Tongue bites are also relatively common in psychogenic non-epileptic seizures.[32] Psychogenic non-epileptic seizures are seizure like behavior without an associated synchronised electrical discharge on EEG and are considered a dissociative disorder.[32]
 Myoclonic seizures involve very brief muscle spasms in either a few areas or all over.[33][34] These sometimes cause the person to fall, which can cause injury.[33] Absence seizures can be subtle with only a slight turn of the head or eye blinking with impaired consciousness;[2] typically, the person does not fall over and returns to normal right after it ends.[2] Atonic seizures involve losing muscle activity for greater than one second,[29] typically occurring on both sides of the body.[29] Rarer seizure types can cause involuntary unnatural laughter (gelastic), crying (dyscrastic), or more complex experiences such as déjà vu.[34]
 About 6% of those with epilepsy have seizures that are often triggered by specific events and are known as reflex seizures.[35] Those with reflex epilepsy have seizures that are only triggered by specific stimuli.[36] Common triggers include flashing lights and sudden noises.[35] In certain types of epilepsy, seizures happen more often during sleep,[37] and in other types they occur almost only when sleeping.[38] In 2017, the International League Against Epilepsy published new uniform guidelines for the classification of seizures as well as epilepsies along with their cause and comorbidities.[39]
 People with epilepsy may experience seizure clusters which may be broadly defined as an acute deterioration in seizure control.[40] The prevalence of seizure clusters is uncertain given that studies have used different definitions to define them.[41] However, estimates suggest that the prevalence may range from 5% to 50% of people with epilepsy.[42] People with refractory epilepsy who have a high seizure frequency are at the greatest risk for having seizure clusters.[43][44][45] Seizure clusters are associated with increased healthcare use, worse quality of life, impaired psychosocial functioning, and possibly increased mortality.[41][46] Benzodiazepines are used as an acute treatment for seizure clusters.[47]
 After the active portion of a seizure (the ictal state) there is typically a period of recovery during which there is confusion, referred to as the postictal period, before a normal level of consciousness returns.[28] It usually lasts 3 to 15 minutes[48] but may last for hours.[49] Other common symptoms include feeling tired, headache, difficulty speaking, and abnormal behavior.[49] Psychosis after a seizure is relatively common, occurring in 6–10% of people.[50] Often people do not remember what happened during this time.[49] Localized weakness, known as Todd's paralysis, may also occur after a focal seizure. It would typically last for seconds to minutes but may rarely last for a day or two.[51]
 Epilepsy can have adverse effects on social and psychological well-being.[26] These effects may include social isolation, stigmatization, or disability.[26] They may result in lower educational achievement and worse employment outcomes.[26] Learning disabilities are common in those with the condition, and especially among children with epilepsy.[26] The stigma of epilepsy can also affect the families of those with the disorder.[31]
 Certain disorders occur more often in people with epilepsy, depending partly on the epilepsy syndrome present. These include depression, anxiety, obsessive–compulsive disorder (OCD),[52] and migraine.[53] Attention deficit hyperactivity disorder (ADHD) affects three to five times more children with epilepsy than children without the condition.[54] ADHD and epilepsy have significant consequences on a child's behavioral, learning, and social development.[55] Epilepsy is also more common in children with autism.[56]
 Approximately, one-in-three people with epilepsy have a lifetime history of a psychiatric disorder.[57] There are believed to be multiple causes for this including pathophysiological changes related to the epilepsy itself as well as adverse experiences related to living with epilepsy (e.g., stigma, discrimination).[58] In addition, it is thought that the relationship between epilepsy and psychiatric disorders is not unilateral but rather bidirectional. For example, people with depression have an increased risk for developing new-onset epilepsy.[59]
 The presence of comorbid depression or anxiety in people with epilepsy is associated with a poorer quality of life, increased mortality, increased healthcare use and a worse response to treatment (including surgical).[60][61][62][63] Anxiety disorders and depression may explain more variability in quality of life than seizure type or frequency.[64] There is evidence that both depression and anxiety disorders are underdiagnosed and undertreated in people with epilepsy.[65]
 Epilepsy can have both genetic and acquired causes, with the interaction of these factors in many cases.[66][67] Established acquired causes include serious brain trauma, stroke, tumours, and brain problems resulting from a previous infection.[66] In about 60% of cases, the cause is unknown.[26][31] Epilepsies caused by genetic, congenital, or developmental conditions are more common among younger people, while brain tumors and strokes are more likely in older people.[26]
 Seizures may also occur as a consequence of other health problems;[30] if they occur right around a specific cause, such as a stroke, head injury, toxic ingestion, or metabolic problem, they are known as acute symptomatic seizures and are in the broader classification of seizure-related disorders rather than epilepsy itself.[68][69]
 Genetics is believed to be involved in the majority of cases, either directly or indirectly.[13][70] Some epilepsies are due to a single gene defect (1–2%); most are due to the interaction of multiple genes and environmental factors.[13] Each of the single gene defects is rare, with more than 200 in all described.[71] Most genes involved affect ion channels, either directly or indirectly.[66] These include genes for ion channels, enzymes, GABA, and G protein-coupled receptors.[33]
 In identical twins, if one is affected, there is a 50–60% chance that the other will also be affected.[13] In non-identical twins, the risk is 15%.[13] These risks are greater in those with generalized rather than focal seizures.[13] If both twins are affected, most of the time they have the same epileptic syndrome (70–90%).[13] Other close relatives of a person with epilepsy have a risk five times that of the general population.[72] Between 1 and 10% of those with Down syndrome and 90% of those with Angelman syndrome have epilepsy.[72]
 Phakomatoses, also known as neurocutaneous disorders, are a group of multisystemic diseases that most prominently affect the skin and central nervous system. They are caused by defective development of the embryonic ectodermal tissue that is most often due to a single genetic mutation. The brain, as well as other neural tissue and the skin, are all derived from the ectoderm and thus defective development may result in epilepsy as well as other manifestations such as autism and intellectual disability. Some types of phakomatoses such as tuberous sclerosis complex and Sturge-Weber syndrome have a higher prevalence of epilepsy relative to others such as neurofibromatosis type 1.[73]
 Tuberous sclerosis complex is an autosomal dominant disorder that is caused by mutations in either the TSC1 or TSC2 gene and it affects approximately 1 in 6,000–10,000 live births.[74][75] These mutations result in the upregulation of the mechanistic target of rapamycin (mTOR) pathway which leads to the growth of tumors in many organs including the brain, skin, heart, eyes and kidneys.[75] In addition, abnormal mTOR activity is believed to alter neural excitability.[76] The prevalence of epilepsy is estimated to be 80-90%.[73][76] The majority of cases of epilepsy present within the first 3 years of life and are medically refractory.[77] Relatively recent developments for the treatment of epilepsy in people with TSC include mTOR inhibitors, cannabidiol and vigabatrin. Epilepsy surgery is often pursued.
 Sturge-Weber syndrome is caused by an activating somatic mutation in the GNAQ gene and it affects approximately 1 in 20,000–50,000 live births.[78] The mutation results in vascular malformations affecting the brain, skin and eyes. The typical presentation includes a facial port-wine birthmark, ocular angiomas and cerebral vascular malformations which are most often unilateral but are bilateral in 15% of cases.[79] The prevalence of epilepsy is 75-100% and is higher in those with bilateral involvement.[79] Seizures typically occur within the first two years of life and are refractory in nearly half of cases.[80] However, high rates of seizure freedom with surgery have been reported in as many as 83%.[81]
 Neurofibromatosis type 1 is the most common phakomatoses and occurs in approximately 1 in 3,000 live births.[82] It is caused by autosomal dominant mutations in the Neurofibromin 1 gene. Clinical manifestations are variable but may include hyperpigmented skin marks, hamartomas of the iris called Lisch nodules, neurofibromas, optic pathway gliomas and cognitive impairment. The prevalence of epilepsy is estimated to be 4–7%.[83] Seizures are typically easier to control with anti-seizure medications relative to other phakomatoses but in some refractory cases surgery may need to be pursued.[84]
 Epilepsy may occur as a result of several other conditions, including tumors, strokes, head trauma, previous infections of the central nervous system, genetic abnormalities, and as a result of brain damage around the time of birth.[30][31] Of those with brain tumors, almost 30% have epilepsy, making them the cause of about 4% of cases.[72] The risk is greatest for tumors in the temporal lobe and those that grow slowly.[72] Other mass lesions such as cerebral cavernous malformations and arteriovenous malformations have risks as high as 40–60%.[72] Of those who have had a stroke, 6–10% develop epilepsy.[85][86] Risk factors for post-stroke epilepsy include stroke severity, cortical involvement, hemorrhage and early seizures.[87][88] Between 6 and 20% of epilepsy is believed to be due to head trauma.[72] Mild brain injury increases the risk about two-fold while severe brain injury increases the risk seven-fold.[72] In those who have experienced a high-powered gunshot wound to the head, the risk is about 50%.[72]
 Some evidence links epilepsy and celiac disease and non-celiac gluten sensitivity, while other evidence does not. There appears to be a specific syndrome that includes coeliac disease, epilepsy, and calcifications in the brain.[89][90] A 2012 review estimates that between 1% and 6% of people with epilepsy have coeliac disease while 1% of the general population has the condition.[90]
 The risk of epilepsy following meningitis is less than 10%; it more commonly causes seizures during the infection itself.[72] In herpes simplex encephalitis the risk of a seizure is around 50%[72] with a high risk of epilepsy following (up to 25%).[91][92] A form of an infection with the pork tapeworm (cysticercosis), in the brain, is known as neurocysticercosis, and is the cause of up to half of epilepsy cases in areas of the world where the parasite is common.[72] Epilepsy may also occur after other brain infections such as cerebral malaria, toxoplasmosis, and toxocariasis.[72] Chronic alcohol use increases the risk of epilepsy: those who drink six units of alcohol per day have a 2.5-fold increase in risk.[72] Other risks include Alzheimer's disease, multiple sclerosis, and autoimmune encephalitis.[72] Getting vaccinated does not increase the risk of epilepsy.[72] Malnutrition is a risk factor seen mostly in the developing world, although it is unclear however if it is a direct cause or an association.[19] People with cerebral palsy have an increased risk of epilepsy, with half of people with spastic quadriplegia and spastic hemiplegia having the condition.[93]
 Normally brain electrical activity is non-synchronous, as large numbers of neurons do not normally fire at the same time, but rather fire in order as signals travel throughout the brain.[2] Neuron activity is regulated by various factors both within the cell and the cellular environment. Factors within the neuron include the type, number and distribution of ion channels, changes to receptors and changes of gene expression.[94] Factors around the neuron include ion concentrations, synaptic plasticity and regulation of transmitter breakdown by glial cells.[94][95]
 The exact mechanism of epilepsy is unknown,[96] but a little is known about its cellular and network mechanisms. However, it is unknown under which circumstances the brain shifts into the activity of a seizure with its excessive synchronization.[97][98][99][100]
 In epilepsy, the resistance of excitatory neurons to fire during this period is decreased.[2] This may occur due to changes in ion channels or inhibitory neurons not functioning properly.[2] This then results in a specific area from which seizures may develop, known as a ""seizure focus"".[2] Another mechanism of epilepsy may be the up-regulation of excitatory circuits or down-regulation of inhibitory circuits following an injury to the brain.[2][3] These secondary epilepsies occur through processes known as epileptogenesis.[2][3] Failure of the blood–brain barrier may also be a causal mechanism as it would allow substances in the blood to enter the brain.[101]
 There is evidence that epileptic seizures are usually not a random event. Seizures are often brought on by factors (also known as triggers) such as stress, excessive alcohol use, flickering light, or a lack of sleep, among others. The term seizure threshold is used to indicate the amount of stimulus necessary to bring about a seizure; this threshold is lowered in epilepsy.[97]
 In epileptic seizures a group of neurons begin firing in an abnormal, excessive,[26] and synchronized manner.[2] This results in a wave of depolarization known as a paroxysmal depolarizing shift.[102] Normally, after an excitatory neuron fires it becomes more resistant to firing for a period of time.[2] This is due in part to the effect of inhibitory neurons, electrical changes within the excitatory neuron, and the negative effects of adenosine.[2]
 Focal seizures begin in one area of the brain while generalized seizures begin in both hemispheres.[30] Some types of seizures may change brain structure, while others appear to have little effect.[103] Gliosis, neuronal loss, and atrophy of specific areas of the brain are linked to epilepsy but it is unclear if epilepsy causes these changes or if these changes result in epilepsy.[103]
 The seizures can be described on different scales, from the cellular level[104] to the whole brain.[105] These are several concomitant factor, which on different scale can ""drive"" the brain to pathological states and trigger a seizure.
 The diagnosis of epilepsy is typically made based on observation of the seizure onset and the underlying cause.[26] An electroencephalogram (EEG) to look for abnormal patterns of brain waves and neuroimaging (CT scan or MRI) to look at the structure of the brain are also usually part of the initial investigations.[26] While figuring out a specific epileptic syndrome is often attempted, it is not always possible.[26] Video and EEG monitoring may be useful in difficult cases.[106]
 Epilepsy is a disorder of the brain defined by any of the following conditions:[10]
 Furthermore, epilepsy is considered to be resolved for individuals who had an age-dependent epilepsy syndrome but are now past that age or those who have remained seizure-free for the last 10 years, with no seizure medicines for the last 5 years.[10]
 This 2014 definition of the International League Against Epilepsy[10] (ILAE) is a clarification of the ILAE 2005 conceptual definition, according to which epilepsy is ""a disorder of the brain characterized by an enduring predisposition to generate epileptic seizures and by the neurobiologic, cognitive, psychological, and social consequences of this condition. The definition of epilepsy requires the occurrence of at least one epileptic seizure.""[12][107]
 It is, therefore, possible to outgrow epilepsy or to undergo treatment that causes epilepsy to be resolved, but with no guarantee that it will not return. In the definition, epilepsy is now called a disease, rather than a disorder. This was a decision of the executive committee of the ILAE, taken because the word disorder, while perhaps having less stigma than does disease, also does not express the degree of seriousness that epilepsy deserves.[10]
 The definition is practical in nature and is designed for clinical use. In particular, it aims to clarify when an ""enduring predisposition"" according to the 2005 conceptual definition is present. Researchers, statistically minded epidemiologists, and other specialized groups may choose to use the older definition or a definition of their own devising. The ILAE considers doing so is perfectly allowable, so long as it is clear what definition is being used.[10]
 The ILAE definition for one seizure needs an understanding of projecting an enduring predisposition to the generation of epileptic seizures.[10] WHO, for instance, chooses to just use the traditional definition of two unprovoked seizures.[11]
 In contrast to the classification of seizures which focuses on what happens during a seizure, the classification of epilepsies focuses on the underlying causes. When a person is admitted to hospital after an epileptic seizure the diagnostic workup results preferably in the seizure itself being classified (e.g. tonic-clonic) and in the underlying disease being identified (e.g. hippocampal sclerosis).[106] The name of the diagnosis finally made depends on the available diagnostic results and the applied definitions and classifications (of seizures and epilepsies) and its respective terminology.
 The International League Against Epilepsy (ILAE) provided a classification of the epilepsies and epileptic syndromes in 1989 as follows:[108]
 This classification was widely accepted but has also been criticized mainly because the underlying causes of epilepsy (which are a major determinant of clinical course and prognosis) were not covered in detail.[109] In 2010 the ILAE Commission for Classification of the Epilepsies addressed this issue and divided epilepsies into three categories (genetic, structural/metabolic, unknown cause)[110] which were refined in their 2011 recommendation into four categories and a number of subcategories reflecting recent technological and scientific advances.[111]
 Cases of epilepsy may be organized into epilepsy syndromes by the specific features that are present. These features include the age that seizures begin, the seizure types, EEG findings, among others. Identifying an epilepsy syndrome is useful as it helps determine the underlying causes as well as what anti-seizure medication should be tried.[30][114]
 The ability to categorize a case of epilepsy into a specific syndrome occurs more often with children since the onset of seizures is commonly early.[69] Less serious examples are benign rolandic epilepsy (2.8 per 100,000), childhood absence epilepsy (0.8 per 100,000) and juvenile myoclonic epilepsy (0.7 per 100,000).[69] Severe syndromes with diffuse brain dysfunction caused, at least partly, by some aspect of epilepsy, are also referred to as developmental and epileptic encephalopathies. These are associated with frequent seizures that are resistant to treatment and cognitive dysfunction, for instance Lennox–Gastaut syndrome (1–2% of all persons with epilepsy),[115] Dravet syndrome (1: 15,000-40,000 worldwide[116]), and West syndrome (1–9: 100,000[117]).[118] Genetics is believed to play an important role in epilepsies by a number of mechanisms. Simple and complex modes of inheritance have been identified for some of them. However, extensive screening have failed to identify many single gene variants of large effect.[119] More recent exome and genome sequencing studies have begun to reveal a number of de novo gene mutations that are responsible for some epileptic encephalopathies, including CHD2 and SYNGAP1[120][121][122] and DNM1, GABBR2, FASN and RYR3.[123]
 Syndromes in which causes are not clearly identified are difficult to match with categories of the current classification of epilepsy. Categorization for these cases was made somewhat arbitrarily.[111] The idiopathic (unknown cause) category of the 2011 classification includes syndromes in which the general clinical features and/or age specificity strongly point to a presumed genetic cause.[111] Some childhood epilepsy syndromes are included in the unknown cause category in which the cause is presumed genetic, for instance benign rolandic epilepsy.[111] Clinical syndromes in which epilepsy is not the main feature (e.g. Angelman syndrome) were categorized symptomatic but it was argued to include these within the category idiopathic.[111] Classification of epilepsies and particularly of epilepsy syndromes will change with advances in research.[111]
 An electroencephalogram (EEG) can assist in showing brain activity suggestive of an increased risk of seizures. It is only recommended for those who are likely to have had an epileptic seizure on the basis of symptoms. In the diagnosis of epilepsy, electroencephalography may help distinguish the type of seizure or syndrome present.[124] In children it is typically only needed after a second seizure unless specified by a specialist. It cannot be used to rule out the diagnosis and may be falsely positive in those without the condition.[124] In certain situations it may be useful to perform the EEG while the affected individual is sleeping or sleep deprived.[106]
 Diagnostic imaging by CT scan and MRI is recommended after a first non-febrile seizure to detect structural problems in and around the brain.[106] MRI is generally a better imaging test except when bleeding is suspected, for which CT is more sensitive and more easily available.[20] If someone attends the emergency room with a seizure but returns to normal quickly, imaging tests may be done at a later point.[20] If a person has a previous diagnosis of epilepsy with previous imaging, repeating the imaging is usually not needed even if there are subsequent seizures.[106][125]
 For adults, the testing of electrolyte, blood glucose and calcium levels is important to rule out problems with these as causes.[106] An electrocardiogram can rule out problems with the rhythm of the heart.[106] A lumbar puncture may be useful to diagnose a central nervous system infection but is not routinely needed.[20] In children additional tests may be required such as urine biochemistry and blood testing looking for metabolic disorders.[106][126] Together with EEG and neuroimaging, genetic testing is becoming one of the most important diagnostic technique for epilepsy, as a diagnosis might be achieved in a relevant proportion of cases with severe epilepsies, both in children and adults.[127] For those with negative genetic testing, in some it might be important to repeat or re-analyze previous genetic studies after 2–3 years.[128]
 A high blood prolactin level within the first 20 minutes following a seizure may be useful to help confirm an epileptic seizure as opposed to psychogenic non-epileptic seizure.[129][130] Serum prolactin level is less useful for detecting focal seizures.[131] If it is normal an epileptic seizure is still possible[130] and a serum prolactin does not separate epileptic seizures from syncope.[132] It is not recommended as a routine part of the diagnosis of epilepsy.[106]
 Diagnosis of epilepsy can be difficult. A number of other conditions may present very similar signs and symptoms to seizures, including syncope, hyperventilation, migraines, narcolepsy, panic attacks and psychogenic non-epileptic seizures (PNES).[133][134] In particular, syncope can be accompanied by a short episode of convulsions.[135] Nocturnal frontal lobe epilepsy, often misdiagnosed as nightmares, was considered to be a parasomnia but later identified to be an epilepsy syndrome.[136] Attacks of the movement disorder paroxysmal dyskinesia may be taken for epileptic seizures.[137] The cause of a drop attack can be, among many others, an atonic seizure.[134]
 Children may have behaviors that are easily mistaken for epileptic seizures but are not. These include breath-holding spells, bedwetting, night terrors, tics and shudder attacks.[134] Gastroesophageal reflux may cause arching of the back and twisting of the head to the side in infants, which may be mistaken for tonic-clonic seizures.[134]
 Misdiagnosis is frequent (occurring in about 5 to 30% of cases).[26] Different studies showed that in many cases seizure-like attacks in apparent treatment-resistant epilepsy have a cardiovascular cause.[135][138] Approximately 20% of the people seen at epilepsy clinics have PNES[20] and of those who have PNES about 10% also have epilepsy;[139] separating the two based on the seizure episode alone without further testing is often difficult.[139]
 While many cases are not preventable, efforts to reduce head injuries,[7] provide good care around the time of birth, and reduce environmental parasites such as the pork tapeworm may be effective.[31] After brain injuries, there is a limited window of time to intervene with treatments to prevent epilepsy, similar to the therapeutic approach used in stroke therapy. Epileptogenesis may occur rapidly, further narrowing this window, but a delayed process known as ""secondary epileptogenesis"" can influence the progression and severity of epilepsy, offering opportunities for intervention even after its onset. Current research focuses on identifying methods and targets to prevent or slow epilepsy development. Promising treatments include drugs such as TrkB inhibitors, losartan, statins, isoflurane, anti-inflammatory and anti-oxidative drugs, the SV2A modulator levetiracetam, and epigenetic interventions.[140] Efforts in one part of Central America to decrease rates of pork tapeworm resulted in a 50% decrease in new cases of epilepsy.[19] Yoga-based Nadi Shodhana Pranayama, also known as Alternate Nostril Breathing, may positively impact the nervous system and help manage seizure disorders. Regular exercise helps balance brain function by providing the body with oxygen and removing carbon dioxide and toxins from the blood.[141]
 Epilepsy can be dangerous when seizure occurs at certain times. The risk of drowning or being involved in a motor vehicle collision is higher. It is also found that people with epilepsy are more likely to have psychological problems.[142] Other complications include aspiration pneumonia and difficulty learning.[143]
 Epilepsy is usually treated with daily medication once a second seizure has occurred,[26][106] while medication may be started after the first seizure in those at high risk for subsequent seizures.[106] Supporting people's self-management of their condition may be useful.[144] In drug-resistant cases different management options may be considered, including special diets, the implantation of a neurostimulator, or neurosurgery.
 Rolling people with an active tonic-clonic seizure onto their side and into the recovery position helps prevent fluids from getting into the lungs.[145] Putting fingers, a bite block or tongue depressor in the mouth is not recommended as it might make the person vomit or result in the rescuer being bitten.[28][145] Efforts should be taken to prevent further self-injury.[28] Spinal precautions are generally not needed.[145]
 If a seizure lasts longer than 5 minutes or if there are more than two seizures in 5 minutes without a return to a normal level of consciousness between them, it is considered a medical emergency known as status epilepticus.[106][146] This may require medical help to keep the airway open and protected;[106] a nasopharyngeal airway may be useful for this.[145] At home the recommended initial medication for seizure of a long duration is midazolam placed in the nose or mouth.[147] Diazepam may also be used rectally.[147] In hospital, intravenous lorazepam is preferred.[106]
 If two doses of benzodiazepines are not effective, other medications such as phenytoin are recommended.[106] Convulsive status epilepticus that does not respond to initial treatment typically requires admission to the intensive care unit and treatment with stronger agents such as midazolam infusion, ketamine, thiopentone or propofol.[106] Most institutions have a preferred pathway or protocol to be used in a seizure emergency like status epilepticus.[106] These protocols have been found to be effective in reducing time to delivery of treatment.[106]
 The mainstay treatment of epilepsy is anticonvulsant medications, possibly for the person's entire life.[26] The choice of anticonvulsant is based on seizure type, epilepsy syndrome, other medications used, other health problems, and the person's age and lifestyle.[147] A single medication is recommended initially;[148] if this is not effective, switching to a single other medication is recommended.[106] Two medications at once is recommended only if a single medication does not work.[106] In about half, the first agent is effective; a second single agent helps in about 13% and a third or two agents at the same time may help an additional 4%.[149] About 30% of people continue to have seizures despite anticonvulsant treatment.[7]
 There are a number of medications available including phenytoin, carbamazepine and valproate. Evidence suggests that phenytoin, carbamazepine, and valproate may be equally effective in both focal and generalized seizures.[150][151] Controlled release carbamazepine appears to work as well as immediate release carbamazepine, and may have fewer side effects.[152][153] In the United Kingdom, carbamazepine or lamotrigine are recommended as first-line treatment for focal seizures, with levetiracetam and valproate as second-line due to issues of cost and side effects.[106][154] Valproate is recommended first-line for generalized seizures with lamotrigine being second-line.[106] In those with absence seizures, ethosuximide or valproate are recommended; valproate is particularly effective in myoclonic seizures and tonic or atonic seizures.[106] If seizures are well-controlled on a particular treatment, it is not usually necessary to routinely check the medication levels in the blood.[106]
 The least expensive anticonvulsant is phenobarbital at around US$5 a year.[19] The World Health Organization gives it a first-line recommendation in the developing world and it is commonly used there.[155][156] Access, however, may be difficult as some countries label it as a controlled drug.[19]
 Adverse effects from medications are reported in 10% to 90% of people, depending on how and from whom the data is collected.[157] Most adverse effects are dose-related and mild.[157] Some examples include mood changes, sleepiness, or an unsteadiness in gait.[157] Certain medications have side effects that are not related to dose such as rashes, liver toxicity, or suppression of the bone marrow.[157] Up to a quarter of people stop treatment due to adverse effects.[157] Some medications are associated with birth defects when used in pregnancy.[106] Many of the common used medications, such as valproate, phenytoin, carbamazepine, phenobarbital, and gabapentin have been reported to cause increased risk of birth defects,[158] especially when used during the first trimester.[159] Despite this, treatment is often continued once effective, because the risk of untreated epilepsy is believed to be greater than the risk of the medications.[159] Among the antiepileptic medications, levetiracetam and lamotrigine seem to carry the lowest risk of causing birth defects.[158]
 Slowly stopping medications may be reasonable in some people who do not have a seizure for two to four years; however, around a third of people have a recurrence, most often during the first six months.[106][160] Stopping is possible in about 70% of children and 60% of adults.[31] Measuring medication levels is not generally needed in those whose seizures are well controlled.[125]
 Epilepsy surgery should be considered for any person with epilepsy who is medically refractory.[16] People with epilepsy are evaluated on a case-by-case basis in centres that are familiar with and have expertise in epilepsy surgery.[16] Results from a 2023 systematic review found that surgical interventions for children aged 1–36 months with drug-resistant epilepsy can lead to significant seizure reduction or freedom, especially when other treatments have failed.[161] Epilepsy surgery may be an option for people with focal seizures that remain a problem despite other treatments.[162][163] These other treatments include at least a trial of two or three medications.[164] The goal of surgery has been total control of seizures.[165] However, most physicians believe that even palliative surgery where the burden of seizures is reduced significantly can help in achieving developmental progress or reversal of developmental stagnation in children with drug-resistant epilepsy and this may be achieved in 60–70% of cases.[164] Common procedures include cutting out the hippocampus via an anterior temporal lobe resection, removal of tumors, and removing parts of the neocortex.[164] Some procedures such as a corpus callosotomy are attempted in an effort to decrease the number of seizures rather than cure the condition.[164] Following surgery, medications may be slowly withdrawn in many cases.[164][162]
 Neurostimulation via neuro-cybernetic prosthesis implantation may be another option in those who are not candidates for surgery, providing chronic, pulsatile electrical stimulation of specific nerve or brain regions, alongside standard care.[106] Three types of neurotherapy have been used in those who do not respond to medications: vagus nerve stimulation (VNS), anterior thalamic stimulation, and closed-loop responsive stimulation (RNS).[5][166][167]
 Non-pharmacological modulation of neurotransmitters via high-level VNS (h-VNS) may reduce seizure frequency in children and adults who do not respond to medical and/or surgical therapy, when compared with low-level VNS (l-VNS).[167] In a 2022 Cochrane review of four randomized controlled trials, with moderate certainty of evidence, people receiving h-VNS treatment were 73% more likely (13% more likely to 164% more likely) to experience a reduction in seizure frequency by at least 50% (the minimum threshold defined for individual clinical response).[167] Potentially 249 (163 to 380) per 1000 people with drug-resistant epilepsy may achieve a 50% reduction in seizures following h-VNS, benefiting an additional 105 per 1000 people compared with l-VNS.[167]
 This outcome was limited by the number of studies available, and the quality of one trial in particular, wherein three people received l-VNS in error. A sensitivity analysis suggested that the best case scenario was that the likelihood of clinical response to h-VNS may be 91% (27% to 189%) higher than those receiving l-VNS. In the worst-case scenario, the likelihood of clinical response to h-VNS was still 61% higher (7% higher to 143% higher) than l-VNS.[167]
 Despite the potential benefit for h-VNS treatment, the Cochrane review also found that the risk of several adverse-effects was greater than those receiving l-VNS. There was moderate certainty of evidence that voice alteration or hoarseness risk may be 2.17(1.49 to 3.17) fold higher than people receiving l-VNS. Dyspnoea risk was also 2.45 (1.07 to 5.60) times that of l-VNS recipients, although the low number of events and studies meant that the certainty of evidence was low. The risk of rebound-withdrawal symptoms, coughing, pain and paraesthesia was unclear.[167]
 There is promising evidence that a ketogenic diet (high-fat, low-carbohydrate, adequate-protein) decreases the number of seizures and eliminates seizures in some; however, further research is necessary.[6] A 2022 systematic review of the literature has found some evidence to support that a ketogenic diet or modified Atkins diet can be helpful in the treatment of epilepsy in some infants.[168] These types of diets may be beneficial for children with drug-resistant epilepsy; the use for adults remains uncertain.[6] The most commonly reported adverse effects were vomiting, constipation and diarrhoea.[6] It is unclear why this diet works.[169] In people with coeliac disease or non-celiac gluten sensitivity and occipital calcifications, a gluten-free diet may decrease the frequency of seizures.[90]
 Avoidance therapy consists of minimizing or eliminating triggers. For example, those who are sensitive to light may have success with using a small television, avoiding video games, or wearing dark glasses.[170] Operant-based biofeedback based on the EEG waves has some support in those who do not respond to medications.[171] Psychological methods should not, however, be used to replace medications.[106]
 Exercise has been proposed as possibly useful for preventing seizures,[172] with some data to support this claim.[173] Some dogs, commonly referred to as seizure dogs, may help during or after a seizure.[174][175] It is not clear if dogs have the ability to predict seizures before they occur.[176]
 There is moderate-quality evidence supporting the use of psychological interventions along with other treatments in epilepsy.[177] This can improve quality of life, enhance emotional wellbeing, and reduce fatigue in adults and adolescents.[177] Psychological interventions may also improve seizure control for some individuals by promoting self-management and adherence.[177]
 As an add-on therapy in those who are not well controlled with other medications, cannabidiol appears to be useful in some children.[178][179] In 2018 the FDA approved this product for Lennox–Gastaut syndrome and Dravet syndrome.[180]
 There are a few studies on the use of dexamethasone for the successful treatment of drug-resistant seizures in both adults and children.[181]
 Alternative medicine, including acupuncture,[182] routine vitamins,[183] and yoga,[184] have no reliable evidence to support their use in epilepsy. Melatonin, as of 2016[update], is insufficiently supported by evidence.[185] The trials were of poor methodological quality and it was not possible to draw any definitive conclusions.[185]
 Several supplements (with varied reliabilities of evidence) have been reported to be helpful for drug-resistant epilepsy. These include high-dose Omega-3, berberine, Manuka honey, reishi and lion's mane mushrooms, curcumin,[186] vitamin E, coenzyme Q-10, and resveratrol. The reason these can work (in theory) is that they reduce inflammation or oxidative stress, two of the major mechanism contributing to epilepsy.[187]
 Women of child-bearing age, including those with epilepsy, are at risk of unintended pregnancies if they are not using an effective form of contraception.[188] Women with epilepsy may experience a temporary increase in seizure frequency when they begin hormonal contraception.[188]
 Some anti-seizure medications interact with enzymes in the liver and cause the drugs in hormonal contraception to be broken down more quickly. These enzyme inducing drugs make hormonal contraception less effective, and this is particularly hazardous if the anti-seizure medication is associated with birth defects.[189] Potent enzyme-inducing anti-seizure medications include carbamazepine, eslicarbazepine acetate, oxcarbazepine, phenobarbital, phenytoin, primidone, and rufinamide. The drugs perampanel and topiramate can be enzyme-inducing at higher doses.[190] Conversely, hormonal contraception can lower the amount of the anti-seizure medication lamotrigine circulating in the body, making it less effective.[188] The failure rate of oral contraceptives, when used correctly, is 1%, but this increases to between 3–6% in women with epilepsy.[189] Overall, intrauterine devices (IUDs) are preferred for women with epilepsy who are not intending to become pregnant.[188]
 Women with epilepsy, especially if they have other medical conditions, may have a slightly lower, but still high, chance of becoming pregnant.[188] Women with infertility have about the same chance of success with in vitro fertilisation or other forms of assisted reproductive technology as women without epilepsy.[188] There may be a higher risk of pregnancy loss.[188]
 Once pregnant, there are two main concerns related to pregnancy. The first concern is about the risk of seizures during pregnancy, and the second concern is that the anti-seizure medications may result in birth defects.[158] Most women with epilepsy must continue treatment with anti-seizure drugs, and the treatment goal is to balance the need to prevent seizures with the need to prevent drug-induced birth defects.[188][191]
 Pregnancy does not seem to change seizure frequency very much.[188] When seizures happen, however, they can cause some pregnancy complications, such as pre-term births or the babies being smaller than usual when they are born.[188]
 All pregnancies have a risk of birth defects, e.g., due to smoking during pregnancy.[188] In addition to this typical level of risk, some anti-seizure drugs significantly increase the risk of birth defects and intrauterine growth restriction, as well as developmental, neurocognitive, and behavioral disorders.[191] Most women with epilepsy receive safe and effective treatment and have typical, healthy children.[191] The highest risks are associated with specific anti-seizure drugs, such as valproic acid and carbamazepine, and with higher doses.[158][188] Folic acid supplementation, such as through prenatal vitamins, reduced the risk.[188] Planning pregnancies in advance gives women with epilepsy an opportunity to switch to a lower-risk treatment program and reduced drug doses.[188]
 Although anti-seizure drugs can be found in breast milk, women with epilepsy can breastfeed their babies, and the benefits usually outweigh the risks.[188]
 Epilepsy cannot usually be cured, but medication can control seizures effectively in about 70% of cases.[7] Of those with generalized seizures, more than 80% can be well controlled with medications while this is true in only 50% of people with focal seizures.[5] One predictor of long-term outcome is the number of seizures that occur in the first six months.[26] Other factors increasing the risk of a poor outcome include little response to the initial treatment, generalized seizures, a family history of epilepsy, psychiatric problems, and waves on the EEG representing generalized epileptiform activity.[192] According to the ILAE epilepsy is considered to be resolved if an individual with epilepsy is seizure free for 10 years and off anticonvulsant for 5 years.[193]
 In the developing world, 75% of people are either untreated or not appropriately treated.[31] In Africa, 90% do not get treatment.[31] This is partly related to appropriate medications not being available or being too expensive.[31]
 People with epilepsy may have a higher risk of premature death compared to those without the condition.[194] This risk is estimated to be between 1.6 and 4.1 times greater than that of the general population.[195] The greatest increase in mortality from epilepsy is among the elderly.[195] Those with epilepsy due to an unknown cause have a relatively low increase in risk.[195]
 Mortality is often related to the underlying cause of the seizures, status epilepticus, suicide, trauma, and sudden unexpected death in epilepsy (SUDEP).[194] Death from status epilepticus is primarily due to an underlying problem rather than missing doses of medications.[194] The risk of suicide is between two and six times higher in those with epilepsy;[196][197] the cause of this is unclear.[196] SUDEP appears to be partly related to the frequency of generalized tonic-clonic seizures[198] and accounts for about 15% of epilepsy-related deaths;[192] it is unclear how to decrease its risk.[198]
Risk factors for SUDEP include nocturnal generalized tonic-clonic seizures, seizures, sleeping alone and medically intractable epilepsy.[199]
 In the United Kingdom, it is estimated that 40–60% of deaths are possibly preventable.[26] In the developing world, many deaths are due to untreated epilepsy leading to falls or status epilepticus.[19]
 Epilepsy is one of the most common serious neurological disorders[200] affecting about 50 million people as of 2021[update].[8][201] It affects 1% of the population by age 20 and 3% of the population by age 75.[17] It is more common in males than females with the overall difference being small.[19][69] Most of those with the disorder (80%) are in low income populations[202] or the developing world.[31]
 The estimated prevalence of active epilepsy (as of 2012[update]) is in the range 3–10 per 1,000, with active epilepsy defined as someone with epilepsy who has had at least one unprovoked seizure in the last five years.[69][203] Epilepsy begins each year in 40–70 per 100,000 in developed countries and 80–140 per 100,000 in developing countries.[31] Poverty is a risk and includes both being from a poor country and being poor relative to others within one's country.[19] In the developed world epilepsy most commonly starts either in the young or in the old.[19] In the developing world its onset is more common in older children and young adults due to the higher rates of trauma and infectious diseases.[19] In developed countries the number of cases a year has decreased in children and increased among the elderly between the 1970s and 2003.[203] This has been attributed partly to better survival following strokes in the elderly.[69]
 The oldest medical records show that epilepsy has been affecting people at least since the beginning of recorded history.[204] Throughout ancient history, the condition was thought to be of a spiritual cause.[204] The world's oldest description of an epileptic seizure comes from a text in Akkadian (a language used in ancient Mesopotamia) and was written around 2000 BC.[24] The person described in the text was diagnosed as being under the influence of a moon god, and underwent an exorcism.[24] Epileptic seizures are listed in the Code of Hammurabi (c. 1790 BC) as reason for which a purchased slave may be returned for a refund,[24] and the Edwin Smith Papyrus (c. 1700 BC) describes cases of individuals with epileptic convulsions.[24]
 The oldest known detailed record of the condition itself is in the Sakikku, a Babylonian cuneiform medical text from 1067–1046 BC.[204] This text gives signs and symptoms, details treatment and likely outcomes,[24] and describes many features of the different seizure types.[204] As the Babylonians had no biomedical understanding of the nature of epilepsy, they attributed the seizures to possession by evil spirits and called for treating the condition through spiritual means.[204] Around 900 BC, Punarvasu Atreya described epilepsy as loss of consciousness;[205] this definition was carried forward into the Ayurvedic text of Charaka Samhita (c. 400 BC).[206]
 The ancient Greeks had contradictory views of the condition. They thought of epilepsy as a form of spiritual possession, but also associated the condition with genius and the divine. One of the names they gave to it was the sacred disease (Ancient Greek: ἠ ἱερὰ νόσος).[24][207] Epilepsy appears within Greek mythology: it is associated with the Moon goddesses Selene and Artemis, who afflicted those who upset them. The Greeks thought that important figures such as Julius Caesar and Hercules had the condition.[24] The notable exception to this divine and spiritual view was that of the school of Hippocrates. In the fifth century BC, Hippocrates rejected the idea that the condition was caused by spirits. In his landmark work On the Sacred Disease, he proposed that epilepsy was not divine in origin and instead was a medically treatable problem originating in the brain.[24][204] He accused those of attributing a sacred cause to the condition of spreading ignorance through a belief in superstitious magic.[24] Hippocrates proposed that heredity was important as a cause, described worse outcomes if the condition presents at an early age, and made note of the physical characteristics as well as the social shame associated with it.[24] Instead of referring to it as the sacred disease, he used the term great disease, giving rise to the modern term grand mal, used for tonic–clonic seizures.[24] Despite his work detailing the physical origins of the condition, his view was not accepted at the time.[204] Evil spirits continued to be blamed until at least the 17th century.[204]
 In Ancient Rome people did not eat or drink with the same pottery as that used by someone who was affected.[208] People of the time would spit on their chest believing that this would keep the problem from affecting them.[208] According to Apuleius and other ancient physicians, to detect epilepsy, it was common to light a piece of gagates, whose smoke would trigger the seizure.[209] Occasionally a spinning potter's wheel was used, perhaps a reference to photosensitive epilepsy.[210]
 In most cultures, persons with epilepsy have been stigmatized, shunned, or even imprisoned. As late as in the second half of the 20th century, in Tanzania and other parts of Africa epilepsy was associated with possession by evil spirits, witchcraft, or poisoning and was believed by many to be contagious.[211] In the Salpêtrière, the birthplace of modern neurology, Jean-Martin Charcot found people with epilepsy side by side with the mentally ill, those with chronic syphilis, and the criminally insane.[212] In Ancient Rome, epilepsy was known as the morbus comitialis or 'disease of the assembly hall' and was seen as a curse from the gods. In northern Italy, epilepsy was traditionally known as Saint Valentine's malady.[213] In at least the 1840s in the United States of America, epilepsy was known as the falling sickness or the falling fits, and was considered a form of medical insanity.[214] Around the same time period, epilepsy was known in France as the haut-mal lit. 'high evil', mal-de terre lit. 'earthen sickness', mal de Saint Jean lit. 'Saint John's sickness', mal des enfans lit. 'child sickness', and mal-caduc lit. 'falling sickness'.[214] People of epilepsy in France were also known as tombeurs lit. 'people who fall', due to the seizures and loss of consciousness in an epileptic episode.[214]
 In the mid-19th century, the first effective anti-seizure medication, bromide, was introduced.[157] The first modern treatment, phenobarbital, was developed in 1912, with phenytoin coming into use in 1938.[215]
 Social stigma is commonly experienced, around the world, by those with epilepsy.[11][216] It can affect people economically, socially and culturally.[216] In India and China, epilepsy may be used as justification to deny marriage.[31] People in some areas still believe those with epilepsy to be cursed.[19] In parts of Africa, such as Tanzania and Uganda, epilepsy is claimed to be associated with possession by evil spirits, witchcraft, or poisoning and is incorrectly believed by many to be contagious.[211][19] Before 1971 in the United Kingdom, epilepsy was considered grounds for the annulment of marriage.[31] The stigma may result in some people with epilepsy denying that they have ever had seizures.[69] A 2024 cross-sectional study revealed that 64.8% of relatives of epilepsy patients experienced moderate stigma and held moderately positive attitudes toward epilepsy. The study found that higher levels of stigma among participants were associated with more negative attitudes toward the condition. Additionally, relatives of patients who experienced frequent seizures (one or more per month) faced greater stigma, while those of patients who did not adhere to their medication regimen exhibited more negative attitudes toward epilepsy.[217]
 Seizures result in direct economic costs of about one billion dollars in the United States.[20] Epilepsy resulted in economic costs in Europe of around 15.5 billion euros in 2004.[26] In India epilepsy is estimated to result in costs of US$1.7 billion or 0.5% of the GDP.[31] It is the cause of about 1% of emergency department visits (2% for emergency departments for children) in the United States.[218]
 Those with epilepsy are at about twice the risk of being involved in a motor vehicular collision and thus in many areas of the world are not allowed to drive or only able to drive if certain conditions are met.[23] Diagnostic delay has been suggested to be a cause of some potentially avoidable motor vehicle collisions since at least one study showed that most motor vehicle accidents occurred in those with undiagnosed non-motor seizures as opposed to those with motor seizures at epilepsy onset.[219] In some places physicians are required by law to report if a person has had a seizure to the licensing body while in others the requirement is only that they encourage the person in question to report it themselves.[23] Countries that require physician reporting include Sweden, Austria, Denmark and Spain.[23] Countries that require the individual to report include the UK and New Zealand, and physicians may report if they believe the individual has not already.[23] In Canada, the United States and Australia the requirements around reporting vary by province or state.[23] If seizures are well controlled most feel allowing driving is reasonable.[220] The amount of time a person must be free from seizures before they can drive varies by country.[220] Many countries require one to three years without seizures.[220] In the United States the time needed without a seizure is determined by each state and is between three months and one year.[220]
 Those with epilepsy or seizures are typically denied a pilot license.[221]
 There are organizations that provide support for people and families affected by epilepsy. The Out of the Shadows campaign, a joint effort by the World Health Organization, the ILAE and the International Bureau for Epilepsy, provides help internationally.[31] In the United States, the Epilepsy Foundation is a national organization that works to increase the acceptance of those with the disorder, their ability to function in society and to promote research for a cure.[227] The Epilepsy Foundation, some hospitals, and some individuals also run support groups in the United States.[228] In Australia, the Epilepsy Foundation provides support, delivers education and training and funds research for people living with epilepsy.
 International Epilepsy Day (World Epilepsy Day) began in 2015 and occurs on the second Monday in February.[229][230]
 Purple Day, a different world-wide epilepsy awareness day for epilepsy, was initiated by a nine-year-old Canadian named Cassidy Megan in 2008, and is every year on 26 March.[231]
 Seizure prediction refers to attempts to forecast epileptic seizures based on the EEG before they occur.[232] As of 2011[update], no effective mechanism to predict seizures has been developed.[232] Although no effective device that can predict seizures is available, the science behind seizure prediction and ability to deliver such a tool has made progress.
 Kindling, where repeated exposures to events that could cause seizures eventually causes seizures more easily, has been used to create animal models of epilepsy.[233]
Different animal models of epilepsy have been characterized in rodents that recapitulate the EEG and behavioral concomitants of different forms of epilepsy, in particular the occurrence of recurrent spontaneous seizures.[234] Because epileptic seizures of different kinds are observed naturally in some of these animals, strains of mice and rats have been selected to be used as genetic models of epilepsy. In particular, several lines of mice and rats display spike-and-wave discharges when EEG recorded and have been studied to understand absence epilepsy.[235] Among these models, the strain of GAERS (Genetic Absence Epilepsy Rats from Strasbourg) was characterized in the 1980s and has helped to understand the mechanisms underlying childhood absence epilepsy.[236]
 Rat brain slices serve as a valuable model for assessing the potential of compounds in reducing epileptiform activity. By evaluating the frequency of epileptiform bursting in hippocampal networks, researchers can identify promising candidates for novel anti-seizure drugs.[237]
 Reductionist views on the mechanisms of epileptiform discharges are often expressed through mathematical models. The simplest of these models are based on a few ordinary differential equations, such as the Epileptor model.[238] The more physiologically explicit Epileptor-2 model[239] replicates brief interictal discharges—observed as clusters of action potential spikes in the activity of individual neurons—and longer ictal discharges, represented as clusters of these shorter discharges. According to this model, brief interictal discharges are characterized as stochastic oscillations of the membrane potential and synaptic resources, while ictal discharges emerge as oscillations in the extracellular concentration of potassium ions and the intracellular concentration of sodium ions. These models demonstrate [240] that ionic dynamics play a decisive role in the generation of pathological activity.
 One of the hypotheses present in the literature is based on inflammatory pathways. Studies supporting this mechanism revealed that inflammatory, glycolipid, and oxidative factors are higher in people with epilepsy, especially those with generalized epilepsy.[241]
 Gene therapy is being studied in some types of epilepsy.[242] Medications that alter immune function, such as intravenous immunoglobulins, may reduce the frequency of seizures when including in normal care as an add-on therapy; however, further research is required to determine whether these medications are very well tolerated in children and in adults with epilepsy.[243] Noninvasive stereotactic radiosurgery is, as of 2012[update], being compared to standard surgery for certain types of epilepsy.[244]
 Epilepsy occurs in a number of other animals including dogs and cats; it is in fact the most common brain disorder in dogs.[245] It is typically treated with anticonvulsants such as levetiracetam, phenobarbital, or bromide in dogs and phenobarbital in cats.[245] Imepitoin is also used in dogs.[246] While generalized seizures in horses are fairly easy to diagnose, it may be more difficult in non-generalized seizures and EEGs may be useful.[247] Juvenile idiopathic epilepsy (JIE) in foals is a condition with varying outcomes, depending on the severity and management of the condition. Some foals eventually outgrow the condition without significant long-term effects, while others may face severe consequences, including death or lifelong complications, if left untreated. This variability highlights the importance of timely intervention and care. Earlier research has pointed to a significant genetic influence in the development of JIE, suggesting that the condition may follow the inheritance pattern of a single-gene trait. These findings underscore the need for further genetic studies to confirm this hypothesis and explore potential breeding strategies to reduce the prevalence of JIE.[248]
 
"
British Parliament,https://en.wikipedia.org/wiki/British_Parliament,"

 The Parliament of the United Kingdom of Great Britain and Northern Ireland[f] is the supreme legislative body of the United Kingdom, and may also legislate for the Crown Dependencies and the British Overseas Territories.[4][5] It meets at the Palace of Westminster in London. Parliament possesses legislative supremacy and thereby holds ultimate power over all other political bodies in the United Kingdom and the Overseas Territories. While Parliament is bicameral, it has three parts: the sovereign, the House of Lords, and the House of Commons.[6] The three parts acting together to legislate may be described as the King-in-Parliament.[7] The Crown normally acts on the advice of the prime minister, and the powers of the House of Lords are limited to only delaying legislation.[8]
 The House of Commons is the elected lower chamber of Parliament, with elections to 650 single-member constituencies held at least every five years under the first-past-the-post system.[9] By constitutional convention, all government ministers, including the prime minister, are members of the House of Commons (MPs), or less commonly the House of Lords, and are thereby accountable to the respective branches of the legislature. Most Cabinet ministers are from the Commons, while junior ministers can be from either house.
 The House of Lords is the upper chamber of Parliament,[10] comprising two types of members. The most numerous are the Lords Temporal, consisting mainly of life peers appointed by the sovereign on the advice of the prime minister,[11] plus up to 92 hereditary peers. The less numerous Lords Spiritual consist of up to 26 bishops of the Church of England. Before the establishment of the Supreme Court of the United Kingdom in 2009, the House of Lords performed judicial functions through the law lords.
 The Parliament of the United Kingdom is one of the oldest legislatures in the world, and is characterised by the stability of its governing institutions and its capacity to absorb change.[12] The Westminster system shaped the political systems of the nations once ruled by the British Empire, and thus has been called the ""mother of parliaments"".[13][g]
 The Parliament of Great Britain was formed in 1707 following the ratification of the Treaty of Union by Acts of Union passed by the Parliament of England (established 1215) and the Parliament of Scotland (c. 1235), both Acts of Union stating, ""That the United Kingdom of Great Britain be represented by one and the same Parliament to be styled The Parliament of Great Britain."" At the start of the 19th century, Parliament was further enlarged by Acts of Union ratified by the Parliament of Great Britain and the Parliament of Ireland, which abolished the latter and added 100 Irish MPs and 32 Lords to the former to create the Parliament of the United Kingdom of Great Britain and Ireland. The Royal and Parliamentary Titles Act 1927 formally amended the name to the ""Parliament of the United Kingdom of Great Britain and Northern Ireland"",[15] five years after the secession of the Irish Free State.
 The United Kingdom of Great Britain and Ireland was created on 1 January 1801, by the merger of the Kingdoms of Great Britain and Ireland under the Acts of Union 1800. The principle of ministerial responsibility to the lower house (Commons) did not develop until the 19th century—the House of Lords was superior to the House of Commons both in theory and in practice. Members of the House of Commons (MPs) were elected in an antiquated electoral system, under which constituencies of vastly different sizes existed. Thus, the borough of Old Sarum, with seven voters, could elect two members, as could the borough of Dunwich, which had almost completely disappeared into the sea due to land erosion.
 Many small constituencies, known as pocket or rotten boroughs, were controlled by members of the House of Lords (peers), who could ensure the election of their relatives or supporters. During the reforms of the 19th century, beginning with the Reform Act 1832, the electoral system for the House of Commons was progressively regularised. No longer dependent on the Lords for their seats, MPs grew more assertive.
 The supremacy of the British House of Commons was reaffirmed in the early 20th century. In 1909, the Commons passed the ""People's Budget"", which made numerous changes to the taxation system which were detrimental to wealthy landowners. The House of Lords, which consisted mostly of powerful landowners, rejected the Budget. On the basis of the Budget's popularity and the Lords' consequent unpopularity, the Liberal Party narrowly won two general elections in 1910.
 Using the result as a mandate, the Liberal Prime Minister, H. H. Asquith, introduced the Parliament Bill, which sought to restrict the powers of the House of Lords. (He did not reintroduce the land tax provision of the People's Budget.) When the Lords refused to pass the bill, Asquith countered with a promise extracted from the King in secret before the second general election of 1910 and requested the creation of several hundred Liberal peers, so as to erase the Conservative majority in the House of Lords. In the face of such a threat, the House of Lords narrowly passed the bill.
 The Parliament Act 1911, as it became, prevented the Lords from blocking a money bill (a bill dealing with taxation), and allowed them to delay any other bill for a maximum of three sessions (reduced to two sessions in 1949), after which it could become law over their objections. However, regardless of the Parliament Acts of 1911 and 1949, the House of Lords has always retained the unrestricted power to veto any bill outright which attempts to extend the life of a parliament.[16]
 The result of the 1918 general election in Ireland showed a landslide victory for the Irish republican party Sinn Féin, who vowed in their manifesto to establish an independent Irish Republic. Accordingly, Sinn Féin MPs, though ostensibly elected to sit in the House of Commons, refused to take their seats in Westminster, and instead assembled in 1919 to proclaim Irish independence and form a revolutionary unicameral parliament for the independent Irish Republic, called Dáil Éireann.
 In 1920, in parallel to the Dáil, the Government of Ireland Act 1920 created home rule parliaments of Northern Ireland and Southern Ireland and reduced the representation of both parts at Westminster. The number of Northern Ireland seats was increased again after the introduction of direct rule in 1973.
 The Irish republicans responded by declaring the elections to these home rule Parliaments, held on the same day in 1921, to be the basis of membership for a new Dáil Éireann. While the elections in Northern Ireland were both contested and won by Unionist parties, in Southern Ireland, all 128 candidates for the Southern Irish seats were returned unopposed. Of these, 124 were won by Sinn Féin and four by independent Unionists representing Dublin University (Trinity College).[17] Since only four MPs sat in the home rule Southern Irish parliament, with the remaining 124 being in the Republic's Second Dáil, the home rule parliament was adjourned sine die without ever having operated.
 In 1922, pursuant to the Anglo-Irish Treaty, the revolutionary Irish Republic was replaced by the Irish Free State, recognised by the United Kingdom as a separate state (and thus, no longer represented in the Westminster Parliament), while Northern Ireland would remain British, and in 1927, parliament was renamed the Parliament of the United Kingdom of Great Britain and Northern Ireland.
 Further reforms to the House of Lords were made in the 20th century. The Life Peerages Act 1958 authorised the regular creation of life peerage dignities. By the 1960s, the regular creation of hereditary peerage dignities had ceased; thereafter, almost all new peers were life peers only.
 The House of Lords Act 1999 removed the automatic right of hereditary peers to sit in the House of Lords, although it made an exception for 92 of them to be elected to life-terms by the other hereditary peers, with by-elections upon their death. The House of Lords is now a chamber that is subordinate to the House of Commons. Additionally, the Constitutional Reform Act 2005 led to abolition of the judicial functions of the House of Lords with the creation of the new Supreme Court of the United Kingdom in October 2009.
 Under the UK's constitution, Parliament is the supreme legislative body of the state. Whilst the privy council can also issue legislation through orders-in-council, this power may be limited by Parliament like all other exercises of the royal prerogative.
 The legislative authority, the King-in-Parliament, has three separate elements: the Monarch, the House of Lords, and the House of Commons. As a result, a bill must be passed by both houses (or just the House of Commons under the Parliament Act 1911) and receive royal assent for it to become law.
 Executive powers (including those granted by legislation or forming part of the prerogative) are not formally exercised by Parliament. However, these powers are in practice exercised on the advice of government ministers, who much be drawn from and be accountable to the Parliament.
 Whilst the monarch is a constitutive element of parliament, they do not debate bills or otherwise contribute to political debate. Their royal assent is required for a bill to become law; however this has not been refused since 1708 and is largely a formality.
 The House of Lords is known formally as ""The Right Honourable The Lords Spiritual and Temporal in Parliament Assembled"", the Lords Spiritual being bishops of the Church of England and the Lords Temporal being Peers of the Realm. The Lords Spiritual and Lords Temporal are considered separate ""estates"", but they sit, debate and vote together.
 Since the passage of the Parliament Acts 1911 and 1949, the legislative powers of the House of Lords have been diminished of that of the House of Commons. Whilst the Lords debates and votes on all bills (except money bills), their refusal to pass a bill may only delay its passage for a maximum of two parliamentary sessions over a year. After this, the bill may receive royal assent and become law without the Lords' consent.
 Like in the House of Commons, the House of Lords may scrutinise governments through asking questions to government ministers that sit in the Lords and through the operation of a small number of select committees.
 The Lords used to also exercise judicial power and acted as the UK's supreme legislative court. Appeals were not heard by the whole body, but a committee of senior judges that were appointed to the Lords to act for this purpose. This power was lost when it was transferred to the newly created Supreme Court of the United Kingdom in 2009.
 Many members of the general public have questioned the need for The House of Lords in today's society.[citation needed] They say it is stopping the political system from evolving within the UK and hampering modernisation.
 The Lords Spiritual of the Lords' currently consists of the archbishops of Canterbury and York, the bishops of London, Durham and Winchester (who sit by right regardless of seniority) and 21 other diocesan bishops of the Church of England, ranked in order of consecration, subject to women being preferred if one is eligible from 2015 to 2025 under the Lords Spiritual (Women) Act 2015. Formerly, the Lords Spiritual  included all of the senior clergymen of the Church of England—archbishops, bishops, abbots and mitred priors.
 The Lords Temporal consists of 92 hereditary peers and all life peers appointed under the Life Peerages Act 1958 (currently numbering around 700).
 Two hereditary peers sit ex officio by virtue of being the Earl Marshal or the Lord Great Chamberlain. Each of the other 90 are elected for life upon a seat becoming vacant. 15 members are elected by the whole House whilst the other 75 are elected by all the hereditary peers, including those not sitting in the Lords. Currently, a standing rule divides these seats between the parties with a replacement peer from one party subject to an election only by peers of that party. Formerly, all hereditary peers were members of the Lords, until the passage of the House of Lords Act 1999 limited their numbers to 92.
 Life peers are appointed by the monarch, on the advice of the prime minister. Typically, these are members of the party of the prime minister, however some peers from other parties are also generally appointed.
 As of 2019, the House consists of 650 members; this total includes the Speaker, who by convention renounces partisan affiliation and does not take part in debates or votes, as well as three Deputy Speakers, who also do not participate in debates or votes but formally retain their party membership. Each Member of Parliament (MP) is chosen by a single constituency by the First-Past-the-Post electoral system. There are 650 constituencies in the United Kingdom, each made up of an average of 65,925 voters. The First-Past-the-Post system means that every constituency elects one MP each (except the constituency of the Speaker, whose seat is uncontested). Each voter assigns one vote for one candidate, and the candidate with the most votes in each constituency is elected as MP to represent their constituency. Members sit for a maximum of five years, although elections are generally called before that maximum limit is reached.
 A party needs to win 326 constituencies (known as ""seats"") to win a majority in the House of Commons. If no party achieves a majority, then a situation of no overall control occurs – commonly known as a ""Hung Parliament"". In case of a Hung Parliament, the party with the most seats has the opportunity to form a coalition with other parties, so their combined seat tally extends past the 326-seat majority.
 The House of Commons is the most powerful of the components of Parliament, particularly due to its sole right to determine taxation and the supply of money to the government. Additionally, the prime minister and leader of the government sits in the House, having acquiring this position by virtue of having the confidence of the other members. This also means that the House is also the primary location in which the government faces scrutiny, as expressed through Question Time and the work of various select committees.
 The State Opening of Parliament is an annual event that marks the commencement of a session of the Parliament of the United Kingdom. It is held in the House of Lords Chamber. Before 2012, it took place in November or December,[18] or, in a general election year, when the new Parliament first assembled. From 2012 onwards, the ceremony has taken place in May or June.
 Upon the signal of the Monarch, the Lord Great Chamberlain raises their wand of office to signal to Black Rod, who is charged with summoning the House of Commons and has been waiting in the Commons lobby. Black Rod turns and, under the escort of the Door-keeper of the House of Lords and an inspector of police, approaches the doors to the Chamber of the Commons. In 1642, King Charles I stormed into the House of Commons in an unsuccessful attempt to arrest the Five Members, who included the celebrated English patriot and leading Parliamentarian John Hampden. This action sparked the English Civil War.[19][20] The wars established the constitutional rights of Parliament, a concept legally established in the Glorious Revolution in 1688 and the subsequent Bill of Rights 1689. Since then, no British monarch has entered the House of Commons when it is in session.[21] On Black Rod's approach, the doors are slammed shut against them, symbolising the rights of parliament and its independence from the monarch.[21] They then strike, with the end of their ceremonial staff (the Black Rod), three times on the closed doors of the Commons Chamber. They are then admitted, and announce the command of the monarch for the attendance of the Commons.[21]
 The monarch reads a speech, known as the Speech from the Throne, which is prepared by the Prime Minister and the Cabinet, outlining the Government's agenda for the coming year. The speech reflects the legislative agenda for which the Government intends to seek the agreement of both Houses of Parliament.
 After the monarch leaves, each Chamber proceeds to the consideration of an ""Address in Reply to His Majesty's Gracious Speech."" But, first, each House considers a bill pro forma to symbolise their right to deliberate independently of the monarch. In the House of Lords, the bill is called the Select Vestries Bill, while the Commons equivalent is the Outlawries Bill. The Bills are considered for the sake of form only, and do not make any actual progress.
 Both houses of the British Parliament are presided over by a speaker, the Speaker of the House for the Commons and the Lord Speaker in the House of Lords.
 For the Commons, the approval of the Sovereign is required before the election of the Speaker becomes valid, but it is, by modern convention, always granted. The Speaker's place may be taken by the Chairman of Ways and Means, the First Deputy Chairman, or the Second Deputy Chairman. (The titles of those three officials refer to the Committee of Ways and Means, a body which no longer exists.)
 Prior to July 2006, the House of Lords was presided over by a Lord Chancellor (a Cabinet member), whose influence as Speaker was very limited (whilst the powers belonging to the Speaker of the House of Commons are vast). However, as part of the Constitutional Reform Act 2005, the position of Speaker of the House of Lords (as it is termed in the Act) was separated from the office of Lord Chancellor (the office which has control over the judiciary as a whole), though the Lords remain largely self-governing. Decisions on points of order and on the disciplining of unruly members are made by the whole body, but by the Speaker alone in the Lower House. Speeches in the House of Lords are addressed to the House as a whole (using the words ""My Lords""), but those in the House of Commons are addressed to the Speaker alone (using ""Mr Speaker"" or ""Madam Speaker""). Speeches may be made to both Houses simultaneously.
 Both Houses may decide questions by voice vote; members shout out ""Aye!"" and ""No!"" in the Commons—or ""Content!"" and ""Not-Content!"" in the Lords—and the presiding officer declares the result. The pronouncement of either Speaker may be challenged, and a recorded vote (known as a division) demanded. (The Speaker of the House of Commons may choose to overrule a frivolous request for a division, but the Lord Speaker does not have that power.) In each House, a division requires members to file into one of the two lobbies alongside the Chamber; their names are recorded by clerks, and their votes are counted as they exit the lobbies to re-enter the Chamber. The Speaker of the House of Commons is expected to be non-partisan, and does not cast a vote except in the case of a tie; the Lord Speaker, however, votes along with the other Lords. Speaker Denison's rule is a convention which concerns how the Speaker should vote should he be required to break a tie.
 Both Houses normally conduct their business in public, and there are galleries where visitors may sit.
 Originally there was no fixed limit on the length of a Parliament, but the Triennial Act 1694 set the maximum duration at three years. As the frequent elections were deemed inconvenient, the Septennial Act 1715 extended the maximum to seven years, but the Parliament Act 1911 reduced it to five. During the Second World War, the term was temporarily extended to ten years by Acts of Parliament. Since the end of the war the maximum has remained five years. Modern Parliaments, however, rarely continued for the maximum duration; normally, they were dissolved earlier. For instance, the 52nd, which assembled in 1997, was dissolved after four years. The Septennial Act was repealed by the Fixed-term Parliaments Act 2011, which established a presumption that a Parliament will last for five years, unless two thirds of the House of Commons votes for an early general election, or the government loses the confidence of the House. This was repealed by the Dissolution and Calling of Parliament Act 2022, which restored the ability for the government to call an early election while keeping five year terms.
 Summary history of terms of the Parliament of the United Kingdom
 Following a general election, a new Parliamentary session begins. Parliament is formally summoned 40 days in advance by the Sovereign, who is the source of parliamentary authority. On the day indicated by the Sovereign's proclamation, the two Houses assemble in their respective chambers. The Commons are then summoned to the House of Lords, where Lords Commissioners (representatives of the Sovereign) instruct them to elect a Speaker. The Commons perform the election; on the next day, they return to the House of Lords, where the Lords Commissioners confirm the election and grant the new Speaker the royal approval in the Sovereign's name.
 The business of Parliament for the next few days of its session involves the taking of the oaths of allegiance. Once a majority of the members have taken the oath in each House, the State Opening of Parliament may take place. The Lords take their seats in the House of Lords Chamber, the Commons appear at the Bar (at the entrance to the Chamber), and the Sovereign takes the seat on the throne. The Sovereign then reads the Speech from the Throne—the content of which is determined by the Ministers of the Crown—outlining the Government's legislative agenda for the upcoming year. Thereafter, each House proceeds to the transaction of legislative business.
 By custom, before considering the Government's legislative agenda, a bill is introduced pro forma in each House—the Select Vestries Bill in the House of Lords and the Outlawries Bill in the House of Commons. These bills do not become laws; they are ceremonial indications of the power of each House to debate independently of the Crown. After the pro forma bill is introduced, each House debates the content of the Speech from the Throne for several days. Once each House formally sends its reply to the Speech, legislative business may commence, appointing committees, electing officers, passing resolutions and considering legislation.
 A session of Parliament is brought to an end by a prorogation. There is a ceremony similar to the State Opening, but much less well known to the general public. Normally, the Sovereign does not personally attend the prorogation ceremony in the House of Lords and is represented by Lords Commissioners. The next session of Parliament begins under the procedures described above, but it is not necessary to conduct another election of a Speaker or take the oaths of allegiance afresh at the beginning of such subsequent sessions. Instead, the State Opening of Parliament proceeds directly. To avoid the delay of opening a new session in the event of an emergency during the long summer recess, Parliament is no longer prorogued beforehand, but only after the Houses have reconvened in the autumn; the State Opening follows a few days later.
 Each Parliament comes to an end, after a number of sessions, in anticipation of a general election. Parliament is dissolved by virtue of the Dissolution and Calling of Parliament Act 2022 and previously the Fixed-term Parliaments Act 2011. Prior to that, dissolution was effected by the Sovereign, always on the advice of the Prime Minister. The Prime Minister could seek dissolution at a time politically advantageous to their party. If the Prime Minister loses the support of the House of Commons, Parliament will dissolve and a new election will be held. Parliaments can also be dissolved if two-thirds of the House of Commons votes for an early election.
 Formerly, the demise of the Sovereign automatically brought a Parliament to an end, the Crown being seen as the caput, principium, et finis (beginning, basis and end) of the body, but this is no longer the case. The first change was during the reign of William and Mary, when it was seen to be inconvenient to have no Parliament at a time when succession to the Crown could be disputed, and an Act was passed that provided that a Parliament was to continue for six months after the death of a Sovereign, unless dissolved earlier. Under the Representation of the People Act 1867 Parliament can now continue for as long as it would otherwise have done in the event of the death of the Sovereign.
 After each Parliament concludes, the Crown issues writs to hold a general election and elect new members of the House of Commons, though membership of the House of Lords does not change.
 Laws can be made by Acts of the United Kingdom Parliament. While Acts can apply to the whole of the United Kingdom including Scotland, due to the continuing separation of Scots law many Acts do not apply to Scotland and may be matched either by equivalent Acts that apply to Scotland alone or, since 1999, by legislation set by the Scottish Parliament relating to devolved matters.
 This has led to a paradox known as the West Lothian question. The existence of a devolved Scottish Parliament means that while Westminster MPs from Scotland may vote directly on matters that affect English constituencies, they may not have much power over their laws affecting their own constituency. Since there is no devolved ""English Parliament"", the converse is not true. Any Act of the Scottish Parliament may be overturned, amended or ignored by Westminster under section 35 of the Scotland Act 1998, and this happened for the first time in January 2023, when the Gender Recognition Reform (Scotland) Bill was prohibited from receiving royal assent. Legislative Consent Motions enables the UK Parliament to vote on issues normally devolved to Scotland, Wales or Northern Ireland, as part of United Kingdom legislation.
 Laws, in draft form known as bills, may be introduced by any member of either House. A bill introduced by a Minister is known as a ""Government Bill""; one introduced by another member is called a ""Private Member's Bill"". A different way of categorising bills involves the subject. Most bills, involving the general public, are called ""public bills"". A bill that seeks to grant special rights to an individual or small group of individuals, or a body such as a local authority, is called a ""Private Bill"". A Public Bill which affects private rights (in the way a Private Bill would) is called a ""Hybrid Bill"", although those that draft bills take pains to avoid this.
 Private Members' Bills make up the majority of bills, but are far less likely to be passed than government bills. There are three methods for an MP to introduce a Private Member's Bill. The Private Members' Ballot (once per Session) put names into a ballot, and those who win are given time to propose a bill. The Ten Minute Rule is another method, where MPs are granted ten minutes to outline the case for a new piece of legislation. Standing Order 57 is the third method, which allows a bill to be introduced without debate if a day's notice is given to the Table Office. Filibustering is a danger, as an opponent of a bill can waste much of the limited time allotted to it. Private Members' Bills have no chance of success if the current government opposes them, but they are used in moral issues: the bills to decriminalise homosexuality and abortion were Private Members' Bills, for example. Governments can sometimes attempt to use Private Members' Bills to pass things it would rather not be associated with. ""Handout bills"" are bills which a government hands to MPs who win Private Members' Ballots.
 Each Bill goes through several stages in each House. The first stage, called the first reading, is a formality. At the second reading, the general principles of the bill are debated, and the House may vote to reject the bill, by not passing the motion ""That the Bill be now read a second time."" Defeats of Government Bills in the Commons are extremely rare, the last being in 2005, and may constitute a motion of no confidence. (Defeats of Bills in the Lords never affect confidence and are much more frequent.)
 Following the second reading, the bill is sent to a committee. In the House of Lords, the Committee of the Whole House or the Grand Committee are used. Each consists of all members of the House; the latter operates under special procedures, and is used only for uncontroversial bills. In the House of Commons, the bill is usually committed to a Public Bill Committee, consisting of between 16 and 50 members, but the Committee of the Whole House is used for important legislation. Several other types of committees, including Select Committees, may be used, but rarely. A committee considers the bill clause by clause, and reports the bill as amended to the House, where further detailed consideration (""consideration stage"" or ""report stage"") occurs. However, a practice which used to be called the ""kangaroo"" (Standing Order 32) allows the Speaker to select which amendments are debated. This device is also used under Standing Order 89 by the committee chairman, to restrict debate in committee. The Speaker, who is impartial as between the parties, by convention selects amendments for debate which represent the main divisions of opinion within the House. Other amendments can technically be proposed, but in practice have no chance of success unless the parties in the House are closely divided. If pressed they would normally be casually defeated by acclamation.
 Once the House has considered the bill, the third reading follows. In the House of Commons, no further amendments may be made, and the passage of the motion ""That the Bill be now read a third time"" is passage of the whole bill. In the House of Lords further amendments to the bill may be moved. After the passage of the third reading motion, the House of Lords must vote on the motion ""That the Bill do now pass."" Following its passage in one House, the bill is sent to the other House. If passed in identical form by both Houses, it may be presented for the Sovereign's Assent. If one House passes amendments that the other will not agree to, and the two Houses cannot resolve their disagreements, the bill will normally fail.
 Since the passage of the Parliament Act 1911 the power of the House of Lords to reject bills passed by the House of Commons has been restricted, with further restrictions were placed by the Parliament Act 1949. If the House of Commons passes a public bill in two successive sessions, and the House of Lords rejects it both times, the Commons may direct that the bill be presented to the Sovereign for his or her Assent, disregarding the rejection of the Bill in the House of Lords. In each case, the bill must be passed by the House of Commons at least one calendar month before the end of the session. The provision does not apply to Private bills or to Public bills if they originated in the House of Lords or if they seek to extend the duration of a Parliament beyond five years. A special procedure applies in relation to bills classified by the Speaker of the House of Commons as ""Money Bills"". A Money Bill concerns solely national taxation or public funds; the Speaker's certificate is deemed conclusive under all circumstances. If the House of Lords fails to pass a Money Bill within one month of its passage in the House of Commons, the Lower House may direct that the Bill be submitted for the Sovereign's Assent immediately.[22]
 Even before the passage of the Parliament Acts, the Commons possessed pre-eminence in cases of financial matters. By ancient custom, the House of Lords may not introduce a bill relating to taxation or Supply, nor amend a bill so as to insert a provision relating to taxation or Supply, nor amend a Supply Bill in any way. The House of Commons is free to waive this privilege, and sometimes does so to allow the House of Lords to pass amendments with financial implications. The House of Lords remains free to reject bills relating to Supply and taxation, but may be over-ruled easily if the bills are Money Bills. (A bill relating to revenue and Supply may not be a Money Bill if, for example, it includes subjects other than national taxation and public funds).
 The last stage of a bill involves the granting of royal assent. Theoretically, the Sovereign may either grant or withhold royal assent (make the bill a law or veto the bill). In modern times the Sovereign always grants royal assent, using the Norman French words ""Le Roy le veult"" (the King wishes it; ""La Reyne"" in the case of a Queen). The last refusal to grant the Assent was in 1708, when Queen Anne withheld her Assent from a bill ""for the settling of Militia in Scotland"", in the words ""La reyne s'avisera"" (the Queen will think it over).
 Thus, every bill obtains the assent of all three components of Parliament before it becomes law (except where the House of Lords is over-ridden under the Parliament Acts 1911 and 1949). The words ""BE IT ENACTED by the King's most Excellent Majesty, by and with the advice and consent of the Lords Spiritual and Temporal, and Commons, in this present Parliament assembled, and by the authority of the same, as follows:-,""[22] or, where the House of Lords' authority has been over-ridden by use of the Parliament Acts, the words ""BE IT ENACTED by King's most Excellent Majesty, by and with the advice and consent of the Commons in this present Parliament assembled, in accordance with the provisions of the Parliament Acts 1911 and 1949, and by the authority of the same, as follows:-"" appear near the beginning of each Act of Parliament. These words are known as the enacting formula.
 Prior to the creation of the Supreme Court of the United Kingdom in 2009, Parliament was the highest court in the realm for most purposes, but the Privy Council had jurisdiction in some cases (for instance, appeals from ecclesiastical courts). The jurisdiction of Parliament arose from the ancient custom of petitioning the Houses to redress grievances and to do justice. The House of Commons ceased considering petitions to reverse the judgements of lower courts in 1399, effectively leaving the House of Lords as the court of last resort. In modern times, the judicial functions of the House of Lords were performed not by the whole House, but by the Lords of Appeal in Ordinary (judges granted life peerage dignities under the Appellate Jurisdiction Act 1876) and by Lords of Appeal (other peers with experience in the judiciary). However, under the Constitutional Reform Act 2005, these judicial functions were transferred to the newly created Supreme Court in 2009, and the Lords of Appeal in Ordinary became the first Justices of the Supreme Court. Peers who hold high judicial office are no longer allowed to vote or speak in the Lords until they retire as justices.
 In the late 19th century, Acts allowed for the appointment of Scottish Lords of Appeal in Ordinary and ended appeal in Scottish criminal matters to the House of Lords, so that the High Court of Justiciary became the highest criminal court in Scotland. There is an argument that the provisions of Article XIX of the Union with England Act 1707 prevent any Court outside Scotland from hearing any appeal in criminal cases: ""And that the said Courts or any other of the like nature after the Unions shall have no power to Cognosce Review or Alter the Acts or Sentences of the Judicatures within Scotland or stop the Execution of the same."" The House of Lords judicial committee usually had a minimum of two Scottish Judges to ensure that some experience of Scots law was brought to bear on Scottish appeals in civil cases, from the Court of Session. The Supreme Court now usually has at least two Scottish judges, together with at least one from Northern Ireland.[23] As Wales is developing its own judicature, it is likely that the same principle will be applied.
 Certain other judicial functions have historically been performed by the House of Lords. Until 1948, it was the body in which peers had to be tried for felonies or high treason; now, they are tried by normal juries. The last occasion of the trial of a peer in the House of Lords was in 1935. When the House of Commons impeaches an individual, the trial takes place in the House of Lords. Impeachments are now possibly defunct, as the last one occurred in 1806. In 2006, a number of MPs attempted to revive the custom, having signed a motion for the impeachment of Tony Blair, but this was unsuccessful.
 The British Government is answerable to the House of Commons. However, neither the Prime Minister nor members of the Government are elected by the House of Commons. Instead, the King requests the person most likely to command the support of a majority in the House, normally the leader of the largest party in the House of Commons, to form a government. So that they may be accountable to the Lower House, the Prime Minister and most members of the Cabinet are, by convention, members of the House of Commons. The last prime minister to be a member of the House of Lords was Alec Douglas-Home, 14th Earl of Home, who became prime minister in 1963. To adhere to the convention under which he was responsible to the Lower House, he disclaimed his peerage and procured election to the House of Commons within days of becoming prime minister.
 Governments have a tendency to dominate the legislative functions of Parliament, by using their in-built majority in the House of Commons, and sometimes using their patronage power to appoint supportive peers in the Lords. In practice, governments can pass any legislation (within reason) in the Commons they wish, unless there is major dissent by MPs in the governing party.
But even in these situations, it is highly unlikely a bill will be defeated, though dissenting MPs may be able to extract concessions from the government. In 1976, Quintin Hogg, Lord Hailsham of St Marylebone created a now widely used name for this behaviour, in an academic paper called ""elective dictatorship"".
 Parliament controls the executive by passing or rejecting its Bills and by forcing Ministers of the Crown to answer for their actions, either at ""Question Time"" or during meetings of the parliamentary committees. In both cases, Ministers are asked questions by members of their Houses, and are obliged to answer.
 Although the House of Lords may scrutinise the executive through Question Time and through its committees, it cannot bring down the Government. A ministry must always retain the confidence and support of the House of Commons. The Lower House may indicate its lack of support by rejecting a Motion of Confidence or by passing a Motion of No Confidence. Confidence Motions are generally originated by the Government to reinforce its support in the House, whilst No Confidence Motions are introduced by the Opposition. The motions sometimes take the form ""That this House has [no] confidence in His Majesty's Government"" but several other varieties, many referring to specific policies supported or opposed by Parliament, are used. For instance, a Confidence Motion of 1992 used the form, ""That this House expresses the support for the economic policy of His Majesty's Government."" Such a motion may theoretically be introduced in the House of Lords, but, as the Government need not enjoy the confidence of that House, would not be of the same effect as a similar motion in the House of Commons; the only modern instance of such an occurrence involves the 'No Confidence' motion that was introduced in 1993 and subsequently defeated.
 Many votes are considered votes of confidence, although not including the language mentioned above. Important bills that form part of the Government's agenda (as stated in the Speech from the Throne) are generally considered matters of confidence. The defeat of such a bill by the House of Commons indicates that a Government no longer has the confidence of that House. The same effect is achieved if the House of Commons ""withdraws Supply,"" that is, rejects the budget.
 Where a Government has lost the confidence of the House of Commons, in other words has lost the ability to secure the basic requirement of the authority of the House of Commons to tax and to spend Government money, the Prime Minister is obliged either to resign, or seek the dissolution of Parliament and a new general election. Otherwise the machinery of government grinds to a halt within days. The third choice – to mount a coup d'état or an anti-democratic revolution – is hardly to be contemplated in the present age. Though all three situations have arisen in recent years even in developed economies, international relations have allowed a disaster to be avoided.
 Where a prime minister has ceased to retain the necessary majority and requests a dissolution, the sovereign can in theory reject his or her request, forcing a resignation and allowing the Leader of the Opposition to be asked to form a new government. This power is used extremely rarely. The conditions that should be met to allow such a refusal are known as the Lascelles Principles. These conditions and principles are constitutional conventions arising from the Sovereign's reserve powers as well as longstanding tradition and practice, not laid down in law.
 In practice, the House of Commons' scrutiny of the Government is very weak.[24] Since the first-past-the-post electoral system is employed in elections, the governing party tends to enjoy a large majority in the Commons; there is often limited need to compromise with other parties.[25] Modern British political parties are so tightly organised that they leave relatively little room for free action by their MPs.[26] In many cases, MPs may be expelled from their parties for voting against the instructions of party leaders.[27] During the 20th century, the Government has lost confidence issues only three times—twice in 1924, and once in 1979.
 In the United Kingdom, question time in the House of Commons lasts for an hour each day from Monday to Thursday (2:30 to 3:30 pm on Mondays, 11:30 am to 12:30 pm on Tuesdays and Wednesdays, and 9:30 to 10:30 am on Thursdays). Each Government department has its place in a rota which repeats every five weeks. The exception to this sequence are the Business Questions (Questions to the Leader of House of Commons), in which questions are answered each Thursday about the business of the House the following week. Also, Questions to the Prime Minister takes place each Wednesday from noon to 12:30 pm.
 In addition to government departments, there are also questions to the Church commissioners.[28] Additionally, each Member of Parliament is entitled to table questions for written answer. Written questions are addressed to the Ministerial head of a government department, usually a Secretary of State, but they are often answered by a Minister of State or Parliamentary Under Secretary of State. Written Questions are submitted to the Clerks of the Table Office, either on paper or electronically, and answers are recorded in The Official Report (Hansard) so as to be widely available and accessible.[28]
 In the House of Lords, a half-hour is set aside each afternoon at the start of the day's proceedings for Lords' oral questions. A peer submits a question in advance, which then appears on the Order Paper for the day's proceedings.[28] The peer shall say: ""My Lords, I beg leave to ask the Question standing in my name on the Order Paper."" The Minister responsible then answers the question. The peer is then allowed to ask a supplementary question and other peers ask further questions on the theme of the original put down on the order paper. (For instance, if the question regards immigration, peers can ask the Minister any question related to immigration during the allowed period.)[28]
 Several different views have been taken of Parliament's sovereignty. According to the jurist Sir William Blackstone, ""It has sovereign and uncontrollable authority in making, confirming, enlarging, restraining, abrogating, repealing, reviving, and expounding of laws, concerning matters of all possible denominations, ecclesiastical, or temporal, civil, military, maritime, or criminal… it can, in short, do every thing that is not naturally impossible.""
 A different view has been taken by the Scottish judge Thomas Cooper, 1st Lord Cooper of Culross. When he decided the 1953 case of MacCormick v. Lord Advocate as Lord President of the Court of Session, he stated, ""The principle of unlimited sovereignty of Parliament is a distinctively English principle and has no counterpart in Scottish constitutional law."" He continued, ""Considering that the Union legislation extinguished the Parliaments of Scotland and England and replaced them by a new Parliament, I have difficulty in seeing why the new Parliament of Great Britain must inherit all the peculiar characteristics of the English Parliament but none of the Scottish."" Nevertheless, he did not give a conclusive opinion on the subject.
 Thus, the question of Parliamentary sovereignty appears to remain unresolved. Parliament has not passed any Act defining its own sovereignty. The European Union (Withdrawal Agreement) Act 2020 states ""It is recognised that the Parliament of the United Kingdom is sovereign"" without qualification or definition.[29] A related possible limitation on Parliament relates to the Scottish legal system and Presbyterian faith, preservation of which were Scottish preconditions to the creation of the unified Parliament. Since the Parliament of the United Kingdom was set up in reliance on these promises, it may be that it has no power to make laws that break them.
 Parliament's power has often been limited by its own Acts, whilst retaining the power to overturn those decisions should it decide to.
 Acts passed in 1921 and 1925 granted the Church of Scotland complete independence in ecclesiastical matters. From 1973 to 2020, under membership of the European Community and European Union, parliament agreed to the position that European law would apply and be enforceable in Britain and that Britain would be subject to the rulings of the European Court of Justice. In the Factortame case, the European Court of Justice ruled that British courts could have powers to overturn British legislation that was not compatible with European law. This position ended with the passing of the European Union (Withdrawal Agreement) Act 2020 and Britain leaving the EU on 31 January 2020.
 Parliament has also created national devolved parliaments and an assembly with differing degrees of legislative authority in Scotland, Wales and Northern Ireland, but not in England, which continues to be governed by the Parliament of the United Kingdom. Parliament still has the power over areas for which responsibility lies with the devolved institutions, but would ordinarily gain the agreement of those institutions to act on their behalf. Similarly, it has granted the power to make regulations to Ministers of the Crown, and the power to enact religious legislation to the General Synod of the Church of England. (Measures of the General Synod and, in some cases proposed statutory instruments made by ministers, must be approved by both Houses before they become law.)
 In every case aforementioned, authority has been conceded by Act of Parliament and may be taken back in the same manner. It is entirely within the authority of Parliament, for example, to abolish the devolved governments in Scotland, Wales and Northern Ireland, or — as happened in 2020 — to leave the EU. However, Parliament also revoked its legislative competence over Australia and Canada with the Australia and Canada Acts: although the Parliament of the United Kingdom could pass an Act reversing its action, it would not take effect in Australia or Canada as the competence of the Imperial Parliament is no longer recognised there in law.[30]
 One well-recognised consequence of Parliament's sovereignty is that it cannot bind future Parliaments; that is, no Act of Parliament may be made secure from amendment or repeal by a future Parliament. For example, although the Act of Union 1800 states that the Kingdoms of Great Britain and Ireland are to be united ""forever,"" Parliament permitted southern Ireland to leave the United Kingdom in 1922.
 Each House of Parliament possesses and guards various ancient privileges. The House of Lords relies on inherent right. In the case of the House of Commons, the Speaker goes to the Lords' Chamber at the beginning of each new Parliament and requests representatives of the Sovereign to confirm the Lower House's ""undoubted"" privileges and rights. The ceremony observed by the House of Commons dates to the reign of King Henry VIII. Each House is the guardian of its privileges, and may punish breaches thereof. The extent of parliamentary privilege is based on law and custom. Sir William Blackstone states that these privileges are ""very large and indefinite,"" and cannot be defined except by the Houses of Parliament themselves.
 The foremost privilege claimed by both Houses is that of freedom of speech in debate; nothing said in either House may be questioned in any court or other institution outside Parliament. Another privilege claimed is that of freedom from arrest; at one time this was held to apply for any arrest except for high treason, felony or breach of the peace but it now excludes any arrest on criminal charges; it applies during a session of Parliament, and 40 days before or after such a session.[31] Members of both Houses are no longer privileged from service on juries.[32]
 Both Houses possess the power to punish breaches of their privilege. Contempt of Parliament—for example, disobedience of a subpoena issued by a committee—may also be punished. The House of Lords may imprison an individual for any fixed period of time, but an individual imprisoned by the House of Commons is set free upon prorogation.[33] The punishments imposed by either House may not be challenged in any court, and the Human Rights Act does not apply.[34]
 Until at least 2015, members of the House of Commons also had the privilege of a separate seating area in the Palace of Westminster canteen, protected by a false partition labelled ""MPs only beyond this point,"" so that they did not have to sit with canteen staff who were taking a break. This provoked mockery from a newly elected 20-year-old MP who described it as ""ridiculous"" snobbery.[35]
 The quasi-official emblem of the Houses of Parliament is a crowned portcullis. The portcullis was originally the badge of various English noble families from the 14th century. It went on to be adopted by the kings of the Tudor dynasty in the 16th century, under whom the Palace of Westminster became the regular meeting place of Parliament. The crown was added to make the badge a specifically royal symbol.
 The portcullis probably first came to be associated with the Palace of Westminster through its use as decoration in the rebuilding of the Palace after the fire of 1512. However, at the time it was only one of many symbols. The widespread use of the portcullis throughout the Palace dates from the 19th century, when Charles Barry and Augustus Pugin used it extensively as a decorative feature in their designs for the new Palace built following the disastrous 1834 fire.
 The crowned portcullis came to be accepted during the early 20th century as the emblem of both houses of parliament. This was simply a result of custom and usage rather than a specific decision. The emblem now appears on all official stationery, publications and papers, and is stamped on various items in use in the Palace of Westminster, such as cutlery, silverware and china.[36] Various shades of red (for the House of Lords) and green (for the House of Commons) are used for visual identification of the houses.
 All public events are broadcast live and on-demand via parliamentlive.tv, which maintains an archive dating back to 4 December 2007.[37] There is also a related official YouTube channel.[38] They are also broadcast live by the independent Euronews English channel.[39] In the UK the BBC has its own dedicated parliament channel, BBC Parliament, which broadcasts 24 hours a day and is also available on BBC iPlayer. It shows live coverage from the House of Commons, House of Lords, the Scottish Parliament, the Northern Ireland Assembly and the Senedd.
"
Virtual representation,https://en.wikipedia.org/wiki/Virtual_representation,"The concept of virtual representation was that the members of the UK Parliament, including the Lords and the Crown-in-Parliament, reserved the right to speak for the interests of all British subjects, rather than for the interests of only the district that elected them or for the regions in which they held peerages and spiritual sway.[1] Virtual representation was the British response to the First Continental Congress in the American colonies. The Second Continental Congress asked for representation in Parliament in the Suffolk Resolves, also known as the first Olive Branch Petition. Parliament claimed that their members had the well being of the colonists in mind. The patriots in the Colonies rejected this premise.
 In the early stages of the American Revolution, colonists in the Thirteen Colonies rejected legislation imposed upon them by the Parliament of Great Britain because the colonies were not represented in Parliament. According to the British constitution, colonists argued, taxes could be levied on British subjects only with their consent. Because the colonists were represented only in their provincial assemblies, they said, only those legislatures could levy taxes in the colonies. This concept was famously expressed as ""No taxation without representation"".
 During the winter of 1764–1765, British MP George Grenville and his lieutenant, Thomas Whately, attempted to explicitly articulate a theory that could justify the lack of representation in colonial taxation.[2] Grenville and Whately's theory, known as ""virtual representation"" put forth that, just like the vast majority of British citizens who could not vote, the colonists were nonetheless virtually represented in Parliament.[2] Thus Grenville defended all the taxes by arguing that the colonists were virtually represented in Parliament, a position that had critics on both sides of the British Empire.[3] Parliament rejected any criticism that virtual representation was constitutionally invalid as a whole, and passed the Declaratory Act in 1766, asserting the right of Parliament to legislate for the colonies ""all cases whatsoever.""
 
The idea of virtual representation ""found little support on either side of the Atlantic"" as a means of solving the constitutional controversy between colonists and Britons.[4]  William Pitt, a defender of colonial rights, ridiculed virtual representation, calling it ""the most contemptible idea that ever entered into the head of a man; it does not deserve serious refutation.""[5]  Pitt said to the House of Commons in 1766, .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}}   Pitt then stated to Parliament that, ""I myself would have cited the two cases of Chester and Durham...to show that, even under former arbitrary reigns, Parliaments were ashamed of taxing a people without their consent, and allowed them representatives...[A] higher example [might be found] in Wales—Wales that never was taxed by Parliament till it was incorporated.[6]  Pitt pointed out that, unlike the ""India company, merchants, stockholders, [and] manufacturers"" who ""have it in their option to be actually represented...have connections with those that elect, and...have influence over them,"" the colonists had no such option, connections or influence.[6]
 Benjamin Franklin told the House of Commons that, ""I know that whenever taxation has occurred in conversation where I have been present, it has appeared to be the opinion of every one that we could not be taxed by a Parliament wherein we were not represented...An external tax is a duty laid on commodities imported; that duty is added to the first cost and other charges on the commodity, and, when it is offered for sale, makes a part of the price. If the people do not like it at that price, they refuse it; they are not obliged to pay it. But an internal tax is forced from the people without their consent if not laid by their own representatives. The Stamp Act says we shall have no commerce, make no exchange of property with each other, neither purchase nor grant, nor recover debts; we shall neither marry nor make our wills, unless we pay such and such sums; and thus it is intended to extort our money from us or ruin us by the consequence of refusing to pay it.""[7]  James Otis Jr. reasoned that the legal liberties of British subjects meant that Parliament should, or could, only tax the colonists if they were actually represented in Westminster.
 At the time of the American Revolution, only England and Wales and Scotland were directly represented in the Parliament of Great Britain among the many parts of the British Empire. The colonial electorate perhaps consisted of 10% to 20% of the total population, or 75% of adult males.[8]   In Britain, by contrast, representation was highly limited due to unequally distributed voting constituencies and property requirements; only 3% of the population, or between 17% and 23% of males, could vote and they were often controlled by local gentry.[9][10][11][12]
 As virtual representation was founded on ""a defect in the Constitution of England,"" namely, the ""Want of a Full Representation of all the People of England,"" it was, therefore, a pernicious notion that had been fabricated for the sole purpose of arguing the colonists ""out of their civil Rights.""[2]  The colonists, and some Britons, consequently condemned the idea of virtual representation as ""a sham"".[13]  Moreover, the poor state of representation in Britain ""was no excuse for taxing the colonists without their consent.""[3]
 
In his influential 1765 pamphlet, Considerations on the Propriety of Imposing Taxes in the British Colonies, Daniel Dulany Jr. of Maryland likewise observed that attempting to tax subjects on the inequitable basis of ""virtual"" representation was unsound because,  Dulany Jr. also wrote that, ""the Impropriety of a Taxation by the British Parliament...[is proven by] the Fact, that not one inhabitant in any Colony is, or can be actually or virtually represented by the British House of Commons.""[15]  Dulany Jr. denied that Parliament had a right ""to impose an internal Tax upon the Colonies, without their consent for the single Purpose of Revenue.""[16]
 
In 1764, the Massachusetts politician James Otis Jr. said that,  In 1765 Otis Jr. attended the Continental Congress, otherwise known as the Stamp Act Congress, along with other colonial delegates. The resolutions of the Congress stated that the Stamp Act had ""a manifest tendency to subvert the rights and liberties of the colonists"" and that ""the only Representatives of the People of these Colonies, are Persons chosen therein by themselves, and that no Taxes ever have been, or can be Constitutionally imposed on them, but by their respective Legislature.""[18]  Furthermore, it was declared that, ""it is unreasonable and inconsistent with the Principles and Spirit of the British Constitution, for the People of Great-Britain, to grant to his Majesty the Property of the Colonists.""[18]
 Sebastian Galiani and Gustavo Torrens propose that virtual representation imposed a dilemma on the British elite, which had a direct influence on the outbreak of the American Revolutionary War.[19] They suggest the call for ""No taxation without representation"" and proposal of the inclusion of American representatives within Parliament, had they actually been implemented, would have encouraged coalition building between Americans and the British opposition (which was opposed to the dominant elite), disrupting the power of the incumbent landed gentry (who made up the elite). Through game theoretic models, Galiani and Torrens show that, once in Parliament, Americans could not feasibly commit to political alliances independent of the British opposition. As a result, mounting pressure for democratic reform would increase, posing a threat to the established British political order. Galiani and Torrens argue that British elites would incur greater losses to their domestic clout from American representation than from simply forfeiting a colony. The implications of forfeiting virtual representation forced the British elite, which dominated the government, to decide between maintaining the rule of the American colonies, which in their minds was infeasible, and engaging in war.
 Cannon argues that for 18th- and 19th-century Britain ""the doctrine of virtual representation was no more than a polite fiction. Indeed the assertion that there were no fundamental differences of interest between rich and poor is hard to reconcile with the determination of the upper classes to reserve political power for men of substance.""[20]
"
Stamp Act 1765,https://en.wikipedia.org/wiki/Stamp_Act_1765,"

 The Stamp Act 1765, also known as the Duties in American Colonies Act 1765 (5 Geo. 3. c. 12), was an act of the Parliament of Great Britain which imposed a direct tax on the British colonies in America and required that many printed materials in the colonies be produced on stamped paper from London which included an embossed revenue stamp.[1][2] Printed materials included legal documents, magazines, playing cards, newspapers, and many other types of paper used throughout the colonies, and it had to be paid in British currency, not in colonial paper money.[3]
 The purpose of the tax was to pay for British military troops stationed in the American colonies after the French and Indian War, but the colonists had never feared a French invasion to begin with, and they contended that they had already paid their share of the war expenses.[4] Colonists suggested that it was actually a matter of British patronage to surplus British officers and career soldiers who should be paid by London.
 The Stamp Act 1765 was very unpopular among colonists. A majority considered it a violation of their rights as Englishmen to be taxed without their consent—consent that only the colonial legislatures could grant. Their slogan was ""No taxation without representation"". Colonial assemblies sent petitions and protests, and the Stamp Act Congress held in New York City was the first significant joint colonial response to any British measure when it petitioned Parliament and the King.
 One member of the British Parliament argued that the American colonists were no different from the 90-percent of Great Britain who did not own property and thus could not vote, but who were nevertheless ""virtually"" represented by land-owning electors and representatives who had common interests with them.[5] Daniel Dulany, a Maryland attorney and politician, disputed this assertion in a widely read pamphlet,  arguing that the relations between the Americans and the English electors were ""a knot too infirm to be relied on"" for proper representation, ""virtual"" or otherwise.[6] Local protest groups established Committees of Correspondence which created a loose coalition from New England to Maryland. Protests and demonstrations increased, often initiated by the Sons of Liberty and occasionally involving hanging of effigies. Very soon, all stamp tax distributors were intimidated into resigning their commissions, and the tax was never effectively collected.[7][8][9]
 Opposition to the Stamp Act 1765 was not limited to the colonies. British merchants and manufacturers pressured Parliament because their exports to the colonies were threatened by boycotts. The act was repealed on 18 March 1766 as a matter of expedience, but Parliament affirmed its power to legislate for the colonies ""in all cases whatsoever"" by also passing the Declaratory Act 1766. A series of new taxes and regulations then ensued—likewise opposed by the Americans. The episode played a major role in defining the 27 colonial grievances that were clearly stated within the text of the Indictment of George III section of the United States Declaration of Independence, enabling the organized colonial resistance which led to the American Revolution in 1775.[10] [11][12]
 The British victory in the Seven Years' War (1756–1763), known in the United States and elsewhere as the French and Indian War, was won at great financial expense. During the war, the British national debt nearly doubled, rising from £72,289,673 in 1755 to almost £129,586,789 by 1764.[13] Post-war expenses were expected to remain high because the Bute ministry decided in early 1763 to keep ten thousand British regulars in the American colonies, which would cost about £225,000 per year, equal to £42 million today.[14][15][16] The primary reason for retaining such a large force was that demobilizing the army would put 1,500 officers out of work, many of whom were well-connected in Parliament.[17][16] This made it politically prudent to retain a large peacetime establishment, but Britons were averse to maintaining a standing army at home so it was necessary to garrison most of the troops elsewhere.[18]
 The outbreak of Pontiac's War in May 1763 led to the Royal Proclamation of 1763 and the added duty of British soldiers to prevent outbreaks of violence between Native Americans and American colonists.[19] 10,000 British troops were dispatched to the American frontier, with a primary motivation of the move being to provide billets for the officers who were part of the British patronage system.[20][21] John Adams wrote disparagingly of the deployment, writing that ""Revenue is still demanded from America, and appropriated to the maintenance of swarms of officers and pensioners in idleness and luxury"".[22]
 George Grenville became prime minister in April 1763 after the failure of the short-lived Bute Ministry, and he had to find a way to pay for this large peacetime army. Raising taxes in Britain was out of the question, since there had been virulent protests in England against the Bute ministry's 1763 cider tax, with Bute being hanged in effigy.[23][24][25] The Grenville ministry, therefore, decided that Parliament would raise this revenue by taxing the American colonists without their consent. This was something new; Parliament had previously passed measures to regulate trade in the colonies, but it had never before directly taxed the colonies to raise revenue.[26]
 Politicians in London had always expected American colonists to contribute to the cost of their own defence. So long as a French threat existed, there was little trouble convincing colonial legislatures to provide assistance. Such help was normally provided through the raising of colonial militias, which were funded by taxes raised by colonial legislatures. Also, the legislatures were sometimes willing to help maintain regular British units defending the colonies. So long as this sort of help was forthcoming, there was little reason for the British Parliament to impose its own taxes on the colonists. But after the peace of 1763, colonial militias were quickly stood down. Militia officers were tired of the disdain shown to them by regular British officers, and were frustrated by the near-impossibility of obtaining regular British commissions; they were unwilling to remain in service once the war was over. In any case, they had no military role, as the Indian threat was minimal and there was no foreign threat. Colonial legislators saw no need for the British troops.
 The Sugar Act 1764 was the first tax in Grenville's program to raise a revenue in America, which was a modification of the Molasses Act 1733. The Molasses Act 1733 had imposed a tax of 6 pence per gallon (equal to £5.24 today) on foreign molasses imported into British colonies. The purpose of the Molasses Act 1733 was not actually to raise revenue, but instead to make foreign molasses so expensive that it effectively gave a monopoly to molasses imported from the British West Indies.[27] It did not work; colonial merchants avoided the tax by smuggling or, more often, bribing customs officials.[28] The Sugar Act 1764 reduced the tax to 3 pence per gallon (equal to £2.24 today) in the hope that the lower rate would increase compliance and thus increase the amount of tax collected.[29] The act also taxed additional imports and included measures to make the customs service more effective.[30]
 American colonists initially objected to the Sugar Act 1764 for economic reasons, but before long they recognized that there were potential constitutional issues involved.[31] The British Constitution guaranteed that taxes could not be levied without the consent of Parliament, but the colonists argued that due to their theoretical Rights as Englishmen, they could not be taxed without their consent, which came in the form of representation in Parliament. The colonists elected no members of Parliament, and so it was seen as a violation of their rights for Parliament to tax them. There was little time to raise this issue in response to the Sugar Act 1764, but it came to be a major objection to the Stamp Act 1765 the following year.
 Parliament announced in April 1764 when the Sugar Act 1764 was passed that they would also consider a stamp tax in the colonies.[32][33][34] Opposition from the colonies was soon forthcoming to this possible tax, but neither members of Parliament nor American agents in Great Britain (such as Benjamin Franklin) anticipated the intensity of the protest that the tax generated.[35][36]
 Stamp acts had been a very successful method of taxation within Great Britain; they generated over £100,000 in tax revenue with very little in collection expenses. By requiring an official stamp on most legal documents, the system was almost self-regulating; a document would be null and void under British law without the required stamp. Imposition of such a tax on the colonies had been considered twice before the Seven Years' War and once again in 1761. Grenville had actually been presented with drafts of colonial stamp acts in September and October 1763, but the proposals lacked the specific knowledge of colonial affairs to adequately describe the documents subject to the stamp. At the time of the passage of the Sugar Act in April 1764, Grenville made it clear that the right to tax the colonies was not in question, and that additional taxes might follow, including a stamp tax.[32][33][34]
 The Glorious Revolution had established the principle of parliamentary supremacy. Control of colonial trade and manufactures extended this principle across the ocean. This belief had never been tested on the issue of colonial taxation, but the British assumed that the interests of the thirteen colonies were so disparate that a joint colonial action was unlikely to occur against such a tax–an assumption that had its genesis in the failure of the Albany Conference in 1754. By the end of December 1764, the first warnings of serious colonial opposition were provided by pamphlets and petitions from the colonies protesting both the Sugar Act 1764 and the proposed stamp tax.[37]
 For Grenville, the first issue was the amount of the tax. Soon after his announcement of the possibility of a tax, he had told American agents that he was not opposed to the Americans suggesting an alternative way of raising the money themselves. However, the only other alternative would be to requisition each colony and allow them to determine how to raise their share. This had never worked before, even during the French and Indian War, and there was no political mechanism in place that would have ensured the success of such cooperation. On 2 February 1765, Grenville met to discuss the tax with Benjamin Franklin, Jared Ingersoll from New Haven, Richard Jackson, agent for Connecticut, and Charles Garth, the agent for South Carolina (Jackson and Garth were also members of Parliament). These colonial representatives had no specific alternative to present; they simply suggested that the determination be left to the colonies. Grenville replied that he wanted to raise the money ""by means the most easy and least objectionable to the Colonies"". Thomas Whately had drafted the Stamp Act, and he said that the delay in implementation had been ""out of Tenderness to the colonies"", and that the tax was judged as ""the easiest, the most equal and the most certain.""[a]
 The debate in Parliament began soon after this meeting. Petitions submitted by the colonies were officially ignored by Parliament. In the debate, Charles Townshend said, 
 This led to Colonel Isaac Barré's response:
 Massachusetts Royal Governor William Shirley assured London in 1755 that American independence could easily be defeated by force. He argued:
 The act was passed by the British Parliament on 22 March 1765 with an effective date of 1 November 1765. It passed 205–49 in the House of Commons and unanimously in the House of Lords.[42] Historians Edmund and Helen Morgan describe the specifics of the tax:
 The high taxes on lawyers and college students were designed to limit the growth of a professional class in the colonies.[43] The stamps had to be purchased with hard currency, which was scarce, rather than the more plentiful colonial paper currency. To avoid draining currency out of the colonies, the revenues were to be expended in America, especially for supplies and salaries of British Army units who were stationed there.[b]
 Two features of the act involving the courts attracted special attention. The tax on court documents specifically included courts ""exercising ecclesiastical jurisdiction."" These type of courts did not currently exist in the colonies and no bishops were currently assigned to the colonies, who would preside over the courts. Many colonists or their ancestors had fled England specifically to escape the influence and power of such state-sanctioned religious institutions, and they feared that this was the first step to reinstating the old ways in the colonies. Some Anglicans in the northern colonies were already openly advocating the appointment of such bishops, but they were opposed by both southern Anglicans and the non-Anglicans who made up the majority in the northern colonies.[45]
 The act allowed admiralty courts to have jurisdiction for trying violators, following the example established by the Sugar Act. However, admiralty courts had traditionally been limited to cases involving the high seas. The Sugar Act seemed to fall within this precedent, but the Stamp Act did not, and the colonists saw this as a further attempt to replace their local courts with courts controlled by England.[46]
 As the act imposed a tax on many different types of paper items, including newspapers, contracts, deeds, wills, claims, indentures and many other types of legal documents, its effect would be felt in many different professions and trades, resulting in wide spread protests from newspapers, citizens, and even attacks on public officials, tax collectors and their offices and homes.[47]
 Grenville started appointing stamp distributors almost immediately after the act passed Parliament. Applicants were not hard to come by because of the anticipated income that the positions promised, and he appointed local colonists to the post. Benjamin Franklin even suggested the appointment of John Hughes as the agent for Pennsylvania, indicating that even Franklin was not aware of the turmoil and impact that the tax was going to generate on American-British relations or that these distributors would become the focus of colonial resistance.[c]
 Debate in the colonies had actually begun in the spring of 1764 over the Stamp Act when Parliament passed a resolution that contained the assertion, ""That, towards further defraying the said Expences, it may be proper to charge certain Stamp Duties in the said Colonies and Plantations."" Both the Sugar Act and the proposed Stamp Act were designed principally to raise revenue from the colonists. The Sugar Act, to a large extent, was a continuation of past legislation related primarily to the regulation of trade (termed an external tax), but its stated purpose was entirely new: to collect revenue directly from the colonists for a specific purpose. The novelty of the Stamp Act was that it was the first internal tax (a tax based entirely on activities within the colonies) levied directly on the colonies by Parliament. It was judged by the colonists to be a more dangerous assault on their rights than the Sugar Act 1764 was, because of its potential wide application to the colonial economy.[50]
 The theoretical issue that soon held center stage was the matter of taxation without representation. Benjamin Franklin had raised this as far back as 1754 at the Albany Congress when he wrote, ""That it is suppos'd an undoubted Right of Englishmen not to be taxed but by their own Consent given thro' their Representatives. That the Colonies have no Representatives in Parliament.""[51] The counter to this argument was the theory of virtual representation. Thomas Whately enunciated this theory in a pamphlet that readily acknowledged that there could be no taxation without consent, but the facts were that at least 75% of British adult males were not represented in Parliament because of property qualifications or other factors. Members of Parliament were bound to represent the interests of all British citizens and subjects, so colonists were the recipients of virtual representation in Parliament, like those disenfranchised subjects in the British Isles.[52] This theory, however, ignored a crucial difference between the unrepresented in Britain and the colonists. The colonists enjoyed actual representation in their own legislative assemblies, and the issue was whether these legislatures, rather than Parliament, were in fact the sole recipients of the colonists' consent with regard to taxation.[53]
 In May 1764, Samuel Adams of Boston drafted the following that stated the common American position:
 Massachusetts appointed a five-member Committee of Correspondence in June 1764 to coordinate action and exchange information regarding the Sugar Act, and Rhode Island formed a similar committee in October 1764. This attempt at unified action represented a significant step forward in colonial unity and cooperation. The Virginia House of Burgesses sent a protest of the taxes to London in December 1764, arguing that they did not have the specie required to pay the tax.[55][56] Massachusetts, New York, New Jersey, Rhode Island, and Connecticut also sent protest to England in 1764. The content of the messages varied, but they all emphasized that taxation of the colonies without colonial assent was a violation of their rights. By the end of 1765, all of the Thirteen Colonies except Georgia and North Carolina had sent some sort of protest passed by colonial legislative assemblies.[57][49]
 The Virginia House of Burgesses reconvened in early May 1765 after news was received of the passage of the act. By the end of May, it appeared that they would not consider the tax, and many legislators went home, including George Washington. Only 30 out of 116 Burgesses remained, but one of those remaining was Patrick Henry who was attending his first session. Henry led the opposition to the Stamp Act; he proposed his resolutions on 29 May 1765, and they were passed in the form of the Virginia Resolves.[58][59] The Resolves stated:
 On 6 June 1765, the Massachusetts Lower House proposed a meeting for the 1st Tuesday of October in New York City:
 There was no attempt to keep this meeting a secret; Massachusetts promptly notified Richard Jackson of the proposed meeting, their agent in England and a member of Parliament.[62]
 John Adams complained that the London ministry was intentionally trying ""to strip us in a great measure of the means of knowledge, by loading the Press, the colleges, and even an Almanack and a News-Paper, with restraints and duties.""[63] The press fought back. By 1760 the fledgling American newspaper industry comprised 24 weekly papers in major cities. Benjamin Franklin had created an informal network so that each one routinely reprinted news, editorials, letters and essays from the others, thus helping form a common American voice. All the editors were annoyed at the new stamp tax they would have to pay on each copy. By informing colonists what the other colonies were saying the press became a powerful opposition force to the Stamp Act. Many circumvented it and most equated taxation without representation with despotism and tyranny, thus providing a common vocabulary of protest for the Thirteen Colonies.[64]
 The August 1, 1768, issue of the Pennsylvania Chronicle, established by William Goddard, printed on the front page a four-column article of an address made at the State House (Independence Hall) against the Stamp Act, and other excessive tax laws passed without colonial representation in the British Parliament.[65]
 The newspapers reported effigy hangings and stamp master resignation speeches. Some newspapers were on the royal payroll and supported the act, but most of the press was free and vocal. Thus William Bradford, the foremost printer in Philadelphia, became a leader of the Sons of Liberty. He added a skull and crossbones with the words, ""the fatal Stamp,"" to the masthead of his Pennsylvania Journal and weekly Advertiser.[66]
 Some of the earliest forms of American propaganda appeared in these printings in response to the law. The articles written in colonial newspapers were particularly critical of the act because of the Stamp Act's disproportionate effect on printers. David Ramsay, a patriot and historian from South Carolina, wrote of this phenomenon shortly after the American Revolution:
 Most printers were critical of the Stamp Act, although a few Loyalist voices did exist. Some of the more subtle Loyalist sentiments can be seen in publications such as The Boston Evening Post, which was run by British sympathizers John and Thomas Fleet. The article detailed a violent protest that occurred in New York in December, 1765, then described the riot's participants as ""imperfect"" and labeled the group's ideas as ""contrary to the general sense of the people.""[68] Vindex Patriae denigrated the colonists as foreign vagabonds and ungrateful Scots-Irish subjects determined to ""strut and claim an independent property to the dunghill"".[69] These Loyalists beliefs can be seen in some of the early newspaper articles about the Stamp Act, but the anti-British writings were more prevalent and seem to have had a more powerful effect.[70]
 Many papers assumed a relatively conservative tone before the act went into effect, implying that they might close if it wasn't repealed. However, as time passed and violent demonstrations ensued, the authors became more vitriolic. Several newspaper editors were involved with the Sons of Liberty, such as William Bradford of The Pennsylvania Journal and Benjamin Edes of The Boston Gazette, and they echoed the group's sentiments in their publications. The Stamp Act went into effect that November and many newspapers printed their editions with black borders about the edges and columns, which sometimes included imagery of tombstones and skeletons, emphasizing that their papers were ""dead"" and would no longer be able to print because of the Stamp Act.[71] However, most of them returned in the upcoming months, defiantly appearing without the stamp of approval that was deemed necessary by the Stamp Act. Printers were greatly relieved when the law was nullified in the following spring, and the repeal asserted their positions as a powerful voice (and compass) for public opinion.[72]
 While the colonial legislatures were acting, the ordinary citizens of the colonies were also voicing their concerns outside of this formal political process. Historian Gary B. Nash wrote:
 A popular pamphlet condemning the Stamp Act was written by Maryland lawyer Daniel Dulany in 1765. It was named Considerations on the Propriety of Imposing Taxes in the British Colonies.[77] In Pokomoke, Maryland, a tax collector was assaulted.[78] In Talbot County, Maryland, a group of unknown citizens released ""Resolutions of the Freemen of Talbot County Maryland"" on November 25, 1765. This proclamation declared that they should enjoy the same rights as British subjects and condemned the Stamp Act. They also declared that they would erect a gallows in front of the county court house with an effigy of a ""stamp informer"" hung in chains, which would remain until the Stamp Act was repealed.[79]
 Early street protests were most notable in Boston. Andrew Oliver was a distributor of stamps for Massachusetts who was hanged in effigy on 14 August 1765 ""from a giant elm tree at the crossing of Essex and Orange Streets in the city's South End."" Also hung was a jackboot painted green on the bottom (""a Green-ville sole""), a pun on both Grenville and the Earl of Bute, the two people most blamed by the colonists.[80] Lieutenant Governor Thomas Hutchinson ordered sheriff Stephen Greenleaf to take down the effigy, but he was opposed by a large crowd. All day the crowd detoured merchants on Orange Street to have their goods symbolically stamped under the elm tree, which later became known as the ""Liberty Tree"".  This date became accepted by the members of the Sons of Liberty in Boston as the date of the founding of their organization.[81]
 Ebenezer MacIntosh was a veteran of the Seven Years' War and a shoemaker. One night, he led a crowd which cut down the effigy of Andrew Oliver and took it in a funeral procession to the Town House where the legislature met. From there, they went to Oliver's office – which they tore down and symbolically stamped the timbers. Next, they took the effigy to Oliver's home at the foot of Fort Hill, where they beheaded it and then burned it – along with Oliver's stable house and coach and chaise. Greenleaf and Hutchinson were stoned when they tried to stop the mob, which then looted and destroyed the contents of Oliver's house. Oliver asked to be relieved of his duties the next day.[82] This resignation, however, was not enough. Oliver was ultimately forced by MacIntosh to be paraded through the streets and to publicly resign under the Liberty Tree.[83]
 As news spread of the reasons for Andrew Oliver's resignation, violence and threats of aggressive acts increased throughout the colonies, as did organized groups of resistance. Throughout the colonies, members of the middle and upper classes of society formed the foundation for these groups of resistance and soon called themselves the Sons of Liberty. These colonial groups of resistance burned effigies of royal officials, forced Stamp Act collectors to resign, and were able to get businessmen and judges to go about without using the proper stamps demanded by Parliament.[84]
 On 16 August, a mob damaged the home and official papers of William Story, the deputy register of the Vice-Admiralty, who then moved to Marblehead, Massachusetts. Benjamin Hallowell, the comptroller of customs, suffered the almost total loss of his home.[85]
 On 26 August, MacIntosh led an attack on Hutchinson's mansion. The mob evicted the family, destroyed the furniture, tore down the interior walls, emptied the wine cellar, scattered Hutchinson's collection of Massachusetts historical papers, and pulled down the building's cupola. Hutchinson had been in public office for three decades; he estimated his loss at £2,218[86] (in today's money, at nearly $250,000). Nash concludes that this attack was more than just a reaction to the Stamp Act:
 Governor Francis Bernard offered a £300 reward for information on the leaders of the mob, but no information was forthcoming. MacIntosh and several others were arrested, but were either freed by pressure from the merchants or released by mob action.[87]
 The street demonstrations originated from the efforts of respectable public leaders such as James Otis, who commanded the Boston Gazette, and Samuel Adams of the ""Loyal Nine"" of the Boston Caucus, an organization of Boston merchants. They made efforts to control the people below them on the economic and social scale, but they were often unsuccessful in maintaining a delicate balance between mass demonstrations and riots. These men needed the support of the working class, but also had to establish the legitimacy of their actions to have their protests to England taken seriously.[88] At the time of these protests, the Loyal Nine was more of a social club with political interests but, by December 1765, it began issuing statements as the Sons of Liberty.[89]
 Rhode Island also experienced street violence. A crowd built a gallows near the Town House in Newport on 27 August, where they carried effigies of three officials appointed as stamp distributors: Augustus Johnson, Dr. Thomas Moffat, and lawyer Martin Howard. The crowd at first was led by merchants William Ellery, Samuel Vernon, and Robert Crook, but they soon lost control. That night, the crowd was led by a poor man named John Weber, and they attacked the houses of Moffat and Howard, where they destroyed walls, fences, art, furniture, and wine. The local Sons of Liberty were publicly opposed to violence, and they refused at first to support Weber when he was arrested. They were persuaded to come to his assistance, however, when retaliation was threatened against their own homes. Weber was released and faded into obscurity.[90]
 Howard became the only prominent American to publicly support the Stamp Act in his pamphlet ""A Colonist's Defence of Taxation"" (1765). After the riots, Howard had to leave the colony, but he was rewarded by the Crown with an appointment as Chief Justice of North Carolina at a salary of £1,000.[91]
 In New York, James McEvers resigned his distributorship four days after the attack on Hutchinson's house. The first shipment of stamps for New York and Connecticut arrived at New York Harbor on 24 October, greeted by a huge crowd of angry colonists, and were kept at Fort George for safe keeping. Placards appeared throughout the city warning that, ""the first man that either distributes or makes use of stamped paper let him take care of his house, person, and effects."" New York merchants met on 31 October and agreed not to sell any English goods until the act was repealed. Crowds took to the streets for four days of demonstrations, uncontrolled by the local leaders, culminating in an attack by two thousand people on Governor Cadwallader Colden's home and the burning of two sleighs and a coach. Various stamp masters, including Zachariah Hood from Maryland, fled to Fort George out of concern for their safety. Unrest in New York City continued through the end of the year, and the local Sons of Liberty had difficulty in controlling crowd actions.[92][93]   Sir Henry Moore, who replaced Colden as provincial governor of New York, met with the influential Isaac Sears, a leader of the Sons of Liberty,  in an effort to maintain peace and restore order to the city. Shortly thereafter Moore opened the gates of the fort as a gesture of good faith and invited people in.[94][95]
 During the Stamp Act 1765 crisis, Archibald McCall (1734–1814) sided against patriots in Westmoreland and Essex County, Virginia.[96] He insisted on collecting the British tax that was placed on stamps and other documents. In reaction, a mob formed and stormed his house in Tappahannock, Virginia. They threw rocks through the windows and McCall was captured, tarred and feathered. The act was an example of ""taxation without representation"" and a leading event to the war against the British.[97]
 In Frederick, Maryland, a court of 12 magistrates ruled the Stamp Act invalid on 23 November 1765, and directed that businesses and colonial officials proceed in all matters without use of the stamps. A week later, a crowd conducted a mock funeral procession for the act in the streets of Frederick. The magistrates have been dubbed the ""12 Immortal Justices,"" and 23 November has been designated ""Repudiation Day"" by the Maryland state legislature. On 1 October 2015, Senator Cardin (D-MD) read into the Congressional Record a statement noting 2015 as the 250th anniversary of the event. Among the 12 magistrates was William Luckett, who later served as lieutenant colonel in the Maryland Militia at the Battle of Germantown.
 Other popular demonstrations occurred in Portsmouth, New Hampshire; Annapolis, Maryland; Wilmington and New Bern, North Carolina; and Charleston, South Carolina. In Philadelphia, Pennsylvania demonstrations were subdued but even targeted Benjamin Franklin's home, although it was not vandalized.[98] By 16 November, twelve of the stamp distributors had resigned. The Georgia distributor did not arrive in America until January 1766, but his first and only official action was to resign.[99]
 The overall effect of these protests was to both anger and unite the American people like never before. Opposition to the act inspired both political and constitutional forms of literature throughout the colonies, strengthened the colonial political perception and involvement, and created new forms of organized resistance. These organized groups quickly learned that they could force royal officials to resign by employing violent measures and threats.[100]
 The main issue was the constitutional rights of Englishmen, so the French in Quebec did not react. Some English-speaking merchants were opposed but were in a fairly small minority. The Quebec Gazette ceased publication until the act was repealed, apparently over the unwillingness to use stamped paper.[101] In neighboring Nova Scotia a number of former New England residents objected, but recent British immigrants and London-oriented business interests based in Halifax, the provincial capital were more influential. The only major public protest was the hanging in effigy of the stamp distributor and Lord Bute. The act was implemented in both provinces, but Nova Scotia's stamp distributor resigned in January 1766, beset by ungrounded fears for his safety. Authorities there were ordered to allow ships bearing unstamped papers to enter its ports, and business continued unabated after the distributors ran out of stamps.[102] The act occasioned some protests in Newfoundland, and the drafting of petitions opposing not only the Stamp Act, but the existence of the customhouse at St. John's, based on legislation dating back to the reign of Edward VI forbidding any sort of duties on the importation of goods related to its fisheries.[103]
 Violent protests were few in the Caribbean colonies. Political opposition was expressed in a number of colonies, including Barbados and Antigua, and by absentee landowners living in Britain. The worst political violence took place on St. Kitts and Nevis. Riots took place on 31 October 1765, and again on 5 November, targeting the homes and offices of stamp distributors; the number of participants suggests that the percentage of St. Kitts' white population involved matched that of Bostonian involvement in its riots. The delivery of stamps to St. Kitts was successfully blocked, and they were never used there. Montserrat and Antigua also succeeded in avoiding the use of stamps; some correspondents thought that rioting was prevented in Antigua only by the large troop presence. Despite vocal political opposition, Barbados used the stamps, to the pleasure of King George. In Jamaica there was also vocal opposition, which included threats of violence. There was much evasion of the stamps, and ships arriving without stamped papers were allowed to enter port. Despite this, Jamaica produced more stamp revenue (£2,000) than any other colony.[104]
 It was during this time of street demonstrations that locally organized groups started to merge into an inter-colonial organization of a type not previously seen in the colonies. The term ""sons of liberty"" had been used in a generic fashion well before 1765, but it was only around February 1766 that its influence extended throughout the colonies as an organized group using the formal name ""Sons of Liberty"", leading to a pattern for future resistance to the British that carried the colonies towards 1776. [e] Historian John C. Miller noted that the name was adopted as a result of Barre's use of the term in his February 1765 speech.[106]
 The organization spread month by month after independent starts in several different colonies. By 6 November, a committee was set up in New York to correspond with other colonies, and in December an alliance was formed between groups in New York and Connecticut. In January, a correspondence link was established between Boston and Manhattan, and by March, Providence had initiated connections with New York, New Hampshire, and Newport. By March, Sons of Liberty organizations had been established in New Jersey, Maryland, and Norfolk, Virginia, and a local group established in North Carolina was attracting interest in South Carolina and Georgia.[107]
 The officers and leaders of the Sons of Liberty ""were drawn almost entirely from the middle and upper ranks of colonial society,"" but they recognized the need to expand their power base to include ""the whole of political society, involving all of its social or economic subdivisions."" To do this, the Sons of Liberty relied on large public demonstrations to expand their base.[108] They learned early on that controlling such crowds was problematical, although they strived to control ""the possible violence of extra-legal gatherings"". The organization professed its loyalty to both local and British established government, but possible military action as a defensive measure was always part of their considerations. Throughout the Stamp Act Crisis, the Sons of Liberty professed continued loyalty to the King because they maintained a ""fundamental confidence"" that Parliament would do the right thing and repeal the tax.[f]
 The Stamp Act Congress was held in New York in October 1765. Twenty-seven delegates from nine colonies were the members of the Congress, and their responsibility was to draft a set of formal petitions stating why Parliament had no right to tax them.[111] Among the delegates were many important men in the colonies. Historian John Miller observes, ""The composition of this Stamp Act Congress ought to have been convincing proof to the British government that resistance to parliamentary taxation was by no means confined to the riffraff of colonial seaports.""[112]
 The youngest delegate was 26-year-old John Rutledge of South Carolina, and the oldest was 65-year-old Hendrick Fisher of New Jersey. Ten of the delegates were lawyers, ten were merchants, and seven were planters or land-owning farmers; all had served in some type of elective office, and all but three were born in the colonies. Four died before the colonies declared independence, and four signed the Declaration of Independence; nine attended the first and second Continental Congresses, and three were Loyalists during the Revolution.[113]
 New Hampshire declined to send delegates, and North Carolina, Georgia, and Virginia were not represented because their governors did not call their legislatures into session, thus preventing the selection of delegates. Despite the composition of the congress, each of the Thirteen Colonies eventually affirmed its decisions.[114][115] Six of the nine colonies represented at the Congress agreed to sign the petitions to the King and Parliament produced by the Congress. The delegations from New York, Connecticut, and South Carolina were prohibited from signing any documents without first receiving approval from the colonial assemblies that had appointed them.[116]
 Massachusetts governor Francis Bernard believed that his colony's delegates to the Congress would be supportive of Parliament. Timothy Ruggles in particular was Bernard's man, and was elected chairman of the Congress. Ruggles' instructions from Bernard were to ""recommend submission to the Stamp Act until Parliament could be persuaded to repeal it.""[117] Many delegates felt that a final resolution of the Stamp Act would actually bring Britain and the colonies closer together. Robert Livingston of New York stressed the importance of removing the Stamp Act from the public debate, writing to his colony's agent in England, ""If I really wished to see America in a state of independence I should desire as one of the most effectual means to that end that the stamp act should be enforced.""[118]
 The Congress met for 12 consecutive days, including Sundays. There was no audience at the meetings, and no information was released about the deliberations.[119][120] The meeting's final product was called ""The Declaration of Rights and Grievances"", and was drawn up by delegate John Dickinson of Pennsylvania. This Declaration raised fourteen points of colonial protest. It asserted that colonists possessed all the rights of Englishmen in addition to protesting the Stamp Act issue, and that Parliament could not represent the colonists since they had no voting rights over Parliament. Only the colonial assemblies had a right to tax the colonies. They also asserted that the extension of authority of the admiralty courts to non-naval matters represented an abuse of power.[121]
 In addition to simply arguing for their rights as Englishmen, the congress also asserted that they had certain natural rights solely because they were human beings. Resolution 3 stated, ""That it is inseparably essential to the freedom of a people, and the undoubted right of Englishmen, that no taxes be imposed on them, but with their own consent, given personally, or by their representatives."" Both Massachusetts and Pennsylvania brought forth the issue in separate resolutions even more directly when they respectively referred to ""the Natural rights of Mankind"" and ""the common rights of mankind"".[g]
 Christopher Gadsden of South Carolina had proposed that the Congress' petition should go only to the king, since the rights of the colonies did not originate with Parliament. This radical proposal went too far for most delegates and was rejected. The ""Declaration of Rights and Grievances"" was duly sent to the king, and petitions were also sent to both Houses of Parliament.[123]
 Grenville was replaced by Lord Rockingham as prime minister on 10 July 1765. News of the mob violence began to reach England in October. Conflicting sentiments were taking hold in Britain at the same time that resistance was building and accelerating in America. Some wanted to strictly enforce the Stamp Act 1765 over colonial resistance, wary of the precedent that would be set by backing down.[124] Others felt the economic effects of reduced trade with America after the Sugar Act 1764 and an inability to collect debts while the colonial economy suffered, and they began to lobby for a repeal of the Stamp Act 1765.[124][125] The colonial protest had included various non-importation agreements among merchants who recognized that a significant portion of British industry and commerce was dependent on the colonial market. This movement had also spread through the colonies; 200 merchants had met in New York City and agreed to import nothing from England until the Stamp Act 1765 was repealed.[126]
 When Parliament met in December 1765, it rejected a resolution offered by Grenville that would have condemned colonial resistance to the enforcement of the act. Outside of Parliament, Rockingham and his secretary Edmund Burke, a member of Parliament himself, organized London merchants who started a committee of correspondence to support repeal of the Stamp Act 1765 by urging merchants throughout the country to contact their local representatives in Parliament. When Parliament reconvened on 14 January 1766, the Rockingham ministry formally proposed repeal. Amendments were considered that would have lessened the financial impact on the colonies by allowing colonists to pay the tax in their own scrip, but this was viewed to be too little and too late.[h]
 William Pitt stated in the parliamentary debate that everything done by the Grenville ministry ""has been entirely wrong"" with respect to the colonies. He further stated, ""It is my opinion that this Kingdom has no right to lay a tax upon the colonies."" Pitt still maintained ""the authority of this kingdom over the colonies, to be sovereign and supreme, in every circumstance of government and legislature whatsoever,"" but he made the distinction that taxes were not part of governing, but were ""a voluntary gift and grant of the Commons alone."" He rejected the notion of virtual representation, as ""the most contemptible idea that ever entered into the head of man.""[129]
 Grenville responded to Pitt:
 Pitt's response to Grenville included, ""I rejoice that America has resisted. Three millions of people, so dead to all the feelings of liberty as voluntarily to submit to be slaves, would have been fit instruments to make slaves of the rest.""[131]
 Between 17 and 27 January, Rockingham shifted the attention from constitutional arguments to economic by presenting petitions complaining of the economic repercussions felt throughout the country. On 7 February, the House of Commons rejected a resolution by 274–134, saying that it would back the king in enforcing the act. Henry Seymour Conway, the government's leader in the House of Commons, introduced the Declaratory Act 1766 in an attempt to address both the constitutional and the economic issues, which affirmed the right of Parliament to legislate for the colonies ""in all cases whatsoever"", while admitting the inexpediency of attempting to enforce the Stamp Act. Only Pitt and three or four others voted against it. Other resolutions passed which condemned the riots and demanded compensation from the colonies for those who suffered losses because of the actions of the mobs.[132]
 The House of Commons heard testimony between 11 and 13 February, the most important witness being Benjamin Franklin on the last day of the hearings. He responded to the question about how the colonists would react if the act was not repealed: ""A total loss of the respect and affection the people of America bear to this country, and of all the commerce that depends on that respect and affection."" A Scottish journalist observed Franklin's answers to Parliament and his effect on the repeal; he later wrote to Franklin, ""To this very Examination, more than to any thing else, you are indebted to the speedy and total Repeal of this odious Law.""[133]
 A repealing bill was introduced on 21 February to repeal the Stamp Act 1765, and it passed by a vote of 276–168. The king gave royal assent to the resulting .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#b1d2ff}@media screen{html.skin-theme-clientpref-night .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .vanchor>:target~.vanchor-text{background-color:#0f4dc9}}Duties in American Colonies Act 1766 on 18 March 1766.[134][135] To celebrate the repeal, the Sons of Liberty in Dedham, Massachusetts erected the Pillar of Liberty with a bust of Pitt on top.[136]
 Some aspects of the resistance to the act provided a sort of rehearsal for similar acts of resistance to the 1767 Townshend Acts, particularly the activities of the Sons of Liberty and merchants in organizing opposition. The Stamp Act Congress was a predecessor to the later Continental Congresses, notably the Second Continental Congress which oversaw the establishment of American independence. The Committees of Correspondence used to coordinate activities were revived between 1772 and 1774 in response to a variety of controversial and unpopular affairs, and the colonies that met at the 1774 First Continental Congress established a non-importation agreement known as the Continental Association in response to Parliamentary passage of the Intolerable Acts.[citation needed]
"
Townshend Acts,https://en.wikipedia.org/wiki/Townshend_Acts,"

 The Townshend Acts (/ˈtaʊnzənd/)[1] or Townshend Duties were a series of British acts of Parliament passed during 1766 and 1767 introducing a series of taxes and regulations to enable administration of the British colonies in America. They are named after the Chancellor of the Exchequer who proposed the program. Historians vary slightly as to which acts they include under the heading ""Townshend Acts"", but five are often listed:[a]
 The purposes of the acts were to
 The Townshend Acts met resistance in the colonies. People debated them in the streets, and in the colonial newspapers. Opponents of the Acts gradually became violent, leading to the Boston Massacre of 1770. The Acts placed an indirect tax on glass, lead, paints, paper, and tea, all of which had to be imported from Britain. This form of revenue generation was Townshend's response to the failure of the Stamp Act 1765, which had provided the first form of direct taxation placed upon the colonies. However, the import duties proved to be similarly controversial. Colonial indignation over the acts was expressed in John Dickinson's Letters from a Farmer in Pennsylvania and in the Massachusetts Circular Letter. There was widespread protest, and American port cities refused to import British goods, so Parliament began to partially repeal the Townshend duties.[5] In March 1770, most of the taxes from the Townshend Acts were repealed by Parliament under Frederick, Lord North. However, the import duty on tea was retained in order to demonstrate to the colonists that Parliament held the sovereign authority to tax its colonies, in accordance with the Declaratory Act 1766. The British government continued to tax the American colonies without providing representation in Parliament. American resentment, corrupt British officials, and abusive enforcement spurred colonial attacks on British ships, including the burning of the Gaspee in 1772. The Townshend Acts' taxation of imported tea was enforced once again by the Tea Act 1773, and this led to the Boston Tea Party in 1773 in which Bostonians destroyed a large shipment of taxed tea. Parliament responded with severe punishments in the Intolerable Acts 1774. The Thirteen Colonies drilled their militia units, and war finally erupted in Lexington and Concord in April 1775, launching the American Revolution.
 Following the Seven Years' War (1756–1763), the British government was deep in debt. To pay a small fraction of the costs of the newly expanded empire, the Parliament of Great Britain decided to levy new taxes on the colonies of British America. Previously, through the Trade and Navigation Acts, Parliament had used taxation to regulate the trade of the empire. But with the Sugar Act 1764, Parliament sought, for the first time, to tax the colonies for the specific purpose of raising revenue. American colonists argued that there were constitutional issues involved.[6]
 The Americans claimed they were not represented in Parliament, but the British government retorted that they had ""virtual representation"", a concept the Americans rejected.[7] This issue, only briefly debated following the Sugar Act, became a major point of contention after Parliament's passage of the Stamp Act 1765. The Stamp Act proved to be wildly unpopular in the colonies, contributing to its repeal the following year, along with the failure to raise substantial revenue.
 Implicit in the Stamp Act dispute was an issue more fundamental than taxation and representation: the question of the extent of Parliament's authority in the colonies.[8] Parliament provided its answer to this question when it repealed the Stamp Act in 1766 by simultaneously passing the Declaratory Act, which proclaimed that Parliament could legislate for the colonies ""in all cases whatsoever"".[9]
 This act was the (joint) first act, passed on 29 June 1767,[11][12] the same day as the Commissioners of Customs Act (see below).
 It placed taxes on glass, lead, ""painters' colors"" (paint), paper, and tea.[14][15][16] It also gave the supreme court of each colony the power to issue ""writs of assistance"",[17] general warrants that could be issued to customs officers and used to search private property for smuggled goods.[18]
 There was an angry response from colonists, who deemed the taxes a threat to their rights as British subjects.[citation needed] The use of writs of assistance was significantly controversial since the right to be secure in one's private property was an established right in Britain.[citation needed][20]
 This act was passed on 29 June 1767.[22] It created a new Customs Board for the North American colonies, to be headquartered in Boston with five customs commissioners. New offices were eventually opened in other ports as well. The board was created to enforce shipping regulations and increase tax revenue. Previously, customs enforcement was handled by the Customs Board back in England. Due to the distance, enforcement was poor, taxes were avoided and smuggling was rampant.
 Once the new Customs Board was in operation, enforcement increased, leading to a confrontation with smuggling colonists. Incidents between customs officials, military personnel and colonists broke out across the colonies, eventually leading to the occupation of Boston by British troops. This led to the Boston Massacre.[24]
 This was the (joint) third of the five acts, passed on 2 July 1767,[27][25] the same day as the Indemnity Act.[27]
 It forbade the New York Assembly and the governor of New York from passing any new bills until they complied with the Quartering Act 1765.[28] That act required New York to provide housing, food and supplies for the British troops stationed there to defend the colony. New York resisted the Quartering Act saying they were being taxed, yet had no direct representation in Parliament.[29] Furthermore, New York didn't think British soldiers were needed any more, since the French and Indian War had come to an end.[30] 
 Before the act was implemented, New York reluctantly agreed to provide some of the soldiers' needs, so it was never applied.[31]
 This Act was passed together with the New York Restraining Act, on 2 July 1767.[27]
 'Indemnity' means 'security or protection against a loss or other financial burden'.[33] The Indemnity Act 1767 reduced taxes on the British East India Company when they imported tea into England. This allowed them to re-export the tea to the colonies more cheaply and resell it to the colonists. Until this time, all items had to be shipped to England first from wherever they were made and then re-exported to their destination, including to the colonies.[34] This followed from the principle of mercantilism in England, which meant the colonies were forced to trade only with England.[35]
 The British East India Company was one of England's largest companies but was on the verge of collapse due to much cheaper smuggled Dutch tea. Part of the purpose of the entire series of Townshend Acts was to save the company from imploding. Since tea smuggling had become a common and successful practice, Parliament realized how difficult it was to enforce the taxing of tea. The Act stated that no more taxes would be placed on tea, and it made the cost of the East India Company's tea less than tea that was smuggled via Holland. It was an incentive for the colonists to purchase the East India Company tea.[36][37]
 This was the last of the five acts passed. It was not passed until 8 March 1768,[39] the year after the other four. Lord Charles Townshend, the Chancellor of the Exchequer, after whom the Townshend Acts were named, had died suddenly in September 1767, and so did not introduce this Act.
 The Act was passed to aid the prosecution of smugglers. It gave admiralty courts, rather than colonial courts, jurisdiction over all matters concerning customs violations and smuggling. Before the Act, customs violators could be tried in an admiralty court in Halifax, Nova Scotia, if royal prosecutors believed they would not get a favourable outcome using a local judge and jury.
 The Vice-Admiralty Court Act added three new admiralty courts in Boston, Philadelphia and Charleston to aid in more effective prosecutions. These courts were run by judges appointed by the Crown and whose salaries were paid, in the first instance, from fines levied.[40] when they found someone guilty. 
 The decisions were made solely by the judge, without the option of trial by jury, which was considered to be a fundamental right of British subjects. In addition, the accused person had to travel to the court of jurisdiction at his own expense; if he did not appear, he was automatically considered guilty.[42]
 
 The first of the Townshend Acts, sometimes simply known as the Townshend Act, was the Revenue Act 1767 (7 Geo 3 c 46).[d][43][44] This act represented the Chatham ministry's new approach to generating tax revenue in the American colonies after the repeal of the Stamp Act in 1766.[5][15] The British government had gotten the impression that because the colonists had objected to the Stamp Act on the grounds that it was a direct (or ""internal"") tax, colonists would therefore accept indirect (or ""external"") taxes, such as taxes on imports.[45] With this in mind, Charles Townshend, the Chancellor of the Exchequer, devised a plan that placed new duties on paper, paint, lead, glass, and tea that were imported into the colonies.[15][16] These were items that were not produced in North America and that the colonists were only allowed to buy from Great Britain.[46]
 The colonists' objection to ""internal"" taxes did not mean that they would accept ""external"" taxes; the colonial position was that any tax laid by Parliament for the purpose of raising revenue was unconstitutional.[45] ""Townshend's mistaken belief that Americans regarded internal taxes as unconstitutional and external taxes constitutional"", wrote historian John Phillip Reid, ""was of vital importance in the history of events leading to the Revolution.""[47] The Townshend Revenue Act received royal assent on 29 June 1767.[12] There was little opposition expressed in Parliament at the time. ""Never could a fateful measure have had a more quiet passage"", wrote historian Peter Thomas.[12]
 The Revenue Act was passed in conjunction with the Indemnity Act 1767 (7 Geo 3 c 56),[e][49] which was intended to make the tea of the British East India Company more competitive with smuggled Dutch tea.[50] The Indemnity Act repealed taxes on tea imported to England, allowing it to be re-exported more cheaply to the colonies. This tax cut in England would be partially offset by the new Revenue Act taxes on tea in the colonies.[51] The Revenue Act also reaffirmed the legality of writs of assistance, or general search warrants, which gave customs officials broad powers to search houses and businesses for smuggled goods.[18]
 The original stated purpose of the Townshend duties was to raise a revenue to help pay the cost of maintaining an army in North America.[52] Townshend changed the purpose of the tax plan, however, and instead decided to use the revenue to pay the salaries of some colonial governors and judges.[53] Previously, the colonial assemblies had paid these salaries, but Parliament hoped to take the ""power of the purse""[54] away from the colonies. According to historian John C. Miller, ""Townshend ingeniously sought to take money from Americans by means of parliamentary taxation and to employ it against their liberties by making colonial governors and judges independent of the assemblies.""[55]
 Some members of Parliament objected because Townshend's plan was expected to generate only £40,000 in yearly revenue, but he explained that once the precedent for taxing the colonists had been firmly established, the program could gradually be expanded until the colonies paid for themselves.[3][56] According to historian Peter Thomas, Townshend's ""aims were political rather than financial"".[56]
 To better collect the new taxes, the Commissioners of Customs Act 1767 (7 Geo 3 c 41) established the American Board of Customs Commissioners, which was modeled on the British Board of Customs.[43] The board was created because of the difficulties the British Board faced in enforcing trade regulations in the distant colonies.[57] Five commissioners were appointed to the board, which was headquartered in Boston.[58] The American Customs Board would generate considerable hostility in the colonies towards the British government. According to historian Oliver Dickerson, ""The actual separation of the continental colonies from the rest of the Empire dates from the creation of this independent administrative board.""[59]
 The American Board of Customs Commissioners was notoriously corrupt, according to historians. Political scientist Peter Andreas argues:
 Historian Edmund Morgan says:
 Historian Doug Krehbiel argues:
 Another measure to enforce the trade laws was the Vice Admiralty Court Act 1768 (8 Geo 3 c 22).[63] Although often included in discussions of the Townshend Acts, this act was initiated by the Cabinet when Townshend was not present and was not passed until after his death.[64] Before this act, there was just one vice admiralty court in North America, located in Halifax, Nova Scotia. Established in 1764, this court proved to be too remote to serve all of the colonies, and so the 1768 Vice Admiralty Court Act created four district courts, which were located at Halifax, Boston, Philadelphia, and Charleston. One purpose of the vice admiralty courts, which did not have juries, was to help customs officials prosecute smugglers since colonial juries were reluctant to convict persons for violating unpopular trade regulations.
 Townshend also faced the problem of what to do about the New York General Assembly, which had refused to comply with the Quartering Act 1765 because its members saw the act's financial provisions as levying an unconstitutional tax.[29] The New York Restraining Act (7 Geo 3 c 59),[f][66] which according to historian Robert Chaffin was ""officially a part of the Townshend Acts"",[3] suspended the power of the Assembly until it complied with the Quartering Act.[67] The Restraining Act never went into effect because, by the time it was passed, the New York Assembly had already appropriated money to cover the costs of the Quartering Act. The Assembly avoided conceding the right of Parliament to tax the colonies by making no reference to the Quartering Act when appropriating this money; they also passed a resolution stating that Parliament could not constitutionally suspend an elected legislature.[68]
 Townshend knew that his program would be controversial in the colonies, but he argued that, ""The superiority of the mother country can at no time be better exerted than now.""[69] The Townshend Acts did not create an instant uproar like the Stamp Act had done two years earlier, but before long, opposition to the programme had become widespread.[70][71] Townshend did not live to see this reaction, having died suddenly on 4 September 1767.[72]
 The most influential colonial response to the Townshend Acts was a series of twelve essays by John Dickinson entitled ""Letters from a Farmer in Pennsylvania"", which began appearing in December 1767.[73] Eloquently articulating ideas already widely accepted in the colonies,[73] Dickinson argued that there was no difference between ""internal"" and ""external"" taxes, and that any taxes imposed on the colonies by Parliament for the sake of raising a revenue were unconstitutional.[74] Dickinson warned colonists not to concede to the taxes just because the rates were low since this would set a dangerous precedent.[75]
 Dickinson sent a copy of his ""Letters"" to James Otis of Massachusetts, informing Otis that ""whenever the Cause of American Freedom is to be vindicated, I look towards the Province of Massachusetts Bay"".[76][g] The Massachusetts House of Representatives began a campaign against the Townshend Acts by first sending a petition to King George asking for the repeal of the Revenue Act, and then sending a letter to the other colonial assemblies, asking them to join the resistance movement.[76] Upon receipt of the Massachusetts Circular Letter, other colonies also sent petitions to the king.[77][78] Virginia and Pennsylvania also sent petitions to Parliament, but the other colonies did not, believing that it might have been interpreted as an admission of Parliament's sovereignty over them.[79] Parliament refused to consider the petitions of Virginia and Pennsylvania.[80]
 In Great Britain, Lord Hillsborough, who had recently been appointed to the newly created office of Colonial Secretary, was alarmed by the actions of the Massachusetts House. In April 1768 he sent a letter to the colonial governors in America, instructing them to dissolve the colonial assemblies if they responded to the Massachusetts Circular Letter. He also sent a letter to Massachusetts Governor Francis Bernard, instructing him to have the Massachusetts House rescind the Circular Letter. By a vote of 92 to 17, the House refused to comply, and Bernard promptly dissolved the legislature.[81][82]
 When news of the outrage among the colonists finally reached Franklin in London he wrote a number of essays in 1768 calling for ""civility and good manners"", even though he did not approve of the measures.[83] In 1770, Franklin continued writing essays against the Townsend Acts and Lord Hillsborough and wrote eleven attacking the Acts that appeared in the Public Advertiser, a London daily newspaper. The essays were published between January 8 and February 19, 1770, and can be found in The Papers of Benjamin Franklin.[84][85]
 Merchants in the colonies, some of them smugglers, organized economic boycotts to put pressure on their British counterparts to work for repeal of the Townshend Acts. Boston merchants organized the first non-importation agreement, which called for merchants to suspend importation of certain British goods effective 1 January 1768. Merchants in other colonial ports, including New York City and Philadelphia, eventually joined the boycott.[86] In Virginia, the non-importation effort was organized by George Washington and George Mason. When the Virginia House of Burgesses passed a resolution stating that Parliament had no right to tax Virginians without their consent, Governor Lord Botetourt dissolved the assembly. The members met at Raleigh Tavern and adopted a boycott agreement known as the ""Association"".[87]
 The non-importation movement was not as effective as promoters had hoped. British exports to the colonies declined by 38 percent in 1769, but there were many merchants who did not participate in the boycott.[88] The boycott movement began to fail by 1770 and came to an end in 1771.[89]
 The newly created American Customs Board was seated in Boston, so it was there that the Board concentrated on enforcing the Townshend Acts.[90] The acts were so unpopular in Boston that the Customs Board requested assistance. Commodore Samuel Hood sent the fifty-gun fourth-rate ship HMS Romney, which arrived in Boston Harbor in May 1768.[91]
 On 10 June 1768, customs officials seized the Liberty, a sloop owned by leading Boston merchant John Hancock, on allegations that the ship had been involved in smuggling. Bostonians, already angry because the captain of the Romney had been impressing local sailors, began to riot. Customs officials fled to Castle William for protection. With John Adams serving as his lawyer, Hancock was prosecuted in a highly publicized trial by a vice-admiralty court, but the charges were eventually dropped.[92][93]
 Given the unstable state of affairs in Massachusetts, Hillsborough instructed Governor Bernard to try to find evidence of treason in Boston.[94] Parliament had determined that the Treason Act 1543 was still in force, which would allow Bostonians to be transported to England to stand trial for treason. Bernard could find no one who was willing to provide reliable evidence, however, and so there were no treason trials.[95] The possibility that American colonists might be arrested and sent to England for trial produced alarm and outrage in the colonies.[96]
 Even before the Liberty riot, Hillsborough had decided to send troops to Boston. On 8 June 1768, he instructed General Thomas Gage, Commander-in-Chief, North America, to send ""such Force as You shall think necessary to Boston"", although he conceded that this might lead to ""consequences not easily foreseen"".[97][98][99] Hillsborough suggested that Gage might send one regiment to Boston, but the Liberty incident convinced officials that more than one regiment would be needed.[100]
 People in Massachusetts learned in September 1768 that troops were on the way.[101] Samuel Adams organized an emergency, extralegal convention of towns and passed resolutions against the imminent occupation of Boston, but on 1 October 1768, the first of four regiments of the British Army began disembarking in Boston, and the Customs Commissioners returned to town.[102] The ""Journal of Occurrences"", an anonymously written series of newspaper articles, chronicled clashes between civilians and soldiers during the military occupation of Boston, apparently with some exaggeration.[103] Tensions rose after Christopher Seider, a Boston teenager, was killed by a customs employee on 22 February 1770.[104] Although British soldiers were not involved in that incident, resentment against the occupation escalated in the days that followed, resulting in the killing of five civilians in the Boston Massacre of 5 March 1770.[105] After the incident, the troops were withdrawn to Castle William.[106]
 On 5 March 1770—the same day as the Boston Massacre, although news traveled slowly at the time, and neither side of the Atlantic was aware of this coincidence—Lord North, the new Prime Minister, presented a motion in the House of Commons that called for partial repeal of the Townshend Revenue Act.[107] Although some in Parliament advocated a complete repeal of the act, North disagreed, arguing that the tea duty should be retained to assert ""the right of taxing the Americans"".[107] After debate, the Repeal Act (10 Geo 3 c 17)[108] received royal assent on 12 April 1770.[109]
 Historian Robert Chaffin argued that little had actually changed:
 The Townshend duty on tea was retained when the 1773 Tea Act was passed, which allowed the East India Company to ship tea directly to the colonies. The Boston Tea Party soon followed, which set the stage for the American Revolution.
"
Royal Proclamation of 1763,https://en.wikipedia.org/wiki/Royal_Proclamation_of_1763,"
 The Royal Proclamation of 1763 was issued by British King George III on 7 October 1763. It followed the Treaty of Paris (1763), which formally ended the Seven Years' War and transferred French territory in North America to Great Britain.[1] The Proclamation at least temporarily forbade all new settlements west of a line drawn along the Appalachian Mountains, which was delineated as an Indian Reserve.[2] Exclusion from the vast region of Trans-Appalachia created discontent between Britain and colonial land speculators and potential settlers. The proclamation and access to western lands was one of the first significant areas of dispute between Britain and the colonies and would become a contributing factor leading to the American Revolution.[3] The 1763 proclamation line is more or less similar to the Eastern Continental Divide, extending from Georgia in the south to the divide's northern terminus near the middle of the north border of Pennsylvania, where it intersects the northeasterly St. Lawrence Divide, and extends further through New England.
 The Royal Proclamation continues to be of legal importance to First Nations in Canada, being the first legal recognition of aboriginal title, rights and freedoms. It is recognized in the Constitution Act, 1982, partly due to direct action by Indigenous peoples of Canada, known as the Constitution Express movement of 1980–1982.[4][5]
 The Seven Years' War and its North American theater, the French and Indian War, ended with the 1763 Treaty of Paris. Under the treaty, all French colonial territory west of the Mississippi River was ceded to Spain. In contrast, all French colonial territory east of the Mississippi River and south of Rupert's Land (save Saint Pierre and Miquelon, which France kept) was ceded to Great Britain. Both Spain and Britain received some French islands in the Caribbean, while France kept Haiti and Guadeloupe.[6][7]
 The Proclamation of 1763 dealt with managing former French territories in North America that Britain acquired following its victory over France in the French and Indian War and regulating colonial settlers' expansion. It established new governments for several areas: the province of Quebec, the new colonies of West Florida and East Florida,[8] and a group of Caribbean islands, Grenada, Tobago, Saint Vincent, and Dominica, collectively referred to as the British Ceded Islands.[9]
 At the outset, the Royal Proclamation of 1763 defined the jurisdictional limits of the British territories of North America, limiting British colonial expansion on the continent. What remained of the Royal Province of New France east of the Great Lakes and the Ottawa River, and south of Rupert's Land, was reorganised under the name ""Quebec."" The territory northeast of the St. John River on the Labrador coast was reassigned to the Newfoundland Colony.[10] The lands west of Quebec and west of a line running along the crest of the Allegheny Mountains became (British) Indian Territory, barred to settlement from colonies east of the line.[11]
 The proclamation line was not intended to be a permanent boundary between the colonists and Native American lands but rather a temporary boundary that could be extended further west in an orderly, lawful manner.[12][13] It was also not designed as an uncrossable boundary; people could cross the line, but not settle past it.[14] Its contour was defined by the headwaters that formed the watershed along the Appalachians. All land with rivers that flowed into the Atlantic was designated for the colonial entities. In contrast, all the land with rivers that flowed into the Mississippi was reserved for the Native American populations. The proclamation outlawed the private purchase of Native American land, which had often created problems. Instead, all future land purchases were to be made by Crown officials ""at some public Meeting or Assembly of the said Indians"". British colonials were forbidden to settle on native lands, and colonial officials were forbidden to grant ground or lands without royal approval. Organized land companies asked for land grants, but were denied by King George III.[15]
 British colonists and land speculators objected to the proclamation boundary since the British government had already assigned land grants to them. Including the wealthy owners of the Ohio company, who protested the line to the governor of Virginia, as they had plans to settle the land to grow the business.[16] Many settlements already existed beyond the proclamation line,[17] some of which had been temporarily evacuated during Pontiac's War, and there were many already granted land claims yet to be settled. For example, George Washington and his Virginia soldiers had been granted lands past the boundary. Prominent American colonials joined with the land speculators in Britain to lobby the government to move the line further west.[3][18]
 The colonists' demands were met and the boundary line was adjusted in a series of treaties with the Native Americans.[19] The first two of these treaties were completed in 1768; the Treaty of Fort Stanwix adjusted the border with the Iroquois Confederacy in the Ohio Country and the Treaty of Hard Labour adjusted the border with the Cherokee in the Carolinas.[20][21] The Treaty of Hard Labour was followed by the Treaty of Lochaber in 1770, adjusting the border between Virginia and the Cherokee.[22] These agreements opened much of what is now Kentucky and West Virginia to British settlement.[23] The land granted by the Virginian and North Carolinian government heavily favored the land companies, seeing as they had more wealthy backers than the poorer settlers who wanted to settle west in hopes of gaining a fortune.[24]
 Many colonists disregarded the proclamation line and settled west, which created tension between them and the Native Americans.[25] Pontiac's Rebellion (1763–1766) was a war involving Native American tribes, primarily from the Great Lakes region, the Illinois Country and the Ohio Country, who were dissatisfied with British postwar policies in the Great Lakes region after the end of the Seven Years' War. They were able to take over a large number of the forts which commanded the waterways involved in trade within the region and export to Great Britain.[26] The proclamation line had been conceived before the onset of Pontiac's Rebellion, but the outbreak of this conflict hastened the process of making it law.[17]
 The Royal Proclamation continued to govern the cession of Indigenous land in British North America, especially Upper Canada and Rupert's Land. Upper Canada created a platform for treaty-making based on the Royal Proclamation. After loyalists moved into land after Britain's defeat in the American Revolution, the first impetus was created out of necessity.[27]
 According to historian Colin Calloway, ""scholars disagree on whether the proclamation recognized or undermined tribal sovereignty"".[28]
 Some see the Royal Proclamation of 1763 as a ""fundamental document"" for First Nations land claims and self-government.[29] It is ""the first legal recognition by the British Crown of Aboriginal rights""[30] and imposes a fiduciary duty of care on the Crown. The intent and promises made to the natives in the proclamation have been argued to be temporary, only meant to appease the Native peoples who were becoming increasingly resentful of ""settler encroachments on their lands""[31] and were capable of becoming a serious threat to British colonial settlement.[32][33] Advice given by a Sir William Johnson, superintendent of Indian Affairs in North America, to the Board of Trade on 30 August 1764, expressed that:
 Anishinaabe jurist John Borrows has written that ""the Proclamation illustrates the British government's attempt to exercise sovereignty over First Nations while simultaneously trying to convince First Nations that they would remain separate from European settlers and have their jurisdiction preserved.""[35] Borrows further writes that the Royal Proclamation along with the subsequent Treaty of Niagara, provide for an argument that ""discredits the claims of the Crown to exercise sovereignty over First Nations""[36] and affirms Aboriginal ""powers of self-determination in, among other things, allocating lands"".[37]
 The functional content of the proclamation was reintroduced into American law by the decision of the U.S. Supreme Court in Johnson v. McIntosh (1823).[citation needed]
 In October 2013, the 250th anniversary of the Royal Proclamation was celebrated in Ottawa with a meeting of Indigenous leaders and Governor-General David Johnston.[38] The Aboriginal movement Idle No More held birthday parties for the document at various locations across Canada.[39]
 The influence of the Royal Proclamation of 1763 on the coming of the American Revolution has been variously interpreted. Many historians argue that the proclamation ceased to be a significant source of tension after 1768 since the aforementioned later treaties opened up extensive lands for settlement. Others have argued that colonial resentment of the proclamation contributed to the growing divide between the colonies and the mother country. Some historians say that even though the boundary was pushed west in subsequent treaties, the British government refused to permit new colonial settlements for fear of instigating a war with Native Americans, which angered colonial land speculators.[40] Others argue that the Royal Proclamation imposed a fiduciary duty of care on the Crown.[41]
 George Washington was given 20,000 acres (81 km2) of land in the Ohio region for his services in the French and Indian War. In 1770, Washington took the lead in securing the rights of himself and his old soldiers in the French War, advancing money to pay expenses for the common cause and using his influence in the proper quarters. In August 1770, it was decided that Washington should personally make a trip to the western region, where he located and surveyed tracts for himself and military comrades. After some dispute, he was eventually granted a patent letter for tracts of land there. The lands involved were open to Virginians under terms of the Treaty of Lochaber of 1770, except for the lands located two miles (3.2 km) south of Fort Pitt, now known as Pittsburgh.[42]
 In the United States, the Royal Proclamation of 1763 ended with the American Revolutionary War because Great Britain ceded the land in question to the United States in the Treaty of Paris (1783). Afterward, the U.S. government faced difficulties preventing frontier violence and eventually adopted policies similar to the Royal Proclamation. The first in a series of Indian Intercourse Acts was passed in 1790, prohibiting unregulated trade and travel in Native American lands. In 1823, the U.S. Supreme Court case Johnson v. McIntosh established that only the U.S. government, and not private individuals, could purchase land from Native Americans.[43]
"
Allegheny Mountains,https://en.wikipedia.org/wiki/Allegheny_Mountains,"
 The Allegheny Mountain Range (/ˌælɪˈɡeɪni/ AL-ig-AY-nee) — also spelled Alleghany or Allegany, less formally the Alleghenies — is part of the vast Appalachian Mountain Range of the Eastern United States and Canada. Historically it represented a significant barrier to westward land travel and development. The Alleghenies have a northeast–southwest orientation, running for about 300 miles (480 km) from north-central Pennsylvania southward, through western Maryland and eastern West Virginia.
 The Alleghenies comprise the rugged western-central portion of the Appalachians. They rise to 4,862 feet (1,482 m) in northeastern West Virginia. In the east, they are dominated by a high, steep escarpment known as the Allegheny Front. In the west, they slope down into the closely associated Allegheny Plateau, which extends into Ohio and Kentucky. The principal settlements of the Alleghenies are Altoona, State College, and Johnstown, Pennsylvania; and Cumberland, Maryland.
 Using the USGS classification of physical geography (physiography), the Allegheny Mountain range is part of the Appalachian Plateau province of the Appalachian Highlands physiographic division.
 The name is derived from the Allegheny River, which drains only a small portion of the Alleghenies in west-central Pennsylvania. The meaning of the word, which comes from the Lenape (Delaware) Native Americans, is not definitively known but is usually translated as ""fine river"". The closest approximation which makes sense is some context from the Jesuit Relations [1] showing that Alligeh was one of several accepted renderings of the name of the Erie people among the early 17th century missionaries among the Native peoples throughout the eastern Great Lakes region, along with Rique, Yenresh and Erichronon. The suffix -ni means ""of the,"" in Lenape, despite the irony that geh is also Iroquoian for ""of the."" So, most likely, Alligehni, or Oligini, would probably be the Lenape name for the original homeland of the Erie people.
 The word ""Allegheny"" was once commonly used to refer to the whole of what are now called the Appalachian Mountains. John Norton used it (spelled variously) around 1810 to refer to the mountains in Tennessee and Georgia.[2] Around the same time, Washington Irving proposed renaming the United States either ""Appalachia"" or ""Alleghania"".[3] In 1861, Arnold Henry Guyot published the first systematic geologic study of the whole mountain range.[4] His map labeled the range as the ""Alleghanies"", but his book was titled On the Appalachian Mountain System. As late as 1867, John Muir—in his book A Thousand Mile Walk to the Gulf—used the word ""Alleghanies"" in referring to the southern Appalachians.
 There was no general agreement about the ""Appalachians"" versus the ""Alleghanies"" until the late 19th century.[3]
 From northeast to southwest, the Allegheny Mountains run about 300 miles (480 km). From west to east, at their widest, they are about 100 miles (160 km). [a] When combining the Allegheny Province with the Kanawa Province, they run 400 miles (640 km).
 The USGS physiographic classification of all land in the United States lists the Allegheny Mountains as a section within the larger Appalachian Plateau province.[5] It may be generally defined to the south by the Allegheny Front, and to the east by the Susquehanna River valley. To the west, the Alleghenies grade down into the dissected Allegheny Plateau. The westernmost ridges are considered to be the Laurel Highlands and Chestnut Ridge in Pennsylvania, and Laurel Mountain and Rich Mountain in West Virginia.
 Big Stone Ridge marks the southern extent of the Alleghenies and is an outlier of Flat Top Mountain, with the Tug Fork river running along its western flank.[6] The land to the south and to the west of the Alleghenies is the Valley and Ridge physiographic province.
 The eastern edge of the Alleghenies is marked by the Allegheny Front, which is also sometimes considered the eastern terminus of the Allegheny Plateau. This great escarpment roughly follows a portion of the Eastern Continental Divide in this area. A number of impressive gorges and valleys drain the Alleghenies: to the east, Smoke Hole Canyon (South Branch Potomac River), and to the west the New River Gorge and the Blackwater and Cheat Canyons. Thus, about half the precipitation falling on the Alleghenies makes its way west to the Mississippi and half goes east to Chesapeake Bay and the Atlantic seaboard.
 The highest ridges of the Alleghenies are just west of the Front, which has an east/west elevational change of up to 3,000 feet (910 m). Absolute elevations of the Allegheny Highlands reach nearly 5,000 feet (1,500 m), with the highest elevations in the southern part of the range. The highest point in the Allegheny Mountains is Spruce Knob (4,863 ft; 1,482 m), on Spruce Mountain in West Virginia. Other notable Allegheny highpoints include Thorny Flat on Cheat Mountain (4,848 ft; 1,478 m), Bald Knob on Back Allegheny Mountain (4,842 ft; 1,476 m), and Mount Porte Crayon (4,770 ft; 1,450 m), all in West Virginia; Dans Mountain (2,898 ft; 883 m) in Maryland, Backbone Mountain (3,360 ft; 1,020 m), the highest point in Maryland; Mount Davis (3,213 ft; 979 m), the highest point in Pennsylvania, and the second highest, Blue Knob (3,146 ft; 959 m).
 There are very few sizable cities in the Alleghenies. The four largest are (in descending order of population): Altoona, State College, Johnstown (all in Pennsylvania) and Cumberland (in Maryland). In the 1970s and '80s, the Interstate Highway System was extended into the northern portion of the Alleghenies, and the region is now served by a network of federal expressways—Interstates 80, 70/76 and 68. Interstate 64 traverses the southern extremity of the range, but the Central Alleghenies (the ""High Alleghenies"" of eastern West Virginia) have posed special problems for highway planners owing to the region's very rugged terrain and environmental sensitivities (see Corridor H). This region is still served by a rather sparse secondary highway system and remains considerably lower in population density than surrounding regions.
 In the telecommunications field, a unique impediment to development in the central Allegheny region is the United States National Radio Quiet Zone (NRQZ), a large rectangle of land—about 13,000 square miles (34,000 km2)—that straddles the border area of Virginia and West Virginia. Created in 1958 by the Federal Communications Commission, the NRQZ severely restricts all omnidirectional and high-power radio transmissions, although cell phone service is allowed throughout much of the area.
 Much of the Monongahela (West Virginia), George Washington (West Virginia, Virginia) and Jefferson (Virginia) National Forests lie within the Allegheny Mountains. (No part of the wooded Alleghenies in Maryland or Pennsylvania, however, is managed by the U.S. Forest Service.) The Alleghenies also include a number of federally designated wilderness areas, such as the Dolly Sods Wilderness, Laurel Fork Wilderness, and Cranberry Wilderness in West Virginia.
 The mostly completed Allegheny Trail, a project of the West Virginia Scenic Trails Association since 1975, runs the length of the range within West Virginia. The northern terminus is at the Mason–Dixon line and the southern is at the West Virginia-Virginia border on Peters Mountain.[7]
 The bedrock of the Alleghenies is mostly sandstone and metamorphosed sandstone, quartzite, which is extremely resistant to weathering. Prominent beds of resistant conglomerate can be found in some areas, such as the Dolly Sods. When it weathers, it leaves behind a pure white quartzite gravel. The rock layers of the Alleghenies were formed during the Appalachian orogeny.
 Because of intense freeze-thaw cycles in the higher Alleghenies, there is little native bedrock exposed in most areas. The ground surface usually rests on a massive jumble of sandstone rocks, with air space between them, that are gradually moving down-slope. The crest of the Allegheny Front is an exception, where high bluffs are often exposed.
 Mineral springs in the High Alleghenies attracted Native Americans and 18th century white settlers and provided a modest incentive to the local economy. The spas developing around these geological features include celebrated resorts that continue to cater to an exclusive clientele, such as The Greenbrier (White Sulphur Springs, West Virginia; hotel built 1858) and The Homestead (Hot Springs, Virginia; original lodge built 1766).
 The High Alleghenies are noted for their forests of red spruce, balsam fir, and mountain ash, trees typically found much farther north. Hardwood forests also include yellow birch, sugar and red maple, eastern hemlock, and black cherry. American beech, pine and hickory can also be found. The forests of the entire region are now almost all second- or third-growth forests, the original trees having been removed in the late 19th and (in West Virginia) early 20th centuries. The wild onion known as the ramp (Allium tricoccum) is also present in the deeper forests.
 Certain isolated areas in the High Alleghenies are well known for their open expanses of sphagnum bogs and heath shrubs (e.g., Dolly Sods, Cranberry Glades). Many plant communities are indeed similar to those of sea-level eastern Canada. But the ecosystems within the Alleghenies are remarkably varied. In recent decades, the many stages of ecologic succession throughout the area have made the region one of enduring interest to botanists.
 The larger megafauna which once inhabited the High Alleghenies—elk, bison, mountain lion—were all exterminated during the 19th century. They survived longer in this area, however, than in other parts of the eastern United States. Naturalist John James Audubon reported that by 1851 a few eastern elk (Cervus canadensis canadiensis) could still be found in the Alleghany Mountains but that by then they were virtually gone from the remainder of their range. Mammals in the Allegheny region today include whitetail deer, chipmunk, raccoon, skunk, groundhog, opossum, weasel, field mouse, flying squirrel, cottontail rabbit, gray foxes, red foxes, gray squirrels, red squirrels and a cave bat. Bobcat, snowshoe hare, wild boar and black bear and coyote are also found in the forests and parks of the Alleghenies. Mink and beaver are much less often seen.
 These mountains and plateau have over 20 species of reptiles represented as lizard, skink, turtle and snake. Some of the icterid birds visit the mountains as well as the hermit thrush and wood thrush. North American migrant birds live throughout the mountains during the warmer seasons. Occasionally, osprey and eagles can be found nesting along the streams. The hawks and owls are the most common birds of prey.
 The water habitats of the Alleghenies hold 24 families of fish. Amphibian species number about 21, among them hellbenders, lungless salamanders, and various toads and frogs. The Alleghenies provide habitat for about 54 species of common invertebrate. These include Gastropoda, slugs, leech, earthworms and grub worm. Cave crayfish (Cambarus nerterius) live alongside a little over seven dozen cave invertebrates.[8]
 The indigenous people inhabiting the Allegheny Mountains emerged from the greater region's archaic and mound building cultures, particularly the Adena and Eastern Woodland peoples with a later Hopewellian influence. These Late Middle Woodland culture people have been called the Montaine (c. A.D. 500 to 1000) culture.[9][10] Their neighbors, the woodland Buck Garden culture, lived in the western valleys of the central Allegheny range. The Montaine sites extend from the tributaries of the upper Potomac River region south to the New River tributaries. These also were influenced by the earlier Armstrong culture of the more southwestern portions northern sub-range of the Ouasioto (Cumberland) Mountains and by the more easterly Virginia Woodland people. The Late Woodland Montaine were less influenced by Hopewellian trade from Ohio, although similarly polished stone tools have been found among the Montaine sites in the Tygart Valley.[11] Small groups of Montaine people appear to have lingered much beyond their classically defined period in parts of the most mountainous valleys.[12]
 The watershed of the Monongahela River is within the northwestern Alleghenies, and it is from it that the Monongahela culture takes its name. The Godwin-Portman site (36AL39) located in Allegheny County, Pennsylvania, had a possible Fort Ancient (c. AD 850 to 1680) presence during the 15th century.[13] Washington Boro ceramics have been found on the Barton (18AG3) and Llewellyn (18AG26) sites in Maryland on the northeastern slopes of the late Susquehannock sequence. The early Monongahela (c. AD 900 to 1630) are called the Drew Tradition in Pennsylvania. According to archeologist Richard L. George: ""I believe that some of the Monongahela were of Algonquin origin.... Other scholars have suggested that Iroquoian speakers were interacting with Late Monongahela people, and additional evidence is presented to confirm this. I conclude that the archaeologically conceived term, Monongahela, likely encompasses speakers of several languages, including Siouan.""[14] According to Dr Maslowski of West Virginia in 2009: ""The New River Drainage and upper Potomac represents the range of the Huffman Phase (Page) hunting and gathering area or when it is found in small amounts on village sites, trade ware or Page women being assimilated into another village (tribe)."" Finally, according to Prof Potter of Virginia, they [the people represented by the Huffman Phase of Page pottery] had occupied the eastern slopes of the Alleghenies on the upper Potomac to the northern, lower Shenandoah Valley region before the A.D. 1300 Luray phase (Algonquian) peoples' ""invasion"". It is thought that these ancient Alleghenians were pushed from the classic Huffman Phase of the eastern slopes of the Alleghenies to the Blue Ridge Mountains in western Virginia, which was eastern Siouan territory.
 In 1669, John Lederer and members of his party became the first Europeans to crest the Blue Ridge Mountains and the first to see the Shenandoah Valley and the Allegheny Mountains beyond.
 The proto-historic Alleghenies can be exampled by the earliest journals of the colonists. According to Batts and Fallows' September, 1671 Expedition, they found Mehetan Indians of Mountain ""Cherokee-Iroquois"" mix on the New River tributaries. This journal does not identify the ""Salt Village"", but, that the ""Mehetan"" were associated with these and today thought to be ""Monetons"", Siouans. However, this journal does not identify the ""Salt Village"" below the Kanawha Falls, but, that simply the ""Mehetan"" were associated with these. He explained, below the ""Salt Villages"", a mass of hostile Indians had, implied, arrived and some believe these to be ""Shanwans"" of Vielles Expedition of 1692~94, ancient Shawnee. In 1669, John Lederer of Maryland for the Virginia Colony and the Tennessee Cherokee had visited the mouth of the Kanawha and reported no hostilities on the lower streams of the Alleghenies. The Mohetan representative through a Siouan translator explained to Mr Batts and Mr Fallon, Colonel Abraham Woods explorers 1671–2, that he (Moheton Native American) could not say much about the people below the ""Salt Village"" because they (Mountain Cherokee) were not associated with them. The Mohetan was armed by this time of 1671 for the Mohetan Representative was given several pouches of ammunition for his and the other's weapons as a token of friendship. Somebody had already been trading within the central Alleghenies before the Virginians historical record begins in the Allegheny Mountains. Some earlier scholars found evidence these Proto-historics were either Cistercians of Spanish Ajacan Occuquan outpost on the Potomac River or Jesuits and their Kahnawake Praying Indians (Mohawk) on the Riviere de la Ronceverte. The ""Kanawha Madonna"" may date from this period or earlier. Where the New River breaks through Peters' Mountain, near Pearisburg Virginia the 1671 journal mentions the ""Moketans had formerly lived"".
 According to a number of early 17th century maps, the Messawomeake or ""Mincquas"" (Dutch) occupied the northern Allegheny Mountains. The ""Shatteras"" (an ancient Tutelo) occupied the Ouasioto Mountains and the earliest term Canaraguy (Kanawhans otherwise Canawest[15]) on the 1671 French map occupied the southerly Alleghenies. They were associated with the Allegheny ""Cherokee"" and Eastern Siouan as trade-movers and canoe transporters. The Calicuas, an ancient most northern Cherokee, migrated or was pushed from the Central Ohio Valley onto the north eastern slopes of the Alleghenies of the ancient Messawomeake, Iroquois tradesmen to 1630s Kent Island, by 1710 maps. Sometime before 1712, the Canawest (""Kanawhans""-""Canallaway""-""Canaragay"") had moved to the upper Potomac and made a Treaty with the newly established trading post of Fort Conolloway which would become a part of western Maryland during the 1740s.
 Prior to European exploration and settlement, trails through the Alleghenies had been transited for many generations by American Indian tribes such as the Iroquois, Shawnee, Delaware, Catawba and others, for purposes of trade, hunting and, especially, warfare.[16] Western Virginia ""Cherokee"" were reported at Cherokee Falls, today's Valley Falls of the Tygart Valley.[17] Indian trader Charles Poke's trading post dates from 1731 with the Calicuas of Cherokee Falls still in the region from the previous century.
 The ""London Scribes"" (The Crown's taxation records) vaguely mentions the colonial Alleghenian location of only a few other early colonial trading locations. A general knowledge of these few outposts are more of traditional telling of some local people. However, an example is the ""Van Metre"" trading house mentioned in an earlier edition of the ""Wonderful West Virginia Magazine"" being on the South Branches of the upper reaches of the Potomac. Another very early trading house appears on a lower Greenbrier Valley map during the earlier decades of the 18th century.
 As early as 1719, new arrivals from Europe began to cross the lower Susquehanna River and settle illegally in defiance of the Board of Property in Pennsylvania, on un-warranted land of the northeastern drainage rivers of the Allegheny Mountains. Several Indian Nations requested the removal of ""Maryland Intruders"".[18] Some of these moved onward as territory opened up beyond the Alleghenies.
 The first permanent European settlers west of the Alleghenies have traditionally been considered to have been two New Englanders: Jacob Marlin and Stephen Sewell, who arrived in the Greenbrier Valley in 1749. They built a cabin together at what would become Marlinton, West Virginia, but after disputing over religion, Sewell moved into a nearby hollowed-out sycamore tree. In 1751, surveyor John Lewis (father of Andrew Lewis) discovered the pair. Sewell eventually settled on the eastern side of Sewell Mountain, near present-day Rainelle, West Virginia.[19] They may well have been the first to settle what was then called the ""western waters""—i.e., in the regions where streams flowed westward to the Gulf of Mexico rather than eastward to the Atlantic.
 Among the first whites to penetrate into the Allegheny Mountains were surveyors attempting to settle a dispute over the extent of lands belonging to either Thomas Fairfax, 6th Lord Fairfax of Cameron or to the English Privy Council. An expedition of 1736 by John Savage established the location of the source of the North Branch Potomac River. In March 1742, a frontiersman named John Howard—along with his son and others—had been commissioned by Governor Gooch to explore the southwest of Virginia as far as the Mississippi River. Following Cedar Creek through the Natural Bridge, they floated in buffalo-skin boats down the New, Coal, Kanawha, and Ohio Rivers to the Mississippi. Although captured by the French before he reached Natchez, Howard was eventually released and (in 1745) was interviewed by Fairfax. Howard's description of the South Branch Potomac River resulted in the definite decision by Fairfax to secure his lands in the region.[20] An expedition under Peter Jefferson and Thomas Lewis in the following year emplaced the ""Fairfax Stone"" at the source of the North Branch and established a line of demarcation (the ""Fairfax Line"") extending from the stone south-east to the headwaters of the Rappahannock River. Lewis' journal of that expedition provides a valuable view of the Allegheny country before its settlement.[21] Jefferson and Joshua Fry's ""Fry-Jefferson Map"" of 1751 accurately depicted the Alleghenies for the first time. In the following decades, pioneer settlers arrived in the Alleghenies, especially during Colonial Virginia's Robert Dinwiddie era (1751–58). These included squatters by the Quit-rent Law. Some had preceded the official surveyors using a ""hack on the tree and field of corn"" marking land ownership approved by the Virginia Colonial Governor who had to be replaced with Governor John Murray, 4th Earl of Dunmore.
 Trans-Allegheny travel had been facilitated when a military trail—Braddock Road—was blazed and opened by the Ohio Company in 1751. (It followed an earlier Indian and pioneer trail known as Nemacolin's Path.) Braddock Road connected Cumberland, Maryland (the upper limit of navigation on the Potomac River) and the forks of the Ohio River (the future Pittsburgh, Pennsylvania). It received its name from the British leader of the French and Indian War (1754–63), General Edward Braddock, who led the ill-fated Braddock expedition four years later.[22]
 In addition to the war, hunting and trading with Indians were primary motivations for white movement across the mountains. Permanent white settlement of the northern Alleghenies was facilitated by the explorations and stories of such noted Marylanders as the Indian fighter and trader Thomas Cresap (1702–90) and the backwoodsman and hunter Meshach Browning (1781–1859).[23] In the late 18th century, a massive migration to the Monongahela River basin took place over three main routes: along the old Braddock Road via Winchester, Virginia; through the Shenandoah Valley to the head of the Cheat River and from there to the Monongahela; and along the Lincoln Highway to Ligonier, Pennsylvania, and thence along Jacob's Creek to the Monongahela. These immigrants were predominantly Scotch-Irish, German, and, to a lesser extent, British stock.[24]
 The Braddock Road was superseded by the Cumberland Road—also called the National Road—one of the first major improved highways in the United States to be built by the federal government. Construction began in 1811 at Cumberland and the road reached Wheeling, Virginia (now West Virginia) on the Ohio River in 1818. Just to the south, the state-funded Staunton and Parkersburg Turnpike was constructed to provide a direct route for the settlements of the Shenandoah Valley to the Ohio River by way of the Tygart Valley and Little Kanawha Rivers. Planned and approved in 1826 and completed in 1848, the Staunton and Parkersburg was maintained by fees (tolls) collected at toll houses placed at regular intervals.
 Construction on the Baltimore and Ohio Railroad began at Baltimore in 1828; the B&O traversed the Alleghenies, changing the economy and society of the Mountains forever.[citation needed] The B&O had reached Martinsburg, (West) Virginia by May 1842, Hancock, (West) Virginia, by June, Cumberland, Maryland, on November 5, 1842, Piedmont, (West) Virginia on July 21, 1851, and Fairmont, (West) Virginia on June 22, 1852. (It finally reached its Ohio River terminus at Wheeling, (West) Virginia on January 1, 1853.)[citation needed]
 The Chesapeake and Ohio Canal — also begun in 1828, but at Georgetown — was also a public work of enormous economic and social significance for the Alleghenies. It approached Hancock, Maryland, by 1839. From the beginning, the B&O Railroad and the Chesapeake and Ohio Canal operated in bitter legal and commercial competition with one another as they vied for rights to the narrow strips of land along the Potomac.[25] When the Canal finally reached Cumberland in 1850, the Railroad had already arrived eight years before.[26] Debt-ridden, the Canal company dropped its plan to continue construction of the next 180 miles (290 km) of the Canal into the Ohio Valley.[27] The company had long realized—especially with the difficult experience of digging the Paw Paw Tunnel—that the original plan of construction over the mountains and all the way down the Youghiogheny River to Pittsburgh was ""wildly unrealistic"".[28]
 Public works financed at the state level were not lacking during this period. The Main Line of Public Works was a transportation network project built between 1826 and 1834 by the Commonwealth of Pennsylvania. It created a railroad and canal system across southern Pennsylvania between Philadelphia and Pittsburgh, intended to transport freight (notably including anthracite coal) and people with greater reliability, speed and capacity. The Main Line of Public Works included the Philadelphia and Columbia Railroad, the Allegheny Portage Railroad and the Pennsylvania Canal system.
 Lying astride the border separating the Union and Confederacy, the Alleghenies were among the areas most directly affected by the American Civil War (1861–1865). One of the earliest campaigns of the War was fought for control of the Staunton and Parkersburg Turnpike and for the access it provided to the B&O Railroad. The Battle of Rich Mountain (July 11, 1861) gave the Federals control of the turnpike, of Tygart's Valley, and of all of the territory of western Virginia to the north and west, including the railroad. (Union General George McClellan's victory in this theater would ultimately bring him promotion to commander the Army of the Potomac.) The Federals fortified at Cheat Summit, and the Confederates established strongholds at Camp Bartow and Camp Allegheny. Here they faced each other warily through the fall of 1861 and the following winter. General Robert E. Lee's attempt to attack Cheat Summit Fort (September 12–15, 1861) and Federal attempts to attack Bartow and Allegheny, all failed to change the strategic stalemate. Finally, the harsh, high elevation winter achieved what the troops had failed to accomplish, and in the spring of 1862 both armies moved on down the pike to the Battle of McDowell (May 8, 1862), and then on to fight what became General Stonewall Jackson's Shenandoah Valley Campaign (spring 1862). Two years later, much of this contested area (along with much else) became part of the new state of West Virginia. The very rugged terrain of the Alleghenies was not at all amenable to a large-scale maneuver war and so the actions that the area witnessed for the remainder of the conflict were generally guerrilla in nature.
 With the further spread of the railroad networks in the 1890s and early 1900s, many new towns developed and thrived in the Alleghenies. The lumbering and coal industries that boomed in the wake of the railroads brought a measure of prosperity to the region, but most of the revenues flowed out of the mountains to the cities of the eastern seaboard where the captains of industry were headquartered. This inequity created a bitter legacy that would last for generations and form the foundation of the mountaineers' poverty and the area's immense environmental degradation.
 The most momentous disaster to afflict the people of the Alleghenies was the Johnstown Flood—locally known as the ""Great Flood of 1889""—which occurred on May 31 of that year after the catastrophic failure of the South Fork Dam on the Little Conemaugh River 14 miles (23 km) upstream of the town of Johnstown, Pennsylvania. The dam broke after several days of extremely heavy rainfall, unleashing 20 million tons of water (18 million cubic meters) from the reservoir known as Lake Conemaugh. (This body of water had been built as part of the Main Line of Public Works, then abandoned.) With a flow rate that temporarily equalled that of the Mississippi River,[29] the flood killed 2,209 people[30] and caused US$17 million of damage (about $425 million in 2012 dollars). The American Red Cross, led by Clara Barton and with 50 volunteers, undertook a major disaster relief effort.[31] Support for victims came from all over the United States and 18 foreign countries. After the flood, survivors suffered a series of legal defeats in their attempts to recover damages from the dam's owners. Public indignation at that failure prompted the development in American law changing a fault-based regime to strict liability.
 In the 1920s and '30s, Allegheny highways were extensively paved to provide access for automobiles.
 From the 1950s to 1992, the United States government maintained a top secret continuity program known as Project Greek Island at The Greenbrier hotel in the Alleghenies of southern West Virginia.
 In August 1963, at the March on Washington for Jobs and Freedom, Dr. Martin Luther King Jr. referenced the Alleghenies—among several in an evocative list of mountains—in his famous ""I Have a Dream"" speech, when he said ""Let freedom ring from the heightening Alleghenies of Pennsylvania!""[32]
 The Flight 93 National Memorial is located at the site of the crash of United Airlines Flight 93—which was hijacked in the September 11 attacks—in Stonycreek Township, Pennsylvania, about 2 miles (3.2 km) north of Shanksville. The memorial honors the passengers and crew of Flight 93, who stopped Al-Qaeda terrorists from reaching their intended target.
"
Speculation,https://en.wikipedia.org/wiki/Speculation,"In finance, speculation is the purchase of an asset (a commodity, goods, or real estate) with the hope that it will become more valuable shortly. It can also refer to short sales in which the speculator hopes for a decline in value.
 Many speculators pay little attention to the fundamental value of a security and instead focus purely on price movements.[1][citation needed] In principle, speculation can involve any tradable good or financial instrument. Speculators are particularly common in the markets for stocks, bonds, commodity futures, currencies, cryptocurrency, fine art, collectibles, real estate, and financial derivatives.
 Speculators play one of four primary roles in financial markets, along with hedgers, who engage in transactions to offset some other pre-existing risk, arbitrageurs who seek to profit from situations where fungible instruments trade at different prices in different market segments, and investors who seek profit through long-term ownership of an instrument's underlying attributes.
 With the appearance of the stock ticker machine in 1867, which removed the need for traders to be physically present on the stock exchange floor, stock speculation underwent a dramatic expansion through the end of the 1920s. The number of shareholders increased, perhaps, from 4.4 million in 1900 to 26 million in 1932.[2]
 The view of what distinguishes investment from speculation and speculation from excessive speculation varies widely among pundits, legislators and academics. Some sources note that speculation is simply a higher-risk form of investment. Others define speculation more narrowly as positions not characterized as hedging.[3] The U.S. Commodity Futures Trading Commission defines a speculator as ""a trader who does not hedge, but who trades with the objective of achieving profits through the successful anticipation of price movements"".[4] The agency emphasizes that speculators serve important market functions, but defines excessive speculation as harmful to the proper functioning of futures markets.[5]
 According to Benjamin Graham in The Intelligent Investor, the prototypical defensive investor is ""one interested chiefly in safety plus freedom from bother"". He adds that ""some speculation is necessary and unavoidable, for, in many common-stock situations, there are substantial possibilities of both profit and loss, and the risks therein must be assumed by someone."" Thus, many long-term investors, even those who buy and hold for decades, may be classified as speculators, excepting only the rare few who are primarily motivated by income or safety of principal and not eventually selling at a profit.[6]
 Nicholas Kaldor[7] has long argued for the price-stabilizing role of speculators, who tend to even out ""price-fluctuations due to changes in the conditions of demand or supply"", by possessing ""better than average foresight"". This view was later echoed by the speculator Victor Niederhoffer, in ""The Speculator as Hero"",[8] who describes the benefits of speculation:
 Another service provided by speculators to a market is that by risking their own capital in the hope of profit, they add liquidity to the market and make it easier or even possible for others to offset risk, including those who may be classified as hedgers and arbitrageurs.
 If any market, such as pork bellies, had no speculators, only producers (hog farmers) and consumers (butchers, etc.) would participate. With fewer players in the market, there would be a larger spread between the current bid and the asking price of pork bellies. Any new entrant in the market who wanted to trade pork bellies would be forced to accept this illiquid market and might trade at market prices with large bid–ask spreads or even face difficulty finding a co-party to buy or sell to.
 By contrast, a commodity speculator may profit from the difference in the spread and, in competition with other speculators, reduce the spread.  Some schools of thought argue that speculators increase the liquidity in a market, and therefore promote an efficient market.[9] This efficiency is difficult to achieve without speculators. Speculators take information and speculate on how it affects prices, producers and consumers, who may want to hedge their risks, needing counterparties if they could find each other without markets it certainly would happen as it would be cheaper. A very beneficial by-product of speculation for the economy is price discovery.
 On the other hand, as more speculators participate in a market, underlying real demand and supply can diminish compared to trading volume, and prices may become distorted.[9]
 Speculators perform a risk-bearing role that can be beneficial to society.  For example, a farmer might consider planting corn on unused farmland. However, he might not want to do so because he is concerned that the price might fall too far by harvest time. By selling his crop in advance at a fixed price to a speculator, he can now hedge the price risk and plant the corn. Thus, speculators can increase production through their willingness to take on risk (not at the loss of profit).
 Speculative hedge funds that do fundamental analysis ""are far more likely than other investors to try to identify a firm's off-balance-sheet exposures"" including ""environmental or social liabilities present in a market or company but not explicitly accounted for in traditional numeric valuation or mainstream investor analysis"". Hence, they make the prices better reflect the true quality of operation of the firms.[10]
 Shorting may act as a ""canary in a coal mine"" to stop unsustainable practices earlier and thus reduce damages and form market bubbles.[10]
 Auctions are a method of squeezing out speculators from a transaction, but they may have their own perverse effects by the winner's curse. The winner's curse is, however, not very significant to markets with high liquidity for both buyers and sellers, as the auction for selling the product and the auction for buying the product occur simultaneously, and the two prices are separated only by a relatively small spread. That mechanism prevents the winner's curse phenomenon from causing mispricing to any degree greater than the spread.
 Speculation is often associated with economic bubbles.[11] A bubble occurs when the price for an asset exceeds its intrinsic value by a significant margin,[12] although not all bubbles occur due to speculation.[13] Speculative bubbles are characterized by rapid market expansion driven by word-of-mouth feedback loops, as initial rises in asset price attract new buyers and generate further inflation.[14] The growth of the bubble is followed by a precipitous collapse fueled by the same phenomenon.[12][15] Speculative bubbles are essentially social epidemics whose contagion is mediated by the structure of the market.[15] Some economists link asset price movements within a bubble to fundamental economic factors such as cash flows and discount rates.[16]
 In 1936, John Maynard Keynes wrote: ""Speculators may do no harm as bubbles on a steady stream of enterprise. But the situation is serious when enterprise becomes the bubble on a whirlpool of speculation. (1936:159)""[17] Keynes himself enjoyed speculation to the fullest, running an early precursor of a hedge fund. As the Bursar of the Cambridge University King's College, he managed two investment funds, one of which, called Chest Fund, invested not only in the then 'emerging' market US stocks, but to a smaller extent periodically included commodity futures and foreign currencies (see Chua and Woodward, 1983). His fund was profitable almost every year, averaging 13% per year, even during the Great Depression, thanks to very modern investment strategies, which included inter-market diversification (it invested in stocks, commodities and currencies) as well as shorting (selling borrowed stocks or futures to profit from falling prices), which Keynes advocated among the principles of successful investment in his 1933 report: ""a balanced investment position... and if possible, opposed risks"".[18]
 It is controversial whether the presence of speculators increases or decreases short-term volatility in a market. Their provision of capital and information may help stabilize prices closer to their true values. On the other hand, crowd behavior and positive feedback loops in market participants may also increase volatility.
 The economic disadvantages of speculation have resulted in a number of attempts over the years to introduce regulations and restrictions to try to limit or reduce the impact of speculators. States often enact such financial regulation in response to a crisis. Note for example the Bubble Act 1720, which the British government passed at the height of the South Sea Bubble to try to stop speculation in such schemes. It remained in place for over a hundred years until repealed in 1825. The Glass–Steagall Act passed in 1933 during the Great Depression in the United States provides another example; most of the Glass-Steagall provisions were repealed during the 1980s and 1990s. The Onion Futures Act bans the trading of futures contracts on onions in the United States, after speculators successfully cornered the market in the mid-1950s; it remains in effect as of 2021[update].
 The Soviet Union regarded any form of private trade with the intent of gaining profit as speculation (Russian: спекуляция) and a criminal offense and punished speculators accordingly with fines, imprisonment, confiscation and/or corrective labor. Speculation was specifically defined in article 154 of the Penal Code of the USSR.[19]
 Some nations have moved to limit foreign ownership of cropland to ensure that food is available for local consumption, while others have leased food land abroad despite receiving aid from the World Food Programme.[20]
 In 1935, the Indian government passed a law allowing the government partial restriction and direct control of food production (Defence of India Act, 1935). It included the ability to restrict or ban the trading in derivatives on food commodities. After achieving independence in 1947, India in the 1950s continued to struggle with feeding its population and the government increasingly restricted trading in food commodities. Just at the time the Forward Markets Commission was established in 1953, the government felt that derivative markets increased speculation, which led to increased food costs and price instabilities. In 1953 it finally prohibited options- and futures-trading altogether.[21]  The restrictions were not lifted until the 1980s.
 In the United States, following passage of the Dodd-Frank Wall Street Reform and Consumer Protection Act of 2010, the Commodity Futures Trading Commission (CFTC) has proposed regulations aimed at limiting speculation in futures markets by instituting position limits. The CFTC offers three basic elements for their regulatory framework: ""the size (or levels) of the limits themselves; the exemptions from the limits (for example, hedged positions) and; the policy on aggregating accounts for purposes of applying the limits"".[22] The proposed position limits would apply to 28 physical commodities traded in various exchanges across the US.[23]
 Another part of the Dodd-Frank Act established the Volcker Rule, which deals with speculative investments of banks that do not benefit their customers. Passed on 21 January 2010, it states that those investments played a key role in the 2007–2008 financial crisis.[24]
 Proposals made in the past to try to limit speculation – but never enacted – included:
"
Boston Tea Party,https://en.wikipedia.org/wiki/Boston_Tea_Party,"


 The Boston Tea Party was an American political and mercantile protest on December 16, 1773, by the Sons of Liberty in Boston in colonial Massachusetts.[2] The target was the Tea Act of May 10, 1773, which allowed the East India Company to sell tea from China in American colonies without paying taxes apart from those imposed by the Townshend Acts. The Sons of Liberty strongly opposed the taxes in the Townshend Act as a violation of their rights. In response, the Sons of Liberty, some disguised as Native Americans, destroyed a shipment of tea sent by the East India Company.
 The demonstrators boarded the ships and threw the chests of tea into the Boston Harbor. The British government considered the protest an act of treason and responded harshly.[3] Days later, the Philadelphia Tea Party, instead of destroying a shipment of tea, sent the ship back to England without unloading. The episodes escalated into the American Revolution, and the Boston Tea Party became an iconic event of American history. Since then other political protests such as the Tea Party movement have referred to themselves as historical successors to the Boston protest of 1773.
 The Tea Party was the culmination of a resistance movement throughout British America against the Tea Act, a tax passed by the British Parliament in 1773. Colonists objected to the Tea Act believing it violated their rights as Englishmen to ""no taxation without representation"", that is, to be taxed only by their own elected representatives and not by a parliament in which they were not represented. The well-connected East India Company also had been granted competitive advantages over colonial tea importers, who resented the move and feared additional infringement on their business.[4] Protesters had prevented the unloading of tea in three other colonies, but in Boston, embattled Royal Governor Thomas Hutchinson refused to allow the tea to be returned to Great Britain.
 The Boston Tea Party was a significant event that helped accelerate and intensify colonial support for the American Revolution. Parliament responded in 1774 with the Intolerable Acts, or Coercive Acts, which, among other provisions, ended local self-government in Massachusetts and closed Boston's commerce. Colonists throughout the Thirteen Colonies responded to the Intolerable Acts with additional acts of protest, and by convening the First Continental Congress in Philadelphia, which petitioned the British monarch for repeal of the acts and coordinated colonial resistance to them, culminating in the October 1774 Continental Association. The crisis escalated, leading to the Battles of Lexington and Concord on April 19, 1775, which marked the beginning of the American Revolutionary War.
 The event was initially known as The Destruction of the Tea.[5] The moniker ""Boston Tea Party"" gained popularity in the early 19th century as the event took on a legendary status in American history. The name succinctly captures the combination of locality (Boston), the commodity involved (tea), and the nature of the event (a political 'party' or gathering as a form of protest). The Boston Tea Party arose from two issues confronting the British Empire: the financial problems of the British East India Company and an ongoing dispute about the extent of Parliament's authority, if any, over the British American colonies without seating any elected representation. The North Ministry's attempt to resolve these issues produced a showdown that eventually resulted in the Revolution, the associated War of Independence, and ultimately the end of British colonialization and the emergence of the United States as a sovereign nation.[6] The Boston Tea Party was the second American tax revolt against the British royal authority, the first occurring in April 1772, in Weare, New Hampshire known as the Pine Tree Riot where colonialists protested heavy fines levied against them for harvesting trees.[7]
 As Europeans developed a taste for tea in the 17th century, rival companies were formed to import the product from China, which was then governed by the Qing dynasty.[8] In 1698, the British Parliament granted the East India Company a monopoly on the importation of tea.[9] When tea became popular in the British colonies, Parliament sought to eliminate foreign competition by passing an act in 1721 that required colonists to import their tea only from Great Britain.[10] The East India Company did not export tea to the colonies; by law, the company was required to sell its tea wholesale at auctions in England. British firms bought this tea and exported it to the colonies, where they resold it to merchants in Boston, New York, Philadelphia, and Charleston.[11]
 Until 1767, the East India Company paid an ad valorem tax of about 25% on tea that it imported into Great Britain.[12] Parliament laid additional taxes on tea sold for consumption in Britain. These high taxes, combined with the fact that tea imported into the Dutch Republic was not taxed by the Dutch government, meant that Britons and British Americans could buy smuggled Dutch tea at much cheaper prices.[13] The biggest market for illicit tea was England—by the 1760s the East India Company was losing £400,000 per year to smugglers in Great Britain[14]—but Dutch tea was also smuggled into British America in significant quantities.[15]
 To help the East India Company compete with smuggled Dutch tea, Parliament passed the Indemnity Act in 1767; the Act lowered the tax on tea consumed in Great Britain and gave the East India Company a refund of the 25% duty on tea that was re-exported to the colonies.[16] To help offset this loss of government revenue, Parliament also passed the Townshend Revenue Act of 1767, which levied new taxes, including one on tea, in the colonies.[17]
 A controversy between Great Britain and the colonies arose in the 1760s when Parliament sought, for the first time, to impose a direct tax on the colonies for the purpose of raising revenue. Some colonists, known in the colonies as American patriots, objected to the new tax program, arguing that it was a violation of the British Constitution. Britons and British Americans agreed that, according to the constitution, British subjects could not be taxed without the consent of their elected representatives. In Great Britain, this meant that taxes could only be levied by Parliament. Colonists, however, did not elect members of Parliament, and so American Whigs argued that the colonies could not be taxed by that body. According to Whigs, colonists could only be taxed by their own colonial assemblies. Colonial protests resulted in the repeal of the Stamp Act in 1766, but in the 1766 Declaratory Act, Parliament continued to insist that it had the right to legislate for the colonies ""in all cases whatsoever"".[citation needed]
 When new taxes were levied in the Townshend Revenue Act of 1767, American patriots again responded with protests and boycotts. Merchants organized a non-importation agreement, and many colonists pledged to abstain from drinking British tea, with activists in New England promoting alternatives, such as domestic Labrador tea.[18] Smuggling continued apace, especially in New York and Philadelphia, where tea smuggling had always been more extensive than in Boston. Dutied British tea continued to be imported into Boston, however, especially by Richard Clarke and the sons of Massachusetts Governor Thomas Hutchinson, until pressure from Massachusetts Whigs compelled them to abide by the non-importation agreement.[19]
 Parliament finally responded to the protests by repealing the Townshend taxes in 1770, except for the tea duty, which Prime Minister Lord North kept to assert ""the right of taxing the Americans"".[20] This partial repeal of the taxes was enough to bring an end to the non-importation movement by October 1770.[21] From 1771 to 1773, British tea was once again imported into the colonies in significant amounts, with merchants paying the Townshend duty of three pence (equivalent to £1.61 in 2023) per pound in weight of tea.[22][23] Boston was the largest colonial importer of legal tea; smugglers still dominated the market in New York and Philadelphia.[24]
 In the 1772 Gaspee affair, colonists attacked and burned a British navy ship enforcing British customs laws off the coast of Newport, Rhode Island.
 The Indemnity Act of 1767, which gave the East India Company a refund of the duty on tea that was re-exported to the colonies, expired in 1772. Parliament passed a new act in 1772 that reduced this refund, effectively leaving a 10% duty on tea imported into Britain.[26] The act also restored the tea taxes within Britain that had been repealed in 1767, and left in place the three pence Townshend duty in the colonies, equal to £1.61 today. With this new tax burden driving up the price of British tea, sales plummeted. The company continued to import tea into Great Britain, however, amassing a huge surplus of product that no one would buy.[27] For these and other reasons, by late 1772 the East India Company, one of Britain's most important commercial institutions, was in a serious financial crisis.[28] The severe famine in Bengal from 1769 to 1773 had drastically reduced the revenue of the East India Company from India bringing the Company to the verge of bankruptcy and the Tea Act of 1773 was enacted to help the East India Company.[29]
 Eliminating some of the taxes was one obvious solution to the crisis. The East India Company initially sought to have the Townshend duty repealed, but the North ministry was unwilling because such an action might be interpreted as a retreat from Parliament's position that it had the right to tax the colonies.[30] More importantly, the tax collected from the Townshend duty was used to pay the salaries of some colonial governors and judges.[31] This was in fact the purpose of the Townshend tax: previously these officials had been paid by the colonial assemblies, but Parliament now paid their salaries to keep them dependent on the British government rather than allowing them to be accountable to the colonists.[32]
 Another possible solution for reducing the growing mound of tea in the East India Company warehouses was to sell it cheaply in Europe. This possibility was investigated, but it was determined that the tea would simply be smuggled back into Great Britain, where it would undersell the taxed product.[33] The best market for the East India Company's surplus tea, so it seemed, was the American colonies, if a way could be found to make it cheaper than the smuggled Dutch tea.[34]
 The North Ministry's solution was the Tea Act, which received the assent of King George on May 10, 1773.[35] This act restored the East India Company's full refund on the duty for importing tea into Britain, and also permitted the company, for the first time, to export tea to the colonies on its own account. This would allow the company to reduce costs by eliminating the middlemen who bought the tea at wholesale auctions in London.[36] Instead of selling to middlemen, the company now appointed colonial merchants to receive the tea on consignment; the consignees would in turn sell the tea for a commission. In July 1773, tea consignees were selected in New York, Philadelphia, Boston, and Charleston.[37] The Tea Act in 1773 authorized the shipment of 5,000 chests of tea (250 tons) to the American colonies. There would be a tax of £1,750 (equal to £283,000 today) to be paid by the importers when the cargo landed. The act granted the East India Company a monopoly on the sale of tea that was cheaper than smuggled tea; its hidden purpose was to force the colonists to pay a tax of 3 pennies on every pound of tea.[38]
 The Tea Act thus retained the three pence Townshend duty on tea imported to the colonies. Some members of Parliament wanted to eliminate this tax, arguing that there was no reason to provoke another colonial controversy. Former Chancellor of the Exchequer William Dowdeswell, for example, warned Lord North that the Americans would not accept the tea if the Townshend duty remained.[39] But North did not want to give up the revenue from the Townshend tax, primarily because it was used to pay the salaries of colonial officials; maintaining the right of taxing the Americans was a secondary concern.[40] According to historian Benjamin Labaree, ""A stubborn Lord North had unwittingly hammered a nail in the coffin of the old British Empire.""[41]
 Even with the Townshend duty in effect, the Tea Act would allow the East India Company to sell tea more cheaply than before, undercutting the prices offered by smugglers, but also undercutting colonial tea importers, who paid the tax and received no refund. In 1772, legally imported Bohea, the most common variety of tea, sold for about 3 shillings (3s) per pound, equal to £24.22 today.[42] After the Tea Act, colonial consignees would be able to sell tea for 2 shillings per pound (2s), just under the smugglers' price of 2 shillings and 1 penny (2s 1d).[43] Realizing that the payment of the Townshend duty was politically sensitive, the company hoped to conceal the tax by making arrangements to have it paid either in London once the tea was landed in the colonies, or have the consignees quietly pay the duties after the tea was sold. This effort to hide the tax from the colonists was unsuccessful.[44]
 In September and October 1773, seven ships carrying East India Company tea were sent to the colonies: four were bound for Boston, and one each for New York, Philadelphia, and Charleston.[45] In the ships were more than 2,000 chests containing nearly 600,000 pounds (270,000 kg) of tea.[46] Americans learned the details of the Tea Act while the ships were en route, and opposition began to mount.[47] Whigs, sometimes calling themselves Sons of Liberty, began a campaign to raise awareness and to convince or compel the consignees to resign, in the same way that stamp distributors had been forced to resign in the 1765 Stamp Act crisis.[48]
 The protest movement that culminated with the Boston Tea Party was not a dispute about high taxes. The price of legally imported tea was actually reduced by the Tea Act of 1773. Protesters were instead concerned with a variety of other issues. The familiar ""no taxation without representation"" argument, along with the question of the extent of Parliament's authority in the colonies, remained prominent.[49] Samuel Adams considered the British tea monopoly to be ""equal to a tax"" and to raise the same representation issue whether or not a tax was applied to it.[50] Some regarded the purpose of the tax program—to make leading officials independent of colonial influence—as a dangerous infringement of colonial rights.[51] This was especially true in Massachusetts, the only colony where the Townshend program had been fully implemented.[52]
 Colonial merchants, some of them smugglers, played a significant role in the protests. Because the Tea Act made legally imported tea cheaper, it threatened to put smugglers of Dutch tea out of business.[53] Legitimate tea importers who had not been named as consignees by the East India Company were also threatened with financial ruin by the Tea Act.[54] Another major concern for merchants was that the Tea Act gave the East India Company a monopoly on the tea trade, and it was feared that this government-created monopoly might be extended in the future to include other goods.[55]
 In New York, Philadelphia, and Charleston protesters compelled the tea consignees to resign. In Charleston, the consignees had been forced to resign by early December, and the unclaimed tea was seized by customs officials.[56] There were mass protest meetings in Philadelphia. Benjamin Rush urged his fellow countrymen to oppose the landing of the tea, because the cargo contained ""the seeds of slavery"".[57][58] By early December, the Philadelphia consignees had resigned, and in late December the tea ship returned to England with its cargo following a confrontation with the ship's captain.[59] The tea ship bound for New York City was delayed by bad weather; by the time it arrived, the consignees had resigned, and the ship returned to England with the tea.[60]
 In every colony except Massachusetts, protesters were able to force the tea consignees to resign or to return the tea to England.[61] In Boston, however, Governor Hutchinson was determined to hold his ground. He convinced the tea consignees, two of whom were his sons, not to back down.[62]
 When the tea ship Dartmouth[a] arrived in the Boston Harbor in late November, Whig leader Samuel Adams called for a mass meeting to be held at Faneuil Hall on November 29, 1773. Thousands of people arrived, so many that the meeting was moved to the larger Old South Meeting House.[63] British law required Dartmouth to unload and pay the duties within twenty days or customs officials could confiscate the cargo (i.e. unload it onto American soil).[64] The mass meeting passed a resolution, introduced by Adams and based on a similar set of resolutions promulgated earlier in Philadelphia, urging the captain of Dartmouth to send the ship back without paying the import duty. Meanwhile, the meeting assigned twenty-five men to watch the ship and prevent the tea – including a number of chests from Davison, Newman and Co. of London – from being unloaded.[65]
 The colonial governor of Massachusetts, Governor Hutchinson, refused to grant permission for the Dartmouth to leave without paying the duty. Two more tea ships, Eleanor and Beaver, arrived in Boston Harbor. On December 16 – the last day of Dartmouth's deadline – approximately 5,000[66]–7,000[67] people out of an estimated population of 16,000[66] had gathered around the Old South Meeting House. After receiving a report that Governor Hutchinson had again refused to let the ships leave, Adams announced that ""This meeting can do nothing further to save the country."" According to a popular story, Adams's statement was a prearranged signal for the ""tea party"" to begin. However, this claim did not appear in print until nearly a century after the event, in a biography of Adams written by his great-grandson, who apparently misinterpreted the evidence.[68] According to eyewitness accounts, people did not leave the meeting until 10–15 minutes after Adams's alleged ""signal"", and Adams in fact tried to stop people from leaving because the meeting was not yet over.[69]
 While Samuel Adams tried to reassert control of the meeting, people poured out of the Old South Meeting House to prepare to take action. In some cases, this involved donning what may have been elaborately prepared Mohawk costumes.[70] While disguising their individual faces was imperative, because of the illegality of their protest, dressing as Mohawk warriors was a specific and symbolic choice. It showed that the Sons of Liberty identified with America, over their official status as subjects of Great Britain.[71]
 That evening, a group of 30 to 130 men, some dressed in the Mohawk warrior disguises, boarded the three vessels and, over the course of three hours, dumped all 342 chests of tea into the water.[72] The precise location of the Griffin's Wharf site of the Tea Party has been subject to prolonged uncertainty; a comprehensive study[73] places it near the foot of Hutchinson Street (today's Pearl Street).[better source needed] The property damage amounted to the destruction of 92,000 pounds (42,000 kg) or 340 chests of tea, reported by the British East India Company worth £9,659 (equivalent to £1,550,322 in 2023[74]), or roughly $1,700,000 in today's money.[75]
The owner of two of the three ships was William Rotch, a Nantucket-born colonist and merchant.[76]
 Another tea ship intended for Boston, the William, ran aground at Cape Cod in December 1773, and its tea was taxed and sold to private parties. In March 1774, the Sons of Liberty received information that this tea was being held in a warehouse in Boston, entered the warehouse and destroyed all they could find. Some of it had already been sold to Davison, Newman and Co. and was being held in their shop. On March 7, Sons of Liberty once again dressed as Mohawks, broke into the shop, and dumped the last remaining tea into the harbor.[77][78]
 Whether or not Samuel Adams helped plan the Boston Tea Party is disputed, but he immediately worked to publicize and defend it.[79] He argued that the Tea Party was not the act of a lawless mob, but was instead a principled protest and the only remaining option the people had to defend their constitutional rights.[80]
 John Adams, Samuel's second cousin and likewise a Founding Father, wrote in his diary on December 17, 1773, that the Boston Tea Party proved a historical moment in the American Revolution, writing:
 In Great Britain, even those politicians considered friends of the colonies were appalled and this act united all parties there against the colonies. The Prime Minister Lord North said, ""Whatever may be the consequence, we must risk something; if we do not, all is over"".[82] The British government felt this action could not remain unpunished, and responded by closing the port of Boston and putting in place other laws known as the ""Intolerable Acts"". Although the first three, the Boston Port Act, the Massachusetts Government Act and the Administration of Justice Act, applied only to Massachusetts, colonists outside that colony feared that their governments could now also be changed by legislative fiat in England. The Intolerable Acts were viewed as a violation of constitutional rights, natural rights, and colonial charters, and united many colonists throughout America.[83]  Benjamin Franklin stated that the East India Company should be paid for the destroyed tea,[84] all ninety thousand pounds (which, at two shillings per pound, came to £9,000, or £1.44 million [2014, approx. $1.7 million US]).[74] Robert Murray, a New York merchant, went to Lord North with three other merchants and offered to pay for the losses, but the offer was turned down.[85] 
 A number of colonists were inspired by the Boston Tea Party to carry out similar acts, such as the burning of Peggy Stewart. The Boston Tea Party eventually proved to be one of the many reactions that led to the American Revolutionary War.[86] In February 1775, Britain passed the Conciliatory Resolution, which ended taxation for any colony that satisfactorily provided for the imperial defense and the upkeep of imperial officers. The tax on tea was repealed with the Taxation of Colonies Act 1778, part of another Parliamentary attempt at conciliation that failed.[citation needed]
 John Adams[88][failed verification] and many other Americans considered tea drinking to be unpatriotic following the Boston Tea Party.[citation needed] Tea drinking declined during and after the Revolution, resulting in a shift to coffee as the preferred hot drink.[citation needed]
 According to historian Alfred Young, the term ""Boston Tea Party"" did not appear in print until 1834.[89] Before that time, the event was usually referred to as the ""destruction of the tea"". According to Young, American writers were for many years apparently reluctant to celebrate the destruction of property, and so the event was usually ignored in histories of the American Revolution. This began to change in the 1830s, however, especially with the publication of biographies of George Robert Twelves Hewes, one of the few still-living participants of the ""tea party"", as it then became known.[90]
 The Boston Tea Party has often been referenced in other political protests. When Mohandas Gandhi led a mass burning of Indian registration cards in South Africa in 1908, a British newspaper compared the event to the Boston Tea Party.[91] When Gandhi met with the Viceroy of India in 1930 after the Indian salt protest campaign, Gandhi took some duty-free salt from his shawl and said, with a smile, that the salt was ""to remind us of the famous Boston Tea Party.""[92]
 American activists from a variety of political viewpoints have invoked the Tea Party as a symbol of protest. In 1973, on the 200th anniversary of the Tea Party, a mass meeting at Faneuil Hall called for the impeachment of President Richard Nixon and protested oil companies in the ongoing oil crisis. Afterwards, protesters boarded a replica ship in Boston Harbor, hanged Nixon in effigy, and dumped several empty oil drums into the harbor.[93] In 1998, two conservative US Congressmen put the federal tax code into a chest marked ""tea"" and dumped it into the harbor.[94]
 In 2006, a libertarian political party called the ""Boston Tea Party"" was founded. In 2007, the Ron Paul ""Tea Party"" money bomb, held on the 234th anniversary of the Boston Tea Party, broke the one-day fund-raising record by raising $6.04 million in 24 hours.[95] Subsequently, these fund-raising ""Tea parties"" grew into the Tea Party movement, which dominated conservative American politics for the next two years, reaching its peak with a voter victory for the Republicans in 2010 who were widely elected to seats in the United States House of Representatives.[citation needed]
 In 2023, the December 16th 1773 organization hosted a 250th anniversary re-enactment of the Tea Party, putting an original bottle of tea on display.[96][97]
 The Boston Tea Party Museum is located on the Congress Street Bridge in Boston. It features reenactments, a documentary, and a number of interactive exhibits. The museum features two replica ships of the period, Eleanor and Beaver. Additionally, the museum possesses one of two known tea chests from the original event, part of its permanent collection.[98]
 The American Antiquarian Society holds in its collection a vial of actual tea-infused harbor water from 1773.[100]
 The Boston Tea Party has been subject of several films:
 It has been subject of The Boston Tea Party, a 1976 play by Allan Albert, and ""Boston Tea Party"", a 1976 song by the Sensational Alex Harvey Band from SAHB Stories.[101]
 In the 2012 video game Assassin's Creed III, the Boston Tea Party is retold through a main story mission in Sequence 6.
"
Intolerable Acts,https://en.wikipedia.org/wiki/Intolerable_Acts,"

 The Intolerable Acts, sometimes referred to as the Insufferable Acts or Coercive Acts, were a series of five punitive laws passed by the British Parliament in 1774 after the Boston Tea Party. The laws aimed to punish Massachusetts colonists for their defiance in the Tea Party protest of the Tea Act, a tax measure enacted by Parliament in May 1773. In Great Britain, these laws were referred to as the Coercive Acts. They were a key development leading to the outbreak of the American Revolutionary War in April 1775.
 Four acts were enacted by Parliament in early 1774 in direct response to the Boston Tea Party of 16 December 1773: Boston Port, Massachusetts Government, Impartial Administration of Justice, and Quartering Acts.[1] The acts took away self-governance and rights that Massachusetts had enjoyed since its founding, triggering outrage and indignation in the Thirteen Colonies.
 The British Parliament hoped these punitive measures would, by making an example of Massachusetts, reverse the trend of colonial resistance to parliamentary authority that had begun with the Sugar Act 1764. A fifth act, the Quebec Act, enlarged the boundaries of what was then the Province of Quebec notably southwestward into the Ohio Country and other future mid-western states, and instituted reforms generally favorable to the francophone Catholic inhabitants of the region. Although unrelated to the other four Acts, it was passed in the same legislative session and seen by the colonists as one of the Intolerable Acts. The Patriots viewed the acts as an arbitrary violation of the rights of Massachusetts, and in September 1774 they organized the First Continental Congress to coordinate a protest. As tensions escalated, the Revolutionary War broke out in April 1775, leading to the declaration of an independent United States of America in July 1776.
 Relations between the Thirteen Colonies and the British Parliament slowly but steadily worsened after the end of the Seven Years' War (French and Indian War) in 1763. The war had plunged the British government deep into debt, and so the British Parliament enacted a series of measures to increase tax revenue from the colonies. Parliament believed that these acts, such as the Stamp Act 1765 and the Townshend Acts of 1767, were legitimate means of having the colonies pay their fair share of the costs of maintaining the British Empire. Although protests led to the repeal of the Stamp and Townshend Acts, Parliament adhered to the position that it had the right to legislate for the colonies ""in all cases whatsoever"" in the Declaratory Act 1766.
 Many colonists argued that under the unwritten British constitution, a British subject's property could not be taken from him (in the form of taxes) without his consent (in the form of representation in government). Therefore, because the colonies were not directly represented in Parliament, it followed that Parliament had no right to levy taxes upon them, a view expressed by the slogan ""No taxation without representation"". After the Townshend Acts, some colonial essayists took this line of thinking even further, and began to question whether Parliament had any legitimate jurisdiction in the colonies at all.[2] This question of the extent of Parliament's sovereignty in the colonies was the issue underlying what became the American Revolution.
 On 16 December 1773, a group of Patriot colonists associated with the Sons of Liberty destroyed 342 chests of tea in Boston, Massachusetts, an act that came to be known as the Boston Tea Party. The colonists partook in this action because Parliament had passed the Tea Act, which granted the British East India Company a monopoly on tea sales in the colonies, thereby saving the company from bankruptcy. This made British tea less expensive. In addition, there was added a small tax.[citation needed] This angered the colonists. News of the Boston Tea Party reached England in January 1774. Parliament responded by passing four laws. Three of the laws were intended to directly punish Massachusetts. This was for the destruction of private property, to restore British authority in Massachusetts, and to otherwise reform colonial government in America.
 On 22 April 1774, Prime Minister Lord North defended the programme in the House of Commons, saying:
 The Boston Port Act was the first of the laws passed in 1774 in response to the Boston Tea Party. It closed the port of Boston until the colonists paid for the destroyed tea and the king was satisfied that order had been restored. Colonists objected that the Port Act punished all of Boston rather than just the individuals who had destroyed the tea, and that they were being punished without having been given an opportunity to testify in their own defense.[4]
 The Massachusetts Government Act provoked even more outrage than the Port Act because it unilaterally took away Massachusetts' charter and brought it under control of the British government. Under the terms of the Government Act, almost all positions in the colonial government were to be appointed by the governor, Parliament, or the king. The act also severely limited town meetings in Massachusetts to one per year, unless the governor called for one. Colonists outside Massachusetts feared that their governments could now also be changed by the legislative fiat of Parliament.[citation needed]
 The Administration of Justice Act allowed the royal governor to order trials of accused royal officials to take place in Great Britain or elsewhere within the Empire if he decided that the defendant could not get a fair trial in Massachusetts. Although the act stipulated for witnesses to be reimbursed after having traveled at their own expense across the Atlantic, it was not stipulated that this would include reimbursement for lost earnings during the period for which they would be unable to work, leaving few with the ability to testify. George Washington called this the ""Murder Act"" because he believed that it allowed officials to harass colonists and then escape justice.[5] Many colonists believed the act was unnecessary because British soldiers had been given a fair trial following the Boston Massacre in 1770.[citation needed]
 The Quartering Act, which applied to all British colonies in North America, sought to create a more effective method of housing British troops. In a previous act, the colonies had been required to provide housing for soldiers, but colonial legislatures had been uncooperative in doing so. The new Quartering Act allowed a governor to house soldiers in other buildings if suitable quarters were not provided. While many sources claim that the Quartering Act allowed troops to be billeted in occupied private homes, historian David Ammerman's 1974 study claimed that this is a myth, and that the act only permitted troops to be quartered in unoccupied buildings.[6]
 Although unrelated to the aforementioned Acts,[7] the Quebec Act, passed in the same parliamentary session, was considered by the colonists to be one of the Intolerable Acts. The Act expanded the territory of the Province of Quebec into the Great Lakes region and much of what is now the Midwestern United States, which appeared to void the land claims of the Ohio Company on the region. The guarantee of free practice of Catholicism, the majority religion in Canada, was seen by colonists as an ""establishment"" of the faith in the colonies which were overwhelmingly Protestant. Furthermore, colonists resented the lenient provisions granted to their erstwhile enemies whom they had fought hard against during the French and Indian War.[8]
 Many colonists saw the Intolerable Acts as a violation of their constitutional rights, their natural rights, and their colonial charters. They, therefore, viewed the acts as a threat to the liberties of all of British America, not just Massachusetts. Legislation denouncing the act (the Loudoun and Fairfax resolves) was swift, and Richard Henry Lee of Virginia described the acts as ""a most wicked System for destroying the liberty of America"".[9]
 The citizens of Boston viewed the Intolerable Acts as unnecessary and cruel punishment, further inflaming hatred toward Britain. Even more Bostonians turned against British rule.[10]
 Great Britain hoped that the Intolerable Acts would isolate radicals in Massachusetts and cause American colonists to concede the authority of Parliament over their elected assemblies. The calculated risk backfired: the harshness of some of the acts made it difficult for colonial moderates to speak in favor of Parliament.[11] Instead, the acts only served to distance the colonies from the Crown, create sympathy for Massachusetts and encourage colonists from the otherwise diverse colonies to form committees of correspondence which sent delegates to the First Continental Congress. The Continental Congress created the Continental Association, an agreement to boycott British goods. Additionally, it was decided that if the Intolerable Acts were not reversed after a year, goods were to stop being exported to Great Britain as well. The Congress also pledged to support Massachusetts in case of attack, which meant that all of the colonies would become involved when the American Revolutionary War began at Lexington and Concord.[12]
"
George Mason,https://en.wikipedia.org/wiki/George_Mason,"

 George Mason (December 11, 1725 [O.S. November 30, 1725] – October 7, 1792) was an American planter, politician, Founding Father, and delegate to the U.S. Constitutional Convention in Philadelphia in 1787, where he was one of three delegates who refused to sign the Constitution. His writings, including substantial portions of the Fairfax Resolves of 1774, the Virginia Declaration of Rights of 1776, and his Objections to this Constitution of Government (1787) opposing ratification, have exercised a significant influence on American political thought and events. The Virginia Declaration of Rights, which Mason principally authored, served as a basis for the United States Bill of Rights, of which he has been deemed a father.
 Mason was born in 1725 in present-day Fairfax County, Virginia. His father drowned when a storm capsized his boat while crossing the Potomac River in 1735 when Mason was about nine years old. His mother managed the family estates until he came of age. In 1750, Mason married, built Gunston Hall, and lived the life of a country squire, supervising his lands, family, and slaves. He briefly served in the House of Burgesses and involved himself in community affairs, sometimes serving with his neighbor George Washington. As tensions grew between Great Britain and the North American colonies, Mason came to support the colonial side, using his knowledge and experience to help the revolutionary cause, finding ways to work around the Stamp Act 1765 and serving in the pro-independence Fourth Virginia Convention in 1775 and the Fifth Virginia Convention in 1776.
 Mason prepared the first draft of the Virginia Declaration of Rights in 1776, and his words formed much of the text adopted by the final Revolutionary Virginia Convention. He also wrote a constitution for the state; Thomas Jefferson and others sought to have the convention adopt their ideas, but Mason's version was nonetheless adopted. During the American Revolutionary War, Mason was a member of the powerful House of Delegates of the Virginia General Assembly, but to the irritation of Washington and others, he refused to serve in the Second Continental Congress in Philadelphia, citing health and family commitments.
 In 1787, Mason was named one of his state's delegates to the Constitutional Convention in Philadelphia, his only lengthy trip outside Virginia. Many clauses in the Constitution were influenced by Mason's input, but he ultimately did not sign the final version, citing the lack of a bill of rights among his most prominent objections. He also wanted an immediate end to the slave trade and a supermajority requirement for navigation acts, fearing that restrictions on shipping might harm Virginia. He failed to attain these objectives in Philadelphia and later at the Virginia Ratifying Convention of 1788. His prominent fight for a bill of rights led fellow Virginian James Madison to introduce the same during the First Congress in 1789; these amendments were ratified in 1791, a year before Mason died. Obscure after his death, Mason later came to be recognized in the 20th and 21st centuries for his contributions to Virginia and the early United States.
 Mason was born in present-day Fairfax County, in the Colony of Virginia, in British America, on December 11, 1725.[1][2][3] Mason's parents owned property in Mason Neck, Virginia and a second property across the Potomac River in Maryland, which had been inherited by his mother.[4][5] 
 Mason's great-grandfather George Mason I was a Cavalier who was born in 1629 in Pershore, Worcestershire, England. Militarily defeated in the English Civil War, Mason and other Cavaliers emigrated to the American colonies in the 1640s and 1650s.[6] Mason I settled in present-day Stafford County, Virginia,[7] where he was awarded land as a reward for bringing his family and servants to the Colony of Virginia, under headright, which awarded 50 acres for each person transported into the Colony.[8] His son, George Mason II (1660–1716), was the first to move to what in 1742 became Fairfax County, then at the frontier between English and Native American controlled areas. George Mason III (1690–1735) like his father and grandfather served in the House of Burgesses and also as county lieutenant.[7] George Mason IV's mother, Ann Thomson Mason, was the daughter of a former Attorney General of Virginia who immigrated from England.[9]
 Colonial Virginia at the time had few roads, and boats carried most commerce on Chesapeake Bay or its tributaries, including the Potomac and Rappahannock rivers. Most settlement took place near the rivers, through which planters could trade with the world. Colonial Virginia initially had few towns, since estates were largely self-sufficient and could obtain what they needed without the need to purchase locally. Even the capital, Williamsburg, had limited economic activity when the legislature was not in session. Local politics was dominated by large landowners, including the Masons.[10] The Virginia economy rose and fell with tobacco, the main crop, which was raised mostly for export to Great Britain.[11]
 In 1736, Mason began his education with a Mr. Williams, who was hired to teach Mason for the price of 1,000 pounds (450 kg) of tobacco per annum. His studies began at his mother's house. But the following year, he boarded with a Mrs. Simpson in Maryland, and Williams continued as his teacher through 1739. By 1740, Mason was at Chopawamsic, under the tutelage of a Dr. Bridges, likely Charles Bridges, who helped develop the British schools run by the Society for the Promotion of Christian Knowledge, and arrived in British America in 1731. Mason and his brother Thomson likely utilized John Francis Mercer's library, one of the largest in Virginia at the time. Conversations with Mercer and book-lovers who gathered around him could have continued his education informally.[12][13]
 The obligations and offices that came with being one of the largest local landowners descended on Mason as they had previously on his father and grandfather. In 1747, Mason was named to the Court of Fairfax County, Virginia, and was elected as a vestryman at Truro Parish, where he served between 1749 and 1785.[14] He took a position among the officers of the Fairfax County militia, eventually rising to the rank of colonel.[15]
 The county court heard civil and criminal cases, and also decided matters such as local taxes. Membership fell to most major landowners. Mason was a justice for much of the rest of his life, though he was excluded because of nonattendance at court from 1752 to 1764, and he resigned in 1789 when continued service meant swearing to uphold a constitution he could not support.[16] Even while a member, he often did not attend. Joseph Horrell, in a journal article on Mason's court service, noted that he was often in poor health and lived the furthest of any of the major estateholders from the Fairfax County courthouse, whether at its original site near today's Tyson's Corner or later in newly founded Alexandria. Robert Rutland, editor of Mason's papers, considered court service a major influence on Mason's later thinking and writing, but Horrell denied it, ""if the Fairfax court provided a course for Mason's early training, he chiefly distinguished himself by skipping classes.""[17]
 Alexandria was one of the towns founded or given corporate status in the mid-18th century in which Mason had interests; he purchased three of the original lots along King and Royal Streets and became a municipal trustee in 1754. He also served as a trustee of Dumfries, in Prince William County, and had business interests there and in Georgetown, on the Maryland side of the Potomac River in present-day Washington, D.C.[18]
 In 1748, he sought a seat in the House of Burgesses; the process was controlled by more senior members of the court and he was not then successful, but he later emerged victorious in 1758.
 On April 4, 1750, Mason married Ann Eilbeck, only child of William and Sarah Eilbeck of Charles County, Maryland. The Masons and Eilbecks had adjacent lands in Maryland and had joined together in real estate transactions; by his death in 1764, William Eilbeck was one of the wealthiest men in Charles County. At the time of his marriage, Mason was living at Dogue's Neck, possibly at Sycamore Point.[19] George and Ann Mason had nine children who survived to adulthood. Ann Mason died in 1773; their marriage, judging by surviving accounts, was a happy one.[20]
 Mason began to build his home, Gunston Hall, in or around 1755. The exterior, typical of local buildings of that time, was probably based on architectural books sent from Britain to America for the use of local builders; one of these craftsmen, perhaps William Waite or James Wren, constructed Gunston Hall.[21] Mason was proud of the gardens, which still surround the house. There were outbuildings, including slave quarters, a schoolhouse, and kitchens, and beyond them four large plantations, forests, and the shops and other facilities that made Gunston Hall mostly self-sufficient.[22]
 Mason avoided overdependence on tobacco as a source of income by leasing much of his land holdings to tenant farmers,[23] and he diversified his crops to grow wheat for export to the British West Indies as Virginia's economy sank because of tobacco overproduction in the 1760s and 1770s. Mason was a pioneer in the Virginia wine industry. Along with Thomas Jefferson, Mason and other Virginians subscribed to a scheme for growing wine grapes in America developed by Filippo Mazzei.[24]
 Mason greatly ultimately expanded the boundaries of Gunston Hall estate, so that it occupied all of Dogue's Neck, which became known as Mason's Neck.[25] One project that Mason was involved in for most of his adult life was the Ohio Company, in which he invested in 1749 and became treasurer in 1752—an office he held forty years until his death in 1792. The Ohio Company had secured a royal grant for 200,000 acres (81,000 ha) to be surveyed near the forks of the Ohio River in present-day Pittsburgh. The challenges of the French and Indian War, Revolutionary War, and competing claims from the Province of Pennsylvania eventually led to the collapse of the Ohio Company. Although the company failed, Mason acquired considerable western lands independently. His defense against the Pennsylvania claims, Selections from the Virginia Charters, written in 1772, originally intended to promote the Ohio Company's claims, was widely applauded as a defense of the rights of Americans against royal decrees. 
 Involvement with the Ohio Company also brought Mason into contact with many prominent Virginians, including George Washington, his Fairfax County neighbor.[26] Mason and George Washington were friends for many years until they finally broke over their differences regarding the federal constitution. Peter R. Henriques, in a journal article on the relationship between Mason and Washington, suggests that Mason cultivated the friendship more than Washington. Mason  sent many more letters and gifts and stayed more often at Washington's residence at Mount Vernon, though this might be  explained in part by the fact that Mount Vernon was located on the road from Gunston Hall to Alexandria. Henriques suggests that as Mason was older, intellectually superior, and the owner of a flourishing plantation as Washington was struggling to establish Mount Vernon, and it would not have been in the future president's character to seek a close relationship with Mason. Washington, however, had deep respect for Mason's intellectual abilities, and sought Mason's advice on several occasions, writing in 1777 when learning that Mason had taken charge of an issue before the General Assembly, ""I know of no person better qualified ... than Colonel Mason, and shall be very happy to hear he has taken it in hand"".[27]
 Despite his involvement in western real estate, Mason saw that land was being cleared and planted with tobacco faster than the market for it could expand, meaning that its price would drop even as more and more capital was tied up in land and slaves. Thus, although a major slaveholder, he opposed the slave system in Virginia. He believed that slave importation, together with the natural population increase, would result in a huge future slave population in Virginia; a system of leased lands, though not as profitable as slave labor, would have ""little Trouble & Risque [risk]"".[28]
 Little is known of Mason's political views prior to the 1760s, when he came to oppose British colonial policies.[29] In 1758, Mason successfully ran for the House of Burgesses when George William Fairfax, holder of one of Fairfax County's two seats, chose not to seek reelection.[30] Also elected were Mason's brother Thomson for Stafford County, George Washington for Frederick County, where he was stationed as commander of Virginia's militia during the French and Indian War, and Richard Henry Lee, who worked closely with Mason through their careers.[31]
 When the House of Burgesses assembled, Mason was initially appointed to a committee concerned with raising additional militia during that time of war. In 1759, he was appointed to the powerful Committee on Privileges and Elections. He was also placed on the Committee on Propositions and Grievances, which mostly considered local matters. Mason dealt with several local concerns, presenting a petition of Fairfax County planters against being assessed for a tobacco wharf at Alexandria, funds they felt should be raised through wharfage fees. He also played a major role as the Burgesses deliberated how to divide Prince William County as settlement expanded; in March 1759, Fauquier County was created by legislative act. In this, Mason opposed the interest of the family of Thomas, Lord Fairfax,  who wanted existing counties expanded instead, including Fairfax. This difference may have contributed to Mason's decision not to seek re-election in 1761.[32] Mason biographer Jeff Broadwater notes that Mason's committee assignments reflected the esteem his colleagues held him in, or at least the potential they saw.  Broadwater did not find it surprising that Mason did not seek re-election, as he did not attend the sessions between 1759 and 1761.[33]
 Although the British were victorious over the French in the war, King George III's government felt that the North American colonies were not paying their way, since little direct tax revenue from the colonies was received. The Sugar Act 1764 had its greatest effect in New England and did not cause widespread objection. The Stamp Act the following year affected all 13 colonies, as it required revenue stamps to be used on papers required in trade and in the law. When word of passage of the Stamp Act reached Williamsburg, the House of Burgesses passed the Virginia Resolves, asserting that Virginians had the same rights as if they resided in Britain, and that they could only be taxed by themselves or their elected representatives. The Resolves were mostly written by a fiery-spoken new member for Louisa County, Patrick Henry.[34]
 Mason slowly moved from being a peripheral figure towards the center of Virginia politics, but his published response to the Stamp Act, which he opposed, is most notable for the inclusion of his anti-slavery views. George Washington or George William Fairfax, the burgesses for Fairfax County, may have asked Mason's advice as to what steps to take in the crisis.[35] Mason drafted an act to allow for one of the most common court actions, replevin, to take place without the use of stamped paper, and sent it to George Washington, by then one of Fairfax County's burgesses, to gain passage. This action contributed to a boycott of the stamps. With the courts and trade paralyzed, the British Parliament repealed the Stamp Act in 1766 but continued to assert the right to tax the colonies.[34]
 Following the repeal, a committee of London merchants issued a public letter to Americans, warning them not to declare victory. Mason published a response in June 1766, satirizing the British position, ""We have, with infinite Difficulty & Fatigue got you excused this one Time; do what your Papa and Mamma bid, & hasten to return your most grateful Acknowledgements for condescending to let you keep what is your own.""[36] The Townshend Acts of 1767 were Britain's next attempt to tax the colonies, placing duties on substances including lead and glass, which provoked calls from the northern colonies for a boycott of British goods. Virginia, more dependent on goods imported from Britain, was less enthusiastic, and as local planters tended to receive goods at their river landings, a boycott would be difficult to enforce. In April 1769, Washington sent a copy of a Philadelphia resolution to Mason, asking his advice on what action Virginia should take. It is unknown who adapted that text for use in Virginia (Broadwater concludes it was Mason) but Mason sent Washington a corrected draft on April 23, 1769. Washington took it to Williamsburg, but the governor, Lord Botetourt, dissolved the legislature because of the radical resolutions it was passing. The Burgesses adjourned to a nearby tavern and there passed a non-importation agreement based on Mason's draft.[37]
 Although the resolution did not threaten to cut off tobacco, as Mason wanted, he worked in the following years for non-importation. The repeal of most of the Townshend duties made his task more difficult. In March 1773, his wife Ann died of illness contracted after another pregnancy. Mason was the sole parent to nine children, and his commitments made him even more reluctant to accept political office that would require him departing Gunston Hall.[38]
 In May 1774, Mason was in Williamsburg on real estate business. Word had just arrived of the passage of the Intolerable Acts, as Americans dubbed the legislative response to the Boston Tea Party, and a group of lawmakers including Lee, Henry, and Jefferson asked Mason to join them in formulating a course of action. The Burgesses passed a resolution for a day of fasting and prayer to obtain divine intervention against ""destruction of our Civil Rights"", but the governor, Lord Dunmore, dissolved the legislature rather than accept it. Mason may have helped write the resolution and likely joined the members after the dissolution when they met at Raleigh Tavern in Williamsburg.[39][40]
 New elections had to be held for burgess and for delegate to the convention which had been called by the rump of the dissolved House of Burgesses, and Fairfax County's were set for July 5, 1774. George Washington planned to run for one seat and tried to get Mason or Bryan Fairfax to seek the other, but both men declined. Although the poll was postponed to July 14 because of poor weather, Washington met that day with other local leaders (including, likely, Mason) in Alexandria and selected a committee to draft a set of resolutions, which Washington hoped would ""define our Constitutional Rights"".[41] The resulting Fairfax Resolves were largely drafted by Mason. He met with Washington on July 17 at Mount Vernon and stayed the night; the two men rode together to Alexandria the following day. The 24 propositions that made up the Resolves protested loyalty to the British Crown but denied the right of Parliament to legislate for colonies that had been settled at private expense and which had received charters from the monarch. The Resolves called for a continental congress. If Americans did not receive redress by November 1, exports, including that of tobacco, would be cut off. The freeholders of Fairfax County approved the Resolves, appointing Mason and Washington to a special committee in the emergency. According to early Virginia historian Hugh Grigsby, at Alexandria, Mason ""made his first great movement on the theatre of the Revolution"".[42]
 Washington took the Resolves to the Virginia Convention in Williamsburg. Although delegates made some changes, the adopted resolution closely tracked both the Fairfax Resolves and the scheme for non-exportation of tobacco Mason had proposed some years earlier. The convention elected delegates to the First Continental Congress in Philadelphia, including Lee, Washington, and Henry, and in October 1774, Congress adopted a similar embargo.[43]
 Much of Mason's efforts in 1774 and 1775 was in organizing a militia independent of the royal government. Washington by January 1775 was drilling a small force, and he and Mason purchased gunpowder for the company. Mason wrote in favor of annual election of militia officers in words that would later echo in the Virginia Declaration of Rights, ""We came equal into this world, and equals shall we go out of it. All men are by nature born equally free and independent.""[44]
 Washington's election as a delegate to the Second Continental Congress created a vacancy in Fairfax County's delegation to the third Virginia Convention. In May 1775, Washington wrote from Philadelphia, urging that the vacancy be filled. By this time, blood had been shed between American patriot militias and the British Army at the Battles of Lexington and Concord, which launched the American Revolutionary War. Mason attempted to avoid election on the grounds of poor health and his obligations as a single parent to nine children. Nevertheless, he was elected and journeyed to Richmond, which, being further inland than Williamsburg, was deemed better protected from possible British attack.[45]
 When the Richmond convention began in July 1775, Mason was assigned to crucial committees, including one attempting to raise an army to protect the colony. According to Robert A. Rutland, ""Sick or healthy, Mason was needed for his ability.""[46] Mason sponsored a non-exportation measure; it was passed by a large majority, though it had to be repealed later in the session to coordinate with one passed by Maryland. Despite pressure from many delegates, Mason refused to consider election as a delegate to the Second Continental Congress in place of Washington after the Congress elected Washington commanding general of the Continental Army. But Mason could not avoid election to the Committee of Safety, a powerful group that took over many functions in the governmental vacuum. Mason proffered his resignation from this committee, but it was refused.[47]
 Illness forced Mason to abstain himself from the Committee of Safety for several weeks in 1775, and he did not attend the fourth convention, held in December 1775 and January 1776. With independence from Britain widely accepted as necessary among prominent Virginians,[5] the fifth convention, to meet in May 1776 at Williamsburg, would need to decide how Virginia would be administered henceforth, as the royal government was dead in all but name. Accordingly, the convention was seen as so important that Lee arranged for his temporary recall from Congress to be a part of the convention, and Jefferson tried but failed to arrange to leave Congress as well. Other notables elected to the convention were Henry, George Wythe, and a young delegate from Orange County, James Madison.[48] Mason was elected for Fairfax County, though with great difficulty.[49]
 That convention, in May 1776, unanimously instructed Jefferson and other Virginia delegates to Congress to seek ""a clear and full Declaration of Independency"".[51] At the same time, the convention resolved to pass a declaration of rights.[52] Ill health delayed Mason's arrival until May 18, 1776, after the vote, but he was appointed to a committee led by Archibald Cary, which was to compose a declaration of rights and constitution. Mason was skeptical that the thirty-person Cary Committee could collectively compose anything worthwhile, but he was surprised at how quickly it moved—though his membership had a role in that speed. On May 24, convention president Edmund Pendleton wrote to Jefferson about the committee's deliberations, ""as Colo.[nel] Mason seems to have the ascendancy in the great work, I have Sanguine hopes it will be framed so as to Answer it's  [sic] end, Prosperity to the Community and Security to Individuals"".[53]
 Mason, working in a room at the Raleigh Tavern, drafted a declaration of rights and plan of government, likely to prevent frivolous plans with no chance of adoption from being put forward. Edmund Randolph later recalled that Mason's draft ""swallowed up all the rest"".[54] The Virginia Declaration of Rights and the 1776 Constitution of Virginia were joint works, but Mason was the main author. Mason likely worked closely with Thomas Ludwell Lee; the earliest surviving draft shows the first ten articles in Mason's handwriting, with the other two written by Lee. The draft for the Declaration of Rights drew on Magna Carta, the English Petition of Right of 1628, and that nation's 1689 Bill of Rights. Mason's first article would be paraphrased by Jefferson soon after in drafting the American Declaration of Independence.[55]
 From the first article, cataloguing the rights of man, Mason derived the following articles, which make clear that the role of government is to secure and protect those rights, and if it fails to do so, the people have a right to amend or abolish it. Property could not be taken for public use without the owner's consent, and a citizen could only be bound by a law accepted by that person or by elected representatives. If accused, a person had the right to a speedy and local trial, based on an accusation made known to him, with the right to call for evidence and witnesses in his favor.[56]
 When the convention began to debate the declaration, it quickly bogged down on the first sentence of Article 1, which some feared would imply that slaves were their masters' equals. This was resolved by the convention adding the words ""when they enter into a state of society"", thus excluding slaves. Mason spoke repeatedly in the five days of debate, using oratory one hearer described as ""neither flowing nor smooth, but his language was strong, his manner most impressive, and strengthened by a bit of biting cynicism when provocation made it seasonable"".[57] The Declaration of Rights was passed by the convention on June 12, 1776.[58]
 There later was a flurry of contradictory statements from convention members (including Mason) about who composed which articles. Randolph credited Henry with Articles 15 and 16, but Article 16 (dealing with religious freedom), had been written by Madison.[59] Mason had imitated English law in drafting language requiring toleration of those of minority religions, but Madison insisted on full religious liberty, and Mason supported Madison's amendment once made.[58]
 The committee draft, likely for the most part written by Mason, received wide publicity (the final version much less so) and Mason's words ""all men are born equally free and independent"" were later reproduced in state constitutions from Pennsylvania to Montana; Jefferson tweaked the prose and included the sentiments in the Declaration of Independence.[60] In 1778, Mason wrote that the Declaration of Rights ""was closely imitated by the other United States"".[61] This was true, as seven of the original states, and Vermont, joined Virginia in promulgating a bill of rights. Four in addition specified rights that were protected, within the body of their constitutions. Feelings were so strong in Massachusetts that voters there in 1778 rejected a constitution drafted by a convention, insisting that a bill of rights had to come first.[62]
 Even before the convention approved the Declaration of Rights, Mason was busy at work on a constitution for Virginia.[55] He was not the only one occupying himself so; Jefferson sent several versions from Philadelphia, one of which supplied the constitution's preamble. Essex County's Meriwether Smith may have prepared a draft, but the text is unknown. As an original writing in Mason's hand is not known, the extent to which the final draft was written by him is uncertain. On June 22, 1776, however, William Fleming sent Jefferson a copy of the draft before the Cary Committee, telling him ""the inclosed  [sic] printed plan was drawn by Colo. G. Mason and by him laid before the committee"".[63]
 Mason had submitted his plan sometime between June 8 and 10, 1776. It named the new state the ""Commonwealth of Virginia"", a name chosen pointedly by Mason to indicate that power stemmed from the people. The constitution provided for a popularly elected House of Delegates, chosen annually by men who owned or leased property, or who had fathered three or more Virginians. Most governmental power resided in the House of Delegates—the governor could not even veto a bill and could only act as head of the state militia on the advice of his Council of State, whose members were elected by the legislature. The draft was considered by the committee, and it issued a report on June 24, at which time Jefferson's preamble and several amendments authored by him were included—George Wythe, who advocated for Jefferson's draft before the committee, found discussion far enough advanced that members were only willing to yield to Jefferson on a few points. The entire convention considered the document between June 26 and 28, and it was signed on June 29. Richard Henry Lee wrote the day prior to the constitution's passage by unanimous vote, ""I have had the pleasure to see our new plan of Government go on well. This day will put a finishing hand to it. 'Tis very much of the democratic kind.""[64]
 When the convention chose Patrick Henry as Virginia's first post-independence governor, Mason led the committee of notables sent to inform Henry of his election.[65] There was criticism of the constitution—Edmund Randolph later wrote that the document's faults indicated that even such a great mind as Mason's was not immune from ""oversights and negligences"": it did not have an amending process and granted two delegates to each county regardless of population.[66] The 1776 constitution remained in force until 1830, when another convention replaced it.[67] According to Henry C. Riely in his journal article on Mason, ""The Virginia Constitution of 1776, whatever may have been the question raised long afterwards as to the contribution of other great leaders, stands, on the authority of Jefferson, Madison, and Randolph—to mention only the highest authority—as his creation.""[68]
 Mason devoted much effort during the American Revolutionary War to safeguarding Fairfax County and the rivers of Virginia, since the British several times raided areas along the Potomac. Control of the rivers and of Chesapeake Bay was urgent as Virginians tried to obtain hard currency by trading tobacco to the French and other European nations. The export of tobacco, generally via the West Indies, allowed Mason and others to obtain, via France and Holland, British-made items such as cloth, clothing patterns, medicines, and hardware.[69]
 Mason served as a member of the House of Delegates from 1776 to 1781, his longest continuous political service outside Fairfax County, which he represented in Richmond.[70] The other Fairfax County seat turned over several times—Washington's stepson Jackie Custis was elected late in the war—but Mason remained the county's choice throughout. Nevertheless, Mason's health often caused him to miss meetings of the legislature, or to arrive days or weeks late.[71] Mason in 1777 was assigned to a committee to revise Virginia's laws, with the expectation that he would take on the criminal code and land law.  Mason served a few months on the committee before resigning on the ground he was not a lawyer; most of the work fell to Jefferson (returned from Philadelphia), Pendleton, and Wythe. Illness caused by a botched smallpox inoculation forced Mason to miss part of the legislature's spring 1777 session; in his absence delegates on May 22 elected him to the Continental Congress. Mason, who may have been angry that Lee had not been chosen, refused on the ground that he was needed at home and did not feel he could resign from the General Assembly without permission from his constituents. Lee was elected in his place.[72]
 This did not end the desire of Virginians to send Mason to the Continental Congress. In 1779, Lee resigned from Congress, expressing the hope that Mason, Wythe, or Jefferson would replace him in Philadelphia. General Washington was frustrated at the reluctance of many talented men to serve in Congress, writing to Benjamin Harrison that the states ""should compel their ablest men to attend Congress ... Where is Mason, Wythe, Jefferson, Nicholas, Pendleton, Nelson?""[73] Washington wrote to Mason directly:
 In spite of Washington's pleas, Mason remained in Virginia, plagued by illness and heavily occupied on the Committee of Safety and elsewhere in defending the Fairfax County area.  Most of the legislation Mason introduced in the House of Delegates was war related, often aimed at raising the men or money needed by Congress for Washington's Continental Army.[74] The new federal and state governments, short on cash, issued paper money. By 1777, the value of Virginia's paper money had dropped precipitously, and Mason developed a plan to redeem the notes with a tax on real estate. Mason was three weeks late in arriving at Richmond because of his illness, to the frustration of Washington, who had faith in Mason's knowledge of financial affairs. Washington wrote to Custis, ""It is much to be wished that a remedy could be applied to the depreciation of our Currency ... I know of no person better qualified to do this than Colonel Mason"".[75]
 Mason retained his interest in western affairs, hoping in vain to salvage the Ohio Company's land grant. He and Jefferson were among the few delegates to be told of George Rogers Clark's expedition to secure control of the lands north of the Ohio River. Mason and Jefferson secured legislation authorizing Governor Henry to defend against unspecified western enemies. The expedition was generally successful, and Mason received a report directly from Clark.[76] Mason sought to remove differences between Virginia and other states, and although he felt the Mason-Dixon line (not named for George Mason), the 1780 settlement of the boundary dispute with Pennsylvania, was unfavorable to Virginia, he voted for it enthusiastically.[77] Also in 1780, Mason remarried, to Sarah Brent, from a nearby plantation, who had never been married and was 52 years old. It was a marriage of convenience, with the new bride able to take some of the burden of parenting Mason's many children off his hands.[78]
 Following the Treaty of Paris in 1783, which established an sovereign United States and largely ended armed hostilities, life along the Potomac River returned to normal. In December 1783, Madison returned to Gunston Hall after returning from the Second Continental Congress in Philadelphia. In 1781, the  Articles of Confederation had tied the states in a loose bond, and Madison sought a more sound federal structure, seeking the proper balance between federal and state rights. He found Mason willing to consider a federal tax; Madison had feared the subject might offend his host and wrote to Jefferson of the evening's conversation. The same month, Mason spent Christmas at Mount Vernon (the only larger estate than his in Fairfax County). A fellow houseguest described Mason as ""slight in figure, but not tall, and has a grand head and clear gray eyes"".[79][80] Mason retained his political influence in Virginia, writing Patrick Henry, who had been elected to the House of Delegates, a letter filled with advice as that body's 1783 session opened.[81]
 Mason scuttled efforts to elect him to the House of Delegates in 1784, writing that sending him to Richmond would be ""an oppressive and unjust invasion of my personal liberty"".  His refusal disappointed Jefferson, who had hoped that the likelihood that the legislature would consider land legislation would attract Mason to Richmond.[80] The legislature nevertheless appointed Mason a commissioner to negotiate with Maryland over navigation of the Potomac. Mason spent much time on this issue and reached agreement with Maryland delegates at the meeting in March 1785 known as the Mount Vernon Conference. Although the meeting at Washington's home came later to be seen as a first step towards the 1787 Constitutional Convention, Mason saw it simply as efforts by two states to resolve differences between them. Mason was appointed to the Annapolis Convention of 1786, at which representatives of all the states were welcome, but like most delegates he did not attend. The sparsely attended Annapolis meeting called for a conference to consider amendments to the Articles of Confederation.[82][83]
 To deter smuggling, Madison proposed a bill to make Norfolk the state's only legal port of entry. Five other ports, including Alexandria, were eventually added, but the Port Act proved unpopular despite the support of Washington. Mason, an opponent of the act, accepted election to the House of Delegates in 1786, and many believed that his influence would prove decisive for the repeal effort. Mason did not come to Richmond during the initial session because of illness, but he did send a petition as a private citizen to the legislature. The Port Act survived, though additional harbors were added as legal entry points.[84]
 Although the Annapolis Convention saw only about a dozen delegates attend, representing only five states, it called for a meeting to be held in Philadelphia in May 1787 to devise amendments to the Articles of Confederation which would result in a more durable constitutional arrangement. Accordingly in December 1786, the Virginia General Assembly elected seven men as the commonwealth's delegation: Washington, Mason, Henry, Randolph, Madison, Wythe, and John Blair. Henry declined appointment, and his place was given to James McClurg. Randolph, who had just been elected governor, sent three notifications of election to Mason, who accepted without any quibbles. The roads were difficult because of spring flooding, and Mason was the last Virginia delegate to arrive, on May 17, three days after the convention's scheduled opening. But it was not until May 25 that the convention formally opened, with the arrival of at least one delegate from ten of the twelve states which sent representatives (Rhode Island sent no one).[85]
 The journey to Philadelphia was Mason's first beyond Virginia and Maryland.[86] According to Josephine T. Pacheco in her article about Mason's role at Philadelphia, ""since Virginia's leaders regarded [Mason] as a wise, trustworthy man, it is not surprising that they chose him as a member of the Virginia delegation, though they must have been surprised when he accepted"".[87] Broadwater suggests that Mason went to Philadelphia because he knew the federal congress needed additional power and because he felt that body could act as a check on the powers of state legislatures.[88] As the Virginians waited for the other delegates to arrive, they met each day and formulated what became known as the Virginia Plan. They also did some sightseeing and were presented to Pennsylvania's president, Benjamin Franklin. Within a week of arrival, Mason was bored with the social events to which the delegates were invited, ""I begin to grow tired of the etiquette and nonsense so fashionable in this city"".[89]
 Going into the convention, Mason wanted to see a more powerful central government than under the Articles, but not one that would threaten local interests. He feared that the more numerous Northern states would dominate the union and would impose restrictions on trade that would harm Virginia, so he sought a supermajority requirement for navigation acts.[90] As was his constant objective, he sought to preserve the liberty he and other free white men enjoyed in Virginia, guarding against the tyranny he and others had decried under British rule. He also sought a balance of powers, seeking thereby to make a durable government; according to historian Brent Tarter, ""Mason designed his home [Gunston Hall] so that no misplaced window or missing support might spoil the effect or threaten to bring down the roof; he tried to design institutions of government in the same way, so that wicked or unprincipled men could not knock loose any safeguards of liberty"".[91]
 Mason had hope, coming into the convention, that it would yield a result that he felt would strengthen the United States. Impressed by the quality of the delegates, Mason expected sound thinking from them, something he did not think he had often encountered in his political career. Still, he felt that the ""hopes of all the Union centre in this Convention"",[92] and wrote to his son George, ""the revolt from Great Britain & the Formations of our new Government at that time, were nothing compared with the great Business now before us.""[93]
 Mason knew few of the delegates who were not from Virginia or Maryland, but his reputation preceded him. Once delegates representing sufficient states had arrived in Philadelphia by late May, the convention held closed sessions at the Pennsylvania State House (today Independence Hall). Washington was elected the convention's president by unanimous vote, and his tremendous personal prestige as the victorious war general helped legitimize the convention but also caused him to abstain from debate. Mason had no such need to remain silent, and only four or five delegates spoke as frequently as he did. Though he ended up not signing the constitution, according to Broadwater, Mason won as many convention debates as he lost.[95]
 In the early days of the convention, Mason supported much of the Virginia Plan, which was introduced by Randolph on May 29. This plan would have a popularly elected lower house which would choose the members of the upper house from lists provided by the states. Most of the delegates had found the weak government under the Articles insufficient, and Randolph proposed that the new federal government should be supreme over the states.[96] Mason agreed that the federal government should be more powerful than the states.[97]
 The Virginia Plan, if implemented, would base representation in both houses of the federal legislature on population. This was unsatisfactory to the smaller states. Delaware's delegates had been instructed to seek an equal vote for each state, and this became the New Jersey Plan, introduced by that New Jersey Governor William Paterson. The divisions in the convention became apparent in late June, when by a narrow vote, the convention voted that representation in the lower house be based on population, but the motion of Connecticut's Oliver Ellsworth for each state to have an equal vote in the upper house failed on a tie. With the convention deadlocked, on July 2, 1787, a grand committee was formed, with one member from each state, to seek a way out.[98] Mason had not taken as strong a position on the legislature as had Madison, and he was appointed to the committee; Mason and Franklin were the most prominent members. The committee met over the convention's July 4 recess and proposed what became known as the Great Compromise: a House of Representatives based on population, in which money bills must originate, and a Senate with equal representation for each state. Records do not survive of Mason's participation in that committee, but the clause requiring money bills to start in the House most likely came from him or was the price of his support, as he had inserted such a clause in the Virginia Constitution, and he defended that clause once convention debate resumed.[99] According to Madison's notes, Mason urged the convention to adopt the compromise:
 By mid-July, as delegates began to move past the stalemate to a framework built upon the Great Compromise, Mason had considerable influence in the convention. North Carolina's William Blount was unhappy that those from his state ""were in Sentiment with Virginia who seemed to take the lead. Madison at their Head tho Randolph and Mason also great"".[101] Mason had failed to carry his proposals that senators must own property and not be in debt to the United States, but he successfully argued that the minimum age for service in Congress should be 25, telling the convention that younger men were too immature.[102] Mason was the first to propose that the national seat of government not be in a state capital lest the local legislature be too influential. He voted against proposals to base representation on a state's wealth or taxes paid and supported regular reapportionment of the House of Representatives.[103]
 On August 6, 1787, the convention received a tentative draft written by a Committee of Detail chaired by South Carolina's John Rutledge; Randolph had represented Virginia. The draft was acceptable to Mason as a basis for discussion, containing such points important to him as the requirement that money bills originate in the House and not be amendable in the Senate. Nevertheless, Mason felt the upper house was too powerful, as it had the powers to make treaties, appoint Supreme Court justices, and adjudicate territorial disputes between the states. The draft lacked provision for a council of revision, something Mason and others considered a serious lack.[104]
 The convention spent several weeks in August debating the powers of Congress. Although Mason was successful in some of his proposals, such as placing the state militias under federal regulation and a ban on Congress passing an export tax, he lost on some that he deemed crucial. These losses included the convention deciding to allow importation of slaves to continue to at least 1800 (later amended to 1808) and to allow a simple majority to pass navigation acts that might require Virginians to export their tobacco in American-flagged ships, when it might be cheaper to use foreign-flagged vessels. The convention also weakened the requirement that money bills begin in the House and not be subject to amendment in the Senate, eventually striking the latter clause after debate that stretched fitfully over weeks. Despite these defeats, Mason continued to work constructively to build a constitution, serving on another grand committee that considered customs duties and ports.[105]
 On August 31, 1787, Massachusetts's Elbridge Gerry spoke against the document as a whole, as did Luther Martin of Maryland. When Gerry moved to postpone consideration of the final document, Mason seconded him, stating, according to Madison, that ""he would sooner chop off his right hand than put it to the Constitution as it now stands"".[106] Still, Mason did not rule out signing it, saying that he wanted to see how certain matters still before the convention were settled before deciding a final position, whether to sign or ask for a second convention. As the final touches were made to the constitution, Mason and Gerry held meetings in the evening to discuss strategy, bringing in delegates representing states from Connecticut to Georgia.[107]
 Mason's misgivings about the constitution were increased on September 12, when Gerry proposed and Mason seconded that there be a committee appointed to write a bill of rights, to be part of the text of the constitution. Connecticut's Roger Sherman noted that the state bills of rights would remain in force, to which Mason responded, ""the Laws of the United States are to be paramount [supreme] to State Bills of Rights."" Although Massachusetts abstained in deference to Gerry, the Virginians showed no desire to conciliate Mason in their votes, as the motion failed with no states in favor and ten opposed.[109] Also on September 12, the Committee on Style submitted its final draft, and Mason began to list objections on his copy. On September 15, as the convention continued a clause-by-clause consideration of the draft, Mason, Randolph, and Gerry stated they would not sign the constitution.[110]
 On September 17, members of the twelve delegations then present in Philadelphia signed the constitution, except for the three men who had stated they would not. As the document was sent to the Articles of Confederation's Congress in New York, Mason sent a copy of his objections to Richard Henry Lee, a member of the Congress.[111]
 Broadwater notes, ""given the difficulty of the task he had set for himself, his stubborn independence, and his lack, by 1787, of any concern for his own political future, it is not surprising that he left Philadelphia at odds with the great majority of his fellow delegates"".[112] Madison recorded that Mason, believing that the convention had given his proposals short shrift in a hurry to complete its work, began his journey back to Virginia ""in an exceeding ill humor"".[113] Mason biographer Helen Hill Miller notes that before Mason returned to Gunston Hall, he was injured in body as well as spirit after an accident on the road.[114] Word of Mason's opposition stance had reached Fairfax County even before the convention ended; most local sentiment was in favor of the document. Washington made a statement urging ratification but otherwise remained silent, knowing he would almost certainly be the first president. Mason sent Washington a copy of his objections,[115] but Washington believed that the choice was ratification or disaster.[116]
 The constitution was to be ratified by state conventions, with nine approvals necessary for it to come into force. In practice, opposition by large states such as New York or Virginia would make it hard for the new government to function.[117] Mason remained a member of the House of Delegates, and in late October 1787, the legislature called a convention for June 1788; in language crafted by John Marshall, it decreed that the Virginia Ratifying Convention would be allowed ""free and ample discussion"".[118] Mason was less influential in his final session in the House of Delegates because of his strong opposition to ratification, and his age (61) may also have caused him to be less effective.[119]
 As smaller states ratified the constitution in late 1787 and early 1788, there was an immense quantity of pamphlets and other written matter for and against approval. Most prominent in support were the pamphlets later collected as The Federalist, written by Madison and two New Yorkers, Alexander Hamilton and John Jay; Mason's objections were widely cited by opponents.[120] Mason had begun his Objections to this Constitution of Government in Philadelphia; in October 1787, it was published, though without his permission. Madison complained that Mason had gone beyond the reasons for opposing he had stated in convention, but Broadwater suggested the major difference was one of tone, since the written work dismissed as useless the constitution and the proposed federal government. Nevertheless, both Lee and Mason believed that if proper amendments were made, the constitution would be a fine instrument of governance.[121] The Objections were widely cited in opposition to ratification,[120] and Mason was criticized for placing his own name on it, at a time when political tracts were signed, if at all, with pen names such as Junius, so that the author's reputation would not influence the debate. Despite this, Mason's Objections were among the most influential Anti-Federalist works, and its opening line, ""There is no Declaration of Rights"", likely their most effective slogan.[122]
 Virginians were reluctant to believe that greatly respected figures such as Washington and Franklin would be complicit in setting up a tyrannical system.[123] There were broad attacks on Mason; the New Haven Gazette suggested that he had not done much for his country during the war, in marked contrast to Washington.[120] Oliver Ellsworth blamed the Virginia opposition on the Lee family, who had long had tensions with the Washington family, and on ""the madness of Mason"".[124] Tarter, in his American National Biography article on Mason, wrote that ""the rigidity of [Mason's] views and his increasingly belligerent personality produced an intolerance and intemperance in his behavior that surprised and angered Madison, with whom he had worked closely at the beginning of the convention, and Washington, who privately condemned Mason's actions during the ratification struggle.""[5]
 In Richmond from October 1787 until January 1788 representing Fairfax County in the House of Delegates, Mason faced difficulties in winning election to the ratifying convention from his home county, since most Fairfax freeholders were Federalist, and local courthouse politics put him at odds with many in Alexandria. The statute governing elections to the convention in Richmond allowed Mason to seek election where he owned property, and he sought election in Stafford County. He assured voters that he did not seek disunion, but rather reform, and spoke against the unamended constitution in strong terms. George Nicholas, a Federalist friend of Mason, believed that Mason felt he could lead Virginia to gain concessions from the other states, and that he was embittered by the continuing attacks on him.[125] On March 10, 1788, Mason finished first in the polls in Stafford County, and veteran delegate Andrew Buchanan won the other seat[126] over Colonel Carter and William Fitzhugh. Two days later, a Richmond essayist criticized Mason and Richard Henry Lee (who did not attend) for the ""barefaced impudence and folly"" of public protests.[127] Mason apparently was the only person elected to that convention for a constituency in which he did not live (although historically many of his ancestors had lived in and represented the county). Voter turnout was low, as many in remote areas without newspapers knew little about the constitution. The Federalists were believed to have a slight advantage in elected delegates; Mason thought that the convention would be unlikely to ratify the document without demanding amendments.[125]
 By the time the Richmond convention opened in June, Randolph had abandoned the Anti-Federalist cause, which damaged efforts by Mason and Henry to co-ordinate with their counterparts in New York. Mason moved that the convention consider the document clause by clause, which may have played into the hands of the Federalists, who feared what the outcome of an immediate vote might be,[128] and who had more able leadership in Richmond, including Marshall and Madison. Nevertheless, Broadwater suggested that as most delegates had declared their views before the election, Mason's motion made little difference. Henry, far more a foe of a strong federal government than was Mason, took the lead for his side in the debate. Mason spoke several times in the discussion, on topics ranging from the pardon power (which he predicted the president would use corruptly) to the federal judiciary, which he warned would lead to suits in the federal courts by citizens against states where they did not live. John Marshall, a future Chief Justice of the United States, downplayed the concern regarding the judiciary, but Mason would later be proved correct in the case of Chisholm v. Georgia (1793), which led to the passage of the Eleventh Amendment.[129]
 The federalists initially did not have a majority, with the balance held by undeclared delegates, mainly from western Virginia (today's Kentucky). The Anti-Federalists suffered repeated blows during the convention due to the defection of Randolph and as news came other states had ratified. Mason led a group of Anti-Federalists which drafted amendments: even the Federalists were open to supporting them, though the constitution's supporters wanted the document drafted in Philadelphia ratified first.[130]
 After some of the Kentuckians had declared for ratification, the convention considered a resolution to withhold ratification pending the approval of a declaration of rights.[130][131]  Supported by Mason but opposed by Madison, Light-Horse Harry Lee, Marshall, Nicholas, Randolph and Bushrod Washington, the resolution failed, 88–80.[131] Mason then voted in the minority as Virginia ratified the constitution on June 25, 1788 by a vote of 89–79.[131] Following the ratification vote, Mason served on a committee chaired by George Wythe, charged with compiling a final list of recommended amendments, and Mason's draft was adopted, but for a few editorial changes. Unreconciled to the result, Mason prepared a fiery written argument, but some felt the tone too harsh and Mason agreed not to publish it.[130]
 Defeated at Richmond, Mason returned to Gunston Hall, where he devoted himself to family, local affairs and his own health, though still keeping up a vigorous correspondence with political leaders. He resigned from the Fairfax County Court after an act passed by the new Congress required officeholders to take an oath to support the constitution, and in 1790 declined a seat in the Senate which had been left vacant by William Grayson's death, stating that his health would not permit him to serve, even if he had no other objection.[132] The seat went to James Monroe, who had supported Mason's Anti-Federalist stance, and who had, in 1789, lost to Madison for a seat in the House of Representatives. Judging by his correspondence, Mason softened his stance towards the new federal government, telling Monroe that the constitution ""wisely & Properly directs"" that ambassadors be confirmed by the Senate.[133] Although Mason predicted that the amendments to be proposed to the states by the First Congress would be ""Milk & Water Propositions"", he displayed ""much Satisfaction"" at what became the Bill of Rights (ratified in 1791) and wrote that if his concerns about the federal courts and other matters were addressed, ""I could cheerfully put my Hand & Heart to the new Government"".[134]
 Washington, who was in 1789 elected the first president, resented Mason's strong stances against the ratification of the constitution, and these differences destroyed their friendship. Although some sources accept that Mason dined at Mount Vernon on November 2, 1788, Peter R. Henriques noted that Washington's diary states that Mr. George Mason was the guest, and as Washington, elsewhere in his diary, always referred to his former colleague at Philadelphia as Colonel Mason, the visitor was likely George Mason V, the son. Mason always wrote positively of Washington, and the president said nothing publicly, but in a letter referred to Mason as a ""quondam friend"" who would not recant his position on the constitution because ""pride on the one hand, and want of manly candour on the other, will not I am certain let him acknowledge error in his opinions respecting it [the federal government] though conviction should flash on his mind as strongly as a ray of light"".[136] Rutland suggested that the two men were alike in their intolerance of opponents and suspicion of their motives.[137]
 Mason had long battled against Alexandria merchants who he felt unfairly dominated the county court, if only because they could more easily get to the courthouse. In 1789, he drafted legislation to move the courthouse to the center of the county, though it did not pass in his lifetime. In 1798, the legislature passed an authorizing act, and the courthouse opened in 1801.[a][134] Most of those at Gunston Hall, both family and slaves, fell ill during the summer of 1792, experiencing chills and fever; when those subsided, Mason caught a chest cold.[138] When Jefferson visited Gunston Hall on October 1, 1792, he found Mason, long a martyr to gout, needing a crutch to walk, though still sound in mind and memory. Additional ailments, possibly pneumonia, set in. Less than a week after Jefferson's visit, on October 7, George Mason died at Gunston Hall, and was subsequently buried on the estate, within sight of the house he had built and of the Potomac River.[139][140]
 Although Mason's death attracted little notice, aside from a few mentions in local newspapers, Jefferson mourned ""a great loss"".[141] Another future president, Monroe, stated that Mason's ""patriotic virtues thro[ugh] the revolution will ever be remembered by the citizens of this country"".[141]
 Mason routinely spoke out against slavery, even before America's independence. In 1773, he wrote that slavery was ""that slow Poison, which is daily contaminating the Minds & Morals of our People. Every Gentlemen here is born a petty Tyrant.""  In 1774, he advocated ending the international slave trade.[142]
 Like nearly all Virginia land owners at the time, Mason owned many slaves. In Fairfax County, only George Washington owned more, and Mason is not known to have freed any, even in his March 1773 will[143] ultimately transcribed into the Fairfax County probate records in October 1792 (the original was then lost). That will divided his slaves among his children (between twenty and three years old when Mason wrote it). Mason would continue trading in land as well as remarry (with a marriage agreement recorded in Prince William County in April 1780),[144] and Virginia legalized manumission in May 1782.[145] The childless Washington, in his will executed shortly before his death, ordered his slaves be freed after his wife's death, and Jefferson manumitted a few slaves, mostly of the Hemings family, including his own children by Sally Hemings.[146] According to Broadwater, ""In all likelihood, Mason believed, or convinced himself, that he had no options.  Mason would have done nothing that might have compromised the financial futures of his nine children.""[147] Peter Wallenstein, in his article about how writers have interpreted Mason, argued that he could have freed some slaves without harming his children's future, if he had wanted to.[148]
 Mason's biographers and interpreters have long differed about how to present his views on slavery-related issues.[149] His descendant Kate Mason Rowland, a charter member of the United Daughters of the Confederacy published a two-volume biography in 1892.[150]  Broadwater noted  that she wrote  ""during the heyday of Jim Crow"" and denied that Mason (her ancestor) was ""an abolitionist in the modern sense of the term"",[147] arguing that Mason ""regretted"" slavery and was against the slave trade, but wanted slavery protected in the constitution.[151] In 1919, Robert C. Mason published a biography of his prominent ancestor and asserted that George Mason ""agreed to free his own slaves and was the first known abolitionist"", refusing to sign the constitution, among other reasons because ""as it stood then it did not abolish slavery or make preparation for its gradual extinction"".[152] Rutland, writing in 1961, asserted that in Mason's final days, ""only the coalition [between New England and the Deep South at the Constitutional Convention] in Philadelphia that had bargained away any hope of eliminating slavery left a residue of disgust.""[153] Catherine Drinker Bowen, in her widely read 1966 account of the Constitutional Convention, Miracle at Philadelphia, contended that Mason believed slaves to be citizens and was ""a fervent abolitionist before the word was coined"".[148]
 Others took a more nuanced view. Pamela C. Copeland and Richard K. MacMaster deemed Mason's views similar to other Virginians of his class: ""Mason's experience with slave labor made him hate slavery but his heavy investment in slave property made it difficult for him to divest himself of a system that he despised"".[154] According to Wallenstein, ""whatever his occasional rhetoric, George Mason was—if one must choose—proslavery, not antislavery. He acted in behalf of Virginia slaveholders, not Virginia slaves"".[148] Broadwater noted, ""Mason consistently voiced his disapproval of slavery. His 1787 attack on slavery echoes a similar speech to the Virginia Convention of 1776.  His conduct was another matter.""[147]
 According to Wallenstein, historians and other writers ""have had great difficulty coming to grips with Mason in his historical context, and they have jumbled the story in related ways, misleading each other and following each other's errors"".[155] Some of this is due to conflation of Mason's views on slavery with that of his desire to ban the African slave trade, which he unquestionably opposed and fought against. His record otherwise is mixed: Virginia banned the importation of slaves from abroad in 1778, while Mason was in the House of Delegates. In 1782, after he had returned to Gunston Hall, it enacted legislation that allowed manumission of adult slaves young enough to support themselves (not older than 45). However, a proposal, supported by Mason, to require freed slaves to leave Virginia within a year or be sold at auction, was defeated.[156] Broadwater asserted, ""Mason must have shared the fears of Jefferson and countless other whites that whites and free blacks could not live together"".[147]
 The contradiction between wanting protection for slave property, while opposing the slave trade, was pointed out by delegates to the Richmond convention such as George Nicholas, a supporter of ratification.[157] Mason stated of slavery, ""it is far from being a desirable property. But it will involve us in great difficulties and infelicity to be now deprived of them.""[158]
 There are sites remembering George Mason in Fairfax County. Gunston Hall, donated to the Commonwealth of Virginia by its last private owner, is now ""dedicated to the study of George Mason, his home and garden, and life in 18th-century Virginia"".[159] George Mason University, with its main campus adjacent to the city of Fairfax, was formerly George Mason College of the University of Virginia from 1959[160] until it received its present name in 1972.[161] A major landmark on the Fairfax campus is a statue of George Mason by Wendy M. Ross, depicted as he presents his first draft of the Virginia Declaration of Rights.[162]
 The George Mason Memorial Bridge, part of the 14th Street Bridge, connects Northern Virginia to Washington, D.C.[163] The George Mason Memorial in West Potomac Park in Washington, also with a statue by Ross, was dedicated on April 9, 2002.[164]
 Mason was honored in 1981 by the United States Postal Service with an 18-cent Great Americans series postage stamp.[165] A bas-relief of Mason appears in the Chamber of the U.S. House of Representatives as one of 23 honoring great lawmakers. Mason's image is located above and to the right of the Speaker's chair; he and Jefferson are the only Americans recognized.[166]
 According to Miller, ""The succession of New World constitutions of which Virginia's, with Mason as its chief architect, was the first, declared the source of political authority to be the people ... in addition to making clear what a government was entitled to do, most of them were prefaced by a list of individual rights of the citizens ... rights whose maintenance was government's primary reason for being. Mason wrote the first of these lists.""[167] Diane D. Pikcunas, in her article prepared for the bicentennial of the U.S. Bill of Rights, wrote that Mason ""made the declaration of rights as his personal crusade"".[168] Tarter deemed Mason ""celebrated as a champion of constitutional order and one of the fathers of the Bill of Rights"".[169] Supreme Court associate justice Sandra Day O'Connor agreed, ""George Mason's greatest contribution to present day Constitutional law was his influence on our Bill of Rights"".[170]
 Mason's legacy extended overseas, doing so even in his lifetime, and though he never visited Europe, his ideals did. Lafayette's
""Declaration of the Rights of Man and of the Citizen"" was written in the early days of the French Revolution under the influence of Jefferson, the U.S. Minister to France. According to historian R.R. Palmer, ""there was in fact a remarkable parallel between the French Declaration and the Virginia Declaration of 1776"".[171] Another scholar, Richard Morris, concurred, deeming the resemblance between the two texts ""too close to be coincidental"": ""the Virginia statesman George Mason might well have instituted an action of plagiarism"".[172]
 Donald J. Senese, in the conclusion to the collection of essays on Mason published in 1989, noted that several factors contributed to Mason's obscurity in the century after his death. Mason was older than many of those who served in Philadelphia and subsequently came into prominence with the new federal government. Mason died soon after the constitution came into force and displayed no ambition for federal office, declining a seat in the Senate. Mason left no extensive paper trail and, unlike Franklin, who authored an autobiography, left no diary like Washington or John Adams. Washington left papers collected into 100 volumes; for Mason, many of whose documents lost to fire, there were only three that endured. Mason fought on the side that failed, both at Philadelphia and Richmond, leaving him a loser in a history written by winners, even his speeches to the Constitutional Convention descend through the pen of Madison, a supporter of ratification. After the Richmond convention, he was, according to Senese, ""a prophet without honor in his own country"".[173]
 The increased scrutiny of Mason which has accompanied his rise from obscurity has meant, according to Tarter, that ""his role in the creation of some of the most important texts of American liberty is not as clear as it seems"".[174] Rutland suggested that Mason showed only ""belated concern over the personal rights of citizens"".[175] Focusing on Mason's dissent from the constitution, Miller pointed to the intersectional bargain struck over navigation acts and the slave trade, ""Mason lost on both counts, and the double defeat was reflected in his attitude thereafter.""[176] Wallenstein concluded, ""the personal and economic interests of Mason's home state took precedence over a bill of rights"".[175]
 Whatever his motivations, Mason proved a forceful advocate for a bill of rights whose Objections helped accomplish his aims. Rutland noted that ""from the opening phrase of his Objections to the Bill of Rights that James Madison offered in Congress two years later, the line is so direct that we can say that Mason forced Madison's hand. Federalist supporters of the Constitution could not overcome the protest caused by Mason's phrase 'There is no declaration of rights'.""[177] O'Connor wrote that ""Mason lost his battle against ratification ... [but] his ideals and political activities have significantly influenced our constitutional jurisprudence.""[178] Wallenstein felt that there is much to be learned from Mason:
"
Fairfax Resolves,https://en.wikipedia.org/wiki/Fairfax_Resolves,"The Fairfax Resolves were a set of resolutions adopted by a committee in Fairfax County in the Colony of Virginia on July 18, 1774, in the early stages of the American Revolution. Written at the behest of George Washington and others, they were authored primarily by George Mason. The resolutions rejected the British Parliament's claim of supreme authority over the American colonies. More than thirty counties in Virginia passed similar resolutions in 1774, ""but the Fairfax Resolves were the most detailed, the most influential, and the most radical.""[1]
 After Parliament passed the Coercive Acts, also known as the Intolerable Acts, to punish Massachusetts for the Boston Tea Party, the Virginia House of Burgesses proclaimed that June 1, 1774, would be a day of ""fasting, humiliation, and prayer"" as a show of solidarity with Boston. In response, Lord Dunmore, the royal governor of Virginia, dissolved the House of Burgesses. The burgesses reconvened at the Raleigh Tavern on May 27 and called for Virginia's counties to elect delegates to a special convention to meet in August. George Washington and Charles Broadwater were elected as Fairfax County's representatives to the convention.
 On July 5, 1774, Washington and others from Fairfax County met in Alexandria, Virginia, to appoint a committee to draft a statement that would, as Washington described it, ""define our Constitutional Rights.""[2] The statement would also formally serve as instructions to Fairfax County's delegates to the Virginia Convention.[3] The committee wrote a draft that was, in all likelihood, primarily the work of George Mason. Mason and Washington met at Washington's Mount Vernon home on July 17, and perhaps revised the resolutions. The following day in Alexandria, the Fairfax Resolves were endorsed in a meeting of freeholders chaired by Washington.[4]
 In the Resolves, the freeholders expressed a desire to remain subjects of the British Empire, but they insisted that ""we will use every means which Heaven hath given us to prevent our becoming its slaves.""
 The short document provided the following:
 The Resolves directed Washington and Broadwater to present the resolutions to the Virginia Convention.
 The Fairfax Resolves, like the many other similar resolutions passed in county meetings throughout the colonies, summarized the feelings of many colonists in mid-1774 — a conviction that their constitutional rights were being violated by British policies. The Resolves also marked a step forward in inter-colonial cooperation as more Americans began to realize that a threat against one colony was a threat against all. Finally, political rivalries in Virginia were muted to some degree, allowing such figures as Washington and Mason to work productively with the more radical Patrick Henry, Richard Henry Lee and others.
 The non-importation protest called for in the Resolves recalled, with some modifications, the Virginia Association, which in turn provided the pattern for the Continental Association which was passed by the First Continental Congress on October 20, 1774.[5]
"
Atlantic slave trade,https://en.wikipedia.org/wiki/Atlantic_slave_trade,"
 The Atlantic slave trade or transatlantic slave trade involved the transportation by slave traders of enslaved African people to the Americas. European slave ships regularly used the triangular trade route and its Middle Passage. Europeans established a coastal slave trade in the 15th century and trade to the Americas began in the 16th century, lasting through the 19th century.[1] The vast majority of those who were transported in the transatlantic slave trade were from Central Africa and West Africa and had been sold by West African slave traders to European slave traders,[2][3] while others had been captured directly by the slave traders in coastal raids.[4][5] European slave traders gathered and imprisoned the enslaved at forts on the African coast and then brought them to the Americas.[6][7] Some Portuguese and Europeans participated in slave raids. As the National Museums Liverpool explains: ""European traders captured some Africans in raids along the coast, but bought most of them from local African or African-European dealers.""[8] Many European slave traders generally did not participate in slave raids because life expectancy for Europeans in sub-Saharan Africa was less than one year during the period of the slave trade because of malaria that was endemic in the African continent.[9] Portuguese coastal raiders found that slave raiding was too costly and often ineffective and opted for established commercial relations.[10]
 The colonial South Atlantic and Caribbean economies were particularly dependent on slave labour for the production of sugarcane and other commodities.[11][12] This was viewed as crucial by those Western European states which were vying with one another to create overseas empires.[13][14] The Portuguese, in the 16th century, were the first to transport slaves across the Atlantic. In 1526, they completed the first transatlantic slave voyage to Brazil, and other Europeans soon followed.[15] Shipowners regarded the slaves as cargo to be transported to the Americas as quickly and cheaply as possible,[13] there to be sold to work on coffee, tobacco, cocoa, sugar, and cotton plantations, gold and silver mines, rice fields, the construction industry, cutting timber for ships, as skilled labour, and as domestic servants.[16] The first enslaved Africans sent to the English colonies were classified as indentured servants, with legal standing similar to that of contract-based workers coming from Britain and Ireland. However, by the middle of the 17th century, slavery had hardened as a racial caste, with African slaves and their future offspring being legally the property of their owners, as children born to slave mothers were also slaves (partus sequitur ventrem). As property, the people were considered merchandise or units of labour, and were sold at markets with other goods and services.[17]
 The major Atlantic slave trading nations, in order of trade volume, were Portugal, Britain, Spain, France, the Netherlands, the United States, and Denmark. Several had established outposts on the African coast, where they purchased slaves from local African leaders.[18] These slaves were managed by a factor, who was established on or near the coast to expedite the shipping of slaves to the New World. Slaves were imprisoned in trading posts known as factories while awaiting shipment. Current estimates are that about 12 million to 12.8 million Africans were shipped across the Atlantic over a span of 400 years.[19][20] The number purchased by the traders was considerably higher, as the passage had a high death rate, with between 1.2 and 2.4 million dying during the voyage, and millions more in seasoning camps in the Caribbean after arrival in the New World. Millions of people also died as a result of slave raids, wars, and during transport to the coast for sale to European slave traders.[21][22][23][24] Near the beginning of the 19th century, various governments acted to ban the trade, although illegal smuggling still occurred. It was generally thought that the transatlantic slave trade ended in 1867, but evidence was later found of voyages until 1873.[25] In the early 21st century, several governments issued apologies for the transatlantic slave trade.
 The Atlantic slave trade developed after trade contacts were established between the ""Old World"" (Afro-Eurasia) and the ""New World"" (the Americas). For centuries, tidal currents had made ocean travel particularly difficult and risky for the ships that were then available. Thus, there had been very little, if any, maritime contact between the peoples living in these continents.[26] In the 15th century, however, new European developments in seafaring technologies, such as the invention of the caravel, resulted in ships being better equipped to deal with the tidal currents, and could begin traversing the Atlantic Ocean; the Portuguese set up a Navigator's School (although there is much debate about whether it existed and if it did, just what it was). Between 1600 and 1800, approximately 300,000 sailors engaged in the slave trade visited West Africa.[27] In doing so, they came into contact with societies living along the west African coast and in the Americas which they had never previously encountered.[28] Historian Pierre Chaunu termed the consequences of European navigation ""disenclavement"", with it marking an end of isolation for some societies and an increase in inter-societal contact for most others.[29][30]
 Historian John Thornton noted, ""A number of technical and geographical factors combined to make Europeans the most likely people to explore the Atlantic and develop its commerce"".[31] He identified these as being the drive to find new and profitable commercial opportunities outside Europe. Additionally, there was the desire to create an alternative trade network to that controlled by the Muslim Ottoman Empire of the Middle East, which was viewed as a commercial, political and religious threat to European Christendom. In particular, European traders wanted to trade for gold, which could be found in western Africa, and to find a maritime route to ""the Indies"" (India), where they could trade for luxury goods such as spices without having to obtain these items from Middle Eastern Islamic traders.[32]
 During the first wave of European colonization, although many of the initial Atlantic naval explorations were led by the Iberian conquistadors, members of many European nationalities were involved, including sailors from Spain, Portugal, France, England, the Italian states, and the Netherlands. This diversity led Thornton to describe the initial ""exploration of the Atlantic"" as ""a truly international exercise, even if many of the dramatic discoveries were made under the sponsorship of the Iberian monarchs"". That leadership later gave rise to the myth that ""the Iberians were the sole leaders of the exploration"".[34]
 European overseas expansion led to the contact between the Old and New Worlds producing the Columbian exchange, named after the Italian explorer Christopher Columbus.[35] It started the global silver trade from the 16th to 18th centuries and led to direct European involvement in the Chinese porcelain trade. It involved the transfer of goods unique to one hemisphere to another. Europeans brought cattle, horses, and sheep to the New World, and from the New World Europeans received tobacco, potatoes, tomatoes, and maize. Other items and commodities becoming important in global trade were the tobacco, sugarcane, and cotton crops of the Americas, along with the gold and silver brought from the American continent not only to Europe but elsewhere in the Old World.[36][37][38][39]
 By the 15th century, slavery had existed in the Iberian Peninsula (Portugal and Spain) of Western Europe throughout recorded history. The Roman Empire had established its system of slavery in ancient times. Historian Benjamin Isaac suggests proto-racism existed in ancient times among Greco-Roman people. Racial prejudices were based on dehumanizing the foreign peoples they conquered through warfare.[40][41][42] Since the fall of the Western Roman Empire, various systems of slavery continued in the successor Islamic and Christian kingdoms of the peninsula through the early modern era of the Atlantic slave trade.[43][44] In 1441–1444, Portuguese traders first captured Africans on the Atlantic coast of Africa (in what is today Mauritania), taking their captives to slavery in Europe, and established a fort for the slave trade at the Bay of Arguin.[45]
 In the Middle Ages, religion and not race was a determining factor for who was considered to be a legitimate target of slavery. While Christians did not enslave Christians and Muslims did not enslave Muslims, both allowed the enslavement of people they regarded to be heretics or insufficiently correct in their religion, which allowed Catholic Christians to enslave Orthodox Christians, and Sunni Muslims to enslave Shia Muslims;[46] similarly both Christians and Muslims approved of enslaving pagans, who came to be a preferred and comparatively profitable target of the slave trade in the Middle Ages:[46] Spain and Portugal were provided with non-Catholic slaves from Eastern Europe via the Balkan slave trade and the Black Sea slave trade.[47]
 In the 15th century, when the Balkan slave trade was taken over by the Ottoman Empire[48] and the Black Sea slave trade was supplanted by the Crimean slave trade and closed off from Europe, Spain and Portugal replaced this source of slaves by importing slaves first from the conquered Canary Islands and then from mainland Africa, initially from Arab slave traders via the Trans-Saharan slave trade from Libya, and then directly from the African West coast through Portuguese outposts, which developed into the Atlantic slave trade[49] and expanded significantly after the establishment of the colonies in the Americas in 1492.[50]
 In the 15th century, Spain enacted a racially discriminatory law named limpieza de sangre, which translates as ""blood purity"" or ""cleanliness of blood"", a proto-racial law. It prevented people with Jewish and Muslim ancestry from settling in the New World. Limpieza de sangre did not guarantee rights for Jews or Muslims who converted to Catholicism. Jews and Muslims who converted to Catholicism were respectively called conversos and moriscos. Some Jews and Muslims converted to Christianity hoping it would grant them rights under Spanish laws. After the ""discovery"" of new lands across the Atlantic, Spain did not want Jews and Muslims immigrating to the Americas because the Spanish Crown worried Muslims and non-Christians might introduce Islam and other religions to Native Americans.[51] The law also led to the enslavement of Jews and Muslims, prevented Jews from entering the country and from joining the military, universities and other civil services.[52][53][54][55][56] Although Jewish conversos and Muslims experienced religious and racial discrimination, some also participated in the slave trade of Africans. In Lisbon during the 16th and 17th centuries, Muslims financed by Jewish conversos traded Africans across the Sahara Desert and enslaved Africans before and during the Atlantic slave trade in Europe and Africa.[57] In New Spain, Spaniards applied limpieza de sangre to Africans and Native Americans and created a racial caste system, believing them to be impure because they were not Christian.[58][59][60]
 Europeans enslaved Muslims and people practicing other religions as a justification to Christianize them. In 1452, Pope Nicholas V issued papal bull Dum Diversas which gave the King of Portugal the right to enslave non-Christians to perpetual slavery. The clause included Muslims in West Africa and legitimized the slave trade under the Catholic church. In 1454, Pope Nicholas issued Romanus Pontifex. ""Written as a logical sequel to Dum Diversas, Romanus Pontifex allowed the European Catholic nations to expand their dominion over 'discovered' land. Possession of non-Christian lands would be justified along with the enslavement of native, non-Christian 'pagans' in Africa and the 'New World.'""[61][62][63] Dum Diversas and Romanus Pontifex may have had an influence with the creation of doctrines supportive of empire building.[64]
 In 1493, the Doctrine of Discovery issued by Pope Alexander VI, was used as a justification by Spain to take lands from non-Christians West of the Azores. The Doctrine of Discovery stated that non-Christian lands should be taken and ruled by Christian nations, and Indigenous people (Africans and Native Americans) living on their lands should convert to Christianity.[65][66] In 1493, Pope Alexander VI issued a papal bull called Inter Caetera which gave Spain and Portugal rights to claim and colonize all non-Christian lands in the Americas and enslave Native Americans and Africans.[67] Inter Caetera also settled a dispute between Portugal and Spain over those lands. The declaration included a north–south divide 100 leagues West of the Cape Verde Islands and gave the Spanish Crown exclusive rights to travel and trade west of that line.[68][69]
 In Portugal and Spain people had been enslaved because of their religious identity, race had not been a developed factor for enslaving people; nonetheless, by the 15th century, Europeans used both race and religion as a justification to enslave sub-Saharan Africans. An increase of enslaved African people from Senegal occurred in the Iberian Peninsula in the 15th century. As the number of Senegalese slaves grew larger Europeans developed new terminologies that associated slavery with skin color. The Spanish city of Seville had the largest African population. ""The Treaty of Alcacuvas in 1479 provided traders the right to supply Spaniards with Africans.""[71]
 In addition, in the 15th century, Dominican friar Annius of Viterbo invoked the curse of Ham, from the biblical story of enslavement, to explain the differences between Europeans and Africans in his writings. Annius, who frequently wrote of the ""superiority of Christians over the Saracens"", claimed that due to the curse imposed upon Black people, they would inevitably remain permanently subjugated by Arabs and other Muslims. He wrote that the fact that so many Africans had been enslaved even by the heretical Muslims was supposed proof of their inferiority. Through these and other writings, European writers established a hitherto unheard of connection between a cursed people, Africa and slavery, which laid the ideological groundwork for justifying the transatlantic slave trade.[72][73] The term ""race"" was used by the English beginning in the 16th century and referred to family, lineage, and breed. The idea of race continued to develop further through the centuries and was used as a justification for the continuation of the slave trade and racial discrimination.[74][75][76][77]
 The Spanish privateer and merchant Amaro Pargo (1678-1747) managed to transport slaves to the Caribbean, although, it is estimated, to a lesser extent than other captains and figures of the time dedicated to this activity.[78] In 1710, the privateer was involved in a complaint by the priest Alonso García Ximénez, who accused him of freeing an African slave named Sebastián, who was transported to Venezuela on one of Amaro's ships. The aforementioned Alonso García granted a power of attorney on July 18, 1715 to Teodoro Garcés de Salazar so that he could demand his return in Caracas. Despite this fact, Amaro Pargo himself also owned slaves in his domestic service.[78]
 Slavery was prevalent in many parts of Africa for many centuries before the beginning of the Atlantic slave trade.[79] Slavery was an important part of the economic structure of Africa although its relative importance and the role and treatment of enslaved people varied considerably by society.[80]
 Millions of enslaved Africans were transported to other parts of Africa, or exported to Europe and Asia prior to the Atlantic slave trade and the European colonization of the Americas.[81][82] The Trans-Saharan slave trade across the Sahara had functioned since antiquity, and continued to do so up until the 20th-century; in 652, the Rashidun Caliphate in Egypt enforced an annual tribute of 400 slaves from the Christian Kingdom of Makuria by the Baqt treaty, which was to be in effect for centuries.[83] It supplied Africans for slavery in the Rashidun Caliphate (632–661), the Umayyad Caliphate (661–750), the Abbasid Caliphate (750–1258) and the Mamluk Sultanate (1258–1517).
 The Atlantic slave trade was not the only slave trade from Africa; as Elikia M'bokolo wrote in Le Monde diplomatique:
 Slaves were marched in shackles to the coasts of Sudan, Ethiopia and Somali, placed upon dhows and trafficked across the Indian Ocean to the Gulf of Aden. Others were carried across the Red Sea to Arabia and Aden, with sick slaves being thrown overboard, or they were marched across the Sahara desert via the Trans-Saharan slave trade route to the Nile, many of them dying from exposure or swollen feet along the way.[86]
 However, estimates are imprecise, which can affect comparison between different slave trades. Two rough estimates by scholars of the numbers African slaves held over twelve centuries in the Muslim world are 11.5 million[87][page needed] and 14 million,[88][89] while other estimates indicate a number between 12 and 15 million African slaves prior to the 20th century.[90]
 According to John K. Thornton, Europeans usually bought enslaved people who had been captured in endemic warfare between African states.[3] Some Africans had made a business out of capturing war captives or members of neighboring ethnic groups and selling them.[91] A reminder of this practice is documented in debates over the trade in the British Parliament in 1806: ""All the old writers ... concur in stating not only that wars are entered into for the sole purpose of making slaves, but that they are fomented by Europeans, with a view to that object.""[92] People living around the Niger River would be transported from these markets to the coast and sold in European trading ports, in exchange for muskets and manufactured goods such as cloth or alcohol.[93] The European demand for slaves provided a new and larger market for the already existing trade.[94] While those held as slaves in their own region of Africa could hope to escape, those shipped away had little chance of returning to their homeland.[95]
 The Atlantic slave trading of Africans began in 1441 with two Portuguese explorers, Nuno Tristão and António Gonçalves. Tristão and Gonçalves sailed to Mauritania in West Africa and kidnapped twelve Africans and returned to Portugal and presented the captive Africans as gifts to Prince Henry the Navigator. By 1460, seven hundred to eight hundred African people were taken annually and imported into Portugal. In Portugal, the Africans taken were used as domestic servants. From 1460 to 1500, the removal of Africans increased as Portugal and Spain built forts along the coast of West Africa. By 1500, Portugal and Spain had taken about 50,000 thousand West Africans. The Africans worked as domestic servants, artisans, and farmers. Other Africans were taken to work the sugar plantations on the Azores, Madeira,[98] Canary, and Cape Verde islands. Europeans participated in African enslavement because of their need for labor, profit, and religious motives.[99][100]
 Upon discovering new lands through their naval explorations, European colonisers soon began to migrate to and settle in lands outside their native continent. Off the coast of Africa, European migrants, under the directions of the Kingdom of Castile, invaded and colonised the Canary Islands during the 15th century, where they converted much of the land to the production of wine and sugar. Along with this, they also captured native Canary Islanders, the Guanches, to use as slaves both on the Islands and across the Christian Mediterranean.[101]
 After the success of Portugal and Spain in the slave trade other European nations followed. In 1530, an English merchant from Plymouth, William Hawkins, visited the Guinea Coast and left with a few slaves. In 1564, Hawkin's son John Hawkins, sailed to the Guinea Coast and his voyage was supported by Queen Elizabeth I. John later turned to piracy and stole 300 Africans from a Spanish slave ship after failures in Guinea trying to capture Africans as most of his men died after fights with the local Africans.[100]
 As historian John Thornton remarked, ""the actual motivation for European expansion and for navigational breakthroughs was little more than to exploit the opportunity for immediate profits made by raiding and the seizure or purchase of trade commodities"".[105] Using the Canary Islands as a naval base, Europeans, at the time primarily Portuguese traders, began to move their activities down the western coast of Africa, performing raids in which slaves would be captured to be later sold in the Mediterranean.[106] Although initially successful in this venture, ""it was not long before African naval forces were alerted to the new dangers, and the Portuguese [raiding] ships began to meet strong and effective resistance"", with the crews of several of them being killed by African sailors, whose boats were better equipped at traversing the west-central African coasts and river systems.[107]
 By 1494, the Portuguese king had entered agreements with the rulers of several West African states that would allow trade between their respective peoples, enabling the Portuguese to ""tap into"" the ""well-developed commercial economy in Africa ... without engaging in hostilities"".[108] ""Peaceful trade became the rule all along the African coast"", although there were some rare exceptions when acts of aggression led to violence. For instance, Portuguese traders attempted to conquer the Bissagos Islands in 1535.[109] In 1571, Portugal, supported by the Kingdom of Kongo, took control of the south-western region of Angola in order to secure its threatened economic interest in the area. Although Kongo later joined a coalition in 1591 to force the Portuguese out, Portugal had secured a foothold on the continent that it continued to occupy until the 20th century.[110] Despite these incidents of occasional violence between African and European forces, many African states ensured that any trade went on in their own terms, for instance, imposing custom duties on foreign ships. In 1525, the Kongolese King Afonso I seized a French vessel and its crew for illegally trading on his coast. In addition, Afonso complained to the king of Portugal that Portuguese slave traders continued to kidnap his people, which was causing depopulation in his kingdom.[111][109]
 Nzinga of Ndongo and Matamba, who ruled as queen of the Ambundu Kingdoms of Ndongo (1624–1663) and Matamba (1631–1663) in present-day Angola, fought a long war against the Portuguese Empire's expansion. Initially, Nzinga accommodated the Portuguese. She converted to Christianity and repositioned the Ndongo Kingdom as an intermediary in the slave trade instead of as a source for slaves. This also provided her with a valuable ally against hostile neighbouring African Kingdoms, however the Portuguese continued to encroach on her Kingdom to expand the slave trade and establish settlements. Nzinga called for the end to the raids, however the Portuguese declared war on Ndongo in 1626. Nzinga allowed sanctuary to runaway slaves from Portuguese controlled territory and organized a military called kilombo against the Portuguese. Within two years, Nzinga's army was defeated and she went into exile. She later conquered the Kingdom of Matamba and entered into an alliance with the Dutch West India Company and former rival African states. With their help, Nzinga was able to reclaim large parts of Ndongo between 1641 and 1647. Nzinga continued to fight the Portuguese until a peace treaty was signed in 1656.[112][113][114]
 Historians have widely debated the nature of the relationship between these African kingdoms and the European traders. The Guyanese historian Walter Rodney (1972) has argued that it was an unequal relationship, with Africans being forced into a ""colonial"" trade with the more economically developed Europeans, exchanging raw materials and human resources (i.e. slaves) for manufactured goods. He argued that it was this economic trade agreement dating back to the 16th century that led to Africa being underdeveloped in his own time.[115] These ideas were supported by other historians, including Ralph Austen (1987).[116] This idea of an unequal relationship was contested by John Thornton (1998), who argued that ""the Atlantic slave trade was not nearly as critical to the African economy as these scholars believed"" and that ""African manufacturing [at this period] was more than capable of handling competition from preindustrial Europe"".[117] However, Anne Bailey, commenting on Thornton's suggestion that Africans and Europeans were equal partners in the Atlantic slave trade, wrote:
 The Atlantic slave trade is customarily divided into two eras, known as the first and second Atlantic systems. Slightly more than 3% of the enslaved people exported from Africa were traded between 1525 and 1600, and 16% in the 17th century.[citation needed]
 The first Atlantic system was the trade of enslaved Africans to, primarily, American colonies of the Portuguese and Spanish empires. Before the 1520s, slavers took Africans to Seville or the Canary Islands and then exported some of them from Spain to its colonies in Hispaniola and Puerto Rico, with 1 to 40 slaves per ship. These supplemented enslaved Native Americans. In 1518, the Spanish king gave permission for ships to go directly from Africa to the Caribbean colonies, and they started taking 200–300 per trip.[119][better source needed]
 During the first Atlantic system, most of these slavers were Portuguese, giving them a near-monopoly. Decisive was the 1494 Treaty of Tordesillas which did not allow Spanish ships in African ports. Spain had to rely on Portuguese ships and sailors to bring slaves across the Atlantic. From 1525, slaves were transported directly from the Portuguese colony of Sao Tomé across the Atlantic to Hispaniola.[120]
 A burial ground in Campeche, Mexico, suggests enslaved Africans had been brought there not long after Hernán Cortés completed the subjugation of Aztec and Mayan Mexico in 1519. The graveyard had been in use from approximately 1550 to the late 17th century.[121]
 In 1562, John Hawkins captured Africans in what is now Sierra Leone and took 300 people to sell in the Caribbean. In 1564, he repeated the process, this time using Queen Elizabeth's own ship, Jesus of Lübeck, and numerous English voyages ensued.[122]
 Around 1560, the Portuguese began a regular slave trade to Brazil. From 1580 until 1640, Portugal was temporarily united with Spain in the Iberian Union. Most Portuguese contractors who obtained the asiento between 1580 and 1640 were conversos.[123] For Portuguese merchants, many of whom were ""New Christians"" or their descendants, the union of crowns presented commercial opportunities in the slave trade to Spanish America.[124][125][page needed]
 Until the middle of the 17th century, Mexico was the largest single market for slaves in Spanish America.[126] While the Portuguese were directly involved in trading enslaved peoples to Brazil, the Spanish Empire relied on the Asiento de Negros system, awarding (Catholic) Genoese merchant bankers the license to trade enslaved people from Africa to their colonies in Spanish America. Cartagena, Veracruz, Buenos Aires, and Hispaniola received the majority of slave arrivals, mainly from Angola.[127] This division of the slave trade between Spain and Portugal upset the British and the Dutch who invested in the British West Indies and Dutch Brazil producing sugar. After the Iberian Union fell apart, Spain prohibited Portugal from directly engaging in the slave trade as a carrier. According the Treaty of Münster the slave trade was opened for the traditional enemies of Spain, losing a large share of the trade to the Dutch, French, and English. For 150 years, Spanish transatlantic traffic was operating at trivial levels. In many years, not a single Spanish slave voyage set sail from Africa. Unlike all of their imperial competitors, the Spanish almost never delivered slaves to foreign territories. By contrast, the British, and the Dutch before them, sold slaves everywhere in the Americas.[128]
 The second Atlantic system was the trade of enslaved Africans by mostly English, French, and Dutch traders and investors.[129] The main destinations of this phase were the Caribbean islands Curaçao, Jamaica and Martinique, as European nations built up economically slave-dependent colonies in the New World.[130][page needed][131] In 1672, the Royal Africa Company was founded. In 1674, the New West India Company became deeper involved in slave trade.[132] From 1677, the Compagnie du Sénégal, used Gorée to house the slaves. The Spanish proposed to get the slaves from Cape Verde, located closer to the demarcation line between the Spanish and Portuguese empire, but this was against the WIC-charter"".[133] The Royal African Company usually refused to deliver slaves to Spanish colonies, though they did sell them to all comers from their factories in Kingston, Jamaica and Bridgetown, Barbados.[134] In 1682, Spain allowed governors from Havana, Porto Bello, Panama, and Cartagena, Colombia to procure slaves from Jamaica.[135]
 By the 1690s, the English were shipping the most slaves from West Africa.[136] By the 18th century, Portuguese Angola had become again one of the principal sources of the Atlantic slave trade.[137] After the end of the War of the Spanish Succession, as part of the provisions of the Treaty of Utrecht (1713), the Asiento was granted to the South Sea Company.[138] Despite the South Sea Bubble, the British maintained this position during the 18th century, becoming the biggest shippers of slaves across the Atlantic.[139][140] It is estimated that more than half of the entire slave trade took place during the 18th century, with the Portuguese, British, and French being the main carriers of nine out of ten slaves abducted in Africa.[141] At the time, slave trading was regarded as crucial to Europe's maritime economy, as noted by one English slave trader: ""What a glorious and advantageous trade this is ... It is the hinge on which all the trade of this globe moves.""[142][143]
 Meanwhile, it became a business for privately owned enterprises, reducing international complications.[126] After 1790, by contrast, captains typically checked out slave prices in at least two of the major markets of Kingston, Havana, and Charleston, South Carolina (where prices by then were similar) before deciding where to sell.[144] For the last sixteen years of the transatlantic slave trade, Spain was the only transatlantic slave-trading empire.[145]
 Following the British Slave Trade Act 1807 and U.S. bans on the African slave trade that same year, it declined, but the period thereafter still accounted for 28.5% of the total volume of the Atlantic slave trade.[146][page needed] Between 1810 and 1860, over 3.5 million slaves were transported, with 850,000 in the 1820s.[147]
 The first side of the triangle was the export of goods from Europe to Africa. A number of African kings and merchants took part in the trading of enslaved people from 1440 to about 1833. For each captive, the African rulers would receive a variety of goods from Europe. These included guns, ammunition, alcohol, indigo dyed Indian textiles, and other factory-made goods.[148] The second leg of the triangle exported enslaved Africans across the Atlantic Ocean to the Americas and the Caribbean Islands. The third and final part of the triangle was the return of goods to Europe from the Americas. The goods were the products of slave plantations and included cotton, sugar, tobacco, molasses and rum.[149] Sir John Hawkins, considered the pioneer of the English slave trade, was the first to run the triangular trade, making a profit at every stop.[150]
 The Atlantic slave trade was the result of, among other things, labour shortage, itself in turn created by the desire of European colonists to exploit New World land and resources for capital profits. Native peoples were at first utilized as slave labour by Europeans until a large number died from overwork and Old World diseases.[151] Furthermore, in the mid-16th century, the Spanish New Laws, prohibited slavery of the Indigenous people. A labour shortage resulted. Alternative sources of labour, such as indentured servitude, failed to provide a sufficient workforce. Many crops could not be sold for profit, or even grown, in Europe. Exporting crops and goods from the New World to Europe often proved to be more profitable than producing them on the European mainland. A vast amount of labour was needed to create and sustain plantations that required intensive labour to grow, harvest, and process prized tropical crops. Western Africa (part of which became known as ""the Slave Coast""), Angola and nearby Kingdoms and later Central Africa, became the source for enslaved people to meet the demand for labour.[152]
 The basic reason for the constant shortage of labour was that, with much cheap land available and many landowners searching for workers, free European immigrants were able to become landowners themselves relatively quickly, thus increasing the need for workers.[153] Labour shortages were mainly met by the English, French and Portuguese with African slave labour.
 Thomas Jefferson attributed the use of slave labour in part to the climate, and the consequent idle leisure afforded by slave labour: ""For in a warm climate, no man will labour for himself who can make another labour for him. This is so true, that of the proprietors of slaves a very small proportion indeed are ever seen to labour.""[154] In a 2015 paper, economist Elena Esposito argued that the enslavement of Africans in colonial America was attributable to the fact that the American south was sufficiently warm and humid for malaria to thrive; the disease had debilitating effects on the European settlers. Conversely, many enslaved Africans were taken from regions of Africa which hosted particularly potent strains of the disease, so the Africans had already developed natural resistance to malaria. This, Esposito argued, resulted in higher malaria survival rates in the American south among enslaved Africans than among European labourers, making them a more profitable source of labour and encouraging their use.[155]
 Historian David Eltis argues that Africans were enslaved because of cultural beliefs in Europe that prohibited the enslavement of cultural insiders, even if there was a source of labour that could be enslaved (such as convicts, prisoners of war and vagrants). Eltis argues that traditional beliefs existed in Europe against enslaving Christians (few Europeans not being Christian at the time) and those slaves that existed in Europe tended to be non-Christians and their immediate descendants (since a slave converting to Christianity did not guarantee emancipation) and thus by the 15th century Europeans as a whole came to be regarded as insiders.
 Eltis argues that while all slave societies have demarked insiders and outsiders, Europeans took this process further by extending the status of insider to the entire European continent, rendering it unthinkable to enslave a European since this would require enslaving an insider. Conversely, Africans were viewed as outsiders and thus qualified for enslavement. While Europeans may have treated some types of labour, such as convict labour, with conditions similar to that of slaves, these labourers would not be regarded as chattel and their progeny could not inherit their subordinate status, thus not making them slaves in the eyes of Europeans. The status of chattel slavery was thus confined to non-Europeans, such as Africans.[156]
 For the British, slaves were no more than animals and could be treated as commodities, so situations like the Zong massacre occurred without any justice for the victims.[157]
 African partners, including rulers, traders and military aristocrats, played a direct role in the slave trade. They sold slaves acquired from wars or through kidnapping to Europeans or their agents.[84] Those sold into slavery were usually from a different ethnic group than those who captured them, whether enemies or just neighbors.[158] These captive slaves were considered ""other"", not part of the people of the ethnic group or ""tribe""; African kings were only interested in protecting their own ethnic group, but sometimes criminals would be sold to get rid of them. Most other slaves were obtained from kidnappings, or through raids that occurred at gunpoint through joint ventures with the Europeans.[84][159] The kingdom of Dahomey supplied war captives to European slave traders.[160] Dahomey King Agaja, who ruled from 1718 to 1740, took control of key trade routes for the Atlantic slave trade by conquering the neighbouring kingdoms of Allada in 1724 and Whydah in 1727.[160] A decrease in the slave trade in the area was observed after this conquest, however Agaja did create significant infrastructure for the slave trade and actively participated in it towards the end of his reign.[161]
 According to Pernille Ipsen, author of Daughters of the Trade: Atlantic Slavers and Interracial Marriage on the Gold Coast, Africans from the Gold Coast (present-day Ghana) also participated in the slave trade through intermarriage, or cassare (taken from Italian, Spanish, or Portuguese), meaning 'to set up house'. It is derived from the Portuguese word 'casar', meaning 'to marry'. Cassare formed political and economic bonds between European and African slave traders. Cassare was a pre-European-contact practice used to integrate the ""other"" from a differing African tribe. Early on in the Atlantic slave trade, it was common for the powerful elite West African families to marry off their women to the European traders in alliance, bolstering their syndicate. The marriages were even performed using African customs, which Europeans did not object to, seeing how important the connections were.[162]
 It is difficult to reconstruct and generalize how Africans residing in Africa understood the Atlantic slave trade, though there is evidence for some societies that African elites and slave traders had awareness of the conditions of the slaves who were transported to the Americas.[163][164] According to Robin Law, the royal elites of the kingdom of Dahomey must have had an ""informed understanding"" of the fates of the Africans they sold into slavery.[163] Dahomey sent diplomats to Brazil and Portugal who returned with information about their trips.[163] In addition, a few royal elites of Dahomey had experienced slavery for themselves in the Americas before returning to their homeland.[163] The only apparent moral issue that the kingdom had with slavery was the enslavement of fellow Dahomeyans, an offense punishable by death, rather than the institution of slavery itself.[163]
 On the Gold Coast, it was common for slave-trading African rulers to encourage their children to learn about Europeans by sending them to sail on European ships, live inside European forts, or travel to Europe or America for an education.[165] Diplomats also traveled to European capital cities. The elites even rescued fellow elites who were tricked into slavery in the Americas by sending demands to the Dutch and the British governments, who complied due to fears of reduced trade and physical harm to hostages.[165] An example is the case of William Ansah Sessarakoo, who was rescued from slavery in Barbados after being recognised by a visiting slave trader of the same Fante ethnic group, and later became a slave trader himself.[166]
 Fenda Lawrence was a slave trader from the Gambia who lived and traded in Georgia and South Carolina as a free person.[167]
 A common assumption by Africans who were unaware of the true purpose of the Atlantic slave trade was that the Europeans were cannibals who planned on cooking and eating their captives.[168] This rumour was a common source of significant distress for enslaved Africans.[168]
 Sometimes trading between Europeans and African leaders was not equal. For example, Europeans influenced Africans to provide more slaves by forming military alliances with warring African societies to instigate more fighting which would provide more war captives to the African rulers to trade as slaves for European consumer goods. Also, Europeans shifted the location of disembarkation points for trade along the African coast to follow military conflicts in West-Central Africa. In areas of Africa where slavery was not prevalent, European slave traders worked and negotiated with African rulers on their terms for trade, and African rulers refused to supply European demands. Africans and Europeans profited from the slave trade; however, African populations, the social, political, and military changes to African societies suffered greatly. For example, Mossi Kingdoms resisted the Atlantic slave trade and refused to participate in the selling of African people. However, as time progressed more European slave traders entered into West Africa and were having more influence in African nations and the Mossi became involved in slave trading in the 1800s.[158][169]
 Although many African nations participated and profited from the Atlantic slave trade, many African nations also resisted such as the Djola and Balanta.[170] Some African nations organized into military resistance movements and fought African slave raiders and European slave traders entering their villages. For example, the Akan, Etsi, Fetu, Eguafo, Agona, and Asebu people organized into the Fante coalition and fought African and European slave raiders and protected themselves from capture and enslavement.[171] Chief Tomba was born in 1700 and his adopted father was a general from the Jalonke-speaking people who fought against the slave trade. Tomba became ruler of the Baga people in present-day Guinea Bissau in West Africa and made alliances with nearby African villages against African and European slave traders. His efforts were unsuccessful: Tomba was captured by African traders and sold into slavery.[172]
 Donna Beatriz Kimpa Vita in Kongo and Senegalese leader Abd al-Qadir, advocated resistance against the forced exportation of Africans.[173] In the 1770s, leader Abdul Kader Khan opposed the Atlantic slave trade through Futa Toro, present-day Senegal. Abdul Kader Khan and Futa Toro nation resisted French slave traders and colonizers who wanted to enslave Africans and Muslims from Futa Toro.[174] Other forms of resistance against the Atlantic slave trade by African nations was migrating to different areas in West Africa such as swamps and lake regions to escape slave raids. In West Africa, Efik slave dealers participated in slave dealing as a form of protection against enslavement.[175] African resistance movements were carried out in every phase of the slave trade to resisting marches to the slave holding stations, resistance at the slave coast, and resistance on slave ships.[176]
 For example, aboard the slave ship Clare, the enslaved Africans revolted and drove the crew from the vessel and took control of the ship and liberated themselves and landed near Cape Coast Castle in present-day Ghana in 1729. On other slave ships enslaved Africans sunk ships, killed the crew, and set fire to ships with explosives. Slave traders and white crewmembers prepared and prevented possible rebellions by loading women, men, and children separately inside slave ships because enslaved children used loose pieces of wood, tools, and any objects they found and passed them to the men to free themselves and fight the crew. According to historical research from the records of slave ship captains, between 1698 and 1807, there were 353 acts of insurrection aboard slave ships. The majority of the rebellions by the Africans were defeated. Igbo slaves on ships committed suicide by jumping overboard as an act of resistance to enslavement. To prevent further suicides, white crewmen placed nets around slave ships to catch enslaved persons that jumped overboard. White captains and crewmen invested in firearms, swivel guns, and ordered ship crews to watch slaves to prevent or prepare for possible slave revolts.[178][179]
 John Newton was a captain of slave ships and recorded in his personal journal how Africans mutinied on ships, and some were successful in overtaking the crew.[180][181] For example, in 1730 the slave ship Little George departed from the Guinea Coast in route to Rhode Island with a cargo of ninety-six enslaved Africans. A few of the slaves slipped out of their iron chains and killed three of the watchmen on deck and imprisoned the captain and the rest of the crew. The Africans received a promise of freedom in a deal made with the captain and his crew. Africans reclaimed the ship and sailed it back to Africa's shore. The captain and crew failed in their attempt to re-enslave the Africans.[182]
 According to research by historian Jane Landers, more rebellions on slave ships occurred when there were large numbers of African women aboard.[183]
 Europeans provided the market for slaves, rarely traveling beyond the coast or entering the African interior, due to fear of disease and native resistance.[184] They typically resided in fortresses on the coasts, where they waited for Africans to provide them captured slaves from the interior in exchange for goods. Cases of European merchants kidnapping free Africans into slavery often resulted in fierce retaliation from Africans, who could momentarily stop trade and even capture or kill Europeans.[185] Europeans who desired safe and uninterrupted trade aimed to prevent kidnapping incidents, and the British passed the ""Acts of Parliament for Regulating the Slave Trade"" in 1750 which outlawed the abduction of free Africans by ""fraud, force, or violence"".[185] According to a source from the Lowcountry Digital Library at the College of Charleston, ""When Portuguese, and later their European competitors, found that peaceful commercial relations alone did not generate enough enslaved Africans to fill the growing demands of the trans-Atlantic slave trade, they formed military alliances with certain African groups against their enemies. This encouraged more extensive warfare to produce captives for trading.""[186]
 In 1778, Thomas Kitchin estimated that Europeans were bringing an estimated 52,000 slaves to the Caribbean yearly, with the French bringing the most Africans to the French West Indies (13,000 out of the yearly estimate).[187] The Atlantic slave trade peaked in the last two decades of the 18th century,[188] during and following the Kongo Civil War.[189] Wars among tiny states along the Niger River's Igbo-inhabited region and the accompanying banditry also spiked in this period.[91] Another reason for surplus supply of enslaved people was major warfare conducted by expanding states, such as the kingdom of Dahomey,[190] the Oyo Empire, and the Ashanti Empire.[191]
 Forms of slavery varied both in Africa and in the New World. In general, slavery in Africa was not heritable—that is, the children of slaves were free—while in the Americas, children of slave mothers were considered born into slavery. This was connected to another distinction: slavery in West Africa was not reserved for racial or religious minorities, as it was in European colonies, although the case was otherwise in places such as Somalia, where Bantus were taken as slaves for the ethnic Somalis.[192][193]
 The treatment of slaves in Africa was more variable than in the Americas. At one extreme, the kings of Dahomey routinely slaughtered slaves in hundreds or thousands in sacrificial rituals, and slaves as human sacrifices were also known in Cameroon.[194][195] On the other hand, slaves in other places were often treated as part of the family, ""adopted children"", with significant rights including the right to marry without their masters' permission.[196] Scottish explorer Mungo Park wrote:
 According to an article in PBS, there were many differences between African slavery and European slavery in the Americas: ""It is important to distinguish between European slavery and African slavery. In most cases, slavery systems in Africa were more like indentured servitude in that the slaves retained some rights and children born to slaves were generally born free. The slaves could be released from servitude and join a family clan. In contrast, European slaves were chattel, or property, who were stripped of their rights. The cycle of slavery was perpetual; children of slaves would, by default, also be slaves.""[198]
 In the Americas, slaves were denied the right to marry freely and masters did not generally accept them as equal members of the family. New World slaves were considered the property of their owners, and slaves convicted of revolt or murder were executed.[199]
 Europeans would buy and ship slaves to the Western Hemisphere from markets across West Africa. The number of enslaved people sold to the New World varied throughout the slave trade. As for the distribution of slaves from regions of activity, certain areas produced far more enslaved people than others. Between 1650 and 1900, 10.2 million enslaved Africans arrived in the Americas from the following regions in the following proportions:[200][page needed]
 Although the slave trade was largely global, there was considerable intracontinental slave trade in which 8 million people were enslaved within the African continent.[201] Of those who did move out of Africa, 8 million were forced out of Eastern Africa to be sent to Asia.[201]
 There were over 173 city-states and kingdoms in the African regions affected by the slave trade between 1502 and 1853, when Brazil became the last Atlantic import nation to outlaw the slave trade. Of those 173, no fewer than 68 could be deemed nation-states with political and military infrastructures that enabled them to dominate their neighbours. Nearly every present-day nation had a pre-colonial predecessor, sometimes an African empire with which European traders had to barter.
 The different ethnic groups brought to the Americas closely correspond to the regions of heaviest activity in the slave trade. Over 45 distinct ethnic groups were taken to the Americas during the trade. Of the 45, the ten most prominent, according to slave documentation of the era and modern genealogical studies are listed below.[202][203][204]
 The transatlantic slave trade resulted in a vast and as yet unknown loss of life for African captives both in and outside the Americas. Estimates have ranged from as low as 2 million[205] to as high 60 million.[206] ""More than a million people are thought to have died"" during their transport to the New World according to a BBC report.[207] More died soon after their arrival. The number of lives lost in the procurement of slaves remains a mystery but may equal or exceed the number who survived to be enslaved.[22]
 The trade led to the destruction of individuals and cultures. Historian Ana Lucia Araujo has noted that the process of enslavement did not end with arrival on Western Hemisphere shores; the different paths taken by the individuals and groups who were victims of the Atlantic slave trade were influenced by different factors—including the disembarking region, the ability to be sold on the market, the kind of work performed, gender, age, religion, and language.[208][209]
 Patrick Manning estimates that about 12 million slaves entered the Atlantic trade between the 16th and 19th centuries, but about 1.5 million died on board ship. About 10.5 million slaves arrived in the Americas. Besides the slaves who died on the Middle Passage, more Africans likely died during the slave raids and wars in Africa and forced marches to ports. Manning estimates that 4 million died inside Africa after capture, and many more died young. Manning's estimate covers the 12 million who were originally destined for the Atlantic, as well as the 6 million destined for Arabian slave markets and the 8 million destined for African markets.[21] Of the slaves shipped to the Americas, the largest share went to Brazil and the Caribbean.[210]
 Canadian scholar Adam Jones characterized the deaths of millions of Africans during the Atlantic slave trade as genocide. He called it ""one of the worst holocausts in human history"", and claims arguments to the contrary such as ""it was in slave owners' interest to keep slaves alive, not exterminate them"" to be ""mostly sophistry"" stating: ""the killing and destruction were intentional, whatever the incentives to preserve survivors of the Atlantic passage for labour exploitation. To revisit the issue of intent already touched on: If an institution is deliberately maintained and expanded by discernible agents, though all are aware of the hecatombs of casualties it is inflicting on a definable human group, then why should this not qualify as genocide?""[211]
 Saidiya Hartman has argued that the deaths of enslaved people was incidental to the acquisition of profit and to the rise of capitalism: ""Death wasn't a goal of its own but just a by-product of commerce, which has the lasting effect of making negligible all the millions of lives lost. Incidental death occurs when life has no normative value, when no humans are involved, when the population is, in effect, seen as already dead.""[212] Hartman highlights how the Atlantic slave trade created millions of corpses but, unlike the concentration camp or the gulag, extermination was not the final objective; it was a corollary to the making of commodities.
 Most of the Atlantic slave trade was carried out by seven nations and most of the slaves were carried to their own colonies in the new world. But there was also significant other trading which is shown in the table below.[213] The records are not complete, and some data is uncertain. The last rows show that there were also smaller numbers of slaves carried to Europe and to other parts of Africa, and at least 1.8 million did not survive the journey and were buried at sea with little ceremony.
 The timeline chart when the different nations transported most of their slaves.
 The regions of Africa from which these slaves were taken is given in the following table, from the same source.
 According to Kimani Nehusi, the presence of European slavers affected the way in which the legal code in African societies responded to offenders. Crimes traditionally punishable by some other form of punishment became punishable by enslavement and sale to slave traders.[214][158] According to David Stannard's American Holocaust, 50% of African deaths occurred in Africa as a result of wars between native kingdoms, which produced the majority of slaves.[22] This includes not only those who died in battles but also those who died as a result of forced marches from inland areas to slave ports on the various coasts.[215] The practice of enslaving enemy combatants and their villages was widespread throughout Western and West Central Africa, although wars were rarely started to procure slaves. The slave trade was largely a by-product of tribal and state warfare as a way of removing potential dissidents after victory or financing future wars.[216][page needed]
 In addition, European nations instigated war between African nations and increased the number of war captives by making alliances with warring nations and shifted trade locations in coastal areas to follow patterns of African military conflicts to acquire more slaves.[158] Some African groups proved particularly adept and brutal at the practice of enslaving, such as Bono State, Oyo, Benin, Igala, Kaabu, Ashanti, Dahomey, the Aro Confederacy and the Imbangala war bands.[217][218][page needed]
 In letters written by the Manikongo, Nzinga Mbemba Afonso, to the King João III of Portugal, he writes that Portuguese merchandise flowing in is what is fueling the trade in Africans. He requests the King of Portugal to stop sending merchandise but should only send missionaries. In one of his letters he writes:
 Before the arrival of the Portuguese, slavery had already existed in the Kingdom of Kongo. Afonso I of Kongo believed that the slave trade should be subject to Kongo law. When he suspected the Portuguese of receiving illegally enslaved persons to sell, he wrote to King João III in 1526 imploring him to put a stop to the practice.[220]
 The kings of Dahomey sold war captives into transatlantic slavery; they would otherwise have been killed in a ceremony known as the Annual Customs. As one of West Africa's principal slave states, Dahomey became extremely unpopular with neighbouring peoples.[221][222][223] Like the Bambara Empire to the east, the Khasso kingdoms depended heavily on the slave trade for their economy. A family's status was indicated by the number of slaves it owned, leading to wars for the sole purpose of taking more captives. This trade led the Khasso into increasing contact with the European settlements of Africa's west coast, particularly the French.[224] Benin grew increasingly rich during the 16th and 17th centuries on the slave trade with Europe; slaves from enemy states of the interior were sold and carried to the Americas in Dutch and Portuguese ships. The Bight of Benin's shore soon came to be known as the ""Slave Coast"".[225]
 King Gezo of Dahomey said in the 1840s:
 In 1807, the UK Parliament passed the Bill that abolished the trading of slaves. The King of Bonny (now in Nigeria) was horrified at the conclusion of the practice:
 After being marched to the coast for sale, enslaved people were held in large forts called factories. The amount of time in factories varied, but Milton Meltzer states in Slavery: A World History that around 4.5% of deaths attributed to the transatlantic slave trade occurred during this phase.[228] In other words, over 820,000 people are believed to have died in African ports such as Benguela, Elmina, and Bonny, reducing the number of those shipped to 17.5 million.[228][page needed]
 After being captured and held in the factories, slaves entered the infamous Middle Passage. Meltzer's research puts this phase of the slave trade's overall mortality at 12.5%.[228] Their deaths were the result of brutal treatment and poor care from the time of their capture and throughout their voyage.[229] Around 2.2 million Africans died during these voyages, where they were packed into tight, unsanitary spaces on ships for months at a time.[230] Measures were taken to stem the onboard mortality rate, such as enforced ""dancing"" (as exercise) above deck and the practice of force-feeding enslaved persons who tried to starve themselves.[215] The conditions on board also resulted in the spread of fatal diseases. Other fatalities were suicides, slaves who escaped by jumping overboard.[215] The slave traders would try to fit anywhere from 350 to 600 slaves on one ship. Before the African slave trade was completely banned by participating nations in 1853, 15.3 million enslaved people had arrived in the Americas.
 Raymond L. Cohn, an economics professor whose research has focused on economic history and international migration,[231] has researched the mortality rates among Africans during the voyages of the Atlantic slave trade. He found that mortality rates decreased over the history of the slave trade, primarily because the length of time necessary for the voyage was declining. ""In the eighteenth century many slave voyages took at least 2½ months. In the nineteenth century, 2 months appears to have been the maximum length of the voyage, and many voyages were far shorter. Fewer slaves died in the Middle Passage over time mainly because the passage was shorter.""[232]
 Despite the vast profits of slavery, the ordinary sailors on slave ships were badly paid and subject to harsh discipline. Mortality of around 20%, a number similar and sometimes greater than those of the slaves,[233] was expected in a ship's crew during the course of a voyage; this was due to disease, flogging, overwork, or slave uprisings.[234] Disease (malaria or yellow fever) was the most common cause of death among sailors. A high crew mortality rate on the return voyage was in the captain's interests as it reduced the number of sailors who had to be paid on reaching the home port.[235]
 The slave trade was hated by many sailors, and those who joined the crews of slave ships often did so through coercion or because they could find no other employment.[236]
 Meltzer also states that 33% of Africans would have died in the first year at the seasoning camps found throughout the Caribbean.[228] Jamaica held one of the most notorious of these camps. Dysentery was the leading cause of death.[237] Captives who could not be sold were inevitably destroyed.[209] Around 5 million Africans died in these camps, reducing the number of survivors to about 10 million.[228] The purpose of seasoning camps were to obliterate the Africans' identities and culture and prepare them for enslavement. In seasoning camps, enslaved Africans learned a new language and adopted new customs. This process of seasoning slaves took about two or three years.[238]
 Over the colony's hundred-year course, about a million slaves succumbed to the conditions of slavery in Haiti.[239] A slave imported into Haiti was expected to die, on average, within 3 years of arrival, and slaves born on the island had a life expectancy of only 15 years.[240]
 In the Caribbean, Dutch Guiana, and Brazil, the death rate of enslaved people was high, and the birth rates were low, slaveholders imported more Africans to sustain the slave population. The rate of natural decline in the slave population ran as high as 5 percent a year. While the death rate of enslaved populations in the United States was the same on Jamaican plantations. In the Danish West Indies, and for most of the Caribbean, mortality rate was high because of the taxing labor of sugar cultivation. Sugar was a major cash crop and as the Caribbean plantations exported sugar to Europe and North America, they needed an enslaved work force to make its production economically viable, so slaves were imported from Africa. Enslaved Africans lived in inhumane conditions and the mortality rate of enslaved children under the age of five was forty percent. Many enslaved persons died from smallpox and intestinal worms contracted from contaminated food and water.[241]
 The Atlantic slave trade exportation of slaves to Cuba was illegal by 1820; however, Cuba continued to import enslaved Africans from Africa until slavery was abolished in 1886. After the abolition of the slave trade to the United States and British colonies in 1807, Florida imported enslaved Africans from Cuba, many landing in Amelia Island. A clandestine slave ferry operated between Havana, Cuba and Pensacola, Florida. Florida remained under Spanish control until 1821 which made it difficult for the United States to cease the smuggling of enslaved Africans from Cuba. In 1821, Florida was ceded to the United States and the smuggling of enslaved Africans continued, and from 1821 to 1841 Cuba became a main supplier of enslaved Africans for the United States. Between 1859 and 1862, slave traders made 40 illegal voyages between Cuba and the United States.[242][243]
 The costs of the shipment of human cargo from Africa and operating costs of the slave trade from Africa into Cuba rose in the mid-19th century. Historian Laird Bergad writes of the Cuban slave trade and slave prices: ""Three interacting factors produced the overwhelming demand for slaves responsible for pushing prices to the high levels[...] The first was the uncertainty surrounding the future of the slave trade itself. The long and persistent British campaign to force an end to the Cuban trade had traditionally been circumvented by collusion between Spanish colonial officials and Cuban slave traders. An additional obstacle to British efforts was the unwillingness of the United States to permit the search of U.S.-flag vessels suspected of involvement in the slave trade"". By the mid-1860s, prices of Africans in their elderly years decreased while prices of younger Africans increased because they were considered to be of prime working age. According to research, in 1860 in Matanzas, about 39.6 percent of slaves sold were young prime aged Africans of either sex; in 1870 the percentage was 74.3 percent. In addition, as the cost of sugar increased so did the price of slaves.[244]
 The life expectancy for Brazil's slave plantation's for African descended slaves was around 23 years.[245][page needed] The trans-Atlantic slave trade into Brazil was outlawed in 1831. To replace the demand for slaves, slaveholders in Brazil turned to slave reproduction. Enslaved women were forced to give birth to eight or more enslaved children. Some slaveholders promised enslaved women their freedom if they gave birth to eight children. In 1873 in the village of Santa Ana, province of Ceará an enslaved woman named Macária was promised her freedom after she gave birth to eight children. An enslaved woman Delfina killed her baby because she did not want her enslaver Manoel Bento da Costa to own her baby and enslave her child. Brazil practiced partus sequitur ventrem to increase the slave population through enslaved female reproduction, because in the 19th century, Brazil needed a large enslaved labor force to work on the sugar plantations in Bahia and the agricultural and mining industries of Minas Gerais, São Paulo, and Rio de Janeiro.[246]
 After the abolition of the Atlantic slave trade to Brazil, the inter-provincial trade increased which slaveholders forced and depended on enslaved women to give birth to as many children as possible to supply the demand for slaves. Abolitionists in Brazil wanted to abolish slavery by removing partus sequitur ventrem because it was used to perpetuate slavery. For example, historian Martha Santos writes of the slave trade, female reproduction, and abolition in Brazil: ""A proposal centered on the 'emancipation of the womb', authored by the influential jurist and politician Agostinho Marques Perdigão Malheiro, was officially endorsed by Pedro II as the most practical means to end slavery in a controlled and peaceful manner. This conservative proposal, a modified version of which became the 'free womb' law passed by Parliament in 1871, did provide for the freedom of children subsequently born of enslaved women, while it forced those children to serve their mothers' masters until age twenty-one, and deferred complete emancipation to a later date"".[247]
 The birth rate was more than 80 percent higher in the United States because of a natural growth in the slave population and slave breeding farms.[248][249][250] Birth rates were low for the first generation of slaves imported from Africa, but, in the US, may have increased in the 19th century to some 55 per thousand, approaching the biological maximum for human populations.[251][252]
 After the prohibition of the trans-atlantic slave trade in 1807, slaveholders in the Deep South of the United States needed more slaves to work in the cotton and sugar fields. To fill the demand for more slaves, slave breeding was practiced in Richmond, Virginia. Richmond sold thousands of enslaved people to slaveholders in the Deep South to work the cotton, rice, and sugar plantations. Virginia was known as a ""breeder state."" A slaveholder in Virginia bragged his slaves produced 6,000 enslaved children for sale. About 300,000 to 350,000 enslaved people were sold from Richmond's slave breeding farms.[253] Slave breeding farms and forced reproduction on enslaved young girls and women caused reproductive health issues. Enslaved women found ways to resist forced reproduction by causing miscarriages and abortions by taking plants and medicines.[254][255]
 Slaveholders tried to control enslaved women's reproduction by encouraging them to have relationships with enslaved men. ""Some slaveholders took matters into their own hands, however, and paired enslaved men and women together with the intent that they would procreate.""[256][257] Enslaved teenage girls gave birth at the ages of fifteen or sixteen years old. Enslaved women gave birth in their early twenties. To meet the demands of slaveholders' needs to birth more slaves, enslaved girls and women had seven or nine children. Enslaved girls and women were forced to give birth to as many slaves as possible. The mortality rate of enslaved mothers and children was high because of poor nutrition, sanitation, lack of medical care, and overwork.[258][259] In the United States a slave's life expectancy was 21 to 22 years, and a black child through the age of 1 to 14 had twice the risk of dying of a white child of the same age.[260]
 Slave breeding replaced the demand for enslaved laborers after the decline of the Atlantic slave trade to the United States which caused an increase in the domestic slave trade. The sailing of slaves in the domestic slave trade is known as ""sold down the river,"" indicating slaves being sold from Louisville, Kentucky which was a slave trading city and supplier of slaves. Louisville, Kentucky, Virginia, and other states in the Upper South supplied slaves to the Deep South carried on boats going down the Mississippi River to Southern slave markets.[261][262][263][264][265] New Orleans, Louisiana became a major slave market in the United States domestic slave trade after the prohibition of the Atlantic slave trade in 1807. Between 1819 and 1860, 71,000 enslaved people were transported to the New Orleans slave market on slave ships that departed from ports in the United States along the Atlantic and Gulf of Mexico to New Orleans to supply the demand for slaves in the Deep South.[266][267]
 Texas participated in the illegal slave trade and imported enslaved persons from Cuba to Galveston Island which was the main illegal slave port in Texas. Texas was part of Mexico from 1821 until 1836, and Cuba continued to supply African slaves to many Latin American countries. After 1821, the smuggling of slaves into Texas increased because of slaveholders' demand for additional enslaved labor. Galveston Island is located in the Gulf of Mexico and is 800 miles away from the slave ports in Cuba and between 60 and 70 miles away from the Louisiana border. Smugglers utilized these geographic locations to their advantage and illegally imported enslaved Africans from Cuba and made a profit by selling Africans to slaveholders in Texas and Louisiana. For example, French pirate and privateer Jean Lafitte, established a colony on Galveston Island in 1817 and participated in privateering for four years and made a profit by smuggling in slaves and sold over 200 Africans to slaveholders in the United States. Lafitte used intermediaries such as the Bowie brothers, John, Resin, and James who contracted with slave traders and planters from the United States who had an interest in buying slaves. From 1818 to 1820, Lafitte and the Bowie brothers made $65,000 smuggling Africans into the Southern states and selling them to planters in Louisiana and Mississippi.[268]
 Historian Ernest Obadele-Starks estimated that after 1807 the number of enslaved Africans smuggled into the United States annually averaged as low as 3,500. New Orleans, Louisiana and Florida were centers for the illegal importation of slaves in the United States because of their close proximity to Cuba and the other Caribbean islands that provided Southern states enslaved labor.[269]
 Many diseases, each capable of killing a large minority or even a majority of a new human population, arrived in the Americas after 1492. They include smallpox, malaria, bubonic plague, typhus, influenza, measles, diphtheria, yellow fever, and whooping cough.[270] During the Atlantic slave trade following the discovery of the New World, diseases such as these are recorded as causing mass mortality.[271] Due to the many diseases in the African continent, Europeans nicknamed Sierra Leone in West Africa ""white man's grave"" because of the number of European deaths from diseases.[272][273]
 From 1819 to 1836, the regions of Africa that had the highest European deaths from malaria were Sierra Leone and Senegal. Out of European deaths per 1,000, 164.66 whites died from malaria in Senegal, and 483 whites died from malaria in Sierra Leone. Sierra Leone had the highest number of whites dying from malaria accounting for 40 percent of deaths each year, because of this it was nicknamed ""white man's grave.""[274] The phrase white man's grave was coined in the 1830s. However, Europeans prior to the creation of the phrase considered Africa a dangerous environment due to tropical heat and the high death rates of people dying from diseases, which was why the phrase was created in the 19th century.[275]
 Malaria thrives in warm and humid climates. In North America malaria did not spread as much because certain climatic regions were not conducive to the disease's survival. European American slaveholders preferred Africans who had immunity to malaria be trafficked to the slave ports. The price of Africans born in regions where malaria was dominate were higher. Historian Elena Esposito explains: ""By looking at the historical prices of African slaves in the United States, we find evidence of a malaria premium granted by resistance to the disease. In fact, we show that on Louisiana plantations, more malaria resistant individuals - those born in regions of Africa with a higher prevalence of malaria - commanded significantly higher prices.""[276]
 Evolutionary history may also have played a role in African people's resistance to diseases in the contitent, and Indigenous peoples/Native Americans lack of resistance to African borne diseases. Compared to Africans and Europeans, New World populations did not have a history of exposure to diseases such as malaria, and therefore, no genetic resistance had been produced as a result of adaptation through natural selection.[277]
 Levels and extent of immunity varies from disease to disease. For smallpox and measles for example, those who survive are equipped with the immunity to combat the disease for the rest of their life in that they cannot contract the disease again. There are also diseases, such as malaria, which do not confer effective lasting immunity.[277]
 Epidemics of smallpox were known for causing a significant decrease in the Indigenous population of the New World.[278] The effects on survivors included pockmarks on the skin which left deep scars, commonly causing significant disfigurement. Some Europeans, who believed the plague of syphilis in Europe to have come from the Americas, saw smallpox as the European revenge against the Natives.[271] Africans and Europeans, unlike the native population, often had lifelong immunity, because they had often been exposed to minor forms of the illness such as cowpox or variola minor disease in childhood. By the late 16th century, there existed some forms of inoculation and variolation in Africa and the Middle East. One practice features Arab traders in Africa ""buying-off"" the disease in which a cloth that had been previously exposed to the sickness was to be tied to another child's arm to increase immunity. Another practice involved taking pus from a smallpox scab and putting it in the cut of a healthy individual in an attempt to have a mild case of the disease in the future rather than the effects becoming fatal.[278]
 The trade of enslaved Africans in the Atlantic has its origins in the explorations of Portuguese mariners down the coast of West Africa in the 15th century. Before that, contact with African slave markets was made to ransom Portuguese who had been captured by the intense North African Barbary pirate attacks on Portuguese ships and coastal villages, frequently leaving them depopulated.[279] The first Europeans to use enslaved Africans in the New World were the Spaniards, who sought auxiliaries for their conquest expeditions and labourers on islands such as Cuba and Hispaniola. The alarming decline in the native population had spurred the first royal laws protecting them (Laws of Burgos, 1512–13). The first enslaved Africans arrived in Hispaniola in 1501.[280] After Portugal had succeeded in establishing sugar plantations (engenhos) in northern Brazil c. 1545, Portuguese merchants on the West African coast began to supply enslaved Africans to the sugar planters. While at first these planters had relied almost exclusively on the native Tupani for slave labour, after 1570 they began importing Africans, as a series of epidemics had decimated the already destabilized Tupani communities. By 1630, Africans had replaced the Tupani as the largest contingent of labour on Brazilian sugar plantations. This ended the European medieval household tradition of slavery, resulted in Brazil's receiving the most enslaved Africans, and revealed sugar cultivation and processing as the reason that roughly 84% of these Africans were shipped to the New World.
 On November 7, 1693, Charles II issued a royal decree, providing sanctuary in Spanish Florida for fugitive slaves from the British colony of South Carolina.[281]
 As Britain rose in naval power and settled continental North America and some islands of the West Indies, they became the leading slave traders.[282] At one stage the trade was the monopoly of the Royal African Company, operating out of London. But, following the loss of the company's monopoly in 1689,[283] Bristol and Liverpool merchants became increasingly involved in the trade.[284][page needed] By the late 18th century, one out of every four ships that left Liverpool harbour was a slave trading ship.[285][page needed] Much of the wealth on which the city of Manchester, and surrounding towns, was built in the late 18th century, and for much of the 19th century, was based on the processing of slave-picked cotton and manufacture of cloth.[286] Other British cities also profited from the slave trade. Birmingham, the largest gun-producing town in Britain at the time, supplied guns to be traded for slaves.[287] 75% of all sugar produced in the plantations was sent to London, and much of it was consumed in the highly lucrative coffee houses there.[285]
 The first slaves to arrive as part of a labour force in the New World reached the island of Hispaniola (now Haiti and the Dominican Republic) in 1502. Cuba received its first four slaves in 1513. Jamaica received its first shipment of 4,000 slaves in 1518.[289] ""Between the 1490s and the 1850s, Latin America, including the Spanish-speaking Caribbean and Brazil, imported the largest number of African slaves to the New World, generating the single-greatest concentration of black populations outside of the African continent.""[290] About 4 million enslaved Africans were transported to the Caribbean by way of the transatlantic slave trade.[291] Cuba, the largest slave colony in Hispanic America, imported 800,000 enslaved Africans and participated in the illegal slave trade longer than any other.[292] Enslaved Africans worked about 16 hours a day on the sugarcane plantations. They brought their traditional religions from West Africa; these developed in the new world as religions that scholars call African diaspora religions.[293]
 Slave exports to Honduras and Guatemala started in 1526. Historian Nigel Bolland writes of the slave trade in Central America: ""The demand for labor in the early Spanish settlements of Hispaniola, Cuba, Panama, and Peru resulted in a large-scale Indian (Indigenous people) slave trade in Central America in the second quarter of the 16th century. Indeed, the first colonial economy of the region was based on slave trading.""[294]
 In the 16th century, the majority of Africans imported to Central America came from present-day Senegambia and other West African regions. Between 1607 and 1640, Portuguese slave traders imported Africans from Angola to Honduras and were sold in Santiago de Guatemala to work in the sugar and indigo plantations. The majority of the Africans working in the plantations were from the Luanda region in Central Africa.[295]
 The first enslaved Africans to reach what would become the United States arrived in July[citation needed] 1526 as part of a Spanish attempt to colonize San Miguel de Gualdape. By November, the 300 Spanish colonists were reduced to 100, and their slaves from 100 to 70[why?]. The enslaved people revolted in 1526 and joined a nearby Native American tribe, while the Spanish abandoned the colony altogether (1527). The area of the future Colombia received its first enslaved people in 1533. El Salvador, Costa Rica, and Florida began their stints in the slave trade in 1541, 1563, and 1581, respectively. According to research, about 40 percent of enslaved Africans arrived at Gadsden's Wharf, which was the largest slave port in the United States.[296]
 In the 17th century in colonial Boston in Massachusetts, about 166 transatlantic voyages embarked out of Boston. Boston imported enslaved people from Africa and exported rum.[297] Peter Faneuil organized and profited from the trans-Atlantic voyages out of Boston and imported manufactured goods from Europe, and imported enslaved people, rum, and sugar from the Caribbean.[298] Connecticut, Massachusetts, and Rhode Island were the three New England states with the largest slave populations. The enslaved population in South Kingston, Rhode Island was thirty percent, in Boston the slave population was ten percent, in New London it was nine percent, and in New York it was 7.2 percent.[299] The earliest documentation of enslaved people in New England was 1638. In Northern American British colonies, Massachusetts Bay colonies was the center for slave trading and colonial Boston was a major slave port in the North importing slaves directly from Africa.[300][301]
 The 17th century saw an increase in shipments. Africans were brought to Point Comfort – several miles downriver from the English colony of Jamestown, Virginia – in 1619. The first kidnapped Africans in English North America were classed as indentured servants and freed after seven years. Virginia law codified chattel slavery in 1656, and in 1662 the colony adopted the principle of partus sequitur ventrem, which classified children of slave mothers as slaves, regardless of paternity. Under British law, children born of white male slave owners and black female slaves would have inherited the father's status and rights. The change to maternal inheritance for slaves guaranteed that anyone born with any slave ancestors was a slave, with no regard to the nature of the relations between the white father and the black mother, consensual or not.[305]
 In addition to African persons, Indigenous peoples of the Americas were trafficked through Atlantic trade routes. The 1677 work The Doings and Sufferings of the Christian Indians, for example, documents English colonial prisoners of war (not, in fact, opposing combatants, but imprisoned members of English-allied forces) being enslaved and sent to Caribbean destinations.[306][307] Captive Indigenous opponents, including women and children, were also sold into slavery at a substantial profit, to be transported to West Indies colonies.[308][309]
 The Spanish and Portuguese colonized South America and enslaved the Indigenous people. They later enslaved Africans brought from West and Central Africa in ships by way of the Atlantic slave trade. Brazil imported 4.8 million enslaved Africans.[310] Africans who escaped slavery there formed quilombos, maroon communities with degrees of self-governance. Palamares, a quilombo community, lasted for 100 years while other communities were quickly removed by the Dutch and Portuguese.[311][312][313] The Africans imported to Brazil were Yoruba, Fon, Bantu and others. Their religions from Africa developed into new world religions in Brazil called Candomblé, Umbanda, Xango, and Macumba.[314]
 Historian Erika Edwards writes of the slave trade in Argentina: ""In 1587 the first slaves arrived in Buenos Aires from Brazil. From 1580 to 1640, the main commercial activity for Buenos Aires was the slave trade. More than 70 percent of the value of all imports arriving in Buenos Aires were enslaved Africans. Slaves came primarily from Brazil via the Portuguese slave trade from Angola and other western states in Africa. Once arriving in Buenos Aires, they could be sent as far as Lima, Peru; slaves were provided to Mendoza, Tucuman, and Salta Jujuy as well as to Chile, Paraguay, and what is today Bolivia and southern Peru.""[315]
 By 1802, Russian colonists noted that ""Boston"" (U.S.-based) skippers were trading African slaves for otter pelts with the Tlingit people in Southeast Alaska.[316]
 Notes:
 In 18th-century France, returns for investors in plantations averaged around 6%; as compared to 5% for most domestic alternatives, this represented a 20% profit advantage. Risks—maritime and commercial—were important for individual voyages. Investors mitigated it by buying small shares of many ships at the same time. In that way, they were able to diversify a large part of the risk away. Between voyages, ship shares could be freely sold and bought.[320]
 By far the most financially profitable West Indian colonies in 1800 belonged to the United Kingdom. After entering the sugar colony business late, British naval supremacy and control over key islands such as Jamaica, Trinidad, the Leeward Islands, and Barbados and the territory of British Guiana gave it an important edge over all competitors; while many British did not make gains, a handful of individuals made small fortunes. This advantage was reinforced when France lost its most important colony, St. Domingue (western Hispaniola, now Haiti), to a slave revolt in 1791[321] and supported revolts against its rival Britain, in the name of liberty after the 1793 French revolution. Before 1791, British sugar had to be protected to compete against cheaper French sugar.
 After 1791, the British islands produced the most sugar, and the British people quickly became the largest consumers. West Indian sugar became ubiquitous as an additive to Indian tea. It has been estimated that the profits of the slave trade and of West Indian plantations created up to one-in-twenty of every pound circulating in the British economy at the time of the Industrial Revolution in the latter half of the 18th century.[322]
 Following the Slavery Abolition Act 1833 which gradually abolished slavery in the British Empire, the UK government took out a loan of £15 million ($4.25 billion in 2023) to compensate former slave owners for the loss of their ""property"" after their slaves were freed. Compensation was not given to the formerly enslaved people.[323][324]
 Historian Walter Rodney has argued that at the start of the slave trade in the 16th century, although there was a technological gap between Europe and Africa, it was not very substantial. Both continents were using Iron Age technology. The major advantage that Europe had was in ship building. During the period of slavery, the populations of Europe and the Americas grew exponentially, while the population of Africa remained stagnant. Rodney contended that the profits from slavery were used to fund economic growth and technological advancement in Europe and the Americas. Based on earlier theories by Eric Williams, he asserted that the industrial revolution was at least in part funded by agricultural profits from the Americas. He cited examples such as the invention of the steam engine by James Watt, which was funded by plantation owners from the Caribbean.[326]
 Other historians have attacked both Rodney's methodology and accuracy. Joseph C. Miller has argued that the social change and demographic stagnation (which he researched on the example of West Central Africa) was caused primarily by domestic factors. Joseph Inikori provided a new line of argument, estimating counterfactual demographic developments in case the Atlantic slave trade had not existed. Patrick Manning has shown that the slave trade did have a profound impact on African demographics and social institutions, but criticized Inikori's approach for not taking other factors (such as famine and drought) into account, and thus being highly speculative.[327]
 The effect of the trade on African societies is much debated, due to the influx of goods to Africans. Proponents of the slave trade, such as Archibald Dalzel, argued that African societies were robust and not much affected by the trade. In the 19th century, European abolitionists, most prominently David Livingstone, took the opposite view, arguing that the fragile local economy and societies were being severely harmed by the trade.[328][329] According to research from historian Nathan Nunn, the underdeveloped infrastructure and economy in Africa is a result of colonization and the slave trade. Nunn wrote: ""...Africa's poor economic performance is a result of postcolonial state failure, the roots of which lie in the underdevelopment and instability of precolonial polities..., because of a lack of significant political development during colonial rule, the limited precolonial political structures continued to exist after independence. As a result, Africa's postindependence leaders inherited nation states that did not have the infrastructure necessary to extend authority and control over the whole country. Many states were, and still are, unable to collect taxes from their citizens, and as a result they are also unable to provide a minimum level of public goods and services"".[330][331][332][333]
 Some African rulers saw an economic benefit from trading their subjects with European slave traders. With the exception of Portuguese-controlled Angola, coastal African leaders ""generally controlled access to their coasts, and were able to prevent direct enslavement of their subjects and citizens"".[335] Thus, as African scholar John Thornton argues, African leaders who allowed the continuation of the slave trade likely derived an economic benefit from selling their subjects to Europeans. The Kingdom of Benin, for instance, participated in the African slave trade, at will, from 1715 to 1735, surprising Dutch traders, who had not expected to buy slaves in Benin.[335]
 The benefit derived from trading slaves for European goods was enough to make the Kingdom of Benin rejoin the trans-Atlantic slave trade after centuries of non-participation. Such benefits included military technology (specifically guns and gunpowder), gold, or simply maintaining amicable trade relationships with European nations. The slave trade was, therefore, a means for some African elites to gain economic advantages.[336] Historian Walter Rodney estimates that by c. 1770, the King of Dahomey was earning an estimated £250,000 per year by selling captive African soldiers and enslaved people to the European slave-traders. Many West African countries also already had a tradition of holding slaves, which was expanded into trade with Europeans.
 The Atlantic trade brought new crops to Africa and more efficient currencies which were adopted by the West African merchants. This can be interpreted as an institutional reform which reduced the cost of doing business. But the developmental benefits were limited as long as the business including slaving.[337]
 Both Thornton and Fage contend that while African political elite may have ultimately benefited from the slave trade, their decision to participate may have been influenced more by what they could lose by not participating. In Fage's article ""Slavery and the Slave Trade in the Context of West African History"", he notes that for West Africans ""... there were really few effective means of mobilizing labour for the economic and political needs of the state"" without the slave trade.[336]
 Historian Eric Williams in 1994 argued that the profits that Britain received from its sugar colonies, or from the slave trade between Africa and the Caribbean, contributed to the financing of Britain's industrial revolution. However, he says that by the time of the abolition of the slave trade in 1807, and the emancipation of the slaves in 1833, the sugar plantations of the British West Indies had lost their profitability, and it was in Britain's economic interest to emancipate the slaves.[338]
 Other researchers and historians have strongly contested what has come to be referred to as the ""Williams thesis"" in academia. David Richardson has concluded that the profits from the slave trade amounted to less than 1% of domestic investment in Britain.[339] Economic historian Stanley Engerman finds that even without subtracting the associated costs of the slave trade (e.g., shipping costs, slave mortality, mortality of British people in Africa, defense costs) or reinvestment of profits back into the slave trade, the total profits from the slave trade and of West Indian plantations amounted to less than 5% of the British economy during any year of the Industrial Revolution.[340]
 Engerman's 5% figure gives as much as possible in terms of benefit of the doubt to the Williams argument, not solely because it does not take into account the associated costs of the slave trade to Britain, but also because it carries the full-employment assumption from economics and holds the gross value of slave trade profits as a direct contribution to Britain's national income.[340] Historian Richard Pares, in an article written before Williams' book, dismisses the influence of wealth generated from the West Indian plantations upon the financing of the Industrial Revolution, stating that whatever substantial flow of investment from West Indian profits into industry there occurred after emancipation, not before. However, each of these works focus primarily on the slave trade or the Industrial Revolution, and not the main body of the Williams thesis, which was on sugar and slavery itself. Therefore, they do not refute the main body of the Williams thesis.[341][342]
 Seymour Drescher and Robert Anstey argue the slave trade remained profitable until the end, and that moralistic reform, not economic incentive, was primarily responsible for abolition. They say slavery remained profitable in the 1830s because of innovations in agriculture. However, Drescher's Econocide wraps up its study in 1823, and does not address the majority of the Williams thesis, which covers the decline of the sugar plantations after 1823, the emancipation of the slaves in the 1830s, and the subsequent abolition of sugar duties in the 1840s. These arguments do not refute the main body of the Williams thesis, which presents economic data to show that the slave trade was minor compared to the wealth generated by sugar and slavery itself in the British Caribbean.[343][342][344][page needed]
 Karl Marx, in his influential economic history of capitalism, Das Kapital, wrote that ""... the turning of Africa into a warren for the commercial hunting of black-skins, signaled the rosy dawn of the era of capitalist production"". He argued that the slave trade was part of what he termed the ""primitive accumulation"" of capital, the 'non-capitalist' accumulation of wealth that preceded and created the financial conditions for Britain's industrialisation.[345]
 The demographic effects of the slave trade is a controversial and highly debated issue. Although scholars such as Paul Adams and Erick D. Langer have estimated that sub-Saharan Africa represented about 18 percent of the world's population in 1600 and only 6 percent in 1900,[346] the reasons for this demographic shift have been the subject of much debate. In addition to the depopulation Africa experienced because of the slave trade, African nations were left with severely imbalanced gender ratios, with females comprising up to 65 percent of the population in hard-hit areas such as Angola.[201] Moreover, many scholars (such as Barbara N. Ramusack) have suggested a link between the prevalence of prostitution in Africa today with the temporary marriages that were enforced during the course of the slave trade.[347]
 Walter Rodney argued that the export of so many people had been a demographic disaster which left Africa permanently disadvantaged when compared to other parts of the world, and it largely explains the continent's continued poverty.[326] He presented numbers showing that Africa's population stagnated during this period, while those of Europe and Asia grew dramatically. According to Rodney, all other areas of the economy were disrupted by the slave trade as the top merchants abandoned traditional industries in order to pursue slaving, and the lower levels of the population were disrupted by the slaving itself.
 Others have challenged this view. J. D. Fage compared the demographic effect on the continent as a whole. David Eltis has compared the numbers to the rate of emigration from Europe during this period. In the 19th century alone over 50 million people left Europe for the Americas, a far higher rate than were ever taken from Africa.[348]
 Other scholars accused Walter Rodney of mischaracterizing the trade between Africans and Europeans. They argue that Africans, or more accurately African elites, deliberately let European traders join in an already large trade in enslaved people and that they were not patronized.[216][page needed]
 As Joseph E. Inikori argues, the history of the region shows that the effects were still quite deleterious. He argues that the African economic model of the period was very different from the European model, and could not sustain such population losses. Population reductions in certain areas also led to widespread problems. Inikori also notes that after the suppression of the slave trade Africa's population almost immediately began to rapidly increase, even prior to the introduction of modern medicines.[349]
 The cultural effects of the transatlantic slave trade in Africa are the reduction of traditional African religious practices. According to research in a 2021 census of religions practiced in Ghana published by the Office of International Religious Freedom, in 2021 the Ghana Embassy reported ""71 percent of the population are Christian, 20 percent Muslim, 3 percent adhere to indigenous or animistic religious beliefs, and 6 percent belong to other religious groups or have no religious beliefs"". Historian Nana Osei Bonsu argued that the transatlantic slave trade not only took millions of Africans from the continent but also caused a decline of traditional African religions and Ghanaian indigenous culture as Europeans believed African people's culture and religions were irrelevant and inferior. The slave trade resulted in the colonization of Africa and its people forcing many Africans to convert to Christianity.[350][351]
 The transatlantic slave trade affected traditional trade routes in West-Central Africa. Africans traded goods and slaves using trade routes in the interior of Africa that connected to the Sahara Desert and the Mediterranean coast where other commodities and enslaved people were traded. These trade routes were used by Africans for centuries and societies and kingdoms developed as a result. Europeans chose to trade primarily along the Atlantic coast because they did not have immunity to malaria that was endemic to the region and ""they could not dominate further than their guns could fire, from ship or fort"". The slave trade also left warlords in charge in African societies as they wanted to trade with Europeans to obtain guns to defeat their enemies and sell them to Europeans.[353][354]
 The European system of monetization implemented with the slave trade replaced cowrie shells, the currency traditionally used among Africans. According to research from the National Park Service: ""European trade goods supplanted former African reliance on indigenous material goods, natural resources and products as the economic basis of their society. At the same time Europeans increasingly required people in exchange for trade goods. Once this stage was reached an African society had little choice but to trade human lives for European goods and guns; guns that had become necessary to wage wars for further captives in order to trade for goods upon which an African society was now dependent"".[355]
 As the European slave trade grew more profitable, the demand for slaves increased, which affected African coastal societies in the following ways: ""Commerce with the world outside Africa changed from overland to sea and coastal villages whose main trades had been fishing and salt production became ports and trading posts"". The trans-Atlantic slave trade resulted in the colonization of Africa. Colonization in Africa continues to have negative effects as some traditional African cultures are erased, along with traditional languages and traditional African religions. After the trans-Atlantic slave trade had ceased, European colonial powers fought over the land and resources in Africa. The development of the antimalarial drug quinine in 1820 enabled Europeans to colonize the interior of Africa.[9][356]
 By the end of the 19th century, European powers laid claim to 90 percent of land in Sub-Saharan Africa during the ""Scramble for Africa"". In this invasion and subsequent colonization, the seven European powers (Britain, France, Germany, Belgium, Spain, Portugal, and Italy) removed African kingdoms of power, created national borders that did not align with the already existing ethnic borders in Africa and forced diverse tribal ethnic groups to coexist and be controlled under one colonial power. This caused an unnatural division of people and was the groundwork for the instability in the African continent beginning in the 20th century into the present day.[357][358][359][360][361]
 The slave forts built along the Gold Coast in Ghana during the years of the slave trade were owned and used by the British colonial administration as their headquarters well into the 20th century.[362] British forts in Ghana were vacated after Ghana gained independence in 1957. The placement of these forts dislocated African societies that lived and fished along the coast. British colonists used the fort to imprison African resistance leaders who organized resistance movements against colonization. In 1900, Yaa Asantewaa (Queen mother and war leader of the Ashanti people), was imprisoned at Elmina Castle because she led a war against the British for possession of the Golden Stool, or Ashanti royal throne.[363][364]
 Walter Rodney states:
 Eric Williams argued that ""A racial twist [was] given to what is basically an economic phenomenon. Slavery was not born of racism: rather, racism was the consequence of slavery.""[365]
 Similarly, John Darwin writes, ""The rapid conversion from white indentured labour to black slavery ... made the English Caribbean a frontier of civility where English (later British) ideas about race and slave labour were ruthlessly adapted to local self-interest.... Indeed, the root justification for the system of slavery and the savage apparatus of coercion on which its preservation depended was the ineradicable barbarism of the slave population, a product, it was argued, of its African origins"".[366]
 Although slavery was practiced in ancient times in various cultures, it did not have a global effect like the transatlantic slave trade and slavery in the Americas created by Europeans. The transatlantic slave trade's legacy is institutional racism on an international scale that led to racial discrimination in educational institutions and public places. In addition, scientific racism was taught in schools and some colleges in the United States and Europe in the 19th century and early 20th centuries that was used as a justification to enslave Africans.[367][368][369][370]
 The Canadian Museum for Human Rights explained how European slavery differed from the slavery practiced by Africans and Native Americans. ""Europeans brought a different kind of slavery to North America, however. Many Europeans saw enslaved people merely as property to be bought and sold. This 'chattel slavery' was a dehumanizing and violent system of abuse and subjugation. Importantly, Europeans viewed slavery in racist terms. Indigenous and African peoples were seen as less than human. This white supremacy justified the violence of slavery for hundreds of years.""[371][372][373][374] Another example from an article from The Wall Street Journal explained, ""New World slavery was a racialized institution in which slaves were black and slave owners were white. In contrast, owners and slaves in the Old World were generally of the same race. Distinctions between enslaved and freeborn people were often framed not in racial terms but in terms of language, culture and religion.""[375][376][377][378] Scientific racism and the history of enslaving sub-Saharan Africans led to anti-black racism that is seen worldwide.[379]
 Abolitionists of African, European, and American descent campaigned against the Atlantic slave trade.[380] Black abolitionists took a more radical approach to abolition than their white counterparts, encouraging strikes, slave rebellions aboard slave ships and on plantations, circulating petitions, telling personal narratives about the horrors of slavery, and advocating for freedom and equal rights for Black people in the African diaspora.[381][382]
 According to sociologist José Lingna Nafafé, the first movement against slavery and the Atlantic slave trade started in the 17th century among Africans in the Portuguese empire. Lourenço da Silva de Mendouça, a royal from Angola's Ndongo Kingdom, campaigned against the slave trade while traveling through Italy, Spain, and the Vatican in Rome. Mendonça petitioned the Vatican, Portugal, Italy, and Spain in 1684 to end the enslavement of Africans, presenting his case to Pope Innocent XI, and demanded the abolition for Africans, New Christians (Jews converted to Christianity) and American Indians. This was a century before  abolitionists William Wilberforce and Thomas Buxton emerged.[383][384][385]
 Slavery's supporters cited Africans enslaving each other and claimed this as evidence of African's inferior nature. A common narrative of the abolition of the Atlantic slave trade portrays European Christians as morally superior and saviors of Africans from enslavement. In addition, Christian narratives also justified the slave trade, and the colonialism that followed British abolition.[388]
 Historian and author Benedetta Rossi states that some African societies implemented laws that prohibited the slave trade and slavery before European contact. Rossi writes: ""...the actions of African critics of slavery were informed by cultural representations and normative traditions that varied from society to society. Second, at the individual level, what actors thought and did about slavery and abolition depended on their position in society: wealthy slaveowners, political rulers, religious authorities, and enslaved persons had different interests and tactics, which they developed in the political and economic circum stances of their times."" African rulers who passed anti-slavery laws only abolished certain forms of slavery. African nations who opposed the slave trade did so for various reasons including cultural, religious, political, and economic motivations.[389][390][391]
 In addition, African resistance to enslavement on slave ships and various rebellions in the Americas sparked debates about abolishing the slave trade and slavery.[392] Abolitionist Olaudah Equiano was a former slave who was kidnapped from present day Nigeria and wrote an autobiography about his life published in 1789 that discussed the horrors of slavery, and gave lectures in Britain advocating abolition of the Atlantic slave trade and chattel slavery. In 1788, Equiano participated in the House of Commons debates about slavery and abolition of the slave trade, wrote letters to the government, and corresponded with parliamentarians.[393][394]
 Author and historian Bronwen Everill writes the British were not the first to abolish the slave trade and African leaders in Sierra Leone had a role in ending the transatlantic slave trade. Formerly enslaved Black Britons founded Sierra Leone in West Africa in 1787, on land inhabited by the Temne people. Over the years, Black Loyalists from North America moved to the colony. The Temne, Susu, and freedmen opposed the slave trade. The Sierra Leone Company in London managed the colony, and Africans and freedmen wanted to establish trade with the Sierre Leone Company without selling people. Sierra Leone's story reveals the British navy's reliance on African entities opposing the slave trade to achieve abolition. Britain took control of the Sierra Leone colony from the Sierra Leone Company, establishing a court and naval patrol to combat the slave trade by seizing ships.[395][396]
 In Britain, America, Portugal, and in parts of Europe, opposition developed against the slave trade. David Brion Davis says that abolitionists assumed ""that an end to slave imports would lead automatically to the amelioration and gradual abolition of slavery"".[397] In Britain and America, opposition to the trade was led by members of the Religious Society of Friends (Quakers), Thomas Clarkson and establishment Evangelicals such as William Wilberforce in Parliament. Many people joined the movement and they began to protest against the trade, but they were opposed by the owners of the colonial holdings.[398] Following Lord Mansfield's decision in 1772, many abolitionists and slave-holders believed that slaves became free upon entering the British isles.[399] However, in reality occasional instances of slavery continued in Britain right up to abolition in the 1830s. The Mansfield ruling on Somerset v Stewart only decreed that a slave could not be transported out of England against his will.[400]
 Under the leadership of Thomas Jefferson, the new U.S. state of Virginia in 1778 became the first slave-owning state and one of the first jurisdictions anywhere to stop the importation of new slaves for sale; it made it a crime for traders to bring in slaves from out of state or from overseas for sale; migrants from within the United States were allowed to bring their own slaves. The new law freed all slaves brought in illegally after its passage and imposed heavy fines on violators.[401][402][403] All the other states in the United States followed suit, although South Carolina reopened its slave trade in 1803.[404]
 Denmark, which had been active in the slave trade, was the first country to ban the trade through legislation in 1792, which took effect in 1803.[405] Britain banned the slave trade in 1807, imposing stiff fines for any slave found aboard a British ship (see Slave Trade Act 1807). The Royal Navy moved to stop other nations from continuing the slave trade and declared that slaving was equal to piracy and was punishable by death. The United States Congress passed the Slave Trade Act of 1794, which prohibited the building or outfitting of ships in the U.S. for use in the slave trade. The U.S. Constitution (Article I, section 9, clause 1) barred a federal prohibition on importing slaves for 20 years; at that time the Act Prohibiting Importation of Slaves prohibited imports on the first day the Constitution permitted: January 1, 1808. It was generally thought that the transatlantic slave trade ended in 1867, but evidence was later found of voyages until 1873.[25]
 Quakers began to campaign against the British Empire's slave trade in the 1780s, and from 1789 William Wilberforce was a driving force in the British Parliament in the fight against the trade. The abolitionists argued that the trade was not necessary for the economic success of sugar in the British West Indian colonies. This argument was accepted by wavering politicians, who did not want to destroy the valuable and important sugar colonies of the British Caribbean. Parliament was also concerned about the success of the Haitian Revolution, and they believed they had to abolish the trade to prevent a similar conflagration from occurring in a British Caribbean colony.[406]
 On 22 February 1807, the House of Commons passed a motion by 283 votes to 16 to abolish the Atlantic slave trade. Hence, the slave trade was abolished, but not the still-economically viable institution of slavery itself, which provided Britain's most lucrative import at the time, sugar. Abolitionists did not move against sugar and slavery itself until after the sugar industry went into terminal decline after 1823.[407]
 The United States passed its own Act Prohibiting Importation of Slaves the next week (March 2, 1807), although probably without mutual consultation. The act only took effect on the first day of 1808; since a compromise clause in the US Constitution (Article 1, Section 9, Clause 1) prohibited federal, although not state, restrictions on the slave trade before 1808. The United States did not, however, abolish its internal slave trade, which became the dominant mode of US slave trading until the 1860s.[408] In 1805 the British Order-in-Council had restricted the importation of slaves into colonies that had been captured from France and the Netherlands.[399] Britain continued to press other nations to end its trade; in 1810 an Anglo-Portuguese treaty was signed whereby Portugal agreed to restrict its trade into its colonies; an 1813 Anglo-Swedish treaty whereby Sweden outlawed its slave trade; the Treaty of Paris 1814 where France agreed with Britain that the trade is ""repugnant to the principles of natural justice"" and agreed to abolish the slave trade in five years; the 1814 Anglo-Dutch treaty where the Dutch outlawed its slave trade.[399]
 Abolitionist opinion in Britain was strong enough in 1807 to abolish the slave trade in all British possessions, although slavery itself persisted in the colonies until 1833.[409] Abolitionists after 1807 focused on international agreements to abolish the slave trade. Foreign Minister Castlereagh switched his position and became a strong supporter of the movement. Britain arranged treaties with Portugal, Sweden and Denmark in the period between 1810 and 1814, whereby they agreed to end or restrict their trading. These were preliminary to the Congress of Vienna negotiations that Castlereagh dominated and which resulted in a general declaration condemning the slave trade.[410]
 The problem was that the treaties and declarations were hard to enforce, given the very high profits available to private interests. As Foreign Minister, Castlereagh cooperated with senior officials to use the Royal Navy to detect and capture slave ships. He used diplomacy to make search-and-seize agreements with all the governments whose ships were trading. There was serious friction with the United States, where the southern slave interest was politically powerful. Washington recoiled at British policing of the high seas. Spain, France and Portugal also relied on the international slave trade to supply their colonial plantations.
 As more and more diplomatic arrangements were made by Castlereagh, the owners of slave ships started flying false flags of nations that had not agreed, especially the United States. It was illegal under American law for American ships to engage in the slave trade, but the idea of Britain enforcing American laws was unacceptable to Washington. Lord Palmerston and other British foreign ministers continued the Castlereagh policies. Eventually, in 1842 in 1845, an arrangement was reached between London and Washington. With the arrival of a staunchly anti-slavery government in Washington in 1861, the Atlantic slave trade was doomed. In the long run, Castlereagh's strategy on how to stifle the slave trade proved successful.[411]
 Prime Minister Palmerston detested slavery, and in Nigeria in 1851 he took advantage of divisions in native politics, the presence of Christian missionaries, and the maneuvers of British consul John Beecroft to encourage the overthrow of King Kosoko. The new King Akitoye was a docile non-slave-trading puppet.[412]
 The Royal Navy's West Africa Squadron, established in 1808, grew by 1850 to a force of some 25 vessels, which were tasked with combating slavery along the African coast.[413] Between 1807 and 1860, the Royal Navy's Squadron seized approximately 1,600 ships involved in the slave trade and freed 150,000 Africans who were aboard these vessels.[414] Several hundred slaves a year were transported by the navy to the British colony of Sierra Leone, where they were made to serve as ""apprentices"" in the colonial economy until the Slavery Abolition Act 1833.[415]
 Even though it was prohibited, in response to the North's reluctance or refusal to enforce the Fugitive Slave Act of 1850, the Atlantic slave trade was ""re-open[ed] ... by way of retaliation"".[417] In 1859, ""the trade in slaves from Africa to the Southern coast of the United States is now carried on in defiance of Federal law and of the Federal Government.""[417]
 The last known slave ship to land on U.S. soil was the Clotilda, which in 1859 illegally smuggled a number of Africans into the town of Mobile, Alabama.[418] The Africans on board were sold as slaves; however, slavery in the U.S. was abolished five years later following the end of the American Civil War in 1865. Cudjoe Lewis, who died in 1935, was long believed to be the last survivor of Clotilda and the last surviving slave brought from Africa to the United States,[419] but recent research has found that two other survivors from Clotilda outlived him, Redoshi (who died in 1937) and Matilda McCrear (who died in 1940).[420][421]
 However, according to Senator Stephen Douglas, Lincoln's opponent in the Lincoln–Douglas debates:
 Abraham Lincoln faced significant constitutional challenges in his fight to abolish slavery, as the U.S. Constitution had provided protections for slavery. Despite these challenges, Lincoln's leadership and the creation of a strong federal government allowed for the eventual abolition of slavery through the Emancipation Proclamation and the passage of the 13th Amendment.[423]
 The last country to ban the Atlantic slave trade was Brazil; a first law was approved in 1831, however it was only enforced in 1850 through the new Eusébio de Queirós Law. Despite the prohibition, it took another three years for the trade to effectively end. Between the first law in 1831 and the effective ban of transatlantic trade in 1850, an estimated 500,000 Africans were enslaved and illegally trafficked to Brazil,[424] and until 1856, the year of the last recorded seizure of a slave ship by the Brazilian authorities, around 38,000 Africans still entered the country as slaves.[425]
 Historians João José Reis, Sidney Chalhoub, Robert W. Slenes and Flávio dos Santos Gomes proposed that another reason for the abolition of the Atlantic slave trade to Brazil was the Malê Revolt in 1835. On January 25, 1835, an estimated 600 free and enslaved Africans armed with guns ran through the streets of Salvador murdering whites and slaveholders. Abolitionists argued that if the slave trade and slavery continued, slave resistance movements would increase, resulting in more deaths. Seventy three percent of the Africans in the Malê revolt were Yoruba men who converted to Islam; some white Brazilians believed they had a spirit of resistance against enslavement.[426]
 The historian Walter Rodney contends that it was a decline in the profitability of the triangular trades that made it possible for certain basic human sentiments to be asserted at the decision-making level in a number of European countries—Britain being the most crucial because it was the greatest carrier of African captives across the Atlantic. Rodney states that changes in productivity, technology, and patterns of exchange in Europe and the Americas informed the decision by the British to end their participation in the trade in 1807.[326]
 Nevertheless, Michael Hardt and Antonio Negri[427] argue that it was neither a strictly economic nor a moral matter. First, because slavery was (in practice) still beneficial to capitalism, providing not only an influx of capital but also disciplining hardship into workers (a form of ""apprenticeship"" to the capitalist industrial plant). The more ""recent"" argument of a ""moral shift"" (the basis of the previous lines of this article) is described by Hardt and Negri as an ""ideological"" apparatus in order to eliminate the sentiment of guilt in western society. Although moral arguments did play a secondary role, they usually had major resonance when used as a strategy to undercut competitors' profits. This argument holds that Eurocentric history has been blind to the most important element in this fight for emancipation, precisely, the constant revolt and the antagonism of slaves' revolts. The most important of those being the Haitian Revolution.
 The shock of this 1804 revolution introduced an essential political argument into the end of the slave trade as slaveholders in North America feared a similar situation could happen in the United States, where enslaved people in the Southern states might free themselves through an armed resistance movement and free all enslaved people. The success of enslaved and free blacks in Haiti in freeing themselves through revolt invoked fear among many whites in North America. St. George Tucker, a Virginian jurist, said this about the Haitian Revolution: ""enough to make one shudder in fear of similar calamities in this country"". Some white Americans and whites in the Caribbean suggested ending the slave trade and slavery to prevent an uprising like the one in Haiti.[428][429] A Jamaican planter, Bryan Edwards, observed the Haitian revolution and argued that the enslaved people who revolted were newly imported slaves from Africa. Edwards and other planters believed the slave revolts in the Caribbean were instigated by these new slaves, and some abolitionists suggested ending the slave trade to prevent further slave insurrections.[430] In Charleston, South Carolina, in 1822, Denmark Vesey and Gullah Jack planned a slave insurrection inspired by the Haitian Revolution.[431]
 The Haitian Revolution affected France's colonial economy. Saint Domingue (Haiti) was France's wealthiest colony and the world's top producer of sugar and coffee; it was also a global leader in cacao and indigo. Enslaved labor made Saint Domingue the wealthiest colony in the world and furnished two-thirds of France's overseas trade—because of Saint Domingue's wealth it was nicknamed ""Pearl of the Antilles."" After free and enslaved people gained their independence from France, France and French slaveholders wanted financial compensation from Haiti in the amount of 150 million francs to compensate for their lost wealth, calling it an ""Independence Debt"" because France had lost its wealthiest colony when Haiti gained independence.[432][433] Haitians defeated the French, British, and Spanish during the revolution. Prior to the revolution, the United States was a major trade partner with Saint Domingue. After the revolution, the United States refused to recognize Haiti as an independent Black nation.[434] Haiti was no longer the main exporter of sugar after the revolution, Cuba became the main supplier of sugar to foreign nations, and Louisiana became a center of sugar production in the United States. Slave revolts affected the economy of the slave trade as slaveholders lost property in enslaved people through death, running away, and a decrease in the production of cash crops resulting in a shift in trade to other nations.[435][436]
 However, both James Stephen and Henry Brougham, 1st Baron Brougham and Vaux, wrote that the slave trade could be abolished for the benefit of the British colonies, and the latter's pamphlet was often used in parliamentary debates in favour of abolition. William Pitt the Younger argued on the basis of these writings that the British colonies would be better off, in their economic position as well as in their security, if the trade was abolished. Consequently, according to historian Christer Petley, abolitionists argued, and even some absentee plantation owners accepted, that the trade could be abolished ""without substantial damage to the plantation economy"". William Grenville, 1st Baron Grenville argued that ""the slave population of the colonies could be maintained without it"". Petley points out that government took the decision to abolish the trade ""with the express intention of improving, not destroying, the still-lucrative plantation economy of the British West Indies.""[437][full citation needed]
 In 1787, the British helped 400 freed slaves, primarily African Americans freed during the American Revolutionary War who had been evacuated to London, to relocate to Sierra Leone. Most of the first group of settlers died due to disease and warfare with Indigenous peoples. About 64 survived to establish the second ""Province of Freedom"" following the failed first attempt at colonization between 1787 and 1789.[438][page needed][439][440]
 In 1792, 1200 Nova Scotian Settlers from Nova Scotia settled and established the Colony of Sierra Leone and the settlement of Freetown; these were newly freed African Americans and their descendants. Many of the adults had left Patriot owners and fought for the British in the Revolutionary War. The Crown had offered slaves freedom who left rebel masters, and thousands joined the British lines. More than 1,200 volunteered to settle and establish the new colony of Freetown, which was established by British abolitionists under the Sierra Leone Company.[438][page needed][441]
 In 1816, a group of wealthy European-Americans, some of whom were abolitionists and others who were racial segregationists, founded the American Colonization Society with the express desire of sending liberated African Americans to West Africa. In 1820, they sent their first ship to Liberia, and within a decade around two thousand African Americans had been settled there. Such resettlement continued throughout the 19th century, increasing following the deterioration of race relations in the Southern states of the US following Reconstruction in 1877.[442]
 The American Colonization Society's proposal to send African Americans to Liberia was not universally popular among African Americans, and the proposal was seen as a plot to weaken the influence of the abolitionist movement.[443][444] The scheme was widely rejected by prominent African-American abolitionists such as James Forten[445] and Frederick Douglass.[2]
 The Rastafari movement, which originated in Jamaica, where 92% of the population are descended from the Atlantic slave trade, has made efforts to publicise the slavery and to ensure it is not forgotten, especially through reggae music.[446]
 In 1998, UNESCO designated 23 August as International Day for the Remembrance of the Slave Trade and its Abolition. Since then there have been a number of events recognizing the effects of slavery.
 At the 2001 World Conference Against Racism in Durban, South Africa, African nations demanded a clear apology for slavery from the former slave-trading countries. Some nations were ready to express an apology, but the opposition, mainly from the United Kingdom, Portugal, Spain, the Netherlands, and the United States blocked attempts to do so. A fear of monetary compensation might have been one of the reasons for the opposition. As of 2009, efforts are underway to create a UN Slavery Memorial as a permanent remembrance of the victims of the Atlantic slave trade.
 In 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the role Africans played in the Atlantic slave trade.[2] Luc Gnacadja, minister of environment and housing for Benin, later said: ""The slave trade is a shame, and we do repent for it.""[447] Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin.[447]
 Denmark had a foothold in Ghana for more than 200 years and trafficked as many as 4,000 enslaved Africans per year.[448]
Danish Foreign Minister, Uffe Ellemann-Jensen declared publicly in 1992:
""I understand why the inhabitants in the West Indian Islands celebrate the day they became part of the U.S. But for Danish people and Denmark the day is a dark chapter. We exploited the slaves in the West Indian Islands during 250 years and made good money on them, but when we had to pay wages, we sold them instead, without even asking the inhabitants (...) That really wasn't a decent thing to do. We could at least have called a referendum, and asked people which nation they wanted to belong to. Instead we just let down the people.""[449]: 69 
 On 30 January 2006, Jacques Chirac (the then French President) said that 10 May would henceforth be a national day of remembrance for the victims of slavery in France, marking the day in 2001 when France passed a law recognising slavery as a crime against humanity.[450]
 President Jerry Rawlings of Ghana apologized for his country's involvement in the slave trade.[2]
 At a UN conference on the Atlantic slave trade in 2001, the Dutch Minister for Urban Policy and Integration of Ethnic Minorities Roger van Boxtel said that the Netherlands ""recognizes the grave injustices of the past."" On 1 July 2013, at the 150th anniversary of the abolition of slavery in the Dutch West Indies, the Dutch government expressed ""deep regret and remorse"" for the involvement of the Netherlands in the Atlantic slave trade. The municipal government of Amsterdam, which co-owned the colony of Surinam, and De Nederlandsche Bank, which was involved in slavery between 1814 and 1863, apologized for their involvement on 1 July 2021 and 1 July 2022, respectively.[451][452]
 A formal apology on behalf of the Dutch government was issued by Prime Minister Mark Rutte in 2022 following a review by an advisory committee. Government ministers were sent to seven former colonies to reiterate the Dutch state's formal apology. Some activists continued to call for Willem-Alexander of the Netherlands to issue an apology.[453][454]
 In 2009, the Civil Rights Congress of Nigeria wrote an open letter to all African chieftains who participated in trade calling for an apology for their role in the Atlantic slave trade: ""We cannot continue to blame the white men, as Africans, particularly the traditional rulers, are not blameless. In view of the fact that the Americans and Europe have accepted the cruelty of their roles and have forcefully apologized, it would be logical, reasonable and humbling if African traditional rulers ... [can] accept blame and formally apologize to the descendants of the victims of their collaborative and exploitative slave trade.""[455]
 On 9 December 1999, Liverpool City Council passed a formal motion apologising for the city's part in the slave trade. It was unanimously agreed that Liverpool acknowledges its responsibility for its involvement in three centuries of the slave trade. The city council has made an unreserved apology for Liverpool's involvement and the continual effect of slavery on Liverpool's black communities.[456]
 On 27 November 2006, British Prime Minister Tony Blair made a partial apology for Britain's role in the African slavery trade. However African rights activists denounced it as ""empty rhetoric"" that failed to address the issue properly. They feel his apology stopped shy to prevent any legal retort.[457] Blair again apologized on 14 March 2007.[458]
 On 24 August 2007, Ken Livingstone (Mayor of London) apologized publicly for London's role in the slave trade. ""You can look across there to see the institutions that still have the benefit of the wealth they created from slavery,"" he said, pointing towards the financial district, before breaking down in tears. He said that London was still tainted by the horrors of slavery. Jesse Jackson praised Mayor Livingstone and added that reparations should be made.[459]
 In 2020, the Bank of England apologized for the role of directors in the Atlantic slave trade and pledged to remove pictures and statues of the 25 bank leaders who owned or traded in slavery.[460][461]
 On 24 February 2007, the Virginia General Assembly passed House Joint Resolution Number 728[462] acknowledging ""with profound regret the involuntary servitude of Africans and the exploitation of Native Americans, and call for reconciliation among all Virginians"". With the passing of that resolution, Virginia became the first of the 50 United States to acknowledge through the state's governing body their state's involvement in slavery. The passing of this resolution came on the heels of the 400th-anniversary celebration of the city of Jamestown, Virginia, which was the first permanent English colony to survive in what would become the United States. Jamestown is also recognized as one of the first slave ports of the American colonies. On 31 May 2007, the Governor of Alabama, Bob Riley, signed a resolution expressing ""profound regret"" for Alabama's role in slavery and apologizing for slavery's wrongs and lingering effects. Alabama is the fourth state to pass a slavery apology, following votes by the legislatures in Maryland, Virginia, and North Carolina.[463]
 On 30 July 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. The language included a reference to the ""fundamental injustice, cruelty, brutality and inhumanity of slavery and Jim Crow"" segregation.[464] On 18 June 2009, the United States Senate issued an apologetic statement decrying the ""fundamental injustice, cruelty, brutality, and inhumanity of slavery"". The news was welcomed by President Barack Obama.[465]
"
Virginia Conventions,https://en.wikipedia.org/wiki/Virginia_Conventions,"
 The Virginia Conventions were assemblies of delegates elected for the purpose of establishing constitutions of fundamental law for the Commonwealth of Virginia superior to General Assembly legislation. Their constitutions and subsequent amendments span four centuries across the territory of modern-day Virginia, West Virginia and Kentucky.
 The first Virginia Conventions held during and just before the American Revolutionary War replaced the British colonial government on the authority of ""the people"" until the initiation of state government under the 1776 Constitution. Subsequent to joining the union of the United States in 1788, Virginia's five unlimited state constitutional conventions took place in 1829–30, 1850, around the time of the Civil War in 1864, 1868, and finally in 1902. These early conventions without restrictions on their jurisdiction were primarily concerned with voting rights and representation in the General Assembly. The Conventions of 1861 on the eve of the American Civil War were called in Richmond for secession and in Wheeling for government loyal to the U.S. Constitution.
 In the 20th century, limited state Conventions were used in 1945 to expand suffrage to members of the armed forces in wartime, and in 1955 to implement ""massive resistance"" to Supreme Court attempts to desegregate public schools. Alternatives to the conventions used commissions for constitutional reform in 1927 for restructuring state government and in 1969 to conform the state constitution with congressional statutes of the Voting Rights Act and U.S. Constitutional law. Each of these 20th century recommendations was placed before the people for ratification in a referendum.
 The First Convention was organized after Lord Dunmore, the colony's royal governor, dissolved the House of Burgesses when that body called for a day of prayer as a show of solidarity with Boston, Massachusetts, when the British government closed the harbor under the Boston Port Act. The Burgesses, who had been elected by propertied freeholders throughout the colony, moved to Raleigh Tavern to continue meeting. The Burgesses declared support for Massachusetts and called for a congress of all the colonies, the Continental Congress. The Burgesses, convened as the First Convention, met on August 1, 1774, and elected officers, banned commerce and payment of debts with Britain, and pledged supplies.  They elected Peyton Randolph, the Speaker of the House of Burgesses, as the President of the convention (a position he held for subsequent conventions until his death in October 1775).[2]
 The Second Convention met in Richmond at St. John's Episcopal Church on March 20, 1775. Delegates again chose a presiding officer and they elected delegates to the Continental Congress. At the convention, Patrick Henry proposed arming the Virginia militia and delivered his ""Give me liberty or give me death!"" speech to rally support for the measure. It was resolved that the colony be ""put into a posture of defence: and that Patrick Henry, Richard Henry Lee, Robert Carter Nicholas, Benjamin Harrison, Lemuel Riddick, George Washington, Adam Stephen, Andrew Lewis, William Christian, Edmund Pendleton, Thomas Jefferson and Isaac Zane, Esquires, be a committee to prepare a plan for the embodying arming and disciplining such a number of men as may be sufficient for that purpose.""[3]
 Between conventions in April 1775, Randolph, who was both the Speaker of the House of Burgesses and President of the Virginia Conventions, negotiated with Lord Dunmore for gunpowder removed from the Williamsburg arsenal to HMS Magdalen during the Gunpowder Incident, which was a confrontation between the Governor's forces and Virginia militia, led by Patrick Henry. The House of Burgesses was called back by Lord Dunmore one last time in June 1775 to address British Prime Minister Lord North's Conciliatory Resolution. Randolph, who was a delegate to the Continental Congress, returned to Williamsburg to take his place as Speaker. Randolph indicated that the resolution had not been sent to the Congress (it had instead been sent to each colony individually in an attempt to divide them and bypass the Continental Congress). The House of Burgesses rejected the proposal, which was also later rejected by the Continental Congress.[4]
 The Third Convention met on July 17, 1775, also at St. John's Church, after Lord Dunmore had fled the capital (following the rejection of North's resolution) and taken refuge on a British warship. Peyton Randolph continued to serve as the President of the convention.[5] The convention created a Committee of Safety to govern as an executive body in the absence of the royal governor (Dunmore). Members of the committee were Edmund Pendleton, George Mason, John Page, Richard Bland, Thomas Ludwell Lee, Paul Carrington, Dudley Digges, William Cabell, Carter Braxton, James Mercer, and John Tabb.[6] The convention also divided Virginia into 16 military districts and resolved to raise regular regiments. The convention ended August 26, 1775, while the Committee of Safety would continue to meet and govern between Convention sessions.[7]
 On November 7, 1775 in Dunmore's Proclamation, Lord Dunmore instituted martial law and offered freedom to enslaved persons who joined the British army.[8] The Royal Ethiopian Regiment was organized.
 The Fourth Convention in Williamsburg met in December 1775 following November's declaration that the colony was in revolt by Lord Dunmore and fighting between his royal forces and militia forces in the Hampton Roads area.[9] Edmund Pendleton served as President of the convention, succeeding Peyton Randolph who had died in October 1775. The Convention declared that Virginians were ready to defend themselves ""against every species of despotism."" The convention passed another ordinance to raise additional troops.[10]
 Back in Britain, in December 1775, the King's Proclamation of Rebellion had declared the colonies outside his protection,[11] but throughout the first four Virginia Conventions, there was no adopted expression in favor of independence from the British Empire.[12]
 By the new year of 1776, George Washington, a delegate in the Virginia Convention and in the Continental Congress, had been appointed in Philadelphia from the First Continental Congress as commander of Continental troops surrounding Boston. Virginia patriots had defeated an advancing British force at the Battle of Great Bridge southeast of Norfolk in December.[13]
 The newly elected Fifth Convention met in Williamsburg from May 6 to July 5, 1776. It elected Edmund Pendleton its presiding officer after his return as president of the First Continental Congress in Philadelphia. There were three parties in the Fifth Convention. The first was mainly made up of wealthy planters, including Robert Carter Nicholas Sr.[14] The second party was made up of the more intellectual types. These included the older generation of George Mason, George Wythe, Edmund Pendleton, and the younger Thomas Jefferson and James Madison.[15] The third party was a minority of young men mainly from western Virginia. This party was led by Patrick Henry and included ""radicals"" who had supported independence earlier than 1775.[16]
 On May 15, the Convention declared that the government of Virginia as ""formerly exercised"" by King George in Parliament was ""totally dissolved"".[17] The Convention adopted a set of three resolutions:  one calling for a declaration of rights for Virginia, one calling for the establishment of a republican constitution, and a third calling for federal relations with whichever other colonies would have them and alliances with whichever foreign countries would have them. It also instructed its delegates to the Continental Congress in Philadelphia to declare independence.[18]
 On June 7, Richard Henry Lee, one of Virginia's delegates to Congress, carried out the instructions to propose independence in the language the convention had commanded him to use: that ""these colonies are, and of right ought to be, free and independent states."" The resolution was followed in Congress by the adoption of the American Declaration of Independence, which reflected its ideas.[19]
 The convention amended, and on June 12 adopted, George Mason's Declaration of Rights, a precursor to the United States Bill of Rights. On June 29, the convention approved the first Constitution of Virginia. The convention chose Patrick Henry as the first governor of the new Commonwealth of Virginia, and he was inaugurated on June 29, 1776. Thus, Virginia had a functioning republican constitution before July 4, 1776.[20]
 The Constitutional Convention convened by the Articles of Confederation Congress in 1787 provided for a ratification process in the states that was duly transmitted by Congress to each state. As Virginians went to the polls to elect delegates to its state convention, six states had ratified including the two other largest states of Pennsylvania and Massachusetts. But Virginia bisected the new nation from the Atlantic Ocean to the Mississippi River; its admission into the prospective union was critical if the United States as a nation-state were to have contiguous continental territory.[21]
 The Convention met from June 2–27, 1788, in the wooden ""Old Capitol"" building at Richmond VA, and elected Edmund Pendleton its presiding officer.[22] The Virginia Ratifying Convention narrowly approved joining the proposed United States under a constitution of supreme national law as authorized by ""We, the people"" of the United States. James Madison led those in favor, Patrick Henry, delegate to the First Continental Convention and Revolutionary wartime governor, led those opposed. Governor Edmund Randolph, who had refused to sign the U.S. Constitution, now chose to support adoption for the sake of national unity. George Mason who had refused to sign the U.S. Constitution due to the lack of a Bill of Rights continued in his opposition.[23] The Virginia ratification included a recommendation for a Bill of Rights, and Madison subsequently led the First Congress to send the Bill of Rights to the states for ratification.[24]
 Patrick Henry questioned the authority of the Philadelphia Convention to presume to speak for ""We, the people"" instead of ""We, the states"". In his view, delegates should have only recommended amendments to the Articles of Confederation. Edmund Randolph had changed from his opposition in the Philadelphia Convention to now supporting adoption for the sake of preserving the Union. He noted that the Confederation was ""totally inadequate"".[26] George Mason countered that a national, consolidated government would overburden Virginians with direct taxes in addition to state taxes, and that government of an extensive territory must necessarily destroy liberty.[27] Madison pointed out that the history of Confederations like that provided in the Articles of Confederation government were inadequate in the long run, both with the ancients and with the modern (1700s) Germans, Dutch and Swiss. They brought ""anarchy and confusion"", disharmony and foreign invasion. Efficient government can only come from direct operation on individuals, it can never flow from negotiations among a confederation's constituent states.[28]
 The Virginia Ratification (Federal) Convention narrowly ratified the U.S. Constitution 89 to 79. Virginians reserved the right to withdraw from the new government as ""the People of the United States"", ""whenever the powers granted unto it should be perverted to their injury or oppression,"" but it also held that failings in the constitution should be remedied by amendment.[29] Unlike the Pennsylvania Convention where the Federalists railroaded the Anti-federalists in an all or nothing choice, in the Virginia Convention the Federalists made efforts to reconcile with the Anti-federalists by recommending amendments to the Federal Constitution like that of Virginia's Bill of Rights preamble to its 1776 Constitution.[30]
 Almost immediately, the Constitution of 1776 was recognized as flawed both for its restriction of the suffrage by property requirements, and for its malapportionment favoring the smaller eastern counties. Between 1801 and 1813, petitioners called on the Assembly to initiate a constitutional convention ten times.[31] Malapportionment in the Assembly was seen by reformers as ""an usurpation of the minority over the majority"" by the slave-owning eastern aristocracy. Partisans argued for apportionment by white population, versus ""federal numbers"" combining white population with three-fifths slaves, versus the existing system counting whites and slaves equally to favor the slave-holding eastern counties.[32]
 The Convention met from October 5, 1829 – January 15, 1830, and elected Philip Pendleton Barbour its presiding officer. The last ""gathering of giants""[33] from the Revolutionary generation included former presidents James Madison and James Monroe, and sitting Chief Justice John Marshall. But three generations were represented among those who would serve in public office including three presidents, seven U.S. senators, fifteen U.S. representatives and four governors. The other delegates to the convention were sitting judges or members of the Virginia General Assembly.[34]
 Conservatives among the Old Republicans such as John Randolph of Roanoke feared any change from the Founders' 1776 Constitution would lead to an ideological anarchy of ""wild abstractions"" imposed by egalitarian ""French Jacobins"" through ""this maggot of innovation"". In answer, John Marshall advanced his view with a petition from the freeholders of Richmond which observed that, ""Virtue, intelligence, are not among the products of the soil. Attachment to [slave] property, often a sordid sentiment, is not to be confounded with the sacred flame of patriots."" Any white male who had served in the War of 1812 or who would serve in the militia in their future defense of the country deserved the right to vote.[35]
 Reformers' efforts to adopt direct popular election of the governor were defeated in favor of continuing election by the General Assembly.[36] Thomas Jefferson Randolph, Thomas Jefferson's grandson, proposed gradual emancipation, a suggestion which never made it out of committee onto the convention floor.[37] The reformers lost on almost every issue. Nevertheless, even with the exaggerated Virginia Senate representation apportioning the delegates, the three most important roll calls were close.  The ""white"" population basis of apportioning the General Assembly failed by two votes. The extension of the vote to all free white males failed by two votes. When the popular election of governor passed on its first vote, it failed on reconsideration. The divisions which would lead to West Virginia's split were evident. Regardless of the various ideologies represented or delegate political affiliation, the final vote 55 for the proposed constitution to 40 against was along an east–west divide. Only one delegate voted yes from west of the Blue Ridge Mountains.[38]
 Following the 1830 Constitution, Virginia began to change politically under the pressure of party competition. Though the planter elite and their representatives in the ruling Democratic ""Richmond Junto"" continued to resist any change, western Democrats and Whigs were more inclined to a white population basis for apportionment in their determination to expand suffrage and to find a more equitable representation between east and west.[39]
 The Convention delegates were a younger generation raised in the Second American Party System of Democrat Jefferson Davis and Whig Henry Clay. Unlike the three generation Convention of 1829–30, the delegates were primarily in their twenties and thirties at the beginning of their careers in the professions and industry, without large land holdings, and without gentry family ties.[40]
 The Convention met from October 14, 1850 – August 1, 1851, and elected John Y. Mason its presiding officer. The Convention featured fierce debates; the arguments raged throughout Virginia in the press and they were widely reported nationally. Direct popular election of the governor was supported by Whig Congressman John Minor Botts. He was opposed by Richmond Junto Democrat Richard L.T. Beale who argued against the natural equality of all men, and the ""plundering propensities"" of the multitude seeking a ""majority of mere numbers"".[41] Although he was for direct election of the Governor, Henry A. Wise was more fearful of the eastern slave-holders' loss of control in the General Assembly. He believed that ""protection of slavery, not the liberalizing of Virginia's Constitution, was the most significant business before the convention.""[42]
 After almost six months of wrangling, the question of apportionment was brought up for a vote. The compromise was to apportion the House of Delegates on the white population basis, giving the western counties a majority, but for the Senate to be apportioned on a modified mixed basis of population and property including slaves, giving the eastern counties a majority. In the remaining two months of the convention, it was agreed to allow direct popular election of the governor, but each office holder would be limited to one term. Constitutional provision for public education was voted down.[43] Voting by secret ballot was rejected, perpetuating viva voce voting.[44]
 Abraham Lincoln's constitutional election reflected the nation's sectional divide, though 82 percent of the electorate had split among the Unionists, Lincoln, Stephen A. Douglas and John Bell. Even before Lincoln's inauguration, the Deep South states that had cast Electoral College votes for John C. Breckinridge resolved to secede from the United States and form the Confederate States of America. The Virginia Assembly called a special convention for the sole purpose of considering secession from the United States. Virginia was deeply divided, returning a convention of delegates amounting to about one-third for secession and two thirds Unionist. But the Unionists would prove to be further divided between those who would be labelled Conditional Unionists who would favor Virginia in the Union only if Lincoln made no move at ""coercion"", and those who would later be called Unconditional Unionists who would be unwavering in their loyalty to the constitutional government of the United States.[45]
 The Convention met from February 3 – December 6, 1861, and elected John Janney its presiding officer. The majority at first voted to remain in the Union, but stayed in session awaiting events. At first, the speeches were mixed between Secessionists advocating leaving the Union, Conditional Unionists holding onto the patriotism of earlier times, and Unconditional Unionists insisting that secession was bad policy and unlawful.[46] On March 4, Abraham Lincoln's inauguration day, Jefferson Davis called up 100,000 militia to serve a year and sent besieging troops to surround Fort Sumter in South Carolina and Fort Pickens in Florida. That same day Waitman T. Willey from trans-Alleghany Monongalia County answered secessionists with a Unionist speech. He defended Virginia's institutions from Northern attacks against slavery, but ""there is no constitutional right of secession ..."" He warned that secession would bring about war, taxes and the abolition of slavery in Virginia.[47]
 John S. Barbour Jr. of the Piedmont's Culpeper County was the first Unionist to break away into the secessionist camp. While ""resolutely protecting slave labor"" he was for encouraging manufacturing and commercial interests in Virginia against those of the North. He asked what would do more to promote Virginia's growth, participation ""in a hostile confederacy in which your [legislative] power will be but 11 out of 150 [with he North], or in a friendly confederacy where it will be 21 out of 89 [with the South]?"" In the South was a government to join ""in full working order, strong, powerful and efficient ..."" Henry A. Wise tried to move the Convention into a ""Spontaneous Southern Rights Convention"" to immediately install a secessionist government in Virginia, but on April 4, almost two-thirds of the Convention voted against secession, and a three-man delegation was sent to consult with Lincoln who had resolved to protect Federal property in the South.[48]
 With the fall of Fort Sumter, Lincoln matched Jefferson Davis' call up of 100,000 men for a year with a call for 75,000 for three months, including 3,500 Virginians to restore Federal property taken in the South by force.[49] But the Unionist bloc lost its Conditional Unionist faction with Lincoln's requisition of troops.[50] The next day, former Governor Henry Wise announced that he had set the ""wheels of revolution"" against the U.S. Government in motion with loyal Virginians seizing both the federal Harper's Ferry Armory and the Gosport Navy Yard at Norfolk. His exhortation resulted in a resolution to secede with a vote 88 for, 55 against.[51]
 The Virginia Secession Ordinance was to ""repeal the ratification of the Constitution of the United States of America, by the State of Virginia.""[52] Two days after the secession resolution and a month before the referendum, the Confederate flag was raised over Virginia's capitol building, a delegation was sent to vote in the Confederate Congress, state militias were activated and a Confederate army was invited to occupy Richmond. Though the ballots from Unionist counties were lost, the total referendum votes counted numbered more than that of the 1860 presidential election by including men voting viva voce aloud in Confederate army camps, approving secession by 128,884 to 32,134.[53]
 Virginia's second Convention of 1861 was a Unionist response to the secessionist movement in Virginia. The First Wheeling Convention meeting at Wheeling, Virginia (now West Virginia), sat on May 13–15. It called for elections to another meeting if Virginia's Ordinance of Secession were to pass referendum. After the vote was taken on May 23, the First Session of the Second Wheeling Convention met from June 11 June 25 to establish the Restored Government of Virginia, electing Arthur I. Boreman its presiding officer.[54] The Second Session of the Second Wheeling Convention met from August 6 to August 21 to call for a new state from the territory of Virginia to be named Kanawha.[55]
 The Second Wheeling Convention included 32 western counties, Alexandria and Fairfax County.[56] Twenty-nine of the convention delegates were members of the Virginia General Assembly as state Delegates or state Senators, such as John J.Davis of Harrison County and Lewis Ruffner of Kanawha County.[57]
 John S. Carlile who had represented transmontane Harrison County as an Unconditional Unionist at the Richmond Secession Convention, was the floor leader at the Second Wheeling Convention who shepherded in the creation of the Restored Virginia Government. On June 14 he expanded on his view of state and federal relations, "" the people of Virginia in establishing government for themselves deemed it best to create two agents. The Federal Government is one, and the State Government is the other ...""  Referencing Article VI of the U.S. Constitution, Carlile observed, ""Any act done or performed by the State agent in conflict with the powers conferred upon the Federal agent is to be null and void ... [the Constitution] provides for its own alteration, amendment or change ... But [the right of secession] was never intended ...""[58]
 On June 17, Carlile attacked the rebellion as treason, he then accounted events at the Richmond Secession Convention in which he had been an Unconditional Unionist. ""For several days before the Convention passed the Ordinance of Secession, it was absolutely besieged; members were threatened with being hung to the lamp posts; their lives were jeopardized; the mob was marching up and down the streets, and surrounding the Capitol, and everything was terror and dismay.""[59] Carlile continued to impeach the legitimacy of Virginia's referendum on secession.[60]
 Republican Francis H. Pierpont of Marion County was elected by the convention as Governor of the Restored Government of Virginia which was recognized by the Lincoln Administration. Unlike in Kentucky and Missouri, the Union armies were unable to reclaim most of the eastern Virginia counties for incorporation into the Restored Government by 1863, and West Virginia was made into its own state.[61]
 Following the creation of West Virginia, the remnant of Restored Virginian government held a Convention of delegates from a few periphery counties occupied by Union forces. The Convention met in Alexandria's U.S. District Court Room from February 13 – April 11, 1864, and elected LeRoy G. Edwards, a slaveholder with three sons in the Confederacy as its presiding officer. The Convention sought direction from President Lincoln whether the General Government would sustain the civil authority, or ""whether the civil is to become, as it is now, subordinate to the military"", so that the convened delegates could support the Administration's war effort in the midst of Grant's Wilderness Campaign. Debate ensued over whether to seek to disenfranchise all supporters of the rebellion, but with an eye to governing after cessation of hostilities, it limited disenfranchisement only to those who had held office in rebel state or Confederate governments.[62]
 After debating whether abolition of slavery would best proceed gradually or at once, with compensation to loyal Union men or without, the Convention resolved to abolish slavery immediately without compensation on April 10, 1864. It abolished the viva voce voting and called for secret balloting for electing state officials. Immediately on its proclamation, the Constitution of 1864 was enforceable only in areas under Union control, but it would serve as Virginia's fundamental law until the Constitution of 1870 went into force.[63]
 A fifty-two-page journal of convention proceedings was published, but the debates were not formally recorded.  W. J. Cowing, secretary of the convention and editor of the pro-Union Alexandria, Virginia State Journal, featured accounts of some debate, but only one edition of the Journal survives covering the convention.[64]
 During Congressional Reconstruction, U.S. General John Schofield administered Virginia as Military District One. After 1866, according to the Radical Reconstruction Acts, a rebelling state which had vacated its delegation in the U.S. Congress was required to incorporate the 14th Amendment into its state constitution before it was allowed to participate again.[65] By the time Schofield called a new state constitutional convention for 1868, three distinct parties had coalesced in Virginia. Radical Republicans, including most ex-slave freedmen, organized to advocate full political and social equality for blacks, but they wanted to exclude ex-Confederates from political participation either in government or at the ballot box. Moderate Unionists including many pre-war Whigs, sought political equality for blacks, but believed that ex-Confederates had to be included in the political community because of their majority in the white population. Conservatives wanted to ensure white control of the state without Radical influence on issues such as public education.[66]
 The convention met from December 3, 1867, to April 17, 1868, at Richmond in the Capitol Building, and elected John C. Underwood its presiding officer. A convention of enfranchised Unionists, freedmen and ex-Confederates was dominated by Radical Republicans. The convention proposed two ""obnoxious clauses"" that provoked widespread opposition, meant to restrict suffrage among ex-Confederates. Negotiations with President Grant resulted in separating the two more controversial proposals, and the remaining constitution was ratified by referendum. It provided for  the vote for African-Americans and public education.[67]
 The convention concerned itself with federal-state relations, with the convention's Committee on the Preamble and Bill of Rights initially stating that, """"the General Government of the United States is paramount to that of an individual state, except as to rights guaranteed to each State by the Constitution of the United States."" But Jacob N. Liggett of Rockingham County voiced the ex-Confederate doctrine that ""the Federal Government is the creature of the acts of the States."" Christopher Y. Thomas of Henry County proposed a compromise, to simply assert Article VI of the U.S. Constitution for Virginia's Bill of Rights, Section 2, that ""the Constitution of the United States, and the laws of Congress passed in pursuance thereof, constitute the supreme law of the land, to which paramount allegiance and obedience are due from every citizen ..."" That was not enough for the Radical majority. Linus M. Nickerson of Fairfax County who had served in a New York infantry regiment successfully added ""this State shall ever remain a member of the United States of America ... and that all attempts from whatever source, or upon whatever pretext, to dissolve said Union ... are unauthorized, and ought to be resisted with the whole power of the State.""[68]
 Radicals in the convention, against the protests of General Schofield, were able to martial an uncompromised majority in their desire to disenfranchise the white ex-Confederate majority in the state. Instead of the moderate Republican position limiting voter restrictions to former U.S. officeholders who had supported rebellion, they sought to guarantee a future government of Union men only. The convention wrote two ""obnoxious clauses"" as they were widely known, that went beyond federal requirements to deny the vote to any office holder in rebel government and an ""iron-clad oath"" testifying that a prospective voter had never ""voluntarily borne arms against the United States.""[69] Following the convention, General Schofield successfully negotiated with President Ulysses S. Grant to propose the referendum on the Radical ""Underwood"" Constitution, but separating its two disenfranchisement ""obnoxious clauses"", allowing voters to decide on them apart from the Constitution.[70] While the referendum on the main body of the Constitution was overwhelmingly approved, the two ""obnoxious clauses"" were defeated by a narrower margin.[71]
 After the end of Reconstruction in the 1870s, Virginia and other states of the former Confederacy restricted the suffrage by segregationist Jim Crow laws. By 1890 Southern states began to hold conventions that constitutionally removed large numbers of whites and most blacks from voter registration. Reformers among the Progressive Democrats seeking to expand the influence of the ""better sort"" of voters gained a majority by appealing to the electorate to overthrow the 1868 Underwood Constitution, which the Richmond Dispatch characterized as ""that miserable apology to organic law which was forced upon Virginians by carpetbaggers, scalawags and Negroes supported by Federal bayonets"".[72]
 In May 1900, the increasing public dismay over the electoral fraud and corruption of the Democratic political machine under the control of U.S. Senator Thomas S. Martin manipulating poor white and black voters led to a narrow victory over his entrenched ""court house crowd"" in a referendum to call a constitutional convention.[73]
 The convention met from June 12, 1901, to June 26, 1902, at Richmond in the Capitol Building and elected John Goode Jr. its presiding officer, a former delegate to the 1861 Secessionist Convention. Progressives sought to reform corrupt political practices of the ruling Martin machine and to regulate railroads and big corporations. Martin delegates agreed to restrict suffrage of African-Americans and illiterate whites, and a State Corporation Commission was established.[74] When the railroads challenged the State Corporation Commission's constitutionality on the grounds it violated separation of powers, the commission was upheld in the Virginia Supreme Court of Appeals.[75]
 The convention imposed a system of poll taxes along with literacy and understanding requirements to vote that had the effect of restricting the electorate. The outcome was almost immediate disenfranchising of blacks and half the previous number of whites voting.[76]
 Following the unlimited Convention of 1901–02, twentieth century constitutional activity turned to a mixture of Governor-appointed constitutional commissions in 1927 and 1968, and limited constitutional conventions called by the General Assembly for very specific purpose. After the century's earliest convention disenfranchising voters in a constitution that was proclaimed, each modification of the Virginia Constitution has been sent to the voters for referendum approval. Virginia's women suffrage movement was unsuccessful until the national ratification of the Twenty-first Amendment, and the General Assembly did not ratify until 1952, but women could vote beginning in 1920.[77]
 The Commission met from July 7, 1926 – February 16, 1927, and Virginia Chief Justice Robert R. Prentis was appointed its chair. Governor Harry F. Byrd Sr., the successor boss of the Democratic Organization in Virginia, sought and gained governmental reform streamlining local government and increasing the power of the governor over the executive, as well as implementing constitutional restrictions on the General Assembly's ability to incur debt.[78]
 The convention met on October 25, 1933, and adjourned that day. It elected C. O'Connor Goolrick as its presiding officer. To answer Congressional legislation, it ratified the 21st Amendment repealing the 18th Amendment so as to allow the sale of alcoholic beverages. The 21st Amendment is the only amendment that required state convention ratification as of that time. The Amendment was ratified nationally by three-fourths of the states on December 5, 1933.[79]
 The convention met from April 30 – May 1, 2, 22, 1945, and elected John J. Wicker Jr., its presiding officer. During World War II, Virginia held a constitutional convention called for the limited purpose of expanding the franchise to members of the armed forces during wartime. Efforts by some delegates to expand the scope of the convention to reduce the voting age below 21 failed.[80]
 The convention met from March 5–7, 1956, and elected John C. Parker its presiding officer. When the Supreme Court ruled segregated public schools unconstitutional, proponents of ""massive resistance"" to racial integration in schools secured a limited constitutional convention for the purpose of state financing of non-sectarian private schools, resulting in segregation academies supported by public funds.[81]
 The Commission met from April 1968 – January 1, 1969, and former Governor Albertis S. Harrison Jr. was appointed its chair. After seven decades since the previous unlimited convention, a constitutional commission was called by Governor Mills E. Godwin Jr. to consolidate piecemeal amendments and to conform with U.S. statutory and constitutional law, especially in the areas of education, voter rights and representation in Congressional and General Assembly districts.[82]
 After the Convention of 1901–02, the General Assembly did not call another general convention in the twentieth century. Two proposals for constitutional amendment since the 1960s that might have been passed by the General Assembly and sent to the voters for ratification referendum have failed to be enacted, but both remain current topics of periodic political discussion. Virginia remains the only state to ban governors serving consecutive terms, and it is only one of two states still selecting both trial and appellate judges by the state legislature.[83]
 Since 1971, additional piecemeal amendments have been added in response to federal developments. Amendments ratified by the voters reduced the voting age to eighteen to conform with the Twenty-sixth Amendment, removed residency requirements for voting, and conformed voter registration to the Motor Voter Act. A legislative session now may be called after a Governor's veto. Virginia joined thirty-two other states in 1996 by amending its Constitution to provide for rights of victims of crime. Since 1996 Virginia and other states have adopted a provision protecting the right of the people to hunt, fish and harvest game.[84]
 In 2006, Virginians aligned with twenty-nine other states seeking to ban homosexual marriage by constitutional amendment. The amendment limited marriage to ""unions between one man and one woman"".[85] This Virginian constitutional provision ran afoul of the U.S. Supreme Court's interpretation of the Fourteenth Amendment in both its due process and equal protections clauses in Obergefell v. Hodges (2015).
 1774–76 
 1788 
 1829–30
 1850 
 1861 
 1868 
 1901–02 
"
First Continental Congress,https://en.wikipedia.org/wiki/First_Continental_Congress,"

 The First Continental Congress was a meeting of delegates of 12 of the Thirteen Colonies held from September 5 to October 26, 1774, at Carpenters' Hall in Philadelphia at the beginning of the American Revolution. The meeting was organized by the delegates after the British Navy implemented a blockade of Boston Harbor and the Parliament of Great Britain passed the punitive Intolerable Acts in response to the Boston Tea Party.[1]
 During the opening weeks of the Congress, the delegates conducted a spirited discussion about how the colonies could collectively respond to the British government's coercive actions, and they worked to make a common cause. As a prelude to its decisions, the Congress's first action was the adoption of the Suffolk Resolves, a measure drawn up by several counties in Massachusetts that included a declaration of grievances, called for a trade boycott of British goods, and urged each colony to set up and train its own militia. A less radical plan was then proposed to create a Union of Great Britain and the Colonies, but the delegates tabled the measure and later struck it from the record of their proceedings.
 The First Continental Congress agreed on a Declaration and Resolves that included the Continental Association, a proposal for an embargo on British trade. They also drew up a Petition to the King pleading for redress of their grievances and repeal of the Intolerable Acts. That appeal was unsuccessful, leading delegates from the colonies to convene the Second Continental Congress, also held in Philadelphia, the following May, shortly after the Battles of Lexington and Concord, to organize the defense of the colonies as the American Revolutionary War.
 The Congress met from September 5 to October 26, 1774, in Carpenters' Hall in Philadelphia with delegates from 12 of the Thirteen Colonies participating. The delegates were elected by the people of the respective colonies, the colonial legislature, or by the Committee of Correspondence of a colony.[2] Loyalist sentiments outweighed Patriot views in Georgia, leading that colony to not immediately join the revolutionary cause until the following year when it sent delegates to the Second Continental Congress.[3]
 Peyton Randolph was elected as president of the Congress on the opening day, and he served through October 22 when ill health forced him to retire, and Henry Middleton was elected in his place for the balance of the session. Charles Thomson, leader of the Philadelphia Committee of Correspondence, was selected as the congressional secretary.[4] The rules adopted by the delegates were designed to guard the equality of participants and to promote free-flowing debate.[2]
 As the deliberations progressed, it became clear that those in attendance were not of one mind concerning why they were there. Conservatives such as Joseph Galloway, John Dickinson, John Jay, and Edward Rutledge believed their task to be forging policies to pressure Parliament to rescind its unreasonable acts. Their ultimate goal was to develop a reasonable solution to the difficulties and bring about reconciliation between the Colonies and Great Britain.  Others such as Patrick Henry, Roger Sherman, Samuel Adams, and John Adams believed their task to be developing a decisive statement of the rights and liberties of the Colonies.  Their ultimate goal was to end what they felt to be the abuses of parliamentary authority and to retain their rights, which had been guaranteed under Colonial charters and the English constitution.[5]
 Roger Sherman denied the legislative authority of Parliament, and Patrick Henry believed that the Congress needed to develop a completely new system of government, independent from Great Britain, for the existing Colonial governments were already dissolved.[6] In contrast to these ideas, Joseph Galloway put forward a ""Plan of Union"" which suggested that an American legislative body should be formed with some authority, whose consent would be required for imperial measures.[6][7]
 In the end, the voices of compromise carried the day. Rather than calling for independence, the First Continental Congress passed and signed the Continental Association in its Declaration and Resolves, which called for a boycott of British goods to take effect in December 1774. After Congress signed on October 20, 1774, embracing non exportation they also planned nonimportation of slaves beginning December 1, which would have abolished the slave trade in the United States of America 33 years before it actually ended.[8]
 The primary accomplishment of the First Continental Congress was a compact among the colonies to boycott British goods beginning on December 1, 1774, unless parliament should rescind the Intolerable Acts.[9] While delegates convened in the First Continental Congress, fifty-one women in Edenton, North Carolina formed their own association (now referred to as the Edenton Tea Party) in response to the Intolerable Acts that focused on producing goods for the colonies.[10] Additionally, Great Britain's colonies in the West Indies were threatened with a boycott unless they agreed to non-importation of British goods.[11] Imports from Britain dropped by 97 percent in 1775, compared with the previous year.[9] Committees of observation and inspection were to be formed in each Colony to ensure compliance with the boycott. It was further agreed that if the Intolerable Acts were not repealed, the colonies would also cease exports to Britain after September 10, 1775.[9]
 The Houses of Assembly of each participating colony approved the proceedings of the Congress, with the exception of New York.[12] The boycott was successfully implemented, but its potential for altering British colonial policy was cut off by the outbreak of hostilities in April 1775.
 Congress also voted to meet again the following year if their grievances were not addressed satisfactorily. Anticipating that there would be cause to convene a second congress, delegates resolved to send letters of invitation to those colonies that had not joined them in Philadelphia, including Quebec, Saint John's Island (now Prince Edward Island), Nova Scotia, Georgia, East Florida, and West Florida.[13] Of these, only Georgia would ultimately send delegates to the next Congress.
"
Continental Association,https://en.wikipedia.org/wiki/Continental_Association,"The Continental Association, also known as the Articles of Association or simply the Association, was an agreement among the American colonies adopted by the First Continental Congress in Philadelphia on October 20, 1774. It was a result of the escalating American Revolution and called for a trade boycott against British merchants by the colonies. Congress hoped that placing economic sanctions on British imports and exports would pressure Parliament into addressing the colonies' grievances, especially repealing the Intolerable Acts, which were strongly opposed by the colonies.[1]
 The Congress adopted a ""non-importation, non-consumption, non-exportation"" agreement as a peaceful means of settling the colonies' disputes with Great Britain. The agreement, which had been suggested by Virginia delegate Richard Henry Lee[2] based on the 1769 Virginia Association initiated by George Washington and written by George Mason, opened with a pledge of loyalty to King George III of Britain, and went on to outline a series of actions opening with a ban on British imports that would begin December 1, 1774. Trade between the colonies and Britain subsequently fell sharply. The British soon responded with the New England Restraining Act which escalated their own economic sanctions. The outbreak of the American Revolutionary War in April 1775 superseded the need to boycott British goods.
 A significant effect of the agreement was that it exhibited the colonies' collective will to act together in their common interests. Abraham Lincoln, in his first inaugural address in 1861, credited the origin of the union which would become the United States to the adoption of the Continental Association. The Union actually may have begun slightly earlier with the First Continental Congress's opening session on September 5, 1774, and from that date on, the colonies acted in accord on a series of agreements leading up to the Congress's closing session seven weeks later.[3] One of the last of these agreements, the most visible symbol of political unity among the colonies up to that time, was the adoption of the Continental Association.[4]
 Parliament passed the Coercive Acts in 1774 to restructure the colonial administration of the Thirteen Colonies and to punish the Province of Massachusetts for the Boston Tea Party. A First Continental Congress was convened at Carpenters' Hall in Philadelphia on September 5, 1774, to coordinate a response to the Intolerable Acts (also known as the Coercive Acts). Twelve colonies were represented at the First Continental Congress, which included George Washington, John Adams, Samuel Adams, Patrick Henry and future Chief Justice  John Jay.  Peyton Randolph was unanimously elected as its president, and Charles Thomson elected secretary. A plan of reconciliation was proposed, but was roundly rejected for concern that Parliament would see the proposal as a colonial acknowledgment that it had the right to regulate colonial trade and impose taxes.[5][6]
 Many Americans saw the Coercive Acts as a violation of the British Constitution and a threat to the liberties of all Thirteen Colonies, not just Massachusetts, and they turned to economic boycotts to protest the oppressive legislation, which involved the ""non-importation"", ""nonexportation"", or ""non-consumption"" of British goods.[5][7]
 On May 13, 1774, the Boston Town Meeting passed a resolution, with Samuel Adams acting as moderator, which called for an economic boycott in response to the Boston Port Act, one of the Coercive Acts. The resolution said:
 Paul Revere often served as messenger, and he carried the Boston resolutions to New York and Philadelphia.[9] Adams also promoted the boycott through existing colonial committees of correspondence, which enabled leaders of each colony to keep in touch.
 One of the first actions of the Congress was the endorsement of the Suffolk Resolves, which called for an embargo on British trade and urged each of the colonies to organize militias.[10] The delegates subsequently drew up a Declaration and Resolves that included the Continental Association, which was approved on October 20, 1774. Based on the earlier Virginia Association, the Association signified the growing cooperation between the colonies. Opening with a profession of allegiance to the king, the document then charged Parliament and lower British officials for creating ""a ruinous system of colony administration"" rather than blaming the king.  The Association alleged that this system was ""evidently calculated for enslaving these colonies,[11] and, with them, the British Empire."" Twelve colonies joined at once; Georgia joined a year later.[7]
 Signed copies of the Articles were sent to the King to present to both houses of Parliament, where they remained for some time mixed in with other letters and documents sent from America.[12]
 The articles of the Continental Association imposed an immediate ban on British tea, and a ban beginning on December 1, 1774, on importing or consuming any goods from Britain, Ireland, and the British West Indies. It also threatened an export ban on any products from the Thirteen Colonies to Britain, Ireland, or the West Indies, to be enacted only if the Intolerable Acts were not repealed by September 10, 1775. The Articles stated that the export ban was being suspended until this date because of the ""earnest desire we have not to injure our fellow-subjects in Great-Britain, Ireland, or the West-Indies.""  All American merchants were to direct their agents abroad to also comply with these restrictions, as would all ship owners. Additionally, article 2 placed a ban on all ships engaged in the slave trade.[13]
 The Association set forth policies by which the colonists would endure the scarcity of goods. Merchants were restricted from price gouging. Local committees of inspection were to be established in the Thirteen Colonies which would monitor compliance. Any individual observed to violate the pledges in the Articles would be condemned in print and ostracised in society ""as the enemies of American liberty."" Colonies would also cease all trade and dealings with any other colony that failed to comply with the bans.
 The colonies also pledged that they would ""encourage frugality, economy, and industry, and promote agriculture, arts and the manufactures of this country, especially that of wool; and will discountenance and discourage every species of extravagance and dissipation"", such as gambling, stage plays, and other frivolous entertainment. It set forth specific instructions on frugal funeral observations, pledging that no one ""will go into any further mourning-dress, than a black crepe or ribbon on the arm or hat, for gentlemen, and a black ribbon and necklace for ladies, and we will discontinue the giving of gloves and scarves at funerals.""
 The Continental Association was signed by 53 of the 56 members of the First Continental Congress.[13][a]
 The Continental Association went into effect on December 1, 1774. Compliance with (and support for) the established boycott was largely enforced through local enforcement committees. By mid-1775, a large majority of Virginia's 61 counties had set up their own enforcement committees. Nearly all other colonies saw similar levels of success in upholding the boycott, with the notable exception of Georgia, where Governor James Wright emphasized the need for British protection from Native Americans.[14]
 The use of public pressure was an overwhelmingly effective tactic in enforcing support for the boycott. Those who went against the boycott or even simply criticized the Association would often find their names slandered in newspapers and town gossip, often forcing those targeted to cave to pressure and publicly apologize. The threat of more direct action also played a role in forcing merchants to comply, with one merchant in Annapolis, Maryland, choosing to burn his own ship full of imported tea rather than attempt to sell it. When enforcement could not be guaranteed, some counties enacted price ceilings to discourage smuggling.[14]
 Georgia waited a year but the other twelve colonies quickly established local enforcement committees; the restrictions were dutifully enforced in the others, and trade with Britain plummeted.[15] Breen states that by early 1775 the local committees of safety, ""increasingly functioned as a revolutionary government"" and British officials no longer were in control.[16] 
 According to Christopher Gould, The Continental Association forced colonials to publicly take sides: Patriots signed and Loyalists did not.  In South Carolina Patriots dominated in Charleston and coastal areas; Loyalists were most numerous in the backcountry. The Continental Association took charge of the boycotts and led to new governmental organizations that supervised Revolutionary activities.  The South Carolina boycott made an exception for rice—it could still be exported but a fraction of sales went to purchase indigo from planters. Gould argues that the plan amounted to price stabilization and a commodity exchange program.[17]
 The King acted by securing an election and buying enough seats at £2500 to control the new Parliament. It then responded by passing the New England Restraining Act which prohibited the northeastern colonies from trading with anyone but Britain and the British West Indies, and they barred colonial ships from the North Atlantic fishing areas. These punitive measures were later extended to most of the other colonies, as well. Britain did not yield to American demands but instead tried to tighten its grip, and the conflict escalated to war. However, the long-term success of the Association was in its effective direction of collective action among the colonies and expression of their common interests.[18]
 In his first inaugural address in 1861, President Abraham Lincoln traced the origin of the union of the states to the Continental Association of 1774:
  The full text of Continental Association at Wikisource
"
American Revolutionary War,https://en.wikipedia.org/wiki/American_Revolutionary_War,"


 The American Revolutionary War (April 19, 1775 – September 3, 1783), also known as the Revolutionary War or American War of Independence, was an armed conflict that was part of the broader American Revolution, in which American Patriot forces organized as the Continental Army and commanded by George Washington defeated the British Army. The conflict was fought in North America, the Caribbean, and the Atlantic Ocean. The war ended with the Treaty of Paris (1783), which resulted in Great Britain ultimately recognizing the independence of the United States of America.
 After the British Empire gained dominance in North America with victory over the French in the Seven Years' War in 1763, tensions and disputes arose between Great Britain and the Thirteen Colonies over a variety of issues, including the Stamp and Townshend Acts. The resulting British military occupation led to the Boston Massacre in 1770. Among further tensions, the British Parliament imposed the Intolerable Acts in mid-1774. A British attempt to disarm the Americans and the resulting Battles of Lexington and Concord in April 1775 ignited the war. In June, the Second Continental Congress formalized Patriot militias into the Continental Army and appointed Washington its commander-in-chief. The British Parliament declared the colonies to be in a state of rebellion in August 1775. The stakes of the war were formalized with passage of the Lee Resolution by the Congress in Philadelphia on July 2, 1776, and the unanimous ratification of the Declaration of Independence on July 4, 1776.
 After a successful siege, Washington's forces drove the British Army out of Boston in March 1776, and British commander in chief William Howe responded by launching the New York and New Jersey campaign. Howe captured New York City in November. Washington responded by clandestinely crossing the Delaware River and winning small but significant victories at Trenton and Princeton. In the summer of 1777, as Howe was poised to capture Philadelphia, the Continental Congress fled to Baltimore. In October 1777, a separate northern British force under the command of John Burgoyne was forced to surrender at Saratoga in an American victory that proved crucial in convincing France and Spain that an independent United States was a viable possibility. France signed a commercial agreement with the rebels, followed by a Treaty of Alliance in February 1778. In 1779, the Sullivan Expedition undertook a scorched earth campaign against the Iroquois who were largely allied with the British. Indian raids on the American frontier, however, continued to be a problem. Also, in 1779, Spain allied with France against Great Britain in the Treaty of Aranjuez, though Spain did not formally ally with the Americans.
 Howe's replacement Henry Clinton intended to take the war against the Americans into the Southern Colonies. Despite some initial success, British General Cornwallis was besieged by a Franco-American force in Yorktown in September and October 1781. Cornwallis was forced to surrender in October. The British wars with France and Spain continued for another two years, but fighting largely ceased in North America. In the Treaty of Paris, ratified on September 3, 1783, Great Britain acknowledged the sovereignty and independence of the United States, bringing the American Revolutionary War to an end. The Treaties of Versailles resolved Great Britain's conflicts with France and Spain and forced Great Britain to cede Tobago, Senegal, and small territories in India to France, and Menorca, West Florida and East Florida to Spain.[43][44]
 The French and Indian War, part of the wider global conflict known as the Seven Years' War, ended with the 1763 Peace of Paris, which expelled France from their possessions in New France.[45] The Royal Proclamation of 1763 was designed to refocus colonial expansion north into Nova Scotia and south into Florida, with the Mississippi River as the dividing line between British and Spanish possessions in America. Settlement was tightly restricted beyond the 1763 limits, and claims west of this line, including by Virginia and Massachusetts, were rescinded.[46] With the exception of Virginia and others deprived of rights to western lands, the colonial legislatures agreed on the boundaries but disagreed on where to set them. Many settlers resented the restrictions entirely, and enforcement required permanent garrisons along the frontier, which led to increasingly bitter disputes over who should pay for them.[47]
 The huge debt incurred by the Seven Years' War and demands from British taxpayers for cuts in government expenditure meant Parliament expected the colonies to fund their own defense.[47] The 1763 to 1765 Grenville ministry instructed the Royal Navy to cease trading smuggled goods and enforce customs duties levied in American ports.[47] The most important was the 1733 Molasses Act; routinely ignored before 1763, it had a significant economic impact since 85% of New England rum exports were manufactured from imported molasses. These measures were followed by the Sugar Act and Stamp Act, which imposed additional taxes on the colonies to pay for defending the western frontier.[48] The taxes proved highly burdensome, particularly for the poorer classes, and quickly became a source of discontent.[49] In July 1765, the Whigs formed the First Rockingham ministry, which repealed the Stamp Act and reduced tax on foreign molasses to help the New England economy, but re-asserted Parliamentary authority in the Declaratory Act.[50]
 However, this did little to end the discontent; in 1768, a riot started in Boston when the authorities seized the sloop Liberty on suspicion of smuggling.[51] Tensions escalated in March 1770 when British troops fired on rock-throwing civilians, killing five in what became known as the Boston Massacre.[52] The Massacre coincided with the partial repeal of the Townshend Acts by the Tory-based North Ministry. North insisted on retaining duty on tea to enshrine Parliament's right to tax the colonies; the amount was minor, but ignored the fact it was that very principle Americans found objectionable.[53]
 In April 1772, colonialists staged the first American tax revolt against British royal authority in Weare, New Hampshire, later referred to as the Pine Tree Riot.[54] This would inspire the design of the Pine Tree Flag. Tensions escalated following the destruction of a customs vessel in the June 1772 Gaspee Affair, then came to a head in 1773. A banking crisis led to the near-collapse of the East India Company, which dominated the British economy; to support it, Parliament passed the Tea Act, giving it a trading monopoly in the Thirteen Colonies. Since most American tea was smuggled by the Dutch, the act was opposed by those who managed the illegal trade, while being seen as another attempt to impose the principle of taxation by Parliament.[55] In December 1773, a group called the Sons of Liberty disguised as Mohawks dumped crates of tea into Boston Harbor, an event later known as the Boston Tea Party. The British Parliament responded by passing the so-called Intolerable Acts, aimed specifically at Massachusetts, although many colonists and members of the Whig opposition considered them a threat to liberty in general. This increased sympathy for the Patriot cause locally, in the British Parliament, and in the London press.[56]
 Throughout the 18th century, the elected lower houses in the colonial legislatures gradually wrested power from their governors.[57] Dominated by smaller landowners and merchants, these assemblies now established ad-hoc provincial legislatures, effectively replacing royal control. With the exception of Georgia, twelve colonies sent representatives to the First Continental Congress to agree on a unified response to the crisis.[58] Many of the delegates feared that a boycott would result in war and sent a Petition to the King calling for the repeal of the Intolerable Acts.[59] After some debate, on September 17, 1774, Congress endorsed the Massachusetts Suffolk Resolves and on October 20 passed the Continental Association, which instituted economic sanctions and a boycott of goods against Britain.[60]
 While denying its authority over internal American affairs, a faction led by James Duane and future Loyalist Joseph Galloway insisted Congress recognize Parliament's right to regulate colonial trade.[60][w] Expecting concessions by the North administration, Congress authorized the colonial legislatures to enforce the boycott; this succeeded in reducing British imports by 97% from 1774 to 1775.[61] However, on February 9 Parliament declared Massachusetts to be in rebellion and instituted a blockade of the colony.[62] In July, the Restraining Acts limited colonial trade with the British West Indies and Britain and barred New England ships from the Newfoundland cod fisheries. The tension led to a scramble for control of militia stores, which each assembly was legally obliged to maintain for defense.[63] On April 19, a British attempt to secure the Concord arsenal culminated in the Battles of Lexington and Concord, which began the Revolutionary War.[64]
 After the Patriot victory at Concord, moderates in Congress led by John Dickinson drafted the Olive Branch Petition, offering to accept royal authority in return for George III mediating in the dispute.[65] However, since the petition was immediately followed by the Declaration of the Causes and Necessity of Taking Up Arms, Colonial Secretary Lord Dartmouth viewed the offer as insincere and refused to present the petition to the king.[66] Although constitutionally correct, since the monarch could not oppose his own government, it disappointed those Americans who hoped he would mediate in the dispute, while the hostility of his language annoyed even Loyalist members of Congress.[65] Combined with the Proclamation of Rebellion, issued on August 23 in response to the Battle at Bunker Hill, it ended hopes of a peaceful settlement.[67]
 Backed by the Whigs, Parliament initially rejected the imposition of coercive measures by 170 votes, fearing an aggressive policy would drive the Americans towards independence.[68] However, by the end of 1774 the collapse of British authority meant both Lord North and George III were convinced war was inevitable.[69] After Boston, Gage halted operations and awaited reinforcements; the Irish Parliament approved the recruitment of new regiments, while allowing Catholics to enlist for the first time.[70] Britain also signed a series of treaties with German states to supply additional troops.[71] Within a year, it had an army of over 32,000 men in America, the largest ever sent outside Europe at the time.[72] The employment of German soldiers against people viewed as British citizens was opposed by many in Parliament and by the colonial assemblies; combined with the lack of activity by Gage, opposition to the use of foreign troops allowed the Patriots to take control of the legislatures.[73]
 Support for independence was boosted by Thomas Paine's pamphlet Common Sense, which was published on January 10, 1776, and argued for American self-government and was widely reprinted.[74] To draft the Declaration of Independence, the Second Continental Congress appointed the Committee of Five: Thomas Jefferson, John Adams, Benjamin Franklin, Roger Sherman, and Robert Livingston.[75] The declaration was written almost exclusively by Jefferson.[76]
 Identifying inhabitants of the Thirteen Colonies as ""one people"", the declaration simultaneously dissolved political links with Britain, while including a long list of alleged violations of ""English rights"" committed by George III. This is also one of the first times that the colonies were referred to as ""United States"", rather than the more common United Colonies.[77]
 On July 2, Congress voted for independence and published the declaration on July 4.[78] At this point, the revolution ceased to be an internal dispute over trade and tax policies and had evolved into a civil war, since each state represented in Congress was engaged in a struggle with Britain, but also split between American Patriots and American Loyalists.[79] Patriots generally supported independence from Britain and a new national union in Congress, while Loyalists remained faithful to British rule. Estimates of numbers vary, one suggestion being the population as a whole was split evenly between committed Patriots, committed Loyalists, and those who were indifferent.[80] Others calculate the split as 40% Patriot, 40% neutral, 20% Loyalist, but with considerable regional variations.[81]
 At the onset of the war, the Second Continental Congress realized defeating Britain required foreign alliances and intelligence-gathering. The Committee of Secret Correspondence was formed for ""the sole purpose of corresponding with our friends in Great Britain and other parts of the world"". From 1775 to 1776, the committee shared information and built alliances through secret correspondence, as well as employing secret agents in Europe to gather intelligence, conduct undercover operations, analyze foreign publications, and initiate Patriot propaganda campaigns.[82] Paine served as secretary, while Benjamin Franklin and Silas Deane, sent to France to recruit military engineers,[83] were instrumental in securing French aid in Paris.[84]
 
 On April 14, 1775, Sir Thomas Gage, Commander-in-Chief, North America and Governor of Massachusetts, received orders to take action against the Patriots. He decided to destroy militia ordnance stored at Concord, Massachusetts, and capture John Hancock and Samuel Adams, who were considered the principal instigators of the rebellion. The operation was to begin around midnight on April 19, in the hope of completing it before the American Patriots could respond.[85][86] However, Paul Revere learned of the plan and notified Captain Parker, commander of the Concord militia, who prepared to resist.[87] The first action of the war, commonly referred to as the shot heard round the world, was a brief skirmish at Lexington, followed by the full-scale Battles of Lexington and Concord. British troops suffered around 300 casualties before withdrawing to Boston, which was then besieged by the militia.[88]
 In May 1775, 4,500 British reinforcements arrived under Generals William Howe, John Burgoyne, and Sir Henry Clinton.[89] On June 17, they seized the Charlestown Peninsula at the Battle of Bunker Hill, a frontal assault in which they suffered over 1,000 casualties.[90] Dismayed at the costly attack which had gained them little,[91] Gage appealed to London for a larger army,[92] but instead was replaced as commander by Howe.[90]
 On June 14, 1775, Congress took control of Patriot forces outside Boston, and Congressional leader John Adams nominated Washington as commander-in-chief of the newly formed Continental Army.[93] On June 16, Hancock officially proclaimed him ""General and Commander in Chief of the army of the United Colonies.""[94] He assumed command on July 3, preferring to fortify Dorchester Heights outside Boston rather than assaulting it.[95] In early March 1776, Colonel Henry Knox arrived with heavy artillery acquired in the Capture of Fort Ticonderoga.[96] Under cover of darkness, on March 5, Washington placed these on Dorchester Heights,[97] from where they could fire on the town and British ships in Boston Harbor. Fearing another Bunker Hill, Howe evacuated the city on March 17 without further loss and sailed to Halifax, Nova Scotia, while Washington moved south to New York City.[98]
 Beginning in August 1775, American privateers raided towns in Nova Scotia, including Saint John, Charlottetown, and Yarmouth. In 1776, John Paul Jones and Jonathan Eddy attacked Canso and Fort Cumberland respectively. British officials in Quebec began negotiating with the Iroquois for their support,[99] while US envoys urged them to remain neutral.[100] Aware of Native American leanings toward the British and fearing an Anglo-Indian attack from Canada, Congress authorized a second invasion in April 1775.[101] After the defeat at the Battle of Quebec on December 31,[102] the Americans maintained a loose blockade of the city until they retreated on May 6, 1776.[103] A second defeat at Trois-Rivières on June 8 ended operations in Quebec.[104]
 British pursuit was initially blocked by American naval vessels on Lake Champlain until victory at Valcour Island on October 11 forced the Americans to withdraw to Fort Ticonderoga, while in December an uprising in Nova Scotia sponsored by Massachusetts was defeated at Fort Cumberland.[105] These failures impacted public support for the Patriot cause,[106] and aggressive anti-Loyalist policies in the New England colonies alienated the Canadians.[107]
 In Virginia, Dunmore's Proclamation on November 7, 1775, promised freedom to any slaves who fled their Patriot masters and agreed to fight for the Crown.[108] British forces were defeated at Great Bridge on December 9 and took refuge on British ships anchored near Norfolk. When the Third Virginia Convention refused to disband its militia or accept martial law, Lord Dunmore ordered the Burning of Norfolk on January 1, 1776.[109]
 The siege of Savage's Old Fields began on November 19 in South Carolina between Loyalist and Patriot militias,[110] and the Loyalists were subsequently driven out of the colony in the Snow Campaign.[111] Loyalists were recruited in North Carolina to reassert British rule in the South, but they were decisively defeated in the Battle of Moore's Creek Bridge.[112] A British expedition sent to reconquer South Carolina launched an attack on Charleston in the Battle of Sullivan's Island on June 28, 1776,[113] but it failed.[114]
 A shortage of gunpowder led Congress to authorize a naval expedition against the Bahamas to secure ordnance stored there.[115] On March 3, 1776, an American squadron under the command of Esek Hopkins landed at the east end of Nassau and encountered minimal resistance at Fort Montagu. Hopkins' troops then marched on Fort Nassau. Hopkins had promised governor Montfort Browne and the civilian inhabitants that their lives and property would not be in any danger if they offered no resistance; they complied. Hopkins captured large stores of powder and other munitions that was so great he had to impress an extra ship in the harbor to transport the supplies back home, when he departed on March 17.[116] A month later, after a brief skirmish with HMS Glasgow, they returned to New London, Connecticut, the base for American naval operations.[117]
 After regrouping at Halifax in Nova Scotia,[118] Howe set sail for New York in June 1776 and began landing troops on Staten Island near the entrance to New York Harbor on July 2. The Americans rejected Howe's informal attempt to negotiate peace on July 30;[119] Washington knew that an attack on the city was imminent and realized that he needed advance information to deal with disciplined British regular troops.
 On August 12, 1776, Patriot Thomas Knowlton was ordered to form an elite group for reconnaissance and secret missions. Knowlton's Rangers, which included Nathan Hale, became the Army's first intelligence unit.[120][x] When Washington was driven off Long Island, he soon realized that he would need to professionalize military intelligence. With aid from Benjamin Tallmadge, Washington launched the six-man Culper spy ring.[123][y] The efforts of Washington and the Culper Spy Ring substantially increased the effective allocation and deployment of Continental regiments in the field.[123] Throughout the war, Washington spent more than 10 percent of his total military funds on military intelligence.[124]
 Washington split the Continental Army into positions on Manhattan and across the East River in western Long Island.[125] On August 27 at the Battle of Long Island, Howe outflanked Washington and forced him back to Brooklyn Heights, but he did not attempt to encircle Washington's forces.[126] Through the night of August 28, Knox bombarded the British. Knowing they were up against overwhelming odds, Washington ordered the assembly of a war council on August 29; all agreed to retreat to Manhattan. Washington quickly had his troops assembled and ferried them across the East River to Manhattan on flat-bottomed freight boats without any losses in men or ordnance, leaving General Thomas Mifflin's regiments as a rearguard.[127]
 Howe met with a delegation from the Second Continental Congress at the September Staten Island Peace Conference, but it failed to conclude peace, largely because the British delegates only had the authority to offer pardons and could not recognize independence.[128] On September 15, Howe seized control of New York City when the British landed at Kip's Bay and unsuccessfully engaged the Americans at the Battle of Harlem Heights the following day.[129] On October 18, Howe failed to encircle the Americans at the Battle of Pell's Point, and the Americans withdrew. Howe declined to close with Washington's army on October 28 at the Battle of White Plains and instead attacked a hill that was of no strategic value.[130]
 Washington's retreat isolated his remaining forces and the British captured Fort Washington on November 16. The British victory there amounted to Washington's most disastrous defeat with the loss of 3,000 prisoners.[131] The remaining American regiments on Long Island fell back four days later.[132] General Henry Clinton wanted to pursue Washington's disorganized army, but he was first required to commit 6,000 troops to capture Newport, Rhode Island, to secure the Loyalist port.[133][z] General Charles Cornwallis pursued Washington, but Howe ordered him to halt.[135]
 The outlook following the defeat at Fort Washington appeared bleak for the American cause. The reduced Continental Army had dwindled to fewer than 5,000 men and was reduced further when enlistments expired at the end of the year.[136] Popular support wavered, and morale declined. On December 20, 1776, the Continental Congress abandoned the revolutionary capital of Philadelphia and moved to Baltimore, where it remained until February 27, 1777.[137] Loyalist activity surged in the wake of the American defeat, especially in New York state.[138]
 In London, news of the victorious Long Island campaign was well received with festivities held in the capital. Public support reached a peak.[139] Strategic deficiencies among Patriot forces were evident: Washington divided a numerically weaker army in the face of a stronger one, his inexperienced staff misread the military situation, and American troops fled in the face of enemy fire. The successes led to predictions that the British could win within a year.[140] The British established winter quarters in the New York City area and anticipated renewed campaigning the following spring.[141]
 On the night of December 25–26, 1776, Washington crossed the Delaware River, leading a column of Continental Army troops from today's Bucks County, Pennsylvania, to today's Mercer County, New Jersey, in a logistically challenging and dangerous operation.
 Meanwhile, the Hessians were involved in numerous clashes with small bands of Patriots and were often aroused by false alarms at night in the weeks before the actual Battle of Trenton. By Christmas they were tired, while a heavy snowstorm led their commander, Colonel Johann Rall, to assume no significant attack would occur.[142] At daybreak on the 26th, the American Patriots surprised and overwhelmed Rall and his troops, who lost over 20 killed including Rall,[143] while 900 prisoners, German cannons and supplies were captured.[144]
 The Battle of Trenton restored the American army's morale, reinvigorated the Patriot cause,[145] and dispelled their fear of what they regarded as Hessian ""mercenaries"".[146] A British attempt to retake Trenton was repulsed at Assunpink Creek on January 2;[147] during the night, Washington outmaneuvered Cornwallis, then defeated his rearguard in the Battle of Princeton the following day. The two victories helped convince the French that the Americans were worthy military allies.[148]
 After his success at Princeton, Washington entered winter quarters at Morristown, New Jersey, where he remained until May[149] and received Congressional direction to inoculate all Patriot troops against smallpox.[150][aa] With the exception of a minor skirmishing between the two armies which continued until March,[152] Howe made no attempt to attack the Americans.[153]
 The 1776 campaign demonstrated that regaining New England would be a prolonged affair, which led to a change in British strategy to isolating the north by taking control of the Hudson River, allowing them to focus on the south where Loyalist support was believed to be substantial.[154] In December 1776, Howe wrote to the Colonial Secretary Lord Germain, proposing a limited offensive against Philadelphia, while a second force moved down the Hudson from Canada.[155] Burgoyne supplied several alternatives, all of which gave him responsibility for the offensive, with Howe remaining on the defensive. The option selected required him to lead the main force south from Montreal down the Hudson Valley, while a detachment under Barry St. Leger moved east from Lake Ontario. The two would meet at Albany, leaving Howe to decide whether to join them.[156] Reasonable in principle, this did not account for the logistical difficulties involved and Burgoyne erroneously assumed Howe would remain on the defensive; Germain's failure to make this clear meant he opted to attack Philadelphia instead.[157]
 With a mixed force of British regulars, professional German soldiers and Canadian militia Burgoyne set out on June 14, 1777 and captured Fort Ticonderoga on July 5. As General Horatio Gates retreated, his troops blocked roads, destroyed bridges, dammed streams, and stripped the area of food.[158] This slowed Burgoyne's progress and forced him to send out large foraging expeditions; one of more than 700 British troops were captured at the Battle of Bennington on August 16.[159] St Leger moved east and besieged Fort Stanwix; despite defeating an American relief force at the Battle of Oriskany on August 6, Burgoyne was abandoned by his Indian allies and withdrew to Quebec on August 22.[160] Now isolated and outnumbered by Gates, Burgoyne continued onto Albany rather than retreating to Fort Ticonderoga, reaching Saratoga on September 13. He asked Clinton for support while constructing defenses around the town.[161]
 Morale among his troops rapidly declined, and an unsuccessful attempt to break past Gates at the Battle of Freeman Farms on September 19 resulted in 600 British casualties.[162] When Clinton advised he could not reach them, Burgoyne's subordinates advised retreat; a reconnaissance in force on October 7 was repulsed by Gates at the Battle of Bemis Heights, forcing them back into Saratoga with heavy losses. By October 11, all hope of British escape had vanished; persistent rain reduced the camp to a ""squalid hell"" and supplies were dangerously low.[163] Burgoyne capitulated on October 17; around 6,222 soldiers, including German forces commanded by General Friedrich Adolf Riedesel, surrendered their arms before being taken to Boston, where they were to be transported to England.[164]
 After securing additional supplies, Howe made another attempt on Philadelphia by landing his troops in Chesapeake Bay on August 24.[165] He now compounded failure to support Burgoyne by missing repeated opportunities to destroy his opponent: despite defeating Washington at the Battle of Brandywine on September 11, he then allowed him to withdraw in good order.[166] After dispersing an American detachment at Paoli on September 20, Cornwallis occupied Philadelphia on September 26, with the main force of 9,000 under Howe based just to the north at Germantown.[167] Washington attacked them on October 4, but was repulsed.[168]
 To prevent Howe's forces in Philadelphia being resupplied by sea, the Patriots erected Fort Mifflin and nearby Fort Mercer on the east and west banks of the Delaware respectively, and placed obstacles in the river south of the city. This was supported by a small flotilla of Continental Navy ships on the Delaware, supplemented by the Pennsylvania State Navy, commanded by John Hazelwood. An attempt by the Royal Navy to take the forts in the October 20 to 22 Battle of Red Bank failed;[169][170] a second attack captured Fort Mifflin on November 16, while Fort Mercer was abandoned two days later when Cornwallis breached the walls.[171] His supply lines secured, Howe tried to tempt Washington into giving battle, but after inconclusive skirmishing at the Battle of White Marsh from December 5 to 8, he withdrew to Philadelphia for the winter.[172]
 On December 19, the Americans followed suit and entered winter quarters at Valley Forge. As Washington's domestic opponents contrasted his lack of battlefield success with Gates' victory at Saratoga,[173] foreign observers such as Frederick the Great were equally impressed with Washington's command at Germantown, which demonstrated resilience and determination.[174] Over the winter, poor conditions, supply problems and low morale resulted in 2,000 deaths, with another 3,000 unfit for duty due to lack of shoes.[175] However, Baron Friedrich Wilhelm von Steuben took the opportunity to introduce Prussian Army drill and infantry tactics to ""model companies"" in each Continental Army regiment, who then instructed their home units.[176] Despite Valley Forge being only twenty miles away, Howe made no effort to attack their camp, an action some critics argue could have ended the war.[177]
 Like his predecessors, French foreign minister Vergennes considered the 1763 Peace a national humiliation and viewed the war as an opportunity to weaken Britain. He initially avoided open conflict, but allowed American ships to take on cargoes in French ports, a technical violation of neutrality.[178] Vergennes persuaded Louis XVI to secretly fund a government front company to purchase munitions for the Patriots, carried in neutral Dutch ships and imported through Sint Eustatius in the Caribbean.[179]
 Many Americans opposed a French alliance, fearing to ""exchange one tyranny for another"", but this changed after a series of military setbacks in early 1776. As France had nothing to gain from the colonies reconciling with Britain, Congress had three choices: making peace on British terms, continuing the struggle on their own, or proclaiming independence, guaranteed by France. Although the Declaration of Independence had wide public support, over 20% of Congressmen voted against an alliance with France.[180] Congress agreed to the treaty with reluctance and as the war moved in their favor increasingly lost interest in it.[181]
 Silas Deane was sent to Paris to begin negotiations with Vergennes, whose key objectives were replacing Britain as the United States' primary commercial and military partner while securing the French West Indies from American expansion.[182] These islands were extremely valuable; in 1772, the value of sugar and coffee produced by Saint-Domingue on its own exceeded that of all American exports combined.[183] Talks progressed slowly until October 1777, when British defeat at Saratoga and their apparent willingness to negotiate peace convinced Vergennes only a permanent alliance could prevent the ""disaster"" of Anglo-American rapprochement. Assurances of formal French support allowed Congress to reject the Carlisle Peace Commission and insist on nothing short of complete independence.[184]
 On February 6, 1778, France and the United States signed the Treaty of Amity and Commerce regulating trade between the two countries, followed by a defensive military alliance against Britain, the Treaty of Alliance. In return for French guarantees of American independence, Congress undertook to defend their interests in the West Indies, while both sides agreed not to make a separate peace; conflict over these provisions would lead to the 1798 to 1800 Quasi-War.[181] Charles III of Spain was invited to join on the same terms but refused, largely due to concerns over the impact of the Revolution on Spanish colonies in the Americas. Spain had complained on multiple occasions about encroachment by American settlers into Louisiana, a problem that could only get worse once the United States replaced Britain.[185]
 Although Spain ultimately made important contributions to American success, in the Treaty of Aranjuez, Charles agreed only to support France's war with Britain outside America, in return for help in recovering Gibraltar, Menorca and Spanish Florida.[186] The terms were confidential since several conflicted with American aims; for example, the French claimed exclusive control of the Newfoundland cod fisheries, a non-negotiable for colonies like Massachusetts.[187] One less well-known impact of this agreement was the abiding American distrust of 'foreign entanglements'; the U.S. would not sign another treaty with France until their NATO agreement of 1949.[181] This was because the US had agreed not to make peace without France, while Aranjuez committed France to keep fighting until Spain recovered Gibraltar, effectively making it a condition of U.S. independence without the knowledge of Congress.[188]
 To encourage French participation in the struggle for independence, the U.S. representative in Paris, Silas Deane promised promotion and command positions to any French officer who joined the Continental Army. Such as Gilbert du Motier, Marquis de Lafayette, whom Congress via Dean appointed a major general,[189][190] on July 31, 1777.[191]
 When the war started, Britain tried to borrow the Dutch-based Scots Brigade for service in America, but pro-Patriot sentiment led the States General to refuse.[192] Although the Republic was no longer a major power, prior to 1774 they still dominated the European carrying trade, and Dutch merchants made large profits shipping French-supplied munitions to the Patriots. This ended when Britain declared war in December 1780, a conflict that proved disastrous to the Dutch economy.[193]
 The British government failed to take into account the strength of the American merchant marine and support from European countries, which allowed the colonies to import munitions and continue trading with relative impunity. While well aware of this, the North administration delayed placing the Royal Navy on a war footing for cost reasons; this prevented the institution of an effective blockade.[194] Traditional British policy was to employ European land-based allies to divert the opposition; in 1778, they were diplomatically isolated and faced war on multiple fronts.[195]
 Meanwhile, George III had given up on subduing America while Britain had a European war to fight.[196] He did not welcome war with France, but he held the British victories over France in the Seven Years' War as a reason to believe in ultimate victory over France.[197] Britain subsequently changed its focus into the Caribbean theater,[198] and diverted major military resources away from America.[199]
 At the end of 1777, Howe resigned and was replaced by Sir Henry Clinton on May 24, 1778; with French entry into the war, he was ordered to consolidate his forces in New York.[199] On June 18, the British departed Philadelphia with the reinvigorated Americans in pursuit; the Battle of Monmouth on June 28 was inconclusive but boosted Patriot morale. That midnight, the newly installed Clinton continued his retreat to New York.[200] A French naval force under Admiral Charles Henri Hector d'Estaing was sent to assist Washington; deciding New York was too formidable a target, in August they launched a combined attack on Newport, with General John Sullivan commanding land forces.[201] The resulting Battle of Rhode Island was indecisive; badly damaged by a storm, the French withdrew to avoid risking their ships.[202] 
 Further activity was limited to British raids on Chestnut Neck and Little Egg Harbor in October.[203] In July 1779, the Americans captured British positions at Stony Point and Paulus Hook.[204] Clinton unsuccessfully tried to tempt Washington into a decisive engagement by sending General William Tryon to raid Connecticut.[205] In July, a large American naval operation, the Penobscot Expedition, attempted to retake Maine but was defeated.[206]
Persistent Iroquois raids in New York and Pennsylvania led to the punitive Sullivan Expedition from July to September 1779. Involving more than 4,000 patriot soldiers, the scorched earth campaign destroyed more than 40 Iroquois villages and 160,000 bushels (4,000 mts) of maize, leaving the Iroquois destitute and destroying the Iroquois confederacy as an independent power on the American frontier. However, 5,000 Iroquois fled to Canada, where, supplied and supported by the British, they continued their raids.[207][208][209]
 During the winter of 1779–1780, the Continental Army suffered greater hardships than at Valley Forge.[210] Morale was poor, public support fell away, the Continental dollar was virtually worthless, the army was plagued with supply problems, desertion was common, and mutinies occurred in the Pennsylvania Line and New Jersey Line regiments over the conditions.[211]
 In June 1780, Clinton sent 6,000 men under Wilhelm von Knyphausen to retake New Jersey, but they were halted by local militia at the Battle of Connecticut Farms; although the Americans withdrew, Knyphausen felt he was not strong enough to engage Washington's main force and retreated.[212] A second attempt two weeks later ended in a British defeat at the Battle of Springfield, effectively ending their ambitions in New Jersey.[213] In July, Washington appointed Benedict Arnold commander of West Point; his attempt to betray the fort to the British failed due to incompetent planning, and the plot was revealed when his British contact John André was captured and executed.[214] Arnold escaped to New York and switched sides, an action justified in a pamphlet addressed ""To the Inhabitants of America""; the Patriots condemned his betrayal, while he found himself almost as unpopular with the British.[215]
 The Southern Strategy was developed by Lord Germain, based on input from London-based Loyalists, including Joseph Galloway. They argued that it made no sense to fight the Patriots in the north where they were strongest, while the New England economy was reliant on trade with Britain. On the other hand, duties on tobacco made the South far more profitable for Britain, while local support meant securing it required small numbers of regular troops. Victory would leave a truncated United States facing British possessions in the south, Canada to the north, and Ohio on their western border; with the Atlantic seaboard controlled by the Royal Navy, Congress would be forced to agree to terms. However, assumptions about the level of Loyalist support proved wildly optimistic.[216]
 Germain ordered Augustine Prévost, the British commander in East Florida, to advance into Georgia in December 1778. Lieutenant-Colonel Archibald Campbell, an experienced officer, captured Savannah on December 29, 1778. He recruited a Loyalist militia of nearly 1,100, many of whom allegedly joined only after Campbell threatened to confiscate their property.[217] Poor motivation and training made them unreliable troops, as demonstrated in their defeat by Patriot militia at the Battle of Kettle Creek on February 14, 1779, although this was offset by British victory at Brier Creek on March 3.[218]
 In June 1779, Prévost launched an abortive assault on Charleston, before retreating to Savannah, an operation notorious for widespread looting by British troops that enraged both Loyalists and Patriots. In October, a joint French and American operation under d'Estaing and General Benjamin Lincoln failed to recapture Savannah.[219] Prévost was replaced by Lord Cornwallis, who assumed responsibility for Germain's strategy; he soon realized estimates of Loyalist support were considerably over-stated, and he needed far more regular forces.[220]
 Reinforced by Clinton, Cornwallis's troops captured Charleston in May 1780, inflicting the most serious Patriot defeat of the war; over 5,000 prisoners were taken and the Continental Army in the south effectively destroyed. On May 29, Lieutenant-Colonel Banastre Tarleton's mainly Loyalist force routed a Continental Army force nearly three times its size under Colonel Abraham Buford at the Battle of Waxhaws. The battle is controversial for allegations of a massacre, which were later used as a recruiting tool by the Patriots.[221]
 Clinton returned to New York, leaving Cornwallis to oversee the south; despite their success, the two men left barely on speaking terms.[222] The Southern strategy depended on local support, but this was undermined by a series of coercive measures. Previously, captured Patriots were sent home after swearing not to take up arms against the king; they were now required to fight their former comrades, while the confiscation of Patriot-owned plantations led formerly neutral ""grandees"" to side with them.[223] Skirmishes at Williamson's Plantation, Cedar Springs, Rocky Mount, and Hanging Rock signaled widespread resistance to the new oaths throughout South Carolina.[224]
 In July 1780, Congress appointed Gates commander in the south; he was defeated at the Battle of Camden on August 16, leaving Cornwallis free to enter North Carolina.[225] Despite battlefield success, the British could not control the countryside and Patriot attacks continued; before moving north, Cornwallis sent Loyalist militia under Major Patrick Ferguson to cover his left flank, leaving their forces too far apart to provide mutual support.[226] In early October, Ferguson was defeated at the Battle of Kings Mountain, dispersing organized Loyalist resistance in the region.[227] Despite this, Cornwallis continued into North Carolina hoping for Loyalist support, while Washington replaced Gates with General Nathanael Greene in December 1780.[228]
 Greene divided his army, leading his main force southeast pursued by Cornwallis; a detachment was sent southwest under Daniel Morgan, who defeated Tarleton's British Legion at Cowpens on January 17, 1781, nearly eliminating it as a fighting force.[229] The Patriots now held the initiative in the south, with the exception of a raid on Richmond led by Benedict Arnold in January 1781.[230] Greene led Cornwallis on a series of countermarches around North Carolina; by early March, the British were exhausted and short of supplies and Greene felt strong enough to fight the Battle of Guilford Court House on March 15. Although victorious, Cornwallis suffered heavy casualties and retreated to Wilmington, North Carolina, seeking supplies and reinforcements.[231]
 The Patriots now controlled most of the Carolinas and Georgia outside the coastal areas; after a minor reversal at the Battle of Hobkirk's Hill, they recaptured Fort Watson and Fort Motte on April 15.[232] On June 6, Brigadier General Andrew Pickens captured Augusta, leaving the British in Georgia confined to Charleston and Savannah.[233] The assumption Loyalists would do most of the fighting left the British short of troops and battlefield victories came at the cost of losses they could not replace. Despite halting Greene's advance at the Battle of Eutaw Springs on September 8, Cornwallis withdrew to Charleston with little to show for his campaign.[234]
 From the beginning of the war, Bernardo de Gálvez, the Governor of Spanish Louisiana, allowed the Americans to import supplies and munitions into New Orleans, then ship them to Pittsburgh.[235] This provided an alternative transportation route for the Continental Army, bypassing the British blockade of the Atlantic Coast.[236]
 In February 1778, an expedition of militia to destroy British military supplies in settlements along the Cuyahoga River was halted by adverse weather.[237] Later in the year, a second campaign was undertaken to seize the Illinois Country from the British. Virginia militia, Canadien settlers, and Indian allies commanded by Colonel George Rogers Clark captured Kaskaskia on July 4 and then secured Vincennes, though Vincennes was recaptured by Quebec Governor Henry Hamilton. In early 1779, the Virginians counter-attacked in the siege of Fort Vincennes and took Hamilton prisoner. Clark secured western British Quebec as the American Northwest Territory in the Treaty of Paris brought the Revolutionary War to an end.[238]
 When Spain joined France's war against Britain in the Anglo-French War in 1779, their treaty specifically excluded Spanish military action in North America. Later that year, however, Gálvez initiated offensive operations against British outposts.[239] First, he cleared British garrisons in Baton Rouge, Louisiana, Fort Bute, and Natchez, Mississippi, and captured five forts.[240] In doing so, Gálvez opened navigation on the Mississippi River north to the American settlement in Pittsburgh.[241]
 On May 25, 1780, British Colonel Henry Bird invaded Kentucky as part of a wider operation to clear American resistance from Quebec to the Gulf Coast. Their advance on New Orleans was repelled by Spanish Governor Gálvez's offensive on Mobile. Simultaneous British attacks were repulsed on St. Louis by the Spanish Lieutenant Governor de Leyba, and on the Virginia County courthouse in Cahokia, Illinois, by Lieutenant Colonel Clark. The British initiative under Bird from Detroit was ended at the rumored approach of Clark.[ab] The scale of violence in the Licking River Valley, was extreme ""even for frontier standards."" It led to English and German settlements, who joined Clark's militia when the British and their hired German soldiers withdrew to the Great Lakes.[242] The Americans responded with a major offensive along the Mad River in August which met with some success in the Battle of Piqua but did not end Indian raids.[243]
 French soldier Augustin de La Balme led a Canadian militia in an attempt to capture Detroit, but they dispersed when Miami natives led by Little Turtle attacked the encamped settlers on November 5.[244][ac] The war in the west stalemated with the British garrison sitting in Detroit and the Virginians expanding westward settlements north of the Ohio River in the face of British-allied Indian resistance.[246]
 In 1781, Galvez and Pollock campaigned east along the Gulf Coast to secure West Florida, including British-held Mobile and Pensacola.[247] The Spanish operations impaired the British supply of armaments to British Indian allies, which effectively suspended a military alliance to attack settlers between the Mississippi River and the Appalachian Mountains.[248][ad]
 In 1782, large scale retaliations between settlers and Native Americans in the region included the Gnadenhutten massacre and the Crawford expedition. The 1782 Battle of Blue Licks was one of the last major engagements of the war. News of the treaty between Great Britain and the United States arrived late that year. By this time, about 7% of Kentucky settlers had been killed in battles against Native Americans, contrasted with 1% of the population killed in the Thirteen Colonies. Lingering resentments led to continued fighting in the west after the war officially ended.
 Clinton spent most of 1781 based in New York City; he failed to construct a coherent operational strategy, partly due to his difficult relationship with Admiral Marriot Arbuthnot.[249] In Charleston, Cornwallis independently developed an aggressive plan for a campaign in Virginia, which he hoped would isolate Greene's army in the Carolinas and cause the collapse of Patriot resistance in the South. This strategy was approved by Lord Germain in London, but neither informed Clinton.[250]
 Washington and Rochambeau discussed their options: Washington wanted to attack the British in New York, and Rochambeau wanted to attack them in Virginia, where Cornwallis's forces were less established.[251] Washington eventually gave way, and Lafayette took a combined Franco-American force into Virginia.[252] Clinton misinterpreted his movements as preparations for an attack on New York and instructed Cornwallis to establish a fortified sea base, where the Royal Navy could evacuate British troops to help defend New York.[253]
 When Lafayette entered Virginia, Cornwallis complied with Clinton's orders and withdrew to Yorktown, where he constructed strong defenses and awaited evacuation.[254] An agreement by the Spanish Navy to defend the French West Indies allowed Admiral François Joseph Paul de Grasse to relocate to the Atlantic seaboard, a move Arbuthnot did not anticipate.[249] This provided Lafayette naval support, while the failure of previous combined operations at Newport and Savannah meant their coordination was planned more carefully.[255] Despite repeated urging from his subordinates, Cornwallis made no attempt to engage Lafayette before he could establish siege lines.[256] Expecting to be withdrawn within a few days, he also abandoned the outer defenses, which were promptly occupied by the besiegers and hastened British defeat.[257]
 On August 31, a Royal Navy fleet under Thomas Graves left New York for Yorktown.[258] After landing troops and munitions for the besiegers on August 30, de Grasse remained in Chesapeake Bay and intercepted him on September 5; although the Battle of the Chesapeake was indecisive in terms of losses, Graves was forced to retreat, leaving Cornwallis isolated.[259] An attempted breakout over York River at Gloucester Point failed due to bad weather.[260] Under heavy bombardment with dwindling supplies, on October 16 Cornwallis sent emissaries to General Washington to negotiate surrender; after twelve hours of negotiations, the terms of surrender were finalized the following day.[261] Responsibility for defeat was the subject of fierce public debate between Cornwallis, Clinton, and Germain. Clinton ultimately took most of the blame and spent the rest of his life in relative obscurity.[262]
 Subsequent to Yorktown, American forces were assigned to supervise the armistice between Washington and Clinton made to facilitate British departure following the January 1782 law of Parliament forbidding any further British offensive action in North America. British-American negotiations in Paris led to signed preliminary agreements in November 1782, which acknowledged U.S. independence. The enacted Congressional war objective, a British withdrawal from North America and cession of these regions to the U.S., was completed in stages in East Coast cities.[263]
 In the U.S. South, Generals Greene and Wayne observed the British remove their troops from Charleston on December 14, 1782.[264] Loyalist provincial militias of whites and free Blacks and Loyalists with slaves were transported to Nova Scotia and the British West Indies.[ae] Native American allies of the British and some freed Blacks were left to escape unaided through the American lines.
 On April 9, 1783, Washington issued orders that ""all acts of hostility"" were to cease immediately. That same day, by arrangement with Washington, Carleton issued a similar order to British troops.[265]  As directed by a Congressional resolution of May 26, 1783, all non-commissioned officers and enlisted were furloughed ""to their homes"" until the ""definitive treaty of peace"", when they would be automatically discharged. The U.S. armies were directly disbanded in the field as of Washington's General Orders on June 2, 1783.[266] Once the Treaty of Paris was signed with Britain on September 3, 1783, Washington resigned as commander-in-chief of the Continental Army.[263] The last British occupation of New York City ended on November 25, 1783, with the departure of Clinton's replacement, General Sir Guy Carleton.[267]
 To win their insurrection, Washington and the Continental Army needed to outlast the British will to fight. To restore British America, the British had to defeat the Continental Army quickly and compel the Second Continental Congress to retract its claim to self-governance.[269] Historian Terry M. Mays of The Citadel identifies three separate types of warfare during the Revolutionary War. The first was a colonial conflict in which objections to imperial trade regulation were as significant as taxation policy. The second was a civil war between American Patriots, American Loyalists, and those who preferred to remain neutral. Particularly in the south, many battles were fought between Patriots and Loyalists with no British involvement, leading to divisions that continued after independence was achieved.[270]
 The third element was a global war between France, Spain, the Dutch Republic, and Britain, with America serving as one of several different war theaters.[270] After entering the Revolutionary War in 1778, France provided the Americans money, weapons, soldiers, and naval assistance, while French troops fought under U.S. command in North America. While Spain did not formally join the war in America, they provided access to the Mississippi River and captured British possessions on the Gulf of Mexico that denied bases to the Royal Navy, retook Menorca and besieged Gibraltar in Europe.[271] Although the Dutch Republic was no longer a major power prior to 1774, they still dominated the European carrying trade, and Dutch merchants made large profits by shipping French-supplied munitions to the Patriots. This ended when Britain declared war in December 1780, and the conflict proved disastrous to the Dutch economy.[272]
 The Second Continental Congress stood to benefit if the Revolution evolved into a protracted war. Colonial state populations were largely prosperous and depended on local production for food and supplies rather than on imports from Britain. The thirteen colonies were spread across most of North American Atlantic seaboard, stretching 1,000 miles. Most colonial farms were remote from the seaports, and control of four or five major ports did not give Britain control over American inland areas. Each state had established internal distribution systems.[273] Motivation was also a major asset: each colonial capital had its own newspapers and printers, and the Patriots enjoyed more popular support than the Loyalists. Britain hoped that the Loyalists would do much of the fighting, but found that the Loyalists did not engage as significantly as they had hoped.[14]
 When the Revolutionary War began, the Second Continental Congress lacked a professional army or navy. However, each of the colonies had a long-established system of local militia, which were combat-tested in support of British regulars in the French and Indian War. The colonial state legislatures independently funded and controlled their local militias.[273] 
 Militiamen were lightly armed, had little training, and usually did not have uniforms. Their units served for only a few weeks or months at a time and lacked the training and discipline of more experienced soldiers. Local county militias were reluctant to travel far from home and were unavailable for extended operations.[274] To compensate for this, the Continental Congress established a regular force known as the Continental Army on June 14, 1775, which proved to be the origin of the modern United States Army, and appointed Washington as its commander-in-chief. However, it suffered significantly from the lack of an effective training program and from largely inexperienced officers.[275]
Each state legislature appointed officers for both county and state militias and their regimental Continental line officers; although Washington was required to accept Congressional appointments, he was permitted to choose and command his own generals, such as Greene; his chief of artillery, Knox; and Alexander Hamilton, the chief of staff.[276] One of Washington's most successful general officer recruits was Steuben, a veteran of the Prussian general staff who wrote the Revolutionary War Drill Manual.[275] The development of the Continental Army was always a work in progress and Washington used both his regulars and state militias throughout the war; when properly employed, the combination allowed them to overwhelm smaller British forces, as they did in battles at Concord, Boston, Bennington, and Saratoga. Both sides used partisan warfare, but the state militias effectively suppressed Loyalist activity when British regulars were not in the area.[274][af]
 Washington designed the overall military strategy in cooperation with Congress, established the principle of civilian supremacy in military affairs, personally recruited his senior officer corps, and kept the states focused on a common goal.[279] Washington initially employed the inexperienced officers and untrained troops in Fabian strategies rather than risk frontal assaults against Britain's professional forces.[280] Over the course of the war, Washington lost more battles than he won, but he never surrendered his troops and maintained a fighting force in the face of British field armies.[281]
 By prevailing European standards, the armies in America were relatively small, limited by lack of supplies and logistics. The British were constrained by the logistical difficulty of transporting troops across the Atlantic and their dependence on local supplies. Washington never directly commanded more than 17,000 men,[282] and the combined Franco-American army in the decisive American victory at Yorktown was only about 19,000.[283] At the beginning of 1776, Patriot forces consisted of 20,000 men, with two-thirds in the Continental Army and the other third in the state militias. About 250,000 American men served as regulars or as militia for the revolutionary cause during the war, but there were never more than 90,000 men under arms at any time.[284]
 On the whole, American officers never equaled their British opponents in tactics and maneuvers, and they lost most of the pitched battles. The great successes at Boston (1776), Saratoga (1777), and Yorktown (1781) were won by trapping the British far from base with a greater number of troops.[276] After 1778, Washington's army was transformed into a more disciplined and effective force, mostly as a product of Baron von Steuben's military training.[275] Immediately after the Continental Army emerged from Valley Forge in June 1778, it proved its ability to match the military capabilities of the British at the Battle of Monmouth, including a Black Rhode Island regiment fending off a British bayonet attack and then counter charging the British for the first time as part of Washington's army.[285] After the Battle of Monmouth, Washington came to realize that saving entire towns was not necessary, but preserving his army and keeping the revolutionary spirit alive was more important. Washington informed Henry Laurens, then president of the Second Continental Congress,[ag] ""that the possession of our towns, while we have an army in the field, will avail them little.""[287]
 Although the Continental Congress was responsible for the war effort and provided supplies to the troops, Washington took it upon himself to pressure Congress and the state legislatures to provide the essentials of war; there was never nearly enough.[288] Congress evolved in its committee oversight and established the Board of War, which included members of the military.[289] Because the Board of War was also a committee ensnared with its own internal procedures, Congress also created the post of Secretary of War, appointing Major General Benjamin Lincoln to the position in February 1781. Washington worked closely with Lincoln to coordinate civilian and military authorities and took charge of training and supplying the army.[290][275]
 During the first summer of the war, Washington began outfitting schooners and other small seagoing vessels to prey on ships supplying the British in Boston.[291] The Second Continental Congress established the Continental Navy on October 13, 1775, and appointed Esek Hopkins as its first commander;[292] for most of the war, the Continental Navy included only a handful of small frigates and sloops, supported by privateers.[293] On November 10, 1775, Congress authorized the creation of the Continental Marines, which ultimately evolved into the United States Marine Corps.[278]
 John Paul Jones became the first American naval hero when he captured HMS Drake on April 24, 1778, the first victory for any American military vessel in British waters.[294] The last such victory was by the frigate USS Alliance, commanded by Captain John Barry. On March 10, 1783, the Alliance outgunned HMS Sybil in a 45-minute duel while escorting Spanish gold from Havana to the Congress in Philadelphia.[295] After Yorktown, all US Navy ships were sold or given away; it was the first time in America's history that it had no fighting forces on the high seas.[296]
 Congress primarily commissioned privateers to reduce costs and to take advantage of the large proportion of colonial sailors found in the British Empire. In total, they included 1,700 ships that successfully captured 2,283 enemy ships to damage the British effort and to enrich themselves with the proceeds from the sale of cargo and the ship itself.[297][ah] About 55,000 sailors served aboard American privateers during the war.[16]
 At the beginning of the war, the Americans had no major international allies, since most nation-states waited to see how the conflict unfolded. Over time, the Continental Army established its military credibility. Battles such as the Battle of Bennington, the Battles of Saratoga, and even defeats such as the Battle of Germantown, proved decisive in gaining the support of powerful European nations, including France, Spain, and the Dutch Republic; the Dutch moved from covertly supplying the Americans with weapons and supplies to overtly supporting them.[299]
 The decisive American victory at Saratoga convinced France, which was already a long-time rival of Britain, to offer the Americans the Treaty of Amity and Commerce. The two nations also agreed to a defensive Treaty of Alliance to protect their trade and also guaranteed American independence from Britain. To engage the United States as a French ally militarily, the treaty was conditioned on Britain initiating a war on France to stop it from trading with the U.S. Spain and the Dutch Republic were invited to join by both France and the United States in the treaty, but neither was responsive to the request.[300]
 On June 13, 1778, France declared war on Great Britain, and it invoked the French military alliance with the U.S., which ensured additional U.S. private support for French possessions in the Caribbean.[ai] Washington worked closely with the soldiers and navy that France would send to America, primarily through Lafayette on his staff. French assistance made critical contributions required to defeat Cornwallis at Yorktown in 1781.[303][aj]
 The British military had considerable experience fighting in North America.[305] However, in previous conflicts they benefited from local logistics and support from the colonial militia. In the American Revolutionary War, reinforcements had to come from Europe, and maintaining large armies over such distances was extremely complex; ships could take three months to cross the Atlantic, and orders from London were often outdated by the time they arrived.[306]
 Prior to the conflict, the colonies were largely autonomous economic and political entities, with no centralized area of ultimate strategic importance.[307] This meant that, unlike Europe where the fall of a capital city often ended wars, that in America continued even after the loss of major settlements such as Philadelphia, the seat of Congress, New York, and Charleston.[308] British power was reliant on the Royal Navy, whose dominance allowed them to resupply their own expeditionary forces while preventing access to enemy ports. However, the majority of the American population was agrarian, rather than urban; supported by the French navy and blockade runners based in the Dutch Caribbean, their economy was able to survive.[309]
Lord North, Prime Minister since 1770, delegated control of the war in North America to Lord George Germain and the Earl of Sandwich, who was head of the Royal Navy from 1771 to 1782. Defeat at Saratoga in 1777 made it clear the revolt would not be easily suppressed, especially after the Franco-American alliance of February 1778. With Spain also expected to join the conflict, the Royal Navy needed to prioritize either the war in America or in Europe; Germain advocated the former, Sandwich the latter.[310]
 North initially backed the Southern strategy attempting to exploit divisions between the mercantile north and slave-owning south, but after the defeat of Yorktown, he was forced to accept that this policy had failed.[311] It was clear the war was lost, although the Royal Navy forced the French to relocate their fleet to the Caribbean in November 1781 and resumed a close blockade of American trade.[312] The resulting economic damage and rising inflation meant the US was now eager to end the war, while France was unable to provide further loans; Congress could no longer pay its soldiers.[313]
The geographical size of the colonies and limited manpower meant the British could not simultaneously conduct military operations and occupy territory without local support. Debate persists over whether their defeat was inevitable; one British statesman described it as ""like trying to conquer a map"".[314] While Ferling argues Patriot victory was nothing short of a miracle,[315] Ellis suggests the odds always favored the Americans, especially after Howe squandered the chance of a decisive British success in 1776, an ""opportunity that would never come again"".[316] The US military history speculates the additional commitment of 10,000 fresh troops in 1780 would have placed British victory ""within the realm of possibility"".[317]
 The expulsion of France from North America in 1763 led to a drastic reduction in British troop levels in the colonies; in 1775, there were only 8,500 regular soldiers among a civilian population of 2.8 million.[318] The bulk of military resources in the Americas were focused on defending sugar islands in the Caribbean; Jamaica alone generated more revenue than all thirteen American colonies combined.[319] With the end of the Seven Years' War, the permanent army in Britain was also cut back, which resulted in administrative difficulties when the war began a decade later.[320]
 Over the course of the war, there were four separate British commanders-in-chief. The first was Thomas Gage, appointed in 1763, whose initial focus was establishing British rule in former French areas of Canada. Many in London blamed the revolt on his failure to take firm action earlier, and he was relieved after the heavy losses incurred at the Battle of Bunker Hill.[321] His replacement was Sir William Howe, a member of the Whig faction in Parliament who opposed the policy of coercion advocated by Lord North; Cornwallis, who later surrendered at Yorktown, was one of many senior officers who initially refused to serve in North America.[322]
 The 1775 campaign showed the British overestimated the capabilities of their own troops and underestimated the colonial militia, requiring a reassessment of tactics and strategy,[323] and allowing the Patriots to take the initiative.[324] Howe's responsibility is still debated; despite receiving large numbers of reinforcements, Bunker Hill seems to have permanently affected his self-confidence and lack of tactical flexibility meant he often failed to follow up opportunities.[325] Many of his decisions were attributed to supply problems, such as his failure to pursue Washington's beaten army.[326] Having lost the confidence of his subordinates, he was recalled after Burgoyne surrendered at Saratoga.[327]
 Following the failure of the Carlisle Commission, British policy changed from treating the Patriots as subjects who needed to be reconciled to enemies who had to be defeated.[328] In 1778, Howe was replaced by Sir Henry Clinton.[329] Regarded as an expert on tactics and strategy,[327] like his predecessors Clinton was handicapped by chronic supply issues.[330] In addition, Clinton's strategy was compromised by conflict with political superiors in London and his colleagues in North America, especially Admiral Mariot Arbuthnot, replaced in early 1781 by Rodney.[249] He was neither notified nor consulted when Germain approved Cornwallis's invasion of the south in 1781 and delayed sending him reinforcements believing the bulk of Washington's army was still outside New York City.[331] After the surrender at Yorktown, Clinton was relieved by Carleton, whose major task was to oversee the evacuation of Loyalists and British troops from Savannah, Charleston, and New York City.[332]
 During the 18th century, states commonly hired foreign soldiers, including Britain.[333] When it became clear additional troops were needed to suppress the revolt in America, it was decided to employ professional German soldiers. There were several reasons for this, including public sympathy for the Patriot cause, a historical reluctance to expand the British army and the time needed to recruit and train new regiments.[334] Many smaller states in the Holy Roman Empire had a long tradition of renting their armies to the highest bidder. The most important was Hesse-Kassel, known as ""the Mercenary State"".[335]
 The first supply agreements were signed by the North administration in late 1775; 30,000 Germans served in the American War.[336] Often generically referred to as ""Hessians"", they included men from many other states, including Hanover and Brunswick.[337] Sir Henry Clinton recommended recruiting Russian troops whom he rated very highly, having seen them in action against the Ottomans; however, negotiations with Catherine the Great made little progress.[338]
 Unlike previous wars their use led to intense political debate in Britain, France, and even Germany, where Frederick the Great refused to provide passage through his territories for troops hired for the American war.[339] In March 1776, the agreements were challenged in Parliament by Whigs who objected to ""coercion"" in general, and the use of foreign soldiers to subdue ""British subjects"".[340] The debates were covered in detail by American newspapers; in May 1776 they received copies of the treaties themselves, provided by British sympathizers and smuggled into North America from London.[341]
 The prospect of foreign German soldiers being used in the colonies bolstered support for independence, more so than taxation and other acts combined; the King was accused of declaring war on his own subjects, leading to the idea there were now two separate governments.[342][343] By apparently showing Britain was determined to go to war, it made hopes of reconciliation seem naive and hopeless, while the employment of what was regarded as ""foreign mercenaries"" became one of the charges levelled against George III in the Declaration of Independence.[339] The Hessian reputation within Germany for brutality also increased support for the Patriot cause among German American immigrants.[344]
 The presence of over 150,000 German Americans meant both sides felt the German soldiers might be persuaded to desert; one reason Clinton suggested employing Russians was that he felt they were less likely to defect. When the first German troops arrived on Staten Island in August 1776, Congress approved the printing of handbills, promising land and citizenship to any willing to join the Patriot cause. The British launched a counter-campaign claiming deserters could be executed.[345] Desertion among the Germans occurred throughout the war, with the highest rate of desertion occurring between the surrender at Yorktown and the Treaty of Paris.[346] German regiments were central to the British war effort; of the estimated 30,000 sent to America, some 13,000 became casualties.[347]
 Wealthy Loyalists convinced the British government that most of the colonists were sympathetic toward the Crown;[348] consequently, British military planners relied on recruiting Loyalists, but had trouble recruiting sufficient numbers as the Patriots had widespread support.[274][ak] Approximately 25,000 Loyalists fought for the British throughout the war.[31] Although Loyalists constituted about twenty percent of the colonial population,[81] they were concentrated in distinct communities. Many of them lived among large plantation owners in the Tidewater region and South Carolina.[81]
 When the British began probing the backcountry in 1777–1778, they were faced with a major problem: any significant level of organized Loyalist activity required a continued presence of British regulars.[349] The available manpower that the British had in America was insufficient to protect Loyalist territory and counter American offensives.[350] The Loyalist militias in the South were constantly defeated by neighboring Patriot militia. The Patriot victory at the Battle of Kings Mountain irreversibly impaired Loyalist militia capability in the South.[231]
 When the early war policy was administered by Howe, the Crown's need to maintain Loyalist support prevented it from using the traditional revolt suppression methods.[351] The British cause suffered when their troops ransacked local homes during an aborted attack on Charleston in 1779 that enraged both Patriots and Loyalists.[219] After Congress rejected the Carlisle Peace Commission in 1778 and Westminster turned to ""hard war"" during Clinton's command, neutral colonists in the Carolinas often allied with the Patriots.[352] Conversely, Loyalists gained support when Patriots intimidated suspected Tories by destroying property or tarring and feathering.[353]
 A Loyalist militia unit—the British Legion—provided some of the best troops in British service.[354] It was commanded by Tarleton and gained a fearsome reputation in the colonies for ""brutality and needless slaughter"".[355][better source needed]
 Women played various roles during the Revolutionary War; they often accompanied their husbands when permitted. For example, throughout the war Martha Washington was known to visit and provide aid to her husband George at various American camps.[356] Women often accompanied armies as camp followers to sell goods and perform necessary tasks in hospitals and camps, and numbered in the thousands during the war.[357]
 Women also assumed military roles: some dressed as men to directly support combat, fight, or act as spies on both sides.[358] Anna Maria Lane joined her husband in the Army. The Virginia General Assembly later cited her bravery: she fought while dressed as a man and ""performed extraordinary military services, and received a severe wound at the battle of Germantown ... with the courage of a soldier"".[359] On April 26, 1777, Sybil Ludington is said to have ridden to alert militia forces to the British's approach; she has been called the ""female Paul Revere"".[360] Whether the ride occurred is questioned.[361][362][363][364] A few others disguised themselves as men. Deborah Sampson fought until her gender was discovered and she was discharged as a result; Sally St. Clair was killed in action.[359]
 When war began, the population of the Thirteen Colonies included an estimated 500,000 slaves, predominantly used as labor on Southern plantations.[365] In November 1775, Lord Dunmore, the royal governor of Virginia, issued a proclamation that promised freedom to any Patriot-owned slaves willing to bear arms. Although the announcement helped to fill a temporary manpower shortage, white Loyalist prejudice meant recruits were eventually redirected to non-combatant roles. The Loyalists' motive was to deprive Patriot planters of labor rather than to end slavery; Loyalist-owned slaves were returned.[366]
 The 1779 Philipsburg Proclamation issued by Clinton extended the offer of freedom to Patriot-owned slaves throughout the colonies. It persuaded entire families to escape to British lines, many of which were employed growing food for the army by removing the requirement for military service. While Clinton organized the Black Pioneers, he also ensured fugitive slaves were returned to Loyalist owners with orders that they were not to be punished.[367] As the war progressed, service as regular soldiers in British units became increasingly common; Black Loyalists formed two regiments of the Charleston garrison in 1783.[368]
 Estimates of the numbers who served the British during the war vary from 25,000 to 50,000, excluding those who escaped during wartime. Thomas Jefferson estimated that Virginia may have lost 30,000 slaves to escapes.[369] In South Carolina, nearly 25,000 slaves (about 30 percent of the enslaved population) either fled, migrated, or died, which significantly disrupted the plantation economies both during and after the war.[370]
 Black Patriots were barred from the Continental Army until Washington convinced Congress in January 1778 that there was no other way to replace losses from disease and desertion. The 1st Rhode Island Regiment formed in February included former slaves whose owners were compensated; however, only 140 of its 225 soldiers were Black and recruitment stopped in June 1788.[371] Ultimately, around 5,000 African Americans served in the Continental Army and Navy in a variety of roles, while another 4,000 were employed in Patriot militia units, aboard privateers, or as teamsters, servants, and spies. After the war, a small minority received land grants or Congressional pensions; many others were returned to their masters post-war despite earlier promises of freedom.[372]
 As a Patriot victory became increasingly likely, the treatment of Black Loyalists became a point of contention; after the surrender of Yorktown in 1781, Washington insisted all escapees be returned but Cornwallis refused. In 1782 and 1783, around 8,000 to 10,000 freed Blacks were evacuated by the British from Charleston, Savannah, and New York; some moved onto London, while 3,000 to 4,000 settled in Nova Scotia.[373] White Loyalists transported 15,000 enslaved Blacks to Jamaica and the Bahamas. The free Black Loyalists who migrated to the British West Indies included regular soldiers from Dunmore's Ethiopian Regiment, and those from Charleston who helped garrison the Leeward Islands.[368]
 Most Native Americans east of the Mississippi River were affected by the war, and many tribes were divided over how to respond. A few tribes were friendly with the colonists, but most Natives opposed the union of the Colonies as a potential threat to their territory. Approximately 13,000 Natives fought on the British side, with the largest group coming from the Iroquois tribes who deployed around 1,500 men.[33]
 Early in July 1776, Cherokee allies of Britain attacked the short-lived Washington District of North Carolina. Their defeat splintered both Cherokee settlements and people, and was directly responsible for the rise of the Chickamauga Cherokee, who perpetuated the Cherokee–American wars against American settlers for decades after hostilities with Britain ended.[374]
 Muscogee and Seminole allies of Britain fought against Americans in Georgia and South Carolina. In 1778, a force of 800 Muscogee destroyed American settlements along the Broad River in Georgia. Muscogee warriors also joined Thomas Brown's raids into South Carolina and assisted Britain during the siege of Savannah.[375] Many Native Americans were involved in the fight between Britain and Spain on the Gulf Coast and along the British side of the Mississippi River. Thousands of Muscogee, Chickasaw, and Choctaw fought in major battles such as the Battle of Fort Charlotte, the Battle of Mobile, and the siege of Pensacola.[376]
 The Iroquois Confederacy was shattered as a result of the American Revolutionary War. The Seneca, Onondaga, and Cayuga tribes sided with the British; members of the Mohawks fought on both sides; and many Tuscarora and Oneida sided with the Americans. To retaliate against raids on American settlement by Loyalists and their Indian allies, the Continental Army dispatched the Sullivan Expedition throughout New York to debilitate the Iroquois tribes that had sided with the British. Mohawk leaders Joseph Louis Cook and Joseph Brant sided with the Americans and the British respectively, which further exacerbated the split.[377]
 In the western theater, conflicts between settlers and Native Americans led to lingering distrust.[378] In the 1783 Treaty of Paris, Great Britain ceded control of the disputed lands between the Great Lakes and the Ohio River, but Native inhabitants were not a part of the peace negotiations.[379] Tribes in the Northwest Territory joined as the Western Confederacy and allied with the British to resist American settlement, and their conflict continued after the Revolutionary War as the Northwest Indian War.[380]
 The terms presented by the Carlisle Peace Commission in 1778 included acceptance of the principle of self-government. Parliament would recognize Congress as the governing body, suspend any objectionable legislation, surrender its right to local colonial taxation, and discuss including American representatives in the House of Commons. In return, all property confiscated from Loyalists would be returned, British debts honored, and locally enforced martial law accepted. However, Congress demanded either immediate recognition of independence or the withdrawal of all British troops; they knew the commission were not authorized to accept these, bringing negotiations to a rapid end.[382]
 On February 27, 1782, a Whig motion to end the offensive war in America was carried by 19 votes.[383] North resigned, obliging the king to invite Lord Rockingham to form a government; a consistent supporter of the Patriot cause, he made a commitment to U.S. independence a condition of doing so. George III reluctantly accepted and the new government took office on March 27, 1782; however, Rockingham died unexpectedly on July 1, and was replaced by Lord Shelburne who acknowledged American independence.[384]
 When Lord Rockingham was elevated to Prime Minister, Congress consolidated its diplomatic consuls in Europe into a peace delegation at Paris. The dean of the delegation was Benjamin Franklin. He had become a celebrity in the French Court, but he was also influential in the courts of Prussia and Austria. Since the 1760s, Franklin had been an organizer of British American inter-colony cooperation, and then served as a colonial lobbyist to Parliament in London. John Adams had been consul to the Dutch Republic and was a prominent early New England Patriot. John Jay of New York had been consul to Spain and was a past president of the Continental Congress. As consul to the Dutch Republic, Henry Laurens had secured a preliminary agreement for a trade agreement. Although active in the preliminaries, he was not a signer of the conclusive treaty.[263]
 The Whig negotiators included long-time friend of Franklin David Hartley, and Richard Oswald, who had negotiated Laurens' release from the Tower of London.[263] The Preliminary Peace signed on November 30 met four key Congressional demands: independence, territory up to the Mississippi, navigation rights into the Gulf of Mexico, and fishing rights in Newfoundland.[263]
 British strategy was to strengthen the U.S. sufficiently to prevent France from regaining a foothold in North America, and they had little interest in these proposals.[385] However, divisions between their opponents allowed them to negotiate separately with each to improve their overall position, starting with the American delegation in September 1782.[386] The French and Spanish sought to improve their position by creating the U.S. dependent on them for support against Britain, thus reversing the losses of 1763.[387] Both parties tried to negotiate a settlement with Britain excluding the Americans; France proposed setting the western boundary of the U.S. along the Appalachians, matching the British 1763 Proclamation Line. The Spanish suggested additional concessions in the vital Mississippi River Basin, but required the cession of Georgia in violation of the Franco-American alliance.[387]
 Facing difficulties with Spain over claims involving the Mississippi River, and from France who was still reluctant to agree to American independence until all her demands were met, John Jay told the British that he was willing to negotiate directly with them, cutting off France and Spain, and Prime Minister Lord Shelburne, in charge of the British negotiations, agreed.[388] Key agreements for the United States in obtaining peace included recognition of US independence; all of the territory east of the Mississippi River, north of Florida and south of Canada; and fishing rights in the Grand Banks, off the coast of Newfoundland and in the Gulf of Saint Lawrence. The United States and Great Britain were each given perpetual access to the Mississippi River.[389][390]
 An Anglo-American Preliminary Peace was formally entered into in November 1782, and Congress endorsed the settlement on April 15, 1783. It announced the achievement of peace with independence, and the conclusive treaty was signed on September 2, 1783, in Paris, effective the following day when Britain signed its treaty with France. John Adams, who helped draft the treaty, claimed it represented ""one of the most important political events that ever happened on the globe"". Ratified respectively by Congress and Parliament, the final versions were exchanged in Paris the following spring.[391] On November 25, the last British troops remaining in the U.S. were evacuated from New York to Halifax.[392]
 The expanse of territory that was now the U.S. included millions of sparsely settled acres south of the Great Lakes line between the Appalachian Mountains and the Mississippi River, much of which was part of Canada. The tentative colonial migration west became a flood during the war.[393]
 Britain's extended post-war policy for the U.S. continued to try to establish an Indian barrier state below the Great Lakes as late as 1814 during the War of 1812. The formally acquired western American lands continued to be populated by Indigenous tribes that had mostly been British allies.[379] In practice the British refused to abandon the forts on territory they formally transferred. Instead, they provisioned military allies for continuing frontier raids and sponsored the Northwest Indian War (1785–1795). British sponsorship of local warfare on the U.S. continued until the Anglo-American Jay Treaty, authored by Hamilton, went into effect on February 29, 1796.[394][al]
 Of the European powers with American colonies adjacent to the newly created U.S., Spain was most threatened by American independence, and it was correspondingly the most hostile to it.[am] Its territory adjacent to the U.S. was relatively undefended, so Spanish policy developed a combination of initiatives. Spanish soft power diplomatically challenged the British territorial cession west to the Mississippi River and the previous northern boundaries of Spanish Florida.[396] It imposed a high tariff on American goods, then blocked American settler access to the port of New Orleans. At the same time, the Spanish also sponsored war within the U.S. by Indian proxies in its Southwest Territory ceded by France to Britain, then Britain to the Americans.[393]
 The total loss of life throughout the conflict is largely unknown. As was typical in wars of the era, diseases such as smallpox claimed more lives than battle. Between 1775 and 1782, a smallpox epidemic throughout North America killed an estimated 130,000.[41][an] Historian Joseph Ellis suggests that Washington having his troops inoculated against the disease was one of his most important decisions.[397]
 Up to 70,000 American Patriots died during active military service.[398] Of these, approximately 6,800 were killed in battle, while at least 17,000 died from disease. The majority of the latter died while prisoners of war of the British, mostly in the prison ships in New York Harbor.[399][ao] The number of Patriots seriously wounded or disabled by the war has been estimated from 8,500 to 25,000.[400]
 The French suffered 2,112 killed in combat in the United States.[401][ap] The Spanish lost 124 killed and 247 wounded in West Florida.[402][aq]
 A British report in 1781 puts their total Army deaths at 6,046 in North America (1775–1779).[41][ar] Approximately 7,774 Germans died in British service in addition to 4,888 deserters; among those labeled German deserters, however, it is estimated that 1,800 were killed in combat.[13][as]
 The American Revolution set an example to overthrow both monarchy and colonial governments. The United States has the world's oldest written constitution, which was used as a model in other countries, sometimes word-for-word. The Revolution inspired revolutions in France, Haiti, Latin America, and elsewhere.[410]
 Although the Revolution eliminated many forms of inequality, it did little to change the status of women, despite the role they played in winning independence. Most significantly, it failed to end slavery. While many were uneasy over the contradiction of demanding liberty for some, yet denying it to others, the dependence of southern states on slave labor made abolition too great a challenge. Between 1774 and 1780, many of the states banned the importation of slaves, but the institution itself continued.[411] In 1782, Virginia passed a law permitting manumission and over the next eight years more than 10,000 slaves were given their freedom.[412] The number of abolitionist movements greatly increased, and by 1804 all the northern states had outlawed it.[413] However,  slavery continued to be a serious social and political issue and caused divisions that would ultimately end in civil war.
 The body of historical writings on the American Revolution cite many motivations for the Patriot revolt.[414] American Patriots stressed the denial of their constitutional rights as Englishmen, especially ""no taxation without representation."" Contemporaries credit the American Enlightenment with laying the intellectual, moral, and ethical foundations for the American Revolution among the Founding Fathers, who were influenced by the classical liberalism of John Locke and other Enlightenment writers and philosophers.
 Two Treatises of Government has long been cited as a major influence on Revolutionary-era American thinking, but historians David Lundberg and Henry F. May contend that Locke's Essay Concerning Human Understanding was far more widely read.[415] Historians since the 1960s have emphasized that the Patriot constitutional argument was made possible by the emergence of an American nationalism that united the Thirteen Colonies. In turn, that nationalism was rooted in a Republican value system that demanded consent of the governed and deeply opposed aristocratic control.[416] In Britain, on the other hand, republicanism was largely a fringe ideology since it challenged the aristocratic control of the British monarchy and political system. Political power was not controlled by an aristocracy or nobility in the 13 colonies; instead, the colonial political system was based on the winners of free elections, which were open at the time to the majority of white men. In analysis of the Revolution, historians in recent decades have often cited three motivations behind it:[417]
 After the first U.S. postage stamp was issued in 1849, the U.S. Postal Service frequently issued commemorative stamps celebrating people and events of the Revolutionary War. The first such stamp was the Liberty Bell issue of 1926.[424]
"
Second Continental Congress,https://en.wikipedia.org/wiki/Second_Continental_Congress,"

 The Second Continental Congress (1775–1781) was the meetings of delegates from the Thirteen Colonies that united in support of the American Revolution and the Revolutionary War, which established American independence from the British Empire. The Congress constituted a new federation that it first named the United Colonies of North America, and in 1776, renamed the United States of America. The Congress began convening in Philadelphia, on May 10, 1775, with representatives from 12 of the 13 colonies, after the Battles of Lexington and Concord. 
 The Second Continental Congress succeeded the First Continental Congress, which had met from September 5 to October 26, 1774, also in Philadelphia. The Second Congress functioned as the de facto federation government at the outset of the Revolutionary War by raising militias, directing strategy, appointing diplomats, and writing petitions such as the Declaration of the Causes and Necessity of Taking Up Arms and the Olive Branch Petition.[1] All 13 colonies were represented by the time the Congress adopted the Lee Resolution, which declared independence from Great Britain on July 2, 1776, and the Congress unanimously agreed to the Declaration of Independence two days later. 
 Congress functioned as the provisional government of the United States of America through March 1, 1781, when congress became what is now often called the Confederation Congress. During this period, it successfully managed the war effort, drafted the Articles of Confederation and Perpetual Union, adopted the first U.S. constitution, secured diplomatic recognition and support from foreign nations, and resolved state land claims west of the Appalachian Mountains.
 Many of the delegates who attended the Second Congress had also attended the First. They again elected Peyton Randolph as president of the Congress and Charles Thomson as secretary.[2] Notable new arrivals included Benjamin Franklin of Pennsylvania and John Hancock of Massachusetts. Within two weeks, Randolph was summoned back to Virginia to preside over the House of Burgesses; Hancock succeeded him as president, and Thomas Jefferson replaced Randolph in the Virginia delegation.[3] The number of participating colonies also grew, as Georgia endorsed the Congress in July 1775 and adopted the continental ban on trade with Britain.[4]
 The First Continental Congress had sent entreaties to King George III to stop the Intolerable Acts. They also created the Continental Association to establish a coordinated protest of these acts, boycotting British goods in protest to them. The Second Continental Congress met on May 10, 1775, to plan further responses if the British government did not repeal or modify the acts; however, the American Revolutionary War had started by that time with the Battles of Lexington and Concord, and the Congress was called upon to take charge of the war effort.
 For the first few months of the Revolutionary War, the patriots carried on their struggle in a largely ad-hoc and uncoordinated manner. Even so, they had numerous successes, seizing numerous British arsenals, driving royal officials out of several colonies, and launching the Siege of Boston in order to prevent the movement by land of British troops stationed there. On June 14, 1775, the Second Continental Congress voted to create the Continental Army out of the militia units around Boston, and the next day unanimously approved a motion naming George Washington of Virginia as its commanding general.[5][6] 
 On July 6, 1775, Congress approved a Declaration of Causes outlining the rationale and necessity for taking up arms in the Thirteen Colonies. Two days later, delegates signed the Olive Branch Petition to King George III affirming the colonies' loyalty to the crown and imploring the king to prevent further conflict. However, by the time British Colonial Secretary Lord Dartmouth received the petition, King George III had already issued a proclamation on August 23, 1775, in response to the Battle of Bunker Hill, declaring elements of Britain's continental American possessions to be in a state of what he called an ""open and avowed rebellion"". As a result, the king refused to receive the petition.[7]
 Georgia had not participated in the First Continental Congress and did not initially send delegates to the Second. But with the Revolutionary War escalating, the residents of St. John's Parish in present-day Liberty County sent Lyman Hall to the gathering in Philadelphia on their behalf.[8] He participated in debates but did not vote, as he did not represent the entire colony.[9] That changed after July 1775, when a provincial Congress decided to send delegates to the Continental Congress and to adopt a ban on trade with Britain.[4]
 The Continental Congress had no explicit legal authority from the British to govern,[10] but it assumed all the functions of a national government, including appointing ambassadors, signing treaties, raising armies, appointing generals, obtaining loans from Europe, issuing paper money called ""Continentals"", and disbursing funds. Congress had no authority to levy taxes and was required to request money, supplies, and troops from the states to support the war effort. Individual states frequently ignored these requests.
 Congress was moving towards declaring independence from the British Empire in 1776, but many delegates lacked the authority from their home governments to take such drastic action. Advocates of independence moved to have reluctant colonial governments revise instructions to their delegations, or even replace those governments which would not authorize independence. On May 10, 1776, Congress passed a resolution recommending that any colony with a government that was not inclined toward independence should form one that was. On May 15, they adopted a more radical preamble to this resolution, drafted by John Adams, which advised throwing off oaths of allegiance and suppressing the authority of the Crown in any colonial government that still derived its authority from the Crown. That same day, the Virginia Convention instructed its delegation in Philadelphia to propose a resolution that called for a declaration of independence, the formation of foreign alliances, and a confederation of the states. The resolution of independence was delayed for several weeks, as advocates of independence consolidated support in their home governments.
 On June 7, 1776, Richard Henry Lee offered the resolution before the Congress, declaring the colonies independent. He urged Congress to resolve ""to take the most effectual measures for forming foreign Alliances"" and to prepare a plan of confederation for the newly independent states.[11] Lee argued that independence was the only way to ensure a foreign alliance since no European monarchs would deal with America if they remained Britain's colonies. American leaders had rejected the divine right of kings in the New World, but recognized the necessity of proving their credibility in the Old World.[12]
 The Congress moved to Baltimore in the winter of 1776–77 to avoid capture by British forces who were advancing on Philadelphia. Henry Fite's tavern was the largest building in Baltimore at the time and provided a comfortable location of sufficient size for Congress to meet. Its site at the western edge of town was beyond easy reach of the British Royal Navy's ships should they attempt to sail up the harbor and the Patapsco River to shell the town. Congress was again forced to flee Philadelphia at the end of September 1777, as British troops seized and occupied the city; they moved to York, Pennsylvania, where they continued their work.
 Congress passed the Articles of Confederation on November 15, 1777, after more than a year of debate, and sent it to the states for ratification. Approval by all 13 states was required for the establishment of the constitution. Jefferson's proposal for a Senate to represent the states and a House to represent the people was rejected, but a similar proposal was adopted later in the United States Constitution. One issue of debate was large states wanting a larger say, nullified by small states who feared tyranny. The small states prevailed, and each state was afforded one vote.[13] Another revolved around the issue of western land claims; states without such claims wanted those with claims to yield them to Congress. As written, western land claims remained in the hands of the individual states. Congress urged the states to give their assent quickly, and most did.[14] The first to ratify was Virginia on December 16, 1777; 12 states had ratified the Articles by February 1779, 14 months into the process.[15] The lone holdout, Maryland, finally ratified the Articles on February 2, 1781, doing so only after Virginia relinquished its claims on land north of the Ohio River to Congress.[14]
"
Philadelphia,https://en.wikipedia.org/wiki/Philadelphia,"

 Philadelphia (/fɪləˈdɛlfi.ə/ ⓘ fill-ə-DEL-fee-ə), colloquially referred to as Philly, is the most populous city in the U.S. state of Pennsylvania[11] and the sixth-most populous city in the United States, with a population of 1,603,797 in the 2020 census. The city is the urban core of the larger Delaware Valley, also known as the Philadelphia metropolitan area, the nation's eighth-largest metropolitan area and seventh-largest combined statistical area with 6.245 million residents and 7.366 million residents, respectively.[12]
 Philadelphia has played an extensive role in United States history. The city was founded in 1682 by William Penn, an English Quaker and advocate of religious freedom, and served as the capital of the Pennsylvania Colony during the colonial era.[3][13] The city went on to play a historic and vital role during the American Revolution and Revolutionary War, serving as the central meeting place for the nation's founding fathers, hosting the First Continental Congress in 1774, preserving the Liberty Bell, and hosting the Second Continental Congress during which the nation's 56 founders formed the Continental Army and elected George Washington as its commander in 1775, and unanimously adopted the Declaration of Independence on July 4, 1776. For nine months, from September 1777 to June 1778, the city was occupied by the British during the Philadelphia campaign.[14] In 1787, the U.S. Constitution was ratified in Philadelphia at the Philadelphia Convention. Philadelphia remained the nation's largest city until 1790, and it served as the nation's first capital from May 10, 1775, until December 12, 1776, and on four subsequent occasions until 1800, when construction of the new national capital in Washington, D.C. was completed.[15]
 Philadelphia maintains extensive contemporary influence in business and industry, culture, sports, and music.[16][17] With 17 four-year universities and colleges in the city, Philadelphia is one of the nation's leading centers for higher education and academic research.[18][19] The city is a national cultural center, hosting more outdoor sculptures and murals than any other city in the nation.[20][21] Fairmount Park, when combined with adjacent Wissahickon Valley Park in the same watershed, is 2,052 acres (830 ha), representing one of the nation's largest and the world's 55th-largest urban park.[22] Philadelphia is known for its arts, culture, cuisine, and colonial and Revolutionary-era history; in 2016, it attracted 42 million domestic tourists who spent $6.8 billion, representing  $11 billion in economic impact to the city and its surrounding Pennsylvania counties.[23] With five professional sports teams and one of the nation's most loyal and passionate fan bases, Philadelphia is often ranked as the nation's best city for professional sports fans.[24][25][26][27] The city has a culturally and philanthropically active LGBTQ+ community. Philadelphia also has played an immensely influential historic and ongoing role in the development and evolution of American music, especially R&B, soul, and rock.[28][29]
 As of 2023[update], the Philadelphia metropolitan area had a gross metropolitan product of US$557.6 billion[9] and is home to 13 Fortune 500 corporate headquarters.[30] Metropolitan Philadelphia ranks as one of the nation's Big Five venture capital hubs, facilitated by its geographic proximity to both the entrepreneurial and financial ecosystems of New York City and the federal regulatory environment of Washington, D.C.[31] Greater Philadelphia is also a biotechnology hub. The Philadelphia Stock Exchange, owned by Nasdaq since 2008, is the nation's oldest stock exchange and a global leader in options trading.[32] 30th Street Station, the city's primary rail station, is the third-busiest Amtrak hub in the nation with over 4.1 million passengers in 2023. The city's multimodal transportation and logistics infrastructure includes Philadelphia International Airport, a major transatlantic gateway and transcontinental hub;[33] the rapidly-growing PhilaPort seaport;[34] and Interstate 95, the spine of the north–south highway system along the U.S. East Coast.
 Philadelphia is a city of many firsts, including the nation's first library (1731),[35] hospital (1751),[35] medical school (1765),[36] national capital (1774),[37] university (by some accounts) (1779),[38] central bank (1781),[39] stock exchange (1790),[35] zoo (1874),[40] and business school (1881).[41] Philadelphia contains 67 National Historic Landmarks, including Independence Hall.[42][43][19] From the city's 17th century founding through the present, Philadelphia has been the birthplace or home to an extensive number of prominent and influential Americans.
 Before the arrival of Europeans in the early 17th century, the Philadelphia area was home to the Lenape Indians in the village of Shackamaxon. They were also called the Delaware Indians,[44] and their historical territory was along the Delaware River watershed, western Long Island, and the Lower Hudson Valley.[a] Most Lenape were pushed out of the region during the 18th century by expanding European colonies, exacerbated by losses from intertribal conflicts.[44] Lenape communities were weakened by newly introduced diseases, mainly smallpox, and conflict with Europeans. The Iroquois occasionally fought the Lenape. Surviving Lenape moved west into the upper Ohio River basin. Following the American Revolutionary War and subsequent formation of an independent United States, the Lenape began moving further west. In the 1860s, the United States government sent most remaining Lenape in the eastern United States to the Indian Territory in present-day Oklahoma and surrounding territories as part of the Indian removal policy.
 Europeans first entered Philadelphia and the surrounding Delaware Valley in the early 17th century. The first settlements were founded by Dutch colonists, who built Fort Nassau on the Delaware River in 1623 in what is now Brooklawn, New Jersey. The Dutch considered the entire Delaware River valley to be part of their New Netherland colony. In 1638, Swedish settlers led by renegade Dutch established the colony of New Sweden at Fort Christina, located in present-day Wilmington, Delaware, and quickly spread out in the valley. In 1644, New Sweden supported the Susquehannocks in their war against Maryland colonists.[45] In 1648, the Dutch built Fort Beversreede on the west bank of the Delaware, south of the Schuylkill River near the present-day Eastwick section of Philadelphia, to reassert their dominion over the area. The Swedes responded by building Fort Nya Korsholm, or New Korsholm, named after a town in Finland with a Swedish majority.
 In 1655, a Dutch military campaign led by New Netherland Director-General Peter Stuyvesant took control of the Swedish colony, ending its claim to independence. The Swedish and Finnish people settlers continued to have their own militia, religion, and court, and to enjoy substantial autonomy under the Dutch. An English fleet captured the New Netherland colony in 1664, though the situation did not change substantially until 1682, when the area was included in William Penn's charter for Pennsylvania.[46]
 In 1681, in partial repayment of a debt, Charles II of England granted Penn a charter for what would become the Pennsylvania colony. Despite the royal charter, Penn bought the land from the local Lenape in an effort to establish good terms with the Native Americans and ensure peace for the colony.[47] Penn made a treaty of friendship with Lenape chief Tammany under an elm tree at Shackamaxon, in what is now the city's Fishtown neighborhood.[3] Penn named the city Philadelphia, which is Greek for ""brotherly love"", derived from the Ancient Greek terms φίλος phílos (beloved, dear) and ἀδελφός adelphós (brother, brotherly). There were a number of cities named Philadelphia in the Eastern Mediterranean during the Greek and Roman periods, including modern Alaşehir, mentioned as the site of an early Christian congregation in the Book of Revelation. As a Quaker, Penn had experienced religious persecution and wanted his colony to be a place where anyone could worship freely. This tolerance, which exceeded that of other colonies, led to better relations with the local native tribes and fostered Philadelphia's rapid growth into America's most important city.[48]
 Penn planned a city on the Delaware River to serve as a port and place for government. Hoping that Philadelphia would become more like an English rural town instead of a city, Penn laid out roads on a grid plan to keep houses and businesses spread far apart with areas for gardens and orchards.
 The city's inhabitants did not follow Penn's plans, however, and instead crowded the present-day Port of Philadelphia on the Delaware River and subdivided and resold their lots.[49] Before Penn left Philadelphia for the final time, he issued the Charter of 1701 establishing it as a city. Though poor at first, Philadelphia became an important trading center with tolerable living conditions by the 1750s. Benjamin Franklin, a leading citizen, helped improve city services and founded new ones that were among the first in the nation, including a fire company, library, and hospital.
 A number of philosophical societies were formed, which were centers of the city's intellectual life, including the Philadelphia Society for Promoting Agriculture (1785), the Pennsylvania Society for the Encouragement of Manufactures and the Useful Arts (1787), the Academy of Natural Sciences (1812), and the Franklin Institute (1824).[50] These societies developed and financed new industries that attracted skilled and knowledgeable immigrants from Europe.
 Philadelphia's importance and central location in the colonies made it a natural center for America's revolutionaries. By the 1750s, Philadelphia surpassed Boston as the largest city and busiest port in British America, and the second-largest city in the entire British Empire after London.[52][53] In 1774, as resentment of the British government's policies towards the colonies and support for independence was burgeoning in the colonies, Philadelphia hosted the First Continental Congress.
 From 1775 to 1781, Philadelphia hosted the Second Continental Congress,[54] which adopted the Declaration of Independence in what was then called the Pennsylvania State House and was later renamed Independence Hall. Historian Joseph Ellis, in 2007, described the Declaration of Independence, written predominantly by Thomas Jefferson, as ""the most potent and consequential words in American history,""[14] and its adoption represented a declaration of war against Great Britain. Since the Declaration's July 4, 1776, adoption, its signing has been cited globally and repeatedly by various peoples of the world seeking independence and liberty. It also has been, since its adoption, the basis for annual celebration by Americans; in 1938, this celebration of the Declaration was formalized as Independence Day, one of only eleven designated U.S. federal holidays.
 After George Washington's defeat at the Battle of Brandywine in Chadds Ford Township, on September 11, 1777, during the Philadelphia campaign, the revolutionary capital of Philadelphia was defenseless and the city prepared for what was perceived to be an inevitable British attack. Because bells could easily be recast into munitions, the Liberty Bell, then known as the Pennsylvania State Bell, and bells from two Philadelphia churches, Christ Church and St. Peter's Church, were hastily taken down and transported by heavily guarded wagon train out of the city. The Liberty Bell was taken to Zion German Reformed Church in Northampton Town, which is present-day Allentown, where it was hidden under the church's floor boards for nine months from September 1777 until departure of British forces from Philadelphia in June 1778.[55] Two Revolutionary War battles, the Siege of Fort Mifflin, fought between September 26 and November 16, 1777, and the Battle of Germantown, fought on October 4, 1777, took place within Philadelphia's city limits.
 In Philadelphia, the Second Continental Congress adopted the Articles of Confederation on November 15, 1777, and the city later served as the meeting place for the Constitutional Convention, which ratified the Constitution in Independence Hall in Philadelphia on September 17, 1787.
 Philadelphia served as capital of the United States for much of the colonial and early post-colonial periods, including for a decade, from 1790 to 1800, while Washington, D.C., was being constructed and prepared to serve as the new national capital.[56] In 1793, the largest yellow fever epidemic in U.S. history killed approximately 4,000 to 5,000 people in Philadelphia, or about ten percent of the city's population at the time.[57][58] The capital of the United States was moved to Washington, D.C. in 1800 upon completion of the White House and U.S. Capitol buildings.
 The state capital was moved from Philadelphia to Lancaster in 1799, then ultimately to Harrisburg in 1812. Philadelphia remained the nation's largest city until the late 18th century. It also was the nation's financial and cultural center until ultimately being eclipsed in total population by New York City in 1790. In 1816, the city's free Black community founded the African Methodist Episcopal Church, the first independent Black denomination in the country, and the first Black Episcopal Church. The free Black community also established many schools for its children with the help of Quakers. Large-scale construction projects for new roads, canals, and railroads made Philadelphia the first major industrial city in the United States.
 Throughout the 19th century, Philadelphia hosted a variety of industries and businesses; the largest was the textile industry. Major corporations in the 19th and early 20th centuries included the Baldwin Locomotive Works, William Cramp & Sons Shipbuilding Company, and the Pennsylvania Railroad.[59] Established in 1870, the Philadelphia Conveyancers' Association was chartered by the state in 1871. Along with the U.S. Centennial in 1876, the city's industry was celebrated in the Centennial Exposition, the first official World's fair in the U.S.
 Immigrants, mostly from Ireland and Germany, settled in Philadelphia and the surrounding districts. These immigrants were largely responsible for the first general strike in North America in 1835, in which workers in the city won the ten-hour workday. The city was a destination for thousands of Irish immigrants fleeing the Great Famine in the 1840s; housing for them was developed south of South Street and later occupied by succeeding immigrants. They established a network of Catholic churches and schools and dominated the Catholic clergy for decades. Anti-Irish, anti-Catholic nativist riots erupted in Philadelphia in 1844. The rise in population of the surrounding districts helped lead to the Act of Consolidation of 1854, which extended the city limits from the 2 square miles (5.2 km2) of Center City to the roughly 134 square miles (350 km2) of Philadelphia County.[60][61]
In the latter half of the 19th century and leading into the 20th century, immigrants from Russia, Eastern Europe, and Italy, and African Americans from the southern U.S. settled in the city.[62]
 Philadelphia was represented by the Washington Grays in the American Civil War. The African-American population of Philadelphia increased from 31,699 to 219,559 between 1880 and 1930, largely stemming from the Great Migration from the South.[63][64]
 By the 20th century, Philadelphia had an entrenched Republican political machine and a complacent population.[clarification needed][65] In 1910, a general strike shut down the entire city.[66]
 In 1917, following outrage over the election-year murder of a Philadelphia police officer, the City Council shrank from two houses to just one.[67] In July 1919, Philadelphia was one of more than 36 industrial cities nationally to suffer a race riot during Red Summer in post-World War I unrest as recent immigrants competed with Blacks for jobs. In the 1920s, the public flouting of Prohibition laws, organized crime, mob violence, and corrupt police involvement in illegal activities led to the appointment of Brig. Gen. Smedley Butler of the U.S. Marine Corps as the city's director of public safety, but political pressure still prevented long-term success in fighting crime and corruption.[68]
 In 1940, non-Hispanic whites constituted 86.8% of the city's population.[69] In 1950, the population peaked at more than two million residents, then began to decline with the restructuring of industry that led to the loss of many middle-class union jobs. In addition, suburbanization enticed many affluent residents to depart the city for its outlying railroad commuting towns and newer housing. The resulting reduction in Philadelphia's tax base and the resources of local government caused the city to struggle through a long period of adjustment, and it approached bankruptcy by the late 1980s.[70][71]
 In 1985, the MOVE Bombing of the Cobbs Creek neighborhood by city helicopters occurred, killing 11 and destroying 61 homes.[72]
 Revitalization and gentrification of neighborhoods began in the late 1970s and continues into the 21st century with much of the development occurring in the Center City and University City neighborhoods. But this expanded a shortage of affordable housing in the city. After many manufacturers and businesses left Philadelphia or shut down, the city started attracting service businesses and began to market itself more aggressively as a tourist destination. Contemporary glass-and-granite skyscrapers were built in Center City beginning in the 1980s. Historic areas such as Old City and Society Hill were renovated during the reformist mayoral era of the 1950s through the 1980s, making both areas among the most desirable Center City neighborhoods. Immigrants from around the world began to enter the U.S. through Philadelphia as their gateway, leading to a reversal of the city's population decline between 1950 and 2000, during which it lost about 25 percent of its residents.[73][74]
 Philadelphia eventually began experiencing a growth in its population in 2007, which continued with incremental annual increases through the present.[75][76] A migration pattern has been established from New York City to Philadelphia by residents opting for a large city with relative proximity and a lower cost of living.[77][78]
 Philadelphia's geographic center is about 40° 0′ 34″ north latitude and 75° 8′ 0″ west longitude. The 40th parallel north passes through neighborhoods in Northeast Philadelphia, North Philadelphia, and West Philadelphia including Fairmount Park. The city encompasses 142.71 square miles (369.62 km2), of which 134.18 square miles (347.52 km2) is land and 8.53 square miles (22.09 km2), or 6%, is water.[79] Natural bodies of water include the Delaware and Schuylkill rivers, lakes in Franklin Delano Roosevelt Park, and Cobbs, Wissahickon, and Pennypack creeks. The largest artificial body of water is East Park Reservoir in Fairmount Park.
 The lowest point is sea level and the highest point is in Chestnut Hill, about 446 feet (136 m) above sea level on Summit Street near the intersection of Germantown Avenue and Bethlehem Pike at: 40.07815 N, 75.20747 W.[80][81] Philadelphia is located on the Atlantic Seaboard Fall Line that separates the Atlantic Plain from the Piedmont.[82] The Schuylkill River's rapids at East Falls were inundated by completion of the dam at Fairmount Water Works.[83]
 The city is the seat of its own county. The city is bordered by six adjacent counties: Montgomery to the northwest; Bucks to the north and northeast; Burlington County, New Jersey to the east; Camden County, New Jersey to the southeast; Gloucester County, New Jersey to the south; and Delaware County to the southwest.
 Philadelphia was created in the 17th century, following the plan by William Penn's surveyor Thomas Holme. Center City is structured with long, straight streets running nearly due east–west and north–south, forming a grid pattern between the Delaware and Schuylkill rivers that is aligned with their courses. The original city plan was designed to allow for easy travel and to keep residences separated by open space that would help prevent the spread of fire.[84] In keeping with the idea of a ""Greene Countrie Towne"", and inspired by the many types of trees that grew in the region, Penn named many of the east–west streets for local trees.[85] Penn planned the creation of five public parks in the city which were renamed in 1824.[84] Centre Square was renamed Penn Square;[86] Northeast Square was renamed Franklin Square; Southeast Square was renamed Washington Square; Southwest Square was renamed Rittenhouse Square; and Northwest Square was renamed Logan Circle/Square.[87] Center City had an estimated 183,240 residents as of 2015[update], making it the second-most populated downtown area in the United States after Midtown Manhattan in New York City.[88]
 Philadelphia's neighborhoods are divided into six large sections that surround Center City: North Philadelphia, Northeast Philadelphia, South Philadelphia, Southwest Philadelphia, West Philadelphia, and Northwest Philadelphia. The city's geographic boundaries have been largely unchanged since these neighborhoods were consolidated in 1854. However, each of these large areas contains numerous neighborhoods, some of whose boundaries derive from the boroughs, townships, and other communities that constituted Pennsylvania County before their inclusion within the city.[89]
 The City Planning Commission, tasked with guiding growth and development of the city, has divided the city into 18 planning districts as part of the Philadelphia2035 physical development plan.[90][91] Much of the city's 1980 zoning code was overhauled from 2007 to 2012 as part of a joint effort between former mayors John F. Street and Michael Nutter. The zoning changes were intended to rectify incorrect zoning maps to facilitate future community development, as the city forecasts an additional 100,000 residents and 40,000 jobs will be added by 2035.
 The Philadelphia Housing Authority (PHA) is the largest landlord in Pennsylvania. Established in 1937, the PHA is the nation's fourth-largest housing authority, serving about 81,000 people with affordable housing, while employing 1,400 on a budget of $371 million.[92] The Philadelphia Parking Authority is responsible for ensuring adequate parking for city residents, businesses, and visitors.[93]
 Philadelphia's architectural history dates back to colonial times and includes a wide range of styles. The earliest structures were constructed with logs, but brick structures were common by 1700. During the 18th century, the cityscape was dominated by Georgian architecture, including Independence Hall and Christ Church.
 In the first decades of the 19th century, Federal and Greek Revival architecture were the dominant styles produced by Philadelphia architects such as Benjamin Latrobe, William Strickland, John Haviland, John Notman, Thomas Walter, and Samuel Sloan.[94] Frank Furness is considered Philadelphia's greatest architect of the second half of the 19th century. His contemporaries included John McArthur Jr., Addison Hutton, Wilson Eyre, the Wilson Brothers, and Horace Trumbauer. In 1871, construction began on the Second Empire-style Philadelphia City Hall. The Philadelphia Historical Commission was created in 1955 to preserve the cultural and architectural history of the city. The commission maintains the Philadelphia Register of Historic Places, adding historic buildings, structures, sites, objects and districts as it sees fit.[95]
 In 1932, Philadelphia became home to the first modern International Style skyscraper in the United States, the PSFS Building, designed by George Howe and William Lescaze. The 548 ft (167 m) City Hall remained the tallest building in the city until 1987 when One Liberty Place was completed. Numerous glass and granite skyscrapers were built in Center City beginning in the late 1980s. In 2007, the Comcast Center surpassed One Liberty Place to become the city's tallest building. The Comcast Technology Center was completed in 2018, reaching a height of 1,121 ft (342 m), as the tallest building in the United States outside of Manhattan and Chicago.[96]
 For much of Philadelphia's history, the typical home has been the row house. The row house was introduced to the United States via Philadelphia in the early 19th century and, for a time, row houses built elsewhere in the United States were known as ""Philadelphia rows"".[94] A variety of row houses are found throughout the city, from Federal-style continuous blocks in Old City and Society Hill to Victorian-style homes in North Philadelphia to twin row houses in West Philadelphia. While newer homes have been built recently, much of the housing dates to the 18th, 19th, and early 20th centuries, which has created problems such as urban decay and vacant lots. Some neighborhoods, including Northern Liberties and Society Hill, have been rehabilitated through gentrification.[97][98]
 As of 2014[update], the city's total park space, including municipal, state, and federal parks in the city, amounts to 11,211 acres (17.5 sq mi).[22] Philadelphia's largest park is Fairmount Park, which includes the Philadelphia Zoo and encompasses 2,052 acres (3.2 sq mi) of the total parkland. Fairmount Park's adjacent Wissahickon Valley Park contains 2,042 acres (3.2 sq mi).[100] Fairmount Park, when combined with Wissahickon Valley Park, is one of the largest contiguous urban park areas in the U.S.[22] The two parks, along with the Colonial Revival, Georgian and Federal-style mansions in them, have been listed as one entity on the National Register of Historic Places since 1972.[101]
 Within the Köppen climate classification, Philadelphia falls under the northern periphery of the humid subtropical climate zone (Köppen Cfa).[102] Within the Trewartha climate classification, Philadelphia has a temperate maritime climate (Do) limited to the north by the continental climate (Dc).[103] Summers are typically hot and muggy. Fall and spring are generally mild, and winter is moderately cold. The plant life hardiness zones are 7a and 7b, reflecting an average annual extreme minimum temperature between 0 and 10 °F (−18 and −12 °C).[104]
 Snowfall is highly variable. Some winters have only light snow while others include major snowstorms. The normal seasonal snowfall averages 22.4 in (57 cm), with rare snowfalls in November or April, and rarely any sustained snow cover.[105] Seasonal snowfall accumulation has ranged from trace amounts in 1972–73, to 78.7 inches (200 cm) in the winter of 2009–10.[105][b] The city's heaviest single-storm snowfall was 30.7 in (78 cm), which occurred in January 1996.[106]
 Precipitation is generally spread throughout the year, with eight to eleven wet days per month,[107] at an average annual rate of 44.1 inches (1,120 mm), but historically ranging from 29.31 in (744 mm) in 1922 to 64.33 in (1,634 mm) in 2011.[105] The most rain recorded in one day occurred on July 28, 2013, when 8.02 in (204 mm) fell at Philadelphia International Airport.[105] Philadelphia has a moderately sunny climate with an average of 2,498 hours of sunshine annually. The percentage of sunshine ranges from 47% in December to 61% in June, July, and August.[108]
 The January daily average temperature is 33.7 °F (0.9 °C). The temperature frequently rises to 50 °F (10 °C) during thaws. July averages 78.7 °F (25.9 °C). Heat waves accompanied by high humidity and heat indices are frequent, with highs reaching or exceeding 90 °F (32 °C) on 30 days of the year. The average window for freezing temperatures is November 6 to April 2,[105] allowing a growing season of 217 days. Early fall and late winter are generally dry, with February having the lowest average precipitation at 2.75 inches (70 mm). The dewpoint in the summer averages between 59.1 and 64.5 °F (15 and 18 °C).[105]
 The highest recorded temperature was 106 °F (41 °C) on August 7, 1918. Temperatures at or above 100 °F (38 °C) are not common, with the last occurrence of such a temperature being July 21, 2019.[109] The lowest officially recorded temperature was −11 °F (−24 °C) on February 9, 1934.[109] Temperatures at or below 0 °F (−18 °C) are rare, with the last such occurrence being January 19, 1994.[105] The record low maximum is 5 °F (−15 °C) on February 10, 1899, and December 30, 1880. The record high minimum is 83 °F (28 °C) on July 23, 2011, and July 24, 2010.[110]
 See or edit raw graph data.
 Philadelphia County received an ozone grade of F and a 24-hour particle pollution rating of D in the American Lung Association's 2017 State of the Air report, which analyzed data from 2013 to 2015.[115][116] The city was ranked 22nd for ozone, 20th for short-term particle pollution, and 11th for year-round particle pollution.[117] According to the same report, the city experienced a significant reduction in high ozone days since 2001—from nearly 50 days per year to fewer than 10—along with fewer days of high particle pollution since 2000—from about 19 days per year to about 3—and an approximate 30% reduction in annual levels of particle pollution since 2000.[116]
 Five of the ten largest combined statistical areas (CSAs) were ranked higher for ozone: Los Angeles (1st), New York City (9th), Houston (12th), Dallas (13th), and San Jose, California (18th). Many smaller CSAs were also ranked higher for ozone, including Sacramento (8th), Las Vegas (10th), Denver (11th), El Paso (16th), and Salt Lake City (20th). Only two of those same ten CSAs, San Jose and Los Angeles, were ranked higher than Philadelphia for both year-round and short-term particle pollution.[117]
 As of the 2020 U.S. Census, there were 1,603,797 people residing in Philadelphia, representing a 1.2% increase from the 2019 census estimate.[76] The racial composition of the city was 39.3% Black alone (42.0% Black alone or in combination), 36.3% White alone (41.9% White alone or in combination), 8.7% Asian alone, 0.4% American Indian and Alaska Native alone, 8.7% some other race, and 6.9% multiracial. 14.9% of residents were Hispanic or Latino.[120]
 34.8% had a bachelor's degree or higher. 23.9% spoke a language other than English at home, the most common of which was Spanish (10.8%). 15.0% of the populations foreign born, roughly half of whom are naturalized U.S. citizens. 3.7% of the population are veterans. The median household income was $52,889 and 22.8% of the population lived in poverty. 49.5% of the population drove alone to work, while 23.2% used public transit, 8.2% carpooled, 7.9% walked, and 7.0% worked from home. The average commute is 31 minutes.[120]
 After the 1950 census, when a record high of 2,071,605 was recorded, the city's population began a long decline. The population dropped to a low of 1,488,710 residents in 2006 before beginning to rise again. Between 2006 and 2017, Philadelphia added 92,153 residents. In 2017, the U.S. Census Bureau estimated that the racial composition of the city was 41.3% Black (non-Hispanic), 34.9% White (non-Hispanic), 14.1% Hispanic or Latino, 7.1% Asian, 0.4% Native American, 0.05% Pacific Islander, and 2.8% multiracial.[121]
 In addition to the city's economic growth, the city's population has been fueled by foreign immigration. According to The Pew Charitable Trusts, the city's foreign-born population increased by 69% between 2000 and 2016 to constitute nearly 20% of Philadelphia's workforce,[125] and it doubled between 1990 and 2017 to constitute 13.8% of the city's total population, with the top five countries of origin being China by a significant margin followed by the Dominican Republic, Jamaica, India, and Vietnam.[126]
 Irish, Italian, German, Polish, English, Russian, Ukrainian, and French ancestries constitute the largest European ethnic groups in the city.[128] Philadelphia has the second-largest Irish and Italian populations in the United States after New York City. South Philadelphia remains one of the largest Italian neighborhoods in the country and is home to the Italian Market.
 The Pennsport neighborhood and Gray's Ferry section of South Philadelphia, home to many Mummer clubs, are well known as Irish neighborhoods. The Kensington, Port Richmond, and Fishtown neighborhoods have historically been heavily Irish and Polish. Port Richmond is a center for the Polish-American community in Philadelphia, and it remains a common destination for Polish immigrants. Northeast Philadelphia, although known for its Irish and Irish-American population, is home to a Jewish and Russian population. Mount Airy in Northwest Philadelphia also contains a Jewish community. Nearby Chestnut Hill is historically known as an Anglo-Saxon Protestant community.
 Philadelphia's Black American population is the fourth-largest in the country after New York City, Chicago, and Houston. West Philadelphia and North Philadelphia are largely African-American neighborhoods, but many are leaving those areas in favor of the Northeast and Southwest sections of Philadelphia. A higher proportion of African-American Muslims reside in Philadelphia than most other major U.S. cities. West Philadelphia and Southwest Philadelphia are home to various Afro-Caribbean and African immigrant communities.[129]
 The Puerto Rican population in Philadelphia is the second-largest on the U.S. mainland after New York City, and the second-fastest growing after Orlando.[130] Eastern North Philadelphia, particularly Fairhill and surrounding areas to the north and east, has one of the highest concentrations of Puerto Ricans outside Puerto Rico, with many large swaths of blocks being close to 100% Puerto Rican.[131][132] Puerto Rican and Dominican populations reside in North Philadelphia and the Northeast, and Mexican and Central American populations exist in South Philadelphia.[133] South American migrants were being transported by bus from Texas to Philadelphia beginning in 2022.[134]
 Philadelphia's Asian American population includes those of Chinese, Indians, Vietnamese, South Koreans, Filipinos, Cambodians, and Indonesians. Over 35,000 Chinese Americans lived in the city in 2015,[135] including a Fuzhounese population. Center City hosts a Chinatown that is served by Chinatown bus lines with service to/from Chinatown, Manhattan.[136] Indians make up the second-largest Asian group in the city of Philadelphia,[137] while making up the largest foreign-born population in the greater Delaware Valley.[138] A Korean community initially settled in the North Philadelphia neighborhood of Olney; however, the primary Koreatown has subsequently shifted further north, straddling the city's border with adjacent Cheltenham in Montgomery County and Cherry Hill in South Jersey. South Philadelphia is home to Vietnamese-Americans in Little Saigon and Cambodian-Americans in Cambodia Town, as well as Thai-American, Indonesian-American, and Chinese-American communities.
 Philadelphia's Gay village near Washington Square is home to a concentration of gay and lesbian-friendly businesses, restaurants, and bars.[139][140]
 In a 2014 study by the Pew Research Center, 68% of the population of the city identified themselves as Christian.[141] Approximately 41% of Christians in the city and area professed attendance at a variety of churches that could be considered Protestant, while 26% professed Catholic beliefs.
 The Protestant Christian community in Philadelphia is dominated by mainline Protestant denominations including the Evangelical Lutheran Church in America, United Church of Christ, the Episcopal Church in the United States, Presbyterian Church (USA) and American Baptist Churches USA. One of the most prominent mainline Protestant jurisdictions is the Episcopal Diocese of Pennsylvania. The African Methodist Episcopal Church was established in Philadelphia. Historically, the city has strong connections to the Quakers, Unitarian Universalism, and the Ethical Culture movement, all of which continue to be represented in the city. The Quaker Friends General Conference is based in Philadelphia. Evangelical Protestants making up less than 15% of the population were also prevalent.
 Evangelical Protestant bodies included the Anglican Church in North America, Lutheran Church—Missouri Synod, Presbyterian Church in America, and National Baptist Convention of America. The Catholic community is primarily served by the Latin Catholic Archdiocese of Philadelphia, the Ukrainian Catholic Archeparchy of Philadelphia, and the Syro-Malankara Catholic Eparchy of the United States of America and Canada, though some independent Catholic churches exist throughout Philadelphia and its suburbs. The Latin Church-based jurisdiction is headquartered in the city, and its see is the Cathedral Basilica of Saints Peter and Paul. The Ukrainian Catholic jurisdiction is headquartered in Philadelphia, and is seated at the Cathedral of the Immaculate Conception.
 Less than 1% of Philadelphia's Christians were Mormons. The remainder of the Christian demographic is spread among smaller Protestant denominations and the Eastern and Oriental Orthodox among others. The Diocese of Eastern Pennsylvania (Orthodox Church in America) and Greek Orthodox Archdiocese of America (Ecumenical Patriarchate) divide the Eastern Orthodox in Philadelphia. The Russian Orthodox St. Andrew's Cathedral is in the city.
The same study says that other religions collectively compose about 8% of the population, including Judaism, Hinduism, Islam, Buddhism, and Sikhism.[142] Philadelphia has the fifth-largest Muslim population among U.S. cities.[143] The remaining 24% claimed no religious affiliation.
 The Philadelphia metropolitan area's Jewish population was estimated at 206,000 in 2001, which was the sixth-largest in the U.S. at that time.[144] Jewish traders were operating in southeastern Pennsylvania long before William Penn. Jews in Philadelphia took a prominent part in the War of Independence. Although the majority of the early Jewish residents were of Portuguese or Spanish descent, some among them had emigrated from Germany and Poland. About the beginning of the 19th century, a number of Jews from the latter countries, finding the services of the Congregation Mickvé Israel unfamiliar to them, resolved to form a new congregation which would use the ritual to which they had been accustomed.
 African diasporic religions are practiced in some Latino and Hispanic and Caribbean communities in North and West Philadelphia.[145][146]
 As of 2010[update], 79.12% (1,112,441) of Philadelphia residents age 5 and older spoke English at home as a primary language, while 9.72% (136,688) spoke Spanish, 1.64% (23,075) Chinese, 0.89% (12,499) Vietnamese, 0.77% (10,885) Russian, 0.66% (9,240) French, 0.61% (8,639) other Asian languages, 0.58% (8,217) African languages, 0.56% (7,933) Cambodian (Mon-Khmer), and Italian was spoken as a main language by 0.55% (7,773) of the population over the age of five. In total, 20.88% (293,544) of Philadelphia's population age 5 and older spoke a mother language other than English.[147]
 Philadelphia is home to many food poverty programs, of which two of the largest are Philabundance which claims to feed 90000 people per week.[148][149][150][151] and Share Food Program which claims to feed 1 million people per month.[152]
 Philadelphia's close geographical and transportation connections to other large metropolitan economies along the Eastern Seaboard of the United States have been cited as offering a significant competitive advantage for business creation and entrepreneurship.[154] The city is the center of economic activity in both Pennsylvania and the four-state Delaware Valley metropolitan region. Five Fortune 500 companies are based in the city. As of 2021[update], the Philadelphia metropolitan area is estimated to produce a gross metropolitan product (GMP) of US$479 billion,[155] an increase from the $445 billion calculated by the Bureau of Economic Analysis for 2017,[156] representing the ninth-largest U.S. metropolitan economy.
 Philadelphia's economic sectors include financial services, health care, biotechnology, information technology, trade and transportation, manufacturing, oil refining, food processing, and tourism. Metropolitan Philadelphia is one of the top five American venture capital hubs, credited to its proximity to New York City's financial and tech and biotechnology ecosystems.[31] Financial activities account for the largest economic sector of the metropolitan area, which is one of the largest health education and research centers in the United States. The city's two largest employers are the federal and city governments. Philadelphia's largest private employer is the University of Pennsylvania followed by Children's Hospital of Philadelphia.[157]
 The Philadelphia Stock Exchange, acquired by Nasdaq in 2007, is a global leader in options trading.[32] The city is home to the headquarters of Comcast, the nation's largest multinational telecommunications corporation; insurance conglomerates Cigna, Colonial Penn, and Independence Blue Cross; as well as food services company Aramark, chemical makers FMC Corporation and Rohm and Haas, pharmaceutical companies GlaxoSmithKline, Amicus Therapeutics, Spark Therapeutics, apparel retailers Five Below and Urban Outfitters and its subsidiary Anthropologie, automotive parts retailer Pep Boys, and stainless steel producer Carpenter Technology Corporation.
 Other corporation headquarters in the city include RiteAid, Crown Holdings, and Brandywine Realty Trust. The headquarters of Boeing Rotorcraft Systems and its main rotorcraft factory are in the Philadelphia suburb of Ridley Park; The Vanguard Group, and the U.S. headquarters of Siemens Healthineers are headquartered in Malvern, Pennsylvania, a Philadelphia suburb. Healthcare conglomerate AmerisourceBergen is located in suburban Conshohocken, Pennsylvania. Across the Delaware River in adjacent Camden County, New Jersey, Campbell Soup Company and Subaru USA are both headquartered in Camden, New Jersey, and TD Bank (USA) is headquartered in nearby suburban Cherry Hill, New Jersey.
 Philadelphia is a hub for information technology and biotechnology.[158] Philadelphia and Pennsylvania are attracting new life sciences ventures.[159] As of 2024, the Delaware Valley ranks as one of the Big Five U.S. venture capital hubs, enabled by its proximity to both the entrepreneurial and financial ecosystems of New York City and to the federal regulatory environment of Washington, D.C.[31][159]
 Philadelphia's history attracts many tourists, with the Independence National Historical Park, which includes the Liberty Bell, Independence Hall, and other historic sites, received over 5 million visitors in 2016.[160] The city welcomed 42 million domestic tourists in 2016 who spent $6.8 billion, generating an estimated $11 billion in total economic impact in the city and surrounding four counties of Pennsylvania.[23] The annual Naked Bike Ride attracts participants from around the United States and internationally to Philadelphia.
 Philadelphia International Airport, a major Transatlantic gateway and transcontinental hub, has undergone a $900 million infrastructural expansion to increase passenger capacity and augment passenger experience, and the airport continues an ongoing capital expenditure program to upgrade facilities and add further amenities.[161][162] The Port of Philadelphia, having experienced the highest percentage growth by tonnage loaded in 2017 among major U.S. seaports, has doubled its shipping capacity to accommodate super-sized post-Panamax shipping vessels since 2018.[163] Philadelphia's 30th Street Station is the third-busiest Amtrak rail hub, following Penn Station in Manhattan and Union Station in Washington, D.C., transporting over 4 million inter-city rail passengers annually.[164]
 Education in Philadelphia is provided by many private and public institutions. The School District of Philadelphia is the local school district, operating public schools, in all of the city.[165] The Philadelphia School District is the eighth-largest school district in the nation[166] with 142,266 students in 218 traditional public schools and 86 charter schools as of 2014[update].[167]
 The city's K-12 enrollment in district–run schools dropped from 156,211 students in 2010 to 130,104 students in 2015. During the same time period, the enrollment in charter schools increased from 33,995 students in 2010 to 62,358 students in 2015.[157] This consistent drop in enrollment led the city to close 24 of its public schools in 2013.[168] During the 2014 school year, the city spent an average of $12,570 per pupil, below the average among comparable urban school districts.[157]
 Graduation rates among district-run schools, meanwhile, steadily increased in the ten years from 2005. In 2005, Philadelphia had a district graduation rate of 52%. This number increased to 65% in 2014, still below the national and state averages. Scores on the state's standardized test, the Pennsylvania System of School Assessment (PSSA) trended upward from 2005 to 2011 but subsequently decreased. In 2005, the district-run schools scored an average of 37.4% on math and 35.5% on reading. The city's schools reached their peak scores in 2011 with 59.0% on math and 52.3% on reading. In 2014, the scores dropped significantly to 45.2% on math and 42.0% on reading.[157]
 Of the city's public high schools, including charter schools, only four performed above the national average on the SAT (1497 out of 2400[169]) in 2014: Masterman, Central, Girard Academic Music Program, and MaST Community Charter School. All other district-run schools were below average.[157]
 Medical and research facilities of the University of Pennsylvania School of Medicine and the Children's Hospital of Philadelphia. Philadelphia has the third-largest student concentration on the East Coast, with more than 120,000 college and university students enrolled within the city and nearly 300,000 in the metropolitan area.[170] More than 80 colleges, universities, trade, and specialty schools are in the Philadelphia region. One of the founding members of the Association of American Universities is in the city, the University of Pennsylvania, an Ivy League institution with claims to be the First university in the United States.[171]
 The city's largest university by student enrollment is Temple University, followed by Drexel University.[172] The city's nationally ranked research universities comprise the University of Pennsylvania, Temple University, Drexel University, and Thomas Jefferson University. Philadelphia is also home to five schools of medicine: Drexel University College of Medicine, Perelman School of Medicine at the University of Pennsylvania, Philadelphia College of Osteopathic Medicine, Temple University School of Medicine, and Thomas Jefferson University's Sidney Kimmel Medical College. Hospitals, universities, and higher education research institutions in Philadelphia's four congressional districts received more than $252 million in National Institutes of Health grants in 2015.[173]
 Other institutions of higher learning within the city's borders include:
 Philadelphia is home to many national historical sites that relate to the founding of the United States. Independence National Historical Park is the center of these historical landmarks and one of the country's 22 UNESCO World Heritage Sites. Independence Hall, where the Declaration of Independence was signed, and the Liberty Bell is housed, are among the city's most popular attractions. Other national historic sites include the homes of Edgar Allan Poe and Thaddeus Kosciuszko, and early government buildings, including the First and the Second Bank of the United States, Fort Mifflin, and the Gloria Dei (Old Swedes') Church.[174] Philadelphia alone has 67 National Historic Landmarks, the third most of any city in the country.[174]
 Philadelphia's major science museums include the Franklin Institute, which contains the Benjamin Franklin National Memorial, the Academy of Natural Sciences, the Mütter Museum, and the University of Pennsylvania Museum of Archaeology and Anthropology. History museums include the National Constitution Center, the Museum of the American Revolution, the Philadelphia History Museum, the National Museum of American Jewish History, the African American Museum in Philadelphia, the Historical Society of Pennsylvania, the Masonic Library and Museum of Pennsylvania in the Masonic Temple, and the Eastern State Penitentiary. Philadelphia is home to the United States's first zoo[175] and hospital,[176] as well as Fairmount Park, one of America's oldest and largest urban parks,[22] founded in 1855.[177]
 The city is home to important archival repositories, including the Library Company of Philadelphia, established in 1731 by Benjamin Franklin at 1314 Locust Street,[178] and the Athenaeum of Philadelphia, founded in 1814.[179] The Presbyterian Historical Society is the country's oldest denominational historical society, organized in 1852.[180]
 The city is home to multiple art museums, including the Pennsylvania Academy of the Fine Arts and the Rodin Museum, which holds the largest collection of work by Auguste Rodin outside France. The city's largest art museum, the Philadelphia Museum of Art, is one of the largest art museums in the world. The long flight of steps to the Art Museum's main entrance became famous after the film Rocky (1976).[181]
 Annual events include the Philadelphia Film Festival, held annually each October, the 6abc Dunkin' Donuts Thanksgiving Day Parade, the nation's longest-running continuously held Thanksgiving Day parade, and the Mummers Parade, the nation's longest continuously held folk parade, which is held every New Year's Day predominantly on Broad Street.
 Areas such as South Street and the Old City section of the city have a vibrant night life. The Avenue of the Arts in Center City contains many restaurants and theaters, such as the Kimmel Center for the Performing Arts, home of the Philadelphia Orchestra, and the Academy of Music, home of Opera Philadelphia and the Pennsylvania Ballet.[181] The Wilma Theatre and the Philadelphia Theatre Company at the Suzanne Roberts Theatre produce a variety of new plays.[182][183] Several blocks to the east are the Lantern Theater Company at St. Stephens Episcopal Church;[184] and the Walnut Street Theatre, a National Historic Landmark stated to be the oldest and most subscribed-to theatre in the English-speaking world, founded in 1809.[185] In May 2019, the Walnut Street Theatre announced a major expansion to begin in 2020.[186] New Freedom Theatre, Pennsylvania's oldest African-American theatre, is located on North Broad Street.
 Philadelphia has more public art than any other American city.[187] In 1872, the Association for Public Art, formerly the Fairmount Park Art Association, was created as the first private association in the United States dedicated to integrating public art and urban planning.[188] In 1959, lobbying by the Artists Equity Association helped create the Percent for Art ordinance, the first for a U.S. city.[189] The program, which has funded more than 200 pieces of public art, is administered by the Philadelphia Office of Arts and Culture, the city's art agency.[190] The city has more murals than any other American city, due to the 1984 creation of the Department of Recreation's Mural Arts Program, which seeks to beautify neighborhoods and provide an outlet for graffiti artists. The program has funded more than 2,800 murals by professional, staff and volunteer artists and educated more than 20,000 youth in underserved neighborhoods throughout Philadelphia.[191]
 The city is home to a number of art organizations, including the regional art advocacy nonprofit Philadelphia Tri-State Artists Equity,[192] the Philadelphia Sketch Club, one of the country's oldest artists' clubs,[193] and The Plastic Club, started by women excluded from the Sketch Club.[194] Many Old City art galleries stay open late on the First Friday event of each month.[195]
 The city is known for its hoagies, stromboli, roast pork sandwich, scrapple, soft pretzels, water ice, Irish potato candy, tastykakes, and the cheesesteak sandwich which was developed by Italian immigrants.[196] The Philadelphia area has many establishments that serve cheesesteaks, including restaurants, taverns, delicatessens and pizza parlors.[197][198][199] The originator of the thinly-sliced steak sandwich in the 1930s, initially without cheese, is Pat's King of Steaks, which faces its rival Geno's Steaks, founded in 1966,[200] across the intersection of 9th Street and Passyunk Avenue in the Italian Market of South Philadelphia.[201]
 McGillin's Olde Ale House, opened in 1860 on Drury Street in Center City, is the oldest continuously operated tavern in the city.[202] The City Tavern is a replica of a historic 18th-century building first opened in 1773, demolished in 1854 after a fire, and rebuilt in 1975 on the same site as part of Independence National Historical Park.[203] The tavern offers authentic 18th-century recipes, served in seven period dining rooms, three wine cellar rooms and an outdoor garden.[204]
 The Reading Terminal Market is a historic food market founded in 1893 in the Reading Terminal building, a designated National Historic Landmark. The enclosed market is one of the oldest and largest markets in the country, hosting over a hundred merchants offering Pennsylvania Dutch specialties, artisan cheese and meat, locally grown groceries, and specialty and ethnic foods.[205]
 The traditional Philadelphia accent is considered by some linguists to be the most distinctive accent in North America.[206] The Philadelphia dialect, which is spread throughout the Delaware Valley and South Jersey, is part of a larger Mid-Atlantic American English family, a designation that also includes the Baltimore accent. Additionally, it shares many similarities with the New York accent. Owing to over a century of linguistic data collected by researchers at the University of Pennsylvania under sociolinguist William Labov, the Philadelphia dialect has been one of the best-studied forms of American English.[207][208][f] The accent is especially found within the Irish American and Italian American working-class neighborhoods.[209] Philadelphia also has its own unique collection of neologisms and slang terms.[210]
 The Philadelphia Orchestra is generally considered one of the top five orchestras in the United States. The orchestra performs at the Kimmel Center[211] and has a summer concert series at the Mann Center for the Performing Arts.[212] Opera Philadelphia performs at the nation's oldest continually operating opera house—the Academy of Music.[181] The Philadelphia Boys Choir & Chorale has performed its music all over the world.[213] The Philly Pops plays orchestral versions of popular jazz, swing, Broadway, and blues songs at the Kimmel Center and other venues within the mid-Atlantic region.[214] The Curtis Institute of Music is one of the world's premier conservatories and among the most selective institutes of higher education in the nation.[215]
 Philadelphia has played a prominent role in the music of the United States. The culture of American popular music has been influenced by significant contributions of Philadelphia area musicians and producers, in both the recording and broadcasting industries. In 1952, the teen dance party program called Bandstand premiered on local television, hosted by Bob Horn. The show was renamed American Bandstand in 1957, when it began national syndication on ABC, hosted by Dick Clark and produced in Philadelphia until 1964 when it moved to Los Angeles.[216] Promoters marketed youthful musical artists known as teen idols to appeal to the young audience. Philadelphia-born singers, including Frankie Avalon, James Darren, Eddie Fisher, Fabian Forte, Bobby Rydell, and South Philly-raised Chubby Checker, topped the music charts, establishing a clean-cut rock and roll image.
 Philly soul music of the late 1960s–1970s is a highly produced version of soul music which led to later forms of popular music such as disco and urban contemporary rhythm and blues.[217] On July 13, 1985, John F. Kennedy Stadium was the American venue for the Live Aid concert.[218] The city also hosted the Live 8 concert, which attracted about 700,000 people to the Benjamin Franklin Parkway on July 2, 2005.[219]
 Notable rock and pop musicians from Philadelphia and its suburbs include Bill Haley & His Comets, Nazz, Todd Rundgren, Hall & Oates, the Hooters, Cinderella, DJ Jazzy Jeff & the Fresh Prince, Ween, Schoolly D, Pink, the Roots, Beanie Sigel, State Property, Lisa ""Left Eye"" Lopes, Meek Mill, Lil Uzi Vert, and others.
 Philadelphia has one of the nation's richest histories in professional sports, dating back to the mid-19th century. Its first professional sports team, the Philadelphia Athletics, a professional baseball team, was founded in 1860.[220] The Athletics were initially an amateur league team that turned professional in 1871. In 1876, the Athletics joined with seven other teams in founding the National League, now the longest continuously operating league in world sports.[221] 
 Philadelphia is one of 12 U.S. cities to have teams in all four major league sports: the Philadelphia Phillies of Major League Baseball (MLB), the Philadelphia Eagles of the National Football League (NFL), the Philadelphia Flyers of the National Hockey League (NHL), and the Philadelphia 76ers of the National Basketball Association (NBA).[222] The Phillies, formed in 1883 as the Quakers and renamed in 1884,[223] are the oldest team continuously playing under the same name in the same city in the history of American professional sports.[224]
 The Philadelphia metro area is also home to the Philadelphia Union of Major League Soccer (MLS), plays their home games at Subaru Park, a soccer-specific stadium in Chester, Pennsylvania.[225][226][227]
 Philadelphia was the second of eight U.S. cities to win titles in all four major leagues, the MLB, NFL, NHL, and NBA. It won a title in soccer in the now-defunct North American Soccer League in 1973. Following the 76ers' victory over the Los Angeles Lakers in the 1983 NBA Finals, however, the city's professional teams and their fans endured 25 years without a championship in any professional sport[228] until the Phillies won the 2008 World Series, defeating the Tampa Bay Rays.[229][230] This quarter century without a championship for any Philadelphia sports team is sometimes described as the Curse of Billy Penn, a reference to a 1987 decision that permitted One Liberty Place to become the first building in city history to surpass the height of William Penn, a statue installed in 1894 atop City Hall.[231]  In 2004, during the city's championship drought, ESPN placed Philadelphia second on its list of ""The Fifteen Most Tortured Sports Cities"".[232][233] The city's sports fans are often both praised and sometimes derided. In 2011, for instance, GQ magazine named Eagles and Phillies fans the nation's worst professional sports fans, describing them as the ""Meanest Fans in America"" in summarizing repeated incidents of their drunken behavior and long history of booing.[234][235]
 After the Phillies won the 2008 World Series, nine years passed without a championship until the Eagles won their first Super Bowl following the 2017 season, defeating the New England Patriots in Super Bowl LII. Seven seasons later, following the 2024 season, the Eagles won their second Super Bowl, defeating the Kansas City Chiefs in Super Bowl LIX.[236] 
 Major professional sports teams that originated in Philadelphia, which later moved to other cities, include the Golden State Warriors basketball team, which played in Philadelphia from 1946 to 1962[237] and the Oakland Athletics baseball team, which was originally the Philadelphia Athletics and played in Philadelphia from 1901 to 1954.[238]
 Philadelphia is home to professional, semi-professional, and elite amateur teams in multiple other sports, including cricket, rugby league, and rugby union. Major running events in the city include the Penn Relays, the Philadelphia Marathon, and the Broad Street Run. The Collegiate Rugby Championship is played annually each June at Talen Energy Stadium in Chester.[239]
 The city also has a rich history in rowing, which has been popular in Philadelphia since the 18th century.[240] On Boathouse Row, a symbol of Philadelphia's rich rowing history, each Big Five member has its own boathouse.[241] Philadelphia hosts numerous local and collegiate rowing clubs and competitions, including the annual Dad Vail Regatta,  the largest intercollegiate rowing event in North America with more than 100 participating U.S. and Canadian colleges and universities;[242] the annual Stotesbury Cup Regatta, which is billed as the world's oldest and largest rowing event for high school students;[243][244] and the Head of the Schuylkill Regatta.[245] The regattas are held on the Schuylkill River and organized by Schuylkill Navy, an association of area rowing clubs that has produced numerous Olympic rowers.[246]
 The Philadelphia Spinners were a professional ultimate team in Major League Ultimate (MLU) until 2016. The Spinners were one of the original eight teams of the American Ultimate Disc League (AUDL), which was founded in 2012. They played at Franklin Field and won the inaugural AUDL championship and the final MLU championship in 2016.[247] The MLU was suspended indefinitely by its investors in December 2016.[248] As of 2018[update], the Philadelphia Phoenix continue to play in the AUDL.[249]
 Philadelphia is home to the Philadelphia Big 5, a group of five NCAA Division I college basketball programs, including La Salle, Penn, Saint Joseph's, Temple, and Villanova universities.[250] The sixth NCAA Division I school in Philadelphia is Drexel University. La Salle won the 1954 championship of the NCAA Division I men's basketball tournament.[251] Villanova won the 1985,[252] 2016,[253] and 2018[254] NCAA Division I men's basketball tournaments. Philadelphia will be one of the eleven US host cities for the 2026 FIFA World Cup.[255]
 Philadelphia County is a legal nullity. All county functions were assumed by the city in 1952.[256] The city has been coterminous with the county since 1854.[61]
 Philadelphia's 1952 Home Rule Charter was written by the City Charter Commission, which was created by the Pennsylvania General Assembly in an act of April 1949, and a city ordinance of June 1949. The existing city council received a proposed draft in February 1951, and the electors approved it in an election held in April 1951.[257] The first elections under the new Home Rule Charter were held in November 1951, and the newly elected officials took office in January 1952.[256]
 The city uses the strong-mayor version of the mayor–council form of government, which is led by one mayor in whom executive authority is vested. The mayor has the authority to appoint and dismiss members of all boards and commissions without the approval of the city council. Elected at-large, the mayor is limited to two consecutive four-year terms, but can run for the position again after an intervening term.[257]
 Philadelphia County is coterminous with the First Judicial District of Pennsylvania. The Philadelphia County Court of Common Pleas is the trial court of general jurisdiction for the city, hearing felony-level criminal cases and civil suits above the minimum jurisdictional limit of $10,000. The court has appellate jurisdiction over rulings from the Municipal and Traffic Courts, and some administrative agencies and boards. The trial division has 70 commissioned judges elected by the voters, along with about one thousand other employees.[258] The court has a family division with 25 judges[259] and an orphans' court with three judges.[260]
 As of 2018[update], the city's District Attorney is Larry Krasner, a Democrat.[261] The last Republican to hold the office is Ronald D. Castille, who left in 1991 and later served as the Chief Justice of the Pennsylvania Supreme Court from 2008 to 2014.[262]
 The Philadelphia Municipal Court handles traffic cases, misdemeanor and felony criminal cases with maximum incarceration of five years, and civil cases involving $12,000 or less ($15,000 in real estate and school tax cases), and all landlord-tenant disputes. The municipal court has 27 judges elected by the voters.[263]
 Pennsylvania's three appellate courts also have sittings in Philadelphia. The Supreme Court of Pennsylvania, the court of last resort in the state, regularly hears arguments in Philadelphia City Hall.[264] The Superior Court of Pennsylvania and the Commonwealth Court of Pennsylvania also sit in Philadelphia several times a year.[265][266] Judges for these courts are elected at large.[267] The state Supreme Court and Superior Court have deputy prothonotary offices in Philadelphia.[268][269]
 Philadelphia is home to the federal United States District Court for the Eastern District of Pennsylvania and the Court of Appeals for the Third Circuit, both of which are housed in the James A. Byrne United States Courthouse.[270][271]
 The current mayor is Cherelle Parker who won the election in November 2023.[272] Parker's predecessor, Jim Kenney, served two terms from 2016 to January 2024.[273] Parker is a member of the Democratic Party. For over seven decades, since 1952, every Philadelphia mayor has been a Democrat.
 Philadelphia City Council is the legislative branch which consists of ten council members representing individual districts and seven members elected at-large, all of whom are elected to four-year terms.[274] Democrats are currently the majority and hold 14 seats including nine of the ten districts and five at-large seats. Republicans hold one seat: the Northeast-based Tenth District. The Working Families Party holds two at-large seats making them the council's minority party. The current council president is Kenyatta Johnson.[275]
 Philadelphia's political structure consists of a system of wards and divisions. There are 66 wards with 11 to 51 divisions each for a total of 1703 divisions. Each division elects two committee people who are supposed to live within the division boundaries, and committee people select a leader for their ward.[276] Democrats and Republicans elect their own committee people every four years. The committee person's role is to serve as a point of contact between voters and party officials and help get out the vote.[277] Most wards are closed which means the ward leader makes sole endorsement decisions; open wards allow committee people to weigh in on these decisions.[278] There are groups such as Open Wards Philadelphia Archived April 2, 2024, at the Wayback Machine and individuals who are working to elect ward leaders who promote an open ward system.[279]
 Philadelphia had historically been a bastion of the Republican Party from the American Civil War until the mid-1930s.[281][282] In 1856, the first Republican National Convention was held at Musical Fund Hall at 808 Locust Street in Philadelphia.[283]
 Democratic registrations increased after the Great Depression; however, the city was not carried by Democrat Franklin D. Roosevelt in his landslide victory of 1932, as Pennsylvania was one of only six states won by Republican Herbert Hoover. Voter turnout surged from 600,000 in 1932 to nearly 900,000 in 1936 and Roosevelt carried Philadelphia with over 60% of the vote. Philadelphia has voted Democratic in every presidential election since 1936. In 2008, Democrat Barack Obama drew 83% of the city's vote. Obama's win was even greater in 2012, capturing 85% of the vote. In 2016, Democrat Hillary Clinton won 82% of the vote.[284]
 As a result of the previously declining population in the city and state,[285] Philadelphia has only three congressional districts of the 18 districts in Pennsylvania, based on the 2010 census apportionment:[286] the 2nd district, represented by Brendan Boyle; the 3rd, represented by Dwight Evans; and the 5th, represented by Mary Gay Scanlon.[287] All three representatives are Democrats, though Republicans still have some support in the city, primarily in the Northeast.[288] Sam Katz ran competitive mayoral races as the Republican nominee in 1999 and 2003, losing to Democrat John Street both times.[289][290]
 Pennsylvania's longest-serving Senator, Arlen Specter,[291] was an alumnus of the University of Pennsylvania who opened his first law practice in Philadelphia.[292] Specter served as a Republican from 1981 and as a Democrat from 2009, losing that party's primary in 2010 and leaving office in January 2011.[293] He was assistant counsel on the Warren Commission in 1964 and the city's district attorney from 1966 to 1974.[292]
 Philadelphia has hosted various national conventions, including in 1848 (Whig), 1856 (Republican), 1872 (Republican), 1900 (Republican), 1936 (Democratic), 1940 (Republican), 1948 (Republican), 1948 (Progressive), 2000 (Republican), and 2016 (Democratic).[294] Philadelphia has been home to one vice president, George M. Dallas,[295] and one general of the American Civil War, George B. McClellan, who won his party's nomination for president but lost in the general election to Abraham Lincoln in 1864.[296] In May 2019, former U.S. Vice President Joe Biden chose Philadelphia to be his 2020 U.S. presidential campaign headquarters.[297]
 ""Green Cities, Clean Water"" is an environmental policy initiative based in Philadelphia that has shown promising results in mitigating the effects of climate change.[298] The researchers on the policy have stated that despite such promising plans of green infrastructure building, ""the city is forecasted to grow warmer, wetter, and more urbanized over the century, runoff and local temperatures will increase on average throughout the city"".[298] Even though landcover predictive models on the effects of the policy initiative have indicated that green infrastructure could be useful at decreasing the amount of runoff in the city over time, the city government would have to expand its current plans and ""consider the cobenefit of climate change adaptation when planning new projects"" in limiting the scope of city-wide temperature increase.[298]
 In a 2015 report by Pew Charitable Trusts, the police districts with the highest rates of violent crime were Frankford (15th district) and Kensington (24th district) in the Near Northeast, and districts to the North (22nd, 25th, and 35th districts), West (19th district) and Southwest (12th district) of Center City. Each of those seven districts recorded more than a thousand violent crimes in 2014. The lowest rates of violent crime occurred in Center City, South Philadelphia, the Far Northeast, and Roxborough districts, the latter of which includes Manayunk.[157]
 Philadelphia had 500, 503 according to some sources, murders in 1990, a rate of 31.5 per 100,000. An average of about 400 murders occurred each year for most of the 1990s. The murder count dropped in 2002 to 288, then rose to 406 by 2006, before dropping slightly to 392 in 2007.[299][300] A few years later, Philadelphia began to see a rapid decline in homicides and violent crime. In 2013, the city had 246 murders, which is a decrease of nearly 40% since 2006.[301]
 In 2014, 248 homicides were committed. The homicide rate rose to 280 in 2015, then fell slightly to 277 in 2016, before rising again to 317 in 2017.[302] Homicides increased dramatically in the late 2010s/early 2020s, reaching 499 homicides in 2020[299] and surpassing the 1990 ""record"" in 2021, with 501st murder on November 27 and 510 by the end of the month.[303] Phillie ended the year with 562 murders, an all-time record. It dropped in 2022 to 514, and significantly further again in 2023, to 410.
[304]
In 2006, Philadelphia's homicide rate of 27.7 per 100,000 people was the highest of the country's 10 most populous cities.[305] In 2012, Philadelphia had the fourth-highest homicide rate among the country's most populous cities. The rate dropped to 16 homicides per 100,000 residents by 2014 placing Philadelphia as the sixth-highest city in the country.[157]
 The number of shootings in the city has declined significantly since the early years of the 21st century. Shooting incidents peaked at 1,857 in 2006 before declining nearly 44 percent to 1,047 shootings in 2014.[157] Major crimes have decreased gradually since a peak in 2006 when 85,498 major crimes were reported. The number of reported major crimes fell 11 percent in three years to 68,815 occurrences in 2014. Violent crimes, which include homicide, rape, aggravated assault, and robbery, decreased 14 percent in three years to 15,771 occurrences in 2014.[157]
 In 2014, Philadelphia enacted an ordinance decriminalizing the possession of less than 30 grams of marijuana or eight grams of hashish; the ordinance gave police officers the discretion to treat possession of these amounts as a civil infraction punishable by a $25 ticket, rather than a crime.[306][307] At the time, Philadelphia was at the largest city in the nation to decriminalize the possession of marijuana.[307] From 2013 to 2018, marijuana arrests in the city dropped by more than 85%.[306] The purchase or sale of marijuana remains a criminal offense in Philadelphia.[307]
 The Philadelphia Fire Department provides fire protection and emergency medical services (EMS). The department's official mission is to protect public safety by quick and professional response to emergencies and the promotion of sound emergency prevention measures. This mandate encompasses all traditional firefighting functions, including fire suppression, with 60 engine companies and 30 ladder companies[308] as well as specialty and support units deployed throughout the city; specialized firefighting units for Philadelphia International Airport and the Port of Philadelphia; investigations conducted by the fire marshal's office to determine the origins of fires and develop preventive strategies; prevention programs to educate the public; and support services including research and planning, management of the fire communications center within the city's 911 system, and operation of the Philadelphia Fire Academy.
 Philadelphia's two major daily newspapers are The Philadelphia Inquirer, first published in 1829—the third-oldest surviving daily newspaper in the country—and the Philadelphia Daily News, first published in 1925.[309] The Daily News has been published as an edition of the Inquirer since 2009.[310] Recent owners of the Inquirer and Daily News have included Knight Ridder, The McClatchy Company, and Philadelphia Media Holdings, with the latter organization declaring bankruptcy in 2010.[311] After two years of financial struggle, the newspapers were sold to Interstate General Media in 2012.[311] The two newspapers had a combined daily circulation of 306,831 and a Sunday circulation of 477,313 in 2013[update], the 18th-largest circulation in the country, and their collective website, Philly.com,[312] was ranked 13th in popularity among online U.S. newspapers by Alexa Internet the same year.[313]
 Smaller publications include the Philadelphia Tribune published five days each week for the African-American community;[314] Philadelphia magazine, a monthly regional magazine;[315] Philadelphia Weekly, a weekly alternative newspaper;[316] Philadelphia Gay News, a weekly newspaper for the LGBT community;[317] The Jewish Exponent, a weekly newspaper for the Jewish community;[318] Al Día, a weekly newspaper for the Latino community;[319] and Philadelphia Metro, a free daily newspaper.[320]
 Student-run newspapers include the University of Pennsylvania's The Daily Pennsylvanian,[321] Temple University's The Temple News,[322] and Drexel University's The Triangle.[323]
 The first experimental radio license was issued in Philadelphia in August 1912 to St. Joseph's College. The first commercial AM radio stations began broadcasting in 1922: first WIP, then owned by Gimbels department store, followed by WFIL, then owned by Strawbridge & Clothier department store, and WOO, a defunct station owned by Wanamaker's department store, as well as WCAU and WDAS.[324]
 As of 2018[update], the FCC lists 28 FM and 11 AM stations for Philadelphia.[325][326] As of December 2017, the ten highest-rated stations in Philadelphia were adult contemporary WBEB-FM (101.1), sports talk WIP-FM (94.1), classic rock WMGK-FM (102.9), urban adult contemporary WDAS-FM (105.3), classic hits WOGL-FM (98.1), album-oriented rock WMMR-FM (93.3), country music WXTU-FM (92.5), all-news KYW-AM (1060), talk radio WHYY-FM (90.9), and urban adult contemporary WRNB-FM (100.3).[327][328] Philadelphia is served by three non-commercial public radio stations: WHYY-FM (NPR),[329] WRTI-FM (classical and jazz),[330] and WXPN-FM (adult alternative music).[331]
 In the 1930s, W3XE, an experimental station owned by Philco, launched as Philadelphia's first television station. In 1939, the station became the nation's first NBC's first affiliate, and later became KYW-TV, the Philadelphia television market's CBS affiliate. In 1952, WFIL, later renamed WPVI, premiered the television show Bandstand, which later became the nationally broadcast American Bandstand hosted by Dick Clark.[332] In the 1960s, WCAU-TV, WFIL-TV, and WHYY-TV were founded.[324]
 Each of the nation's commercial networks has an affiliate in Philadelphia: KYW-TV 3 (CBS), WPVI-TV 6 (ABC), WCAU 10 (NBC), WPHL-TV 17 (The CW with MyNetworkTV on DT2), WFPA-CD 28 (UniMás), WTXF-TV 29 (Fox), WPSG 57 (Independent), WWSI 62 (Telemundo), and WUVP-DT 65 (Univision). The region is served also by public broadcasting stations WPPT-TV in Philadelphia, WHYY-TV in Wilmington, Delaware and Philadelphia, WLVT-TV in the Lehigh Valley, and NJTV in New Jersey.[333]
 Since September 2024, Philadelphia is the nation's largest television market where at least one of the six English networks are shown at a station not owned by a particular network's associated parent company. The major Spanish language networks are Univision (WUVP-DT), UniMás (WFPA-CD), and Telemundo (WWSI-TV).[333]
 As of 2023, the Philadelphia media market is the fifth-largest in North America with over 7.8 million viewers[334]
 Philadelphia is served by SEPTA, which operates buses, trains, rapid transit (as both subways and elevated trains), trolleys, and trackless trolleys (electric buses) throughout Philadelphia, the four Pennsylvania suburban counties of Bucks, Chester, Delaware, and Montgomery, in addition to service to Mercer County, New Jersey (Trenton) and New Castle County, Delaware (Wilmington and Newark, Delaware).[335] The city's subway system consists of two routes: the subway section of the Market–Frankford Line running east–west under Market Street which opened in 1905 to the west and 1908 to the east of City Hall,[336] and the Broad Street Line running north–south beneath Broad Street which opened in stages from 1928 to 1938.[337]
 Beginning in the 1980s, large sections of the SEPTA Regional Rail service to the far suburbs of Philadelphia were discontinued due to a lack of funding for equipment and infrastructure maintenance.[338][339][340]
 Philadelphia's 30th Street Station is a major railroad station on Amtrak's Northeast Corridor with 4.4 million passengers in 2017 making it the third-busiest station in the country after New York City's Pennsylvania Station and Washington's Union Station.[341] 30th Street Station offers access to Amtrak,[342] SEPTA,[343] and NJ Transit lines.[344] Over 12 million SEPTA and NJ Transit rail commuters use the station each year, and more than 100,000 people on an average weekday.[341]
 The PATCO Speedline provides rapid transit service to Camden, Collingswood, Westmont, Haddonfield, Woodcrest (Cherry Hill), Ashland (Voorhees), and Lindenwold, New Jersey, from stations on Locust Street between 16th and 15th, 13th and 12th, and 10th and 9th streets, and on Market Street at 8th Street.[345]
 Philadelphia is served by two airports. Philadelphia International Airport (PHL), the larger of the two, is 7 mi (11 km) south-southwest of Center City on the boundary with Delaware County, and provides scheduled domestic and international air service.[346] As of 2023, Philadelphia International Airport is the 21st-busiest airport in the nation with over 13.6 million passengers. It is also among the world's busiest airports measured by traffic movements, including takeoffs and landings.[347] Over 30 million passengers pass through the airport annually on 25 airlines, including all major domestic carriers. The airport has nearly 500 daily departures to over 120 destinations worldwide.[346] SEPTA's Airport Line provides direct service between Center City railroad stations and Philadelphia International Airport.[348]
 Philadelphia's second major airport, Northeast Philadelphia Airport (PNE), is a general aviation relief airport in Northeast Philadelphia, which provides general and corporate aviation.[349]
 William Penn planned Philadelphia with numbered streets traversing north and south, and streets named for trees, including Chestnut, Walnut, and Mulberry (since renamed Arch Streets, traversing east and west. The two main streets were named Broad Street, the north–south artery, later designated Pennsylvania Route 611, and High Street, the east–west artery, which was later renamed Market Street, converging at Centre Square which later became the site of City Hall.[350]
 Interstate 95, also known as the Delaware Expressway, traverses the southern and eastern edges of the city along the Delaware River as the main north–south controlled-access highway, and connects Philadelphia with Newark, New Jersey and New York City to the north and Baltimore and Washington, D.C. to the south. The city is served by Interstate 76, also known as the Schuylkill Expressway, which runs along the Schuylkill River, intersecting the Pennsylvania Turnpike at King of Prussia and providing access to Harrisburg and points west. Interstate 676, also known as Vine Street Expressway, links I-95 and I-76 through Center City, running below street level between the eastbound and westbound lanes of Vine Street. Entrance and exit ramps for the Benjamin Franklin Bridge are near the eastern end of the expressway just west of the I-95 interchange.[351]
 Roosevelt Boulevard and Expressway, also known as U.S. 1, connects Northeast Philadelphia with Center City via I-76 through Fairmount Park. Woodhaven Road, also known as Route 63, and Cottman Avenue, also known as Route 73, serve the neighborhoods of Northeast Philadelphia, running between I-95 and the Roosevelt Boulevard. Fort Washington Expressway, also known as Route 309, extends north from the city's northern border, serving Montgomery and Bucks Counties. U.S. Route 30, also known as Lancaster Avenue, extends west from West Philadelphia to Lancaster.[351]
 Interstate 476, locally called the Blue Route,[352] traverses Delaware County, bypassing the city to the west and serving the city's western suburbs, providing a direct route to Allentown, the Poconos, and points north. Interstate 276, the Pennsylvania Turnpike's Delaware River extension, is a bypass and commuter route north of the city, which links to the New Jersey Turnpike and New York City.[351]
 Delaware River Port Authority operates four bridges in the Philadelphia area, each of which cross the Delaware River to South Jersey: Walt Whitman Bridge (I-76), the Benjamin Franklin Bridge (I-676 and U.S. 30), Betsy Ross Bridge (New Jersey Route 90), and Commodore Barry Bridge (U.S. 322 in Delaware County, south of the city.[353] The Burlington County Bridge Commission maintains two additional bridges that cross the Delaware River. Tacony–Palmyra Bridge connects PA Route 73 in the Tacony section of Northeast Philadelphia with New Jersey Route 73 in Palmyra in Burlington County. Burlington–Bristol Bridge connects NJ Route 413/U.S. Route 130 in Burlington, New Jersey with PA Route 413/U.S. 13 in Bristol Township, north of Philadelphia.[354]
 The Greyhound terminal is at 1001 Filbert Street (at 10th Street) in Center City, southeast of the Pennsylvania Convention Center and south of Chinatown.[355] Several other bus operators provide service at the Greyhound terminal including Fullington Trailways,[356] Martz Trailways,[357] Peter Pan Bus Lines,[358] and NJ Transit buses.[359]
 Other intercity bus services include Megabus with stops at 30th Street Station and the visitor center for Independence Hall,[360] BoltBus (operated by Greyhound) at 30th Street Station,[361] OurBus at various stops in the city.
 Since the early days of rail transportation in the United States, Philadelphia has served as a hub for several major rail companies, particularly the Pennsylvania Railroad and the Reading Railroad. The Pennsylvania Railroad first operated Broad Street Station, then 30th Street Station and Suburban Station, and the Reading Railroad operated Reading Terminal, now part of the Pennsylvania Convention Center. The two companies also operated competing commuter rail systems in the area. The two systems now operate as a single system under the control of SEPTA, the regional transit authority. Additionally, the PATCO Speedline subway system and NJ Transit's Atlantic City Line operate successor services to South Jersey.[362]
 In 1911, Philadelphia had nearly 4,000 electric trolleys running on 86 lines.[363] In 2005, SEPTA reintroduced trolley service to the Girard Avenue Line, Route 15.[364] SEPTA operates six subway-surface trolleys that run on street-level tracks in West Philadelphia and subway tunnels in Center City, along with two surface trolleys in adjacent suburbs.[365]
 Philadelphia is a regional hub of the federally-owned Amtrak system, with 30th Street Station being a primary stop on the Washington-Boston Northeast Corridor and the Keystone Corridor to Harrisburg and Pittsburgh. 30th Street also serves as a major station for services via the Pennsylvania Railroad's former Pennsylvania Main Line to Chicago. As of 2018[update], 30th Street is Amtrak's third-busiest station in the country, after New York City and Washington.[164]
 In 1815, Philadelphia began sourcing its water via the Fairmount Water Works on the Schuylkill River, the nation's first major urban water supply system. In 1909, the Water Works was decommissioned as the city transitioned to modern sand filtration methods.[366] Philadelphia Water Department (PWD) provides drinking water, wastewater collection, and stormwater services for Philadelphia, as well as surrounding counties. PWD draws about 57 percent of its drinking water from the Delaware River and the balance from the Schuylkill River.[367] The city has two filtration plants on the Schuylkill River and one on the Delaware River. The three plants can treat up to 546 million gallons of water per day, while the total storage capacity of the combined plant and distribution system exceeds one billion gallons. The wastewater system consists of three water pollution control plants, 21 pumping stations, and about 3,657 miles (5,885 km) of sewers.[367]
 Exelon subsidiary PECO Energy Company, founded as the Brush Electric Light Company of Philadelphia in 1881 and renamed Philadelphia Electric Company (PECO) in 1902, provides electricity to about 1.6 million customers and more than 500,000 natural gas customers in the southeastern Pennsylvania area including the city of Philadelphia and most of its suburbs.[368] PECO is the largest electric and natural gas utility in the state with 472 power substations and nearly 23,000 miles (37,000 km) of electric transmission and distribution lines and 12,000 miles (19,000 km) of natural gas transmission, distribution, and service lines.[369]
 Philadelphia Gas Works (PGW), overseen by the Pennsylvania Public Utility Commission, is the nation's largest municipally-owned natural gas utility. PGW serves over 500,000 homes and businesses in the Philadelphia area.[370] Founded in 1836, the company came under city ownership in 1987 and has been providing the majority of gas distributed within city limits. In 2014, the City Council refused to conduct hearings on a $1.86 billion sale of PGW, part of a two-year effort that was proposed by the mayor. The refusal led to the prospective buyer terminating its offer.[371][372]
 Southeastern Pennsylvania was assigned the 215 area code in 1947 when the North American Numbering Plan of the Bell System went into effect. The geographic area covered by the code was split nearly in half in 1994 when area code 610 was created, with the city and its northern suburbs retaining 215. Overlay area code 267 was added to the 215 service area in 1997, and 484 was added to the 610 area in 1999. A plan in 2001 to introduce a third overlay code to both service areas, area code 445 to 215 and area code 835 to 610, was delayed and later rescinded.[373] Area code 445 was implemented as an overlay for area codes 215 and 267 starting on February 3, 2018.[374]
 Philadelphia also has three partnership cities or regions:[383]
 Philadelphia has eight official sister cities as designated by the Citizen Diplomacy International (CDI) of Philadelphia:[383] Philadelphia has dedicated landmarks to its sister cities. The Sister Cities Park, a site of 0.5 acres (2,400 sq yd) at 18th and Benjamin Franklin Parkway in Logan Square, was dedicated in June 1976. The park was built to commemorate Philadelphia's first two sister city relationships, with Tel Aviv and Florence. Toruń Triangle, honoring the sister city relationship with Toruń, Poland, was constructed in 1976, west of the United Way building at 18th Street and Benjamin Franklin Parkway. Sister Cities Park was redesigned and reopened in 2012, featuring an interactive fountain honoring Philadelphia's sister and partnership cities, a café and visitor center, children's play area, outdoor garden, boat pond, and a pavilion built to environmentally friendly standards.[387][388]
 The Chinatown Gate, erected in 1984 and crafted by artisans from Tianjin, stands astride 10th Street, on the north side of its intersection with Arch Street, as a symbol of the sister city relationship. The CDI of Philadelphia has participated in the U.S. Department of State's ""Partners for Peace"" project with Mosul, Iraq,[389] and in accepting visiting delegations from dozens of other countries.[390]
"
Continental Army,https://en.wikipedia.org/wiki/Continental_Army,"

 The Continental Army was the army of the United Colonies representing the Thirteen Colonies and later the United States during the American Revolutionary War. It was formed on June 14, 1775, by a resolution passed by the Second Continental Congress, meeting in Philadelphia after the war's outbreak. The Continental Army was created to coordinate military efforts of the colonies in the war against the British, who sought to maintain control over the American colonies. General George Washington was appointed commander-in-chief of the Continental Army and maintained this position throughout the war.
 The Continental Army was supplemented by local militias and volunteer troops that were either loyal to individual states or otherwise independent. Most of the Continental Army was disbanded in 1783 after the Treaty of Paris formally ended the war. The Continental Army's 1st and 2nd Regiments went on to form what was to become the Legion of the United States in 1792, which ultimately served as the foundation for the creation of the United States Army.
 The Continental Army consisted of soldiers from all the Thirteen Colonies and, after 1776, from all 13 states. The American Revolutionary War began at the Battles of Lexington and Concord on April 19, 1775, at a time when the colonial revolutionaries had no standing army. Previously, each colony had relied upon the militia (which was made up of part-time citizen-soldiers) for local defense; or the raising of temporary provincial troops during such crises as the French and Indian War of 1754–1763. As tensions with Great Britain increased in the years leading to the war, colonists began to reform their militias in preparation for the perceived potential conflict. Training of militiamen increased after the passage of the Intolerable Acts in 1774. Colonists such as Richard Henry Lee proposed forming a national militia force, but the First Continental Congress rejected the idea.[2]
 On April 23, 1775, the Massachusetts Provincial Congress authorized the raising of a colonial army consisting of 26 company regiments. New Hampshire, Rhode Island, and Connecticut soon raised similar but smaller forces. On June 14, 1775, the Second Continental Congress decided to proceed with the establishment of a Continental Army for purposes of common defense, adopting the forces already in place outside Boston (22,000 troops) and New York (5,000). It also raised the first ten companies of Continental troops on a one-year enlistment, riflemen from Pennsylvania, Maryland, and Virginia to be used as light infantry. The Pennsylvania riflemen became the 1st Continental Regiment in January 1776. On June 15, 1775, Congress elected by unanimous vote George Washington as Commander-in-Chief, who accepted and served throughout the war without any compensation except for reimbursement of expenses.[3] As the Continental Congress increasingly adopted the responsibilities and posture of a legislature for a sovereign state, the role of the Continental Army became the subject of considerable debate. Some Americans had a general aversion to maintaining a standing army; but on the other hand, the requirements of the war against the British required the discipline and organization of a modern military. As a result, the army went through several distinct phases, characterized by official dissolution and reorganization of units.
 The Continental Army's forces included several successive armies or establishments:
 Military affairs were at first managed by the Continental Congress in plenary session, although specific matters were prepared by a number of ad hoc committees. In June 1776 a five-member standing committee, the Board of War and Ordnance, was established in order to replace the ad hoc committees. The five members who formed the Board fully participated in the plenary activities of Congress as well as in other committees and were unable to fully engage in the administrative leadership of the Continental Army. A new Board of War was therefore formed in October 1777, of three commissioners not member of Congress. Two more commissioners, not members of Congress, were shortly thereafter added, but in October 1778, the membership was set to three commissioners not members of Congress and two commissioners members of Congress. In early 1780, the Quartermaster General, the Commissary General of Purchase, and the Commissary General of Issue were put under the direction of the Board. The Office of the Secretary at War was created in February 1781, although the Office did not start its work until Benjamin Lincoln assumed the office in October 1781.[5]
 On June 15, 1775, Congress elected by unanimous vote George Washington as Commander-in-Chief, who accepted and served throughout the war without any compensation except for reimbursement of expenses.[3]  Washington, as commander-in-chief, was supported by a chief administrative officer, the Adjutant General. Horatio Gates held the position (1775–1776), Joseph Reed (1776–1777), George Weedon and Isaac Budd Dunn (1777), Morgan Connor 1777, Timothy Pickering (1777–1778), Alexander Scammell (1778–1781), and Edward Hand (1781–1783).[6] An Inspector General assisted the Commander-in-Chief through periodically inspecting and reporting on the condition of troops. The first incumbent was Thomas Conway (1777–1778), followed by Baron von Steuben 1778–1784, under whom the position became that of a de facto chief of staff.[7] The Judge Advocate General assisted the commander-in-chief with the administration of military justice, but he did not, as his modern counterpart, give legal advise. William Tudor was the first appointee.[8] He was followed by John Laurance in 1777 and Thomas Edwards in 1781[9] The Mustermaster General kept track by name of every officer and man serving in the army. The first mustermaster was Stephen Moylan.[10] He was followed by Gunning Bedford Jr. 1776–1777 and Joseph Ward.[9]
 Units of the Continental Army were assigned to any one of the territorial departments to decentralize command and administration. In general there were seven territorial departments,[11] although their boundaries were subject to change and they were not all in existence throughout the war. The Department of New York (later the Northern Department) was created when Congress made Philip Schuyler its commander on June 15, 1775. The Southern and Middle Departments were added in February 1776. Several others were added the same year. A major general appointed by Congress commanded each department. Under his command came all Continental Army units within the territorial limits of the department, as well as state troops and militia – if released by the governor of the state.[12]
 All troops under the department commander were designated as an army; hence troops in the Northern Department were called the Northern Army, in the Southern Department the Southern Army, etc. The department commander could be field commander or he could appoint another officer to command the troops in the field. Depending on the size of the army, it could be divided into wings or divisions (of typically three brigades) that were temporary organizations, and brigades (of two to five regiments) that in effect were permanent organizations and the basic tactical unit of the Continental Army.[13]
 An infantry regiment in the Continental Army typically consisted of 8 to 10 companies, each commanded by a captain. Field officers usually included a colonel, a lieutenant colonel, and a major. A regimental staff was made up of an adjutant, quartermaster, surgeon, surgeon's mate, paymaster, and chaplain. Infantry regiments were often called simply regiments or battalions.[14] The regiment's fighting strength consisted of a single battalion of 728 officers and enlisted men at full strength.[15] Cavalry and artillery regiments were organized in a similar manner. A company of cavalry was frequently called a troop. An artillery company contained specialized soldiers, such as bombardiers, gunners, and matrosses.[14] A continental cavalry regiment had a nominal strength of 280 officers and men, but the actual strength was usually less than 150 men and even fewer horses.[16] Artificers were civilian or military mechanics and artisans employed by the army  to provide services. They included blacksmiths, coopers, carpenters, harnessmakers, and wheelwrights.[14]
 In June 1775, Congress created the position of Quartermaster General, after the British example. He was charged with opening and maintaining the lines of advance and retreat, laying out camps and assigning quarters. His responsibilities included furnishing the army with materiel and supplies, although the supply of arms, clothing, and provisions fell under other departments. The transportation of all supplies, even those provided by other departments, came under his ambit. The Quartermaster General served with the main army under General Washington, but was directly responsible to Congress. Deputy quartermasters were appointed by Congress to serve with separate armies, and functioned independently of the Quartermaster General. Thomas Mifflin served as Quartermaster General (1775–1776 and 1776–1778), Stephen Moylan (1776), Nathanael Green (1778–1780), and Timothy Pickering (from 1780).[17]
 Congress also created the position of Commissary General of Stores and Provisions directly responsible to Congress, with Joseph Trumbull as the first incumbent. In 1777, Congress divided the department into two, a Commissary General of Purchases, with four deputies, and a Commissary General of Issues, with three deputies. William Buchanan was head of the Purchase Department (1777–1778), Jeremiah Wadsworth (1778–1779), and Ephraim Blaine (1779–1781). In 1780, the department became subordinated to the Superintendent of Finance, although Blaine retained his position. Charles Stewart served as Commissary General of Issues (1777–1782).[18]
 The responsibility for procuring arms and ammunition at first rested with various committees of Congress. In 1775, a field organization, usually known as the Military Branch of the Commissariat of Military Stores, was made responsible for distribution and care of ordnance in the field. In 1777, Congress established a Commissary General of Military Stores. Known as the Civil Branch, this organization was responsible for handling arsenals, laboratories, and some procurement under the general supervision of the Board of War. Later in the war, a Surveyor of Ordnance was made responsible for inspecting foundries, magazines, ordnance shops, and field ordnance. In July 1777, the Board of War was authorized to purchase artillery.[19]
 Congress created a hospital department in July 1775 as a part of the Continental Army's administrative structure. It came under the Director General of the Hospital Department, chosen by Congress but serving under the Commander-in-Chief, and was staffed by four surgeons, an apothecary, twenty surgeon's mates, a nurse for every ten patients, a matron to supervise the nurses, a clerk, and two storekeepers. The department was reorganized in 1777; deputy director generals were added to the administrative structure; commissaries of hospitals were established to provide food and  forage; and apothecary generals were established to procure and distribute medicines.[20] The first director general was Benjamin Church (1775), he was followed by John Morgan (1775–1777), William Shippen (1777–1781), and John Cochran (1781).[9]
 Keeping the continentals clothed was a difficult task and to do this Washington appointed James Mease, a merchant from Philadelphia, as Clothier General. Mease worked closely with state-appointed agents to purchase clothing and things such as cow hides to make clothing and shoes for soldiers. Mease eventually resigned in 1777 and had compromised much of the organization of the Clothing Department. After this, on many accounts, the soldiers of the Continental Army were often poorly clothed, had few blankets, and often did not even have shoes. The problems with clothing and shoes for soldiers were often not the result of not having enough but of organization and lack of transportation. To reorganize the Board of War was appointed to sort out the clothing supply chain. During this time they sought out the help of France and for the remainder of the war, clothing was coming from over-sea procurement.[21]
 The disbursing of money to pay soldiers and suppliers were the function of the Paymaster-General. James Warren was the first incumbent of this office.[22] His successor was William Palfrey in 1776, who was followed by John Pierce Jr. in 1781.[9]
 The Continental Army lacked the discipline typically expected of an army. When they first assembled, the count of how many soldiers George Washington had was delayed a little over a week. Instead of obeying their commanders and officers without question, each unit was a community that had democratically chosen its leaders. The regiments, coming from different states, were uneven in numbers. Logically, they should be evened, which would mean moving soldiers around. In the spirit of American republicanism, if George Washington separated the soldiers from the officers they had chosen they did not believe they should have to serve. George Washington had to give in to the soldiers and negotiate with them. He needed them to have an army.[24]
 Soldiers in the Continental Army were volunteers; they agreed to serve in the army and standard enlistment periods lasted from one to three years. Early in the war, the enlistment periods were short, as the Continental Congress feared the possibility of the Continental Army evolving into a permanent army. The army never numbered more than 48,000 men overall and 13,000 troops in one area. The turnover proved a constant problem, particularly in the winter of 1776–1777, and longer enlistments were approved. As the new country (not yet fully independent) had no money, the government agreed to give grants to the soldiers which they could exchange for money.[25] In 1781 and 1782, Patriot officials and officers in the Southern Colonies repeatedly implemented policies that offered slaves as rewards for recruiters who managed to enlist a certain number of volunteers in the Continental Army; in January 1781, Virginia's General Assembly passed a measure which announced that voluntary enlistees in the Virginia Line's regiments would be given a ""healthy sound negro"" as a reward.[25]
 The officers of both the Continental Army and the state militias were typically yeoman farmers with a sense of honor and status and an ideological commitment to oppose the policies of the British Crown.[26]  The enlisted men were very different. They came from the working class or minority groups (English, Ulster Protestant, Black or of African descent). They were motivated to volunteer by specific contracts that promised bounty money; regular pay at good wages; food, clothing, and medical care; companionship; and the promise of land ownership after the war. By 1780, more than 30,000 men served in the Continental army, but the lack of resources and proper training resulted in the deaths of over 13,000 soldiers.[27] By 1781–1782, threats of mutiny and actual mutinies were becoming serious.[28][29] Up to a fourth of Washington's army were of Scots-Irish (English and Scottish descent) Ulster origin, many being recent arrivals and in need of work.[30]
 The Continental Army was racially integrated, a condition the United States Army would not see again until the late 1940s. During the Revolution, African American slaves were promised freedom in exchange for military service by both the Continental and British armies.[31][32][33] Approximately 6,600 people of color (including African American, indigenous, and multiracial men) served with the colonial forces, and made up one-fifth of the Northern Continental Army.[34][35]
 In addition to the Continental Army regulars, state militia units were assigned for short-term service and fought in campaigns throughout the war. Sometimes the militia units operated independently of the Continental Army, but often local militias were called out to support and augment the Continental Army regulars during campaigns. The militia troops developed a reputation for being prone to premature retreats, a fact that General Daniel Morgan integrated into his strategy at the Battle of Cowpens and used to fool the British in 1781.[36]
 The financial responsibility for providing pay, food, shelter, clothing, arms, and other equipment to specific units was assigned to states as part of the establishment of these units. States differed in how well they lived up to these obligations. There were constant funding issues and morale problems as the war continued. This led to the army offering low pay, often rotten food, hard work, cold, heat, poor clothing and shelter, harsh discipline, and a high chance of becoming a casualty.[37]
 At the time of the siege of Boston, the Continental Army at Cambridge, Massachusetts, in June 1775, is estimated to have numbered from 14,000 to 16,000 men from New England (though the actual number may have been as low as 11,000 because of desertions). Until Washington's arrival, it remained under the command of Artemas Ward. The British force in Boston was increasing by fresh arrivals. It numbered then about 10,000 men. The British controlled Boston and defended it with their fleet, but they were outnumbered and did not attempt to challenge the American control of New England. Washington selected young Henry Knox, a self-educated strategist, to take charge of the artillery from an abandoned British fort in upstate New York, and dragged across the snow to and placed them in the hills surrounding Boston in March 1776.[38] The British situation was untenable. They negotiated an uneventful abandonment of the city and relocated their forces to Halifax in Canada. Washington relocated his army to New York. For the next five years, the main bodies of the Continental and British armies campaigned against one another in New York, New Jersey, and Pennsylvania. These campaigns included the notable battles of Trenton, Princeton, Brandywine, Germantown, and Morristown, among many others.
 The army increased its effectiveness and success rate through a series of trials and errors, often at a great human cost. General Washington and other distinguished officers were instrumental leaders in preserving unity, learning and adapting, and ensuring discipline throughout the eight years of war. In the winter of 1777–1778, with the addition of Baron von Steuben, a Prussian expert, the training and discipline of the Continental Army was dramatically upgraded to modern European standards through the Regulations for the Order and Discipline of the Troops of the United States.[39] This was during the infamous winter at Valley Forge. Washington always viewed the Army as a temporary measure and strove to maintain civilian control of the military, as did the Continental Congress, though there were minor disagreements about how this was to be carried out.
 Throughout its existence, the Army was troubled by poor logistics, inadequate training, short-term enlistments, interstate rivalries, and Congress's inability to compel the states to provide food, money, or supplies. In the beginning, soldiers enlisted for a year, largely motivated by patriotism; but as the war dragged on, bounties and other incentives became more commonplace. Major and minor mutinies—56 in all—diminished the reliability of two of the main units late in the war.[40]
 The French played a decisive role in 1781 as Washington's Army was augmented by a French expeditionary force under Lieutenant General Rochambeau and a squadron of the French navy under the Comte de Barras. By disguising his movements, Washington moved the combined forces south to Virginia without the British commanders in New York realizing it. This resulted in the capture of the main British invasion force in the south at the Siege of Yorktown, which resulted in the American and their allied victory in the land war in North America and assured independence.
 A small residual force remained at West Point and some frontier outposts until Congress created the United States Army by their resolution of June 3, 1784. Although Congress declined on May 12 to make a decision on the peace establishment, it did address the need for some troops to remain on duty until the British evacuated New York City and several frontier posts. The delegates told Washington to use men enlisted for fixed terms as temporary garrisons. A detachment of those men from West Point reoccupied New York without incident on November 25. When Steuben's effort in July to negotiate a transfer of frontier forts with Major General Frederick Haldimand collapsed, however, the British maintained control over them, as they would into the 1790s. That failure and the realization that most of the remaining infantrymen's enlistments were due to expire by June 1784 led Washington to order Knox, his choice as the commander of the peacetime army, to discharge all but 500 infantry and 100 artillerymen before winter set in. The former regrouped as 1st American Regiment,  under Colonel Henry Jackson of Massachusetts. The single artillery company, New Yorkers under Major John Doughty, came from remnants of the 2nd Continental Artillery Regiment.
 Congress issued a proclamation on October 18, 1783, which approved Washington's reductions. On November 2, Washington, then at Rockingham near Rocky Hill, New Jersey, released his Farewell Orders issued to the Armies of the United States of America to the Philadelphia newspapers for nationwide distribution to the furloughed men. In the message, he thanked the officers and men for their assistance and reminded them that ""the singular interpositions of Providence in our feeble condition were such, as could scarcely escape the attention of the most unobserving; while the unparalleled perseverance of the Armies of the United States, through almost every possible suffering and discouragement for the space of eight long years, was little short of a standing Miracle.""[60]
 Washington believed that the blending of persons from every colony into ""one patriotic band of Brothers"" had been a major accomplishment, and he urged the veterans to continue this devotion in civilian life. Washington said farewell to his remaining officers on December 4 at Fraunces Tavern in New York City. On December 23 he appeared in Congress, then sitting at Annapolis, and returned his commission as commander-in-chief: ""Having now finished the work assigned me, I retire from the great theatre of Action; and bidding an Affectionate farewell to this August body under whose orders I have so long acted, I here offer my Commission, and take my leave of all the employments of public life."" Congress ended the War of American Independence on January 14, 1784, by ratifying the definitive peace treaty that had been signed in Paris on September 3.
 Monthly pay of the officers and soldiers of the continental line as established by the resolutions of Congress, fixing the arrangement of the Continental Army May 27, 1778, which rate of pay continued to the end of the war.[61]
 During the American Revolutionary War, the Continental Army initially wore ribbons, cockades, and epaulettes of various colors as an ad hoc form of rank insignia, as General George Washington wrote in 1775:
 In 1776, captains were to have buff or white cockades.
 Later on in the war, the Continental Army established its own uniform with a black and white cockade among all ranks. Infantry officers had silver and other branches gold insignia:
"
John Adams,https://en.wikipedia.org/wiki/John_Adams,"



 John Adams (October 30, 1735 – July 4, 1826) was an American Founding Father who served as the second president of the United States from 1797 to 1801. Before his presidency, he was a leader of the American Revolution that achieved independence from Great Britain. During the latter part of the Revolutionary War and in the early years of the new nation, he served the U.S. government as a senior diplomat in Europe. Adams was the first person to hold the office of vice president of the United States, serving from 1789 to 1797. He was a dedicated diarist and regularly corresponded with important contemporaries, including his wife and adviser Abigail Adams and his friend and political rival Thomas Jefferson.
 A lawyer and political activist prior to the Revolution, Adams was devoted to the right to counsel and presumption of innocence. He defied anti-British sentiment and successfully defended British soldiers against murder charges arising from the Boston Massacre. Adams was a Massachusetts delegate to the Continental Congress and became a leader of the revolution. He assisted Jefferson in drafting the Declaration of Independence in 1776 and was its primary advocate in Congress. As a diplomat, he helped negotiate a peace treaty with Great Britain and secured vital governmental loans. Adams was the primary author of the Massachusetts Constitution in 1780, which influenced the United States Constitution, as did his essay Thoughts on Government.
 Adams was elected to two terms as vice president under President George Washington and was elected as the United States' second president in 1796. He was the only president elected under the banner of the Federalist Party. Adams's term was dominated by the issue of the French Revolutionary Wars, and his insistence on American neutrality led to fierce criticism from both the Jeffersonian Republicans and from some in his own party, led by his rival Alexander Hamilton. Adams signed the controversial Alien and Sedition Acts and built up the Army and Navy in an undeclared naval war with France. He was the first president to reside in the White House.
 In his bid in 1800 for reelection to the presidency, opposition from Federalists and accusations of despotism from Jeffersonians led to Adams losing to his vice president and former friend Jefferson, and he retired to Massachusetts. He eventually resumed his friendship with Jefferson by initiating a continuing correspondence. He and Abigail started the Adams political family, which includes their son John Quincy Adams, the sixth president. John Adams died on July 4, 1826 – the fiftieth anniversary of the adoption of the Declaration of Independence. New Englanders Adams and his son are the only presidents of the first twelve who never owned slaves. Historians and scholars have favorably ranked his administration.
 John Adams was born on October 30, 1735,[a] to John Adams Sr. and Susanna Boylston. He had two younger brothers, Peter and Elihu.[4] Adams was born on the family farm in Braintree, Massachusetts.[5][b] His mother was from a leading medical family of present-day Brookline, Massachusetts. His father was a deacon in the Congregational Church, a farmer, a cordwainer, and a lieutenant in the militia.[6] Adams often praised his father and recalled their close relationship.[7] Adams's great-great-grandfather Henry Adams immigrated to Massachusetts from Braintree, Essex, England, around 1638.[6]
 Adams's formal education began at age six at a dame school, conducted at a teacher's home and centered on The New England Primer. He then attended Braintree Latin School under Joseph Cleverly, where studies included Latin, rhetoric, logic, and arithmetic. Adams's early education included incidents of truancy, a dislike for his master, and a desire to become a farmer, but his father commanded that he remain in school. Deacon Adams hired a new schoolmaster named Joseph Marsh, and his son responded positively.[8] Adams later noted that ""As a child I enjoyed perhaps the greatest of blessings that can be bestowed upon men – that of a mother who was anxious and capable to form the characters of her children.""[9]
 At age sixteen, Adams entered Harvard College in 1751, studying under Joseph Mayhew.[10] As an adult, Adams was a keen scholar, studying the works of ancient writers such as Thucydides, Plato, Cicero, and Tacitus in their original languages.[11] Though his father expected him to be a minister,[12] after his 1755 graduation with an A.B. degree, he taught school temporarily in Worcester, while pondering his permanent vocation. In the next four years, he began to seek prestige, craving ""Honour or Reputation"" and ""more defference from [his] fellows"", and was determined to be ""a great Man"". He decided to become a lawyer, writing his father that he found among lawyers ""noble and gallant achievements"" but, among the clergy, the ""pretended sanctity of some absolute dunces"". He had reservations about his self-described ""trumpery"" and failure to share the ""happiness of [his] fellow men"".[13]
 When the French and Indian War began in 1754, Adams, aged nineteen, felt guilty he was the first in his family not to be a militia officer; he said ""I longed more ardently to be a Soldier than I ever did to be a Lawyer"".[14]
 In 1756, Adams began reading law under James Putnam, a leading lawyer in Worcester.[15] In 1758, he earned an A.M. from Harvard,[16] and in 1759 was admitted to the bar.[17] He developed an early habit of diary writing; this included his impressions of James Otis Jr.'s 1761 challenge to the legality of British writs of assistance, which allowed British officials to search a home without notice or reason. Otis's argument against the writs inspired Adams to the cause of the American colonies.[18]
 In 1763, Adams explored aspects of political theory in seven essays written for Boston newspapers. Under the pen name ""Humphrey Ploughjogger"", he ridiculed the selfish thirst for power he perceived among the Massachusetts colonial elite.[19] Adams was initially less well known than his older cousin Samuel Adams, but his influence emerged from his work as a constitutional lawyer, his analysis of history, and his dedication to republicanism. Adams often found his own irascible nature a constraint in his political career.[12]
 In the late 1750s, Adams fell in love with Hannah Quincy; he was poised to propose but was interrupted by friends, and the moment was lost. In 1759, he met 15-year-old Abigail Smith, his third cousin,[20] through his friend Richard Cranch, who was courting Abigail's older sister. Adams initially was not impressed with Abigail and her two sisters, writing that they were not ""fond, nor frank, nor candid"".[21]
 In time, Adams grew close to Abigail. They were married on October 25, 1764, despite the opposition of Abigail's mother. The pair shared a love of books and proved honest in their praise and criticism of each other. After his father's death in 1761, Adams had inherited a .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);clip-path:polygon(0px 0px,0px 0px,0px 0px);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}9+1⁄2-acre (3.8 ha) farm and a house where they lived until 1783.[22][23]
 John and Abigail had six children: Abigail (known as ""Nabby"") in 1765,[24] John Quincy in 1767,[25] Susanna in 1768, Charles in 1770, Thomas in 1772,[26] and Elizabeth in 1777.[27] Susanna died when she was one year old,[26] while Elizabeth was stillborn.[27] All three of Adams's sons became lawyers. Charles and Thomas were unsuccessful, became alcoholics, and died at a relatively young age. In contrast, John Quincy excelled and launched a political career, eventually becoming president himself.[28]
 Adams rose to prominence leading widespread opposition to the Stamp Act. The Act was imposed by the British Parliament without consulting the American legislatures. It required payment of a direct tax by the colonies for stamped documents,[29][30] and was designed to pay for the costs of Britain's war with France. Power of enforcement was given to British vice admiralty courts, rather than common law courts.[31][30] These Admiralty courts acted without juries and were greatly disliked.[29] The Act was despised for both its monetary cost and implementation without colonial consent, and encountered violent resistance, preventing its enforcement.[31] Adams authored the ""Braintree Instructions"" in 1765, in a letter sent to the representatives of Braintree in the Massachusetts legislature. It explained that the Act should be opposed since it denied two fundamental rights guaranteed to all Englishmen (and which all free men deserved): to be taxed only by consent and to be tried by a jury of one's peers. The instructions were a succinct and forthright defense of colonial rights and liberties, and served as a model for other towns.[32]
 Adams also reprised his pen name ""Humphrey Ploughjogger"" in opposition to the Stamp Act in August of that year. Included were four articles to the Boston Gazette. The articles were republished in The London Chronicle in 1768 as True Sentiments of America, or A Dissertation on the Canon and Feudal Law. He also spoke in December before the governor and council, pronouncing the Stamp Act invalid in the absence of Massachusetts representation at Parliament.[33][34] He noted that many protests were sparked by a popular sermon of Boston minister Jonathan Mayhew, invoking Romans 13 to justify insurrection.[35] While Adams strongly opposed the Act in writing, he rebuffed attempts by Samuel Adams, a leader in the popular protest movements, to involve him in mob actions and public demonstrations.[36] In 1766, a town meeting of Braintree elected Adams as a selectman.[37]
 With the repeal of the Stamp Act in early 1766, tensions with Britain temporarily eased.[38] Putting politics aside, Adams moved his family to Boston in April 1768 to focus on his law practice. The family rented a house on Brattle Street that was known locally as the ""White House"". He, Abigail, and the children lived there for a year, then moved to Cold Lane; later they moved again to a larger house in Brattle Square in the center of the city.[25] In 1768, Adams successfully defended the merchant John Hancock, who was accused of violating British acts of trade in the Liberty Affair.[39] With the death of Jeremiah Gridley and the mental collapse of James Otis Jr., Adams became Boston's most prominent lawyer.[37]
 Britain's passage of the Townshend Acts in 1767 revived tensions, and an increase in mob violence led the British to dispatch more troops to the colonies.[40] On March 5, 1770, when a lone British sentry was accosted by a mob, eight of his fellow soldiers reinforced him, and the crowd around them grew to several hundred. The soldiers were struck with snowballs, ice, and stones, and in the chaos the soldiers opened fire, killing five civilians, in the infamous Boston Massacre. The accused soldiers were arrested on charges of murder. When no other attorneys would come to their defense, Adams was impelled to do so despite the risk to his reputation. He believed no person should be denied the right to counsel and a fair trial. The trials were delayed so that passions could cool.[41]
 The week-long trial of the commander, Captain Thomas Preston, began on October 24 and ended in his acquittal, because it was impossible to prove that he had ordered his soldiers to fire.[42] The remaining soldiers were tried in December when Adams made his famed argument regarding jury decisions: ""Facts are stubborn things; and whatever may be our wishes, our inclinations, or the dictates of our passion, they cannot alter the state of facts and evidence.""[43] Adams won an acquittal for six of the soldiers. Two, who had fired directly into the crowd, were convicted of manslaughter. Adams was paid a small sum by his clients.[22]
 According to biographer John E. Ferling, during jury selection Adams ""expertly exercised his right to challenge individual jurors and contrived what amounted to a packed jury. Not only were several jurors closely tied through business arrangements to the British army, but five ultimately became Loyalist exiles."" While Adams's defense was helped by a weak prosecution, he ""performed brilliantly.""[44] Ferling surmises that Adams was encouraged to take the case in exchange for political office; one of Boston's seats opened three months later in the Massachusetts legislature, and Adams was the town's choice to fill the vacancy.[45]
 The prosperity of his law practice increased from this exposure, as did the demands on his time. In 1771, Adams moved his family to Braintree, Massachusetts, but kept his office in Boston; he noted ""Now my family is away, I feel no Inclination at all, no Temptation, to be any where but at my Office."" After some time in the capital, he became disenchanted with the rural and ""vulgar"" Braintree as a home for his family – in August 1772, he moved them back to Boston. He purchased a large brick house on Queen Street, not far from his office.[46] In 1774, Adams and Abigail returned the family to the farm due to the increasingly unstable situation in Boston, and Braintree remained their permanent Massachusetts home.[47]
 Adams, who had been among the more conservative of the Founding Fathers, persistently held that while British actions against the colonies had been wrong, open insurrection was unwarranted and peaceful petition with the view of remaining part of Great Britain was preferable.[48] His ideas began to change around 1772, as the British Crown assumed payment of the salaries of Governor Thomas Hutchinson and his judges instead of the Massachusetts legislature. Adams wrote in the Gazette that these measures would destroy judicial independence and place the colonial government in closer subjugation to the Crown. After discontent among members of the legislature, Hutchinson delivered a speech warning that Parliament's powers over the colonies were absolute and that any resistance was illegal. John Adams, Samuel, and Joseph Hawley drafted a resolution adopted by the House of Representatives threatening independence as an alternative to tyranny. The resolution argued that the colonists had never been under the sovereignty of Parliament: their charter, as well as their allegiance, was exclusive to the King.[49]
 The Boston Tea Party, a demonstration against the Tea Act and the British East India Company's tea monopoly over American merchants, took place on December 16, 1773. Protestors demolished 342 chests of tea worth about ten thousand pounds on the British schooner Dartmouth, anchored in Boston harbor. The Dartmouth owners briefly retained Adams as legal counsel regarding their liability for the destroyed shipment. Adams applauded the destruction of the tea, calling it the ""grandest Event"" in the history of the colonial protest movement,[50] and writing in his diary that it was an ""absolutely and indispensably"" necessary action.[51]
 In 1774, at the instigation of Samuel Adams, the First Continental Congress was convened in response to the Intolerable Acts, a series of deeply unpopular measures intended to punish Massachusetts, centralize authority in Britain, and prevent rebellion in other colonies. Four delegates were chosen by the Massachusetts legislature, including John Adams, who agreed to attend,[52] despite an emotional plea from his friend, Attorney General Jonathan Sewall, not to.[53]
 Shortly after he arrived in Philadelphia, Adams was placed on the 23-member Grand Committee tasked with drafting a letter of grievances to King George III. The committee soon split into conservative and radical factions.[54] Although the Massachusetts delegation was largely passive, Adams criticized conservatives such as Joseph Galloway, James Duane, and Peter Oliver who advocated a conciliatory policy towards the British or felt that the colonies had a duty to remain loyal to Britain, although his views at the time aligned with those of conservative John Dickinson. Adams sought the repeal of objectionable policies, but at this stage he continued to see benefits in maintaining the ties with Britain.[55] He renewed his push for the right to a jury trial.[56] He complained of what he considered the pretentiousness of the other delegates, writing to Abigail, ""I believe if it was moved and seconded that We should come to a Resolution that Three and two make five We should be entertained with Logick and Rhetorick, Law, History, Politicks and Mathematicks, concerning the Subject for two whole Days, and then We should pass the Resolution unanimously in the Affirmative.""[57] Adams ultimately helped engineer a compromise between the conservatives and the radicals.[58] The Congress disbanded in October after sending the petition to the King and showing its displeasure with the Intolerable Acts by endorsing the Suffolk Resolves, which called for a boycott of British goods.[59]
 Adams's absence was hard on Abigail, who was left alone to care for the family. She still encouraged her husband in his task, writing: ""You cannot be, I know, nor do I wish to see you an inactive Spectator, but if the Sword be drawn I bid adieu to all domestick felicity, and look forward to that Country where there is neither wars nor rumors of War in a firm belief that thro the mercy of its King we shall both rejoice there together.""[60]
 News of the opening hostilities with the British at the Battles of Lexington and Concord made Adams hope that independence would soon become a reality. Three days after the battle, he rode into a militia camp and, while reflecting positively on the high spirits of the men, was distressed by their poor condition and lack of discipline.[61] A month later, Adams returned to Philadelphia for the Second Continental Congress as the leader of the Massachusetts delegation.[62] He moved cautiously at first, noting that the Congress was divided between Loyalists, those favoring independence, and those hesitant to take any position.[63] He became convinced that Congress was moving in the proper direction – away from Great Britain. Publicly, Adams supported ""reconciliation if practicable,"" but privately agreed with Benjamin Franklin's confidential observation that independence was inevitable.[64]
 In June 1775, with a view of promoting union among the colonies against Great Britain, he nominated George Washington of Virginia as commander-in-chief of the army then assembled around Boston.[65] He praised Washington's ""skill and experience"" as well as his ""excellent universal character.""[66] Adams opposed various attempts, including the Olive Branch Petition, aimed at finding peace.[67] Invoking the already-long list of British actions against the colonies, he wrote, ""In my opinion Powder and Artillery are the most efficacious, Sure, and infallibly conciliatory Measures We can adopt.""[68] After his failure to prevent the petition from being enacted, he wrote a private letter derisively referring to Dickinson as a ""piddling genius."" The letter was intercepted and published in Loyalist newspapers. The well-respected Dickinson refused to greet Adams and he was for a time largely ostracized.[69] Ferling writes, ""By the fall of 1775 no one in Congress labored more ardently than Adams to hasten the day when America would be separate from Great Britain.""[64] In October 1775, Adams was appointed chief judge of the Massachusetts Superior Court, but he never served, and resigned in February 1777.[65] In response to queries from other delegates, Adams wrote the 1776 pamphlet Thoughts on Government, which laid out an influential framework for republican constitutions.[70]
 Throughout the first half of 1776, Adams grew increasingly impatient with what he perceived to be the slow pace of declaring independence.[71] In the Second Continental Congress in Philadelphia, he helped push through a plan to outfit armed ships to launch raids on enemy vessels. Later in the year, he drafted the first set of regulations for the provisional navy.[72] Adams drafted the preamble to the Lee Resolution of colleague Richard Henry Lee.[73] He developed a rapport with delegate Thomas Jefferson of Virginia, who had been slower to support independence but by early 1776 agreed that it was necessary.[74] On June 7, 1776, Adams seconded the Lee Resolution, which stated that the colonies were ""free and independent states.""[75]
 Prior to independence being declared, Adams organized a Committee of Five charged with drafting a Declaration of Independence. He chose himself, Jefferson, Benjamin Franklin, Robert R. Livingston and Roger Sherman.[76] Jefferson thought Adams should write the document, but Adams persuaded the committee to choose Jefferson. Many years later, Adams recorded his reasoning to Jefferson: ""Reason first, you are a Virginian, and a Virginian ought to appear at the head of this business. Reason second, I am obnoxious, suspected, and unpopular. You are very much otherwise. Reason third, you can write ten times better than I can.""[77] The Committee left no minutes, and the drafting process itself remains uncertain. Accounts written years later by Jefferson and Adams, although frequently cited, are often contradictory.[78] Although the first draft was written primarily by Jefferson, Adams assumed a major role.[79] On July 1, the resolution was debated in Congress. It was expected to pass, but opponents such as Dickinson made a strong effort to oppose it. Jefferson, a poor debater, remained silent while Adams argued for its adoption.[80] Many years later, Jefferson hailed Adams as ""the pillar of [the Declaration's] support on the floor of Congress, [its] ablest advocate and defender against the multifarious assaults it encountered.""[81] On July 2, Congress officially voted for independence. Twelve colonies voted in the affirmative, while New York abstained. Dickinson was absent.[82][83] On July 3, Adams wrote to Abigail that ""yesterday was decided the greatest question which was ever debated in America, and a greater perhaps never was nor will be decided among men."" He predicted that ""[t]he second day of July, 1776, will be the most memorable epoch in the history of America,"" and would be celebrated annually.[84] Congress approved the Declaration of Independence on July 4.[85]
 During the congress, Adams sat on ninety committees, chairing twenty-five, an unmatched workload among the congressmen. As Benjamin Rush reported, he was acknowledged ""to be the first man in the House.""[86] In June 1776, Adams became head of the Board of War and Ordnance, charged with recording the officers in the army and their ranks, the disposition of troops throughout the colonies, and ammunition.[87] He was referred to as a ""one man war department,"" working up to eighteen-hour days and mastering the details of raising, equipping and fielding an army under civilian control.[88] Adams functioned as a de facto Secretary of War. He kept extensive correspondences with Continental Army officers concerning supplies, munitions, and tactics. Adams emphasized to them the role of discipline in keeping an army orderly.[89] He authored the ""Plan of Treaties,"" laying out Congress's requirements for a treaty with France.[88] He was worn out by the rigor of his duties and longed to return home. His finances were unsteady, and the money that he received as a delegate failed to cover his expenses. However, the crisis caused by the defeat of the American soldiers kept him at his post.[90]
 After defeating the Continental Army at the Battle of Long Island on August 27, 1776, British Admiral Richard Howe determined that a strategic advantage was at hand, and requested that Congress send representatives to negotiate peace. A delegation consisting of Adams, Franklin, and Edward Rutledge met with Howe at the Staten Island Peace Conference on September 11.[91][92] Howe's authority was premised on the states' submission, so the parties found no common ground. When Lord Howe stated he could view the American delegates only as British subjects, Adams replied, ""Your lordship may consider me in what light you please, ... except that of a British subject.""[93] Adams learned many years later that his name was on a list of people specifically excluded from Howe's pardon-granting authority.[94] Adams was unimpressed with Howe and predicted American success.[95] He was able to return home to Braintree in October before leaving in January 1777 to resume his duties in Congress.[96]
 Adams advocated in Congress that independence was necessary to establish trade, and conversely, trade was essential for the attainment of independence; he specifically urged negotiation of a commercial treaty with France. He was appointed, along with Franklin, Dickinson, Benjamin Harrison from Virginia, and Robert Morris from Pennsylvania, ""to prepare a plan of treaties to be proposed to foreign powers."" While Jefferson was writing the Declaration of Independence, Adams worked on the Model Treaty, which authorized a commercial agreement with France but contained no provisions for formal recognition or military assistance. The treaty adhered to the provision that ""free ships make free goods,"" allowing neutral nations to trade reciprocally while exempting an agreed-upon list of contraband. By late 1777, America's finances were in tatters, and that September a British army had defeated General Washington and captured Philadelphia. More Americans came to determine that mere commercial ties between the U.S. and France would not be enough, and that military assistance would be needed. The defeat of the British at Saratoga was expected to help induce France to agree to an alliance.[97]
 In November 1777, Adams learned that he was to be named commissioner to France, replacing Silas Deane and joining Franklin and Arthur Lee in Paris to negotiate an alliance with the French. James Lovell invoked Adams's ""inflexible integrity"" and the need to have a youthful man who could counterbalance Franklin's age. On November 27, Adams accepted, wasting no time. Abigail was left in Massachusetts to manage their home, but it was agreed that 10-year-old John Quincy would go with Adams, for the experience was ""of inestimable value"" to his maturation.[98] On February 17, 1778, Adams set sail aboard the frigate Boston, commanded by Captain Samuel Tucker.[99] The trip was stormy and treacherous. The ship was pursued by British vessels, with Adams personally taking up arms to help capture one. A cannon malfunction wounded several sailors and killed one. On April 1, the Boston arrived in France, where Adams learned that France had agreed to an alliance with the United States on February 6.[100] Adams was annoyed by the other two commissioners: Lee, whom he thought paranoid and cynical, and the popular and influential Franklin, whom he found lethargic and overly deferential to the French.[101] He assumed a less visible role but helped manage the delegation's finances and record-keeping.[102] Frustrated by the perceived lack of commitment on the part of the French, Adams wrote a letter to French foreign minister Vergennes in December, arguing for French naval support in North America. Franklin toned down the letter, but Vergennes ignored it.[103] In September 1778, Congress increased Franklin's powers by naming him minister plenipotentiary to France while Lee was sent to Spain. Adams received no instructions. Frustrated by the apparent slight, he departed France with John Quincy on March 8, 1779.[104] On August 2, they arrived in Braintree.[105]
 In late 1779, Adams was appointed as the sole minister charged with negotiations to establish a commercial treaty with Britain and end the war.[106] Following the Massachusetts constitutional convention, he departed for France in November,[107] accompanied by his sons John Quincy and 9-year-old Charles.[108] A leak forced the ship to land in Ferrol, Spain, and Adams and his party spent six weeks travelling overland to Paris.[109] Constant disagreement between Lee and Franklin eventually resulted in Adams assuming the role of tie-breaker in almost all votes on commission business. He increased his usefulness by mastering French. Lee was eventually recalled. Adams closely supervised his sons' education while writing to Abigail about once every ten days.[110]
 In contrast to Franklin, Adams viewed the Franco-American alliance pessimistically. The French, he believed, were involved for their own self-interest, and he grew frustrated by what he saw as their sluggishness in providing substantial aid. The French, Adams wrote, meant to keep their hands ""above our chin to prevent us from drowning, but not to lift our heads out of water.""[111] In March 1780, Congress, trying to curb inflation, voted to devalue the dollar. Vergennes summoned Adams for a meeting. In a letter sent in June, he insisted that fluctuation of the dollar value without an exception for French merchants was unacceptable and requested that Adams write to Congress asking it to ""retrace its steps."" Adams bluntly defended the decision, not only claiming that the French merchants were doing better than Vergennes implied but voicing other grievances he had with the French. The alliance had been made over two years before. During that period, an army under the comte de Rochambeau had been sent to assist Washington, but it had yet to do anything of significance and America was expecting French warships. These were needed, Adams wrote, to contain the British armies in the port cities and contend with the powerful British Navy. However, the French Navy had been sent not to the United States but to the West Indies to protect French interests there. France, Adams believed, needed to commit itself more fully to the alliance. Vergennes responded that he would deal only with Franklin, who sent a letter back to Congress critical of Adams.[112] Adams then left France of his own accord.[113]
 In mid-1780, Adams traveled to the Dutch Republic. One of the few other republics at the time, Adams thought it might be sympathetic to the American cause. Securing a Dutch loan could increase American independence from France and pressure Britain into peace. At first, Adams had no official status, but in July he was formally given permission to negotiate for a loan and took up residence in Amsterdam in August. Adams was originally optimistic and greatly enjoyed the city, but soon became disappointed. The Dutch, fearing British retaliation, refused to meet Adams. Before he had arrived, the British found out about secret aid the Dutch had sent to the Americans and authorized reprisals against their ships, which only increased their apprehension. Word had also reached Europe of American battlefield defeats. After five months of not meeting with a single Dutch official, Adams in early 1781 pronounced Amsterdam ""the capital of the reign of Mammon.""[114] He was finally invited to present his credentials as ambassador to the Dutch government at The Hague on April 19, 1781, but they did not promise any assistance. In the meantime, Adams thwarted an attempt by neutral European powers to mediate the war without consulting the United States.[115] In July, Adams consented to the departure of both of his sons; John Quincy went with Adams's secretary Francis Dana to Saint Petersburg as a French interpreter, in an effort to seek recognition from Russia, and a homesick Charles returned home with Adams's friend Benjamin Waterhouse.[116] In August, shortly after being removed from his position of sole head of peace treaty negotiations, Adams had ""a major nervous breakdown.""[117] That November, he learned that American and French troops had decisively defeated the British at Yorktown. The victory was in large part due to the assistance of the French Navy, which vindicated Adams's stand for increased naval assistance.[118]
 News of the American triumph at Yorktown convulsed Europe. In January 1782, after recovering, Adams arrived at The Hague to demand that the States General answer his petitions. His efforts stalled, and he took his cause to the people, successfully capitalizing on popular pro-American sentiment. Several provinces began recognizing American independence. On April 19, the States General formally recognized American independence and acknowledged Adams as ambassador.[119] On June 11, with the aid of the Dutch Patriotten leader Joan van der Capellen tot den Pol, Adams negotiated a loan of five million guilders. In October, he negotiated a treaty of amity and commerce.[120] The house that Adams bought during this stay in the Netherlands became the first American embassy on foreign soil.[121]
 After negotiating the loan with the Dutch, Adams was re-appointed as the American commissioner to negotiate the war-ending treaty, the Treaty of Paris. Vergennes and France's minister to the United States, Anne-César de La Luzerne, disapproved of Adams, so Franklin, Thomas Jefferson, John Jay, and Henry Laurens were appointed to collaborate with Adams, although Jefferson did not initially go to Europe and Laurens was posted to the Dutch Republic following his imprisonment in the Tower of London.[122]
 In the final negotiations, securing fishing rights off Newfoundland and Cape Breton Island proved both very important and very difficult. In response to very strict restrictions proposed by the British, Adams insisted that not only should American fishermen be allowed to travel as close to shore as desired, but that they should be allowed to cure their fish on the shores of Newfoundland.[123] This, and other statements, prompted Vergennes to secretly inform the British that France did not feel compelled to ""sustain [these] pretentious ambitions."" Overruling Franklin and distrustful of Vergennes, Jay and Adams decided not to consult with France, instead dealing directly with the British.[124] During these negotiations, Adams mentioned to the British that his proposed fishing terms were more generous than those offered by France in 1778 and that accepting would foster goodwill between Britain and the United States while putting pressure on France. Britain agreed, and the two sides worked out other provisions afterward. Vergennes was angered when he learned from Franklin of the American duplicity, but did not demand renegotiation. He was surprised at how much the Americans could extract. The independent negotiations also allowed the French to plead innocence to their Spanish allies, whose demands for Gibraltar might have caused significant problems.[125] On September 3, 1783, the treaty was signed and American independence was recognized.[126]
 Adams was appointed the first American ambassador to Great Britain in 1785.[127] After arriving in London from Paris, Adams had his first audience with King George III on June 1, which he meticulously recorded in a letter to Foreign Minister Jay the next day. The pair's exchange was respectful; Adams promised to do all that he could to restore friendship and cordiality ""between People who, tho Seperated [sic] by an Ocean and under different Governments have the Same Language, a Similar Religion and kindred Blood,"" and the King agreed to ""receive with Pleasure, the Assurances of the friendly Dispositions of the United States."" The King added that although ""he had been the last to consent"" to American independence, he had always done what he thought was right. He startled Adams by commenting that ""There is an Opinion, among Some People, that you are not the most attached of all Your Countrymen, to the manners of France."" Adams replied, ""That Opinion sir, is not mistaken... I have no Attachments but to my own Country."" King George responded, ""An honest Man will never have any other.""[128]
 Adams was joined by Abigail in London. Suffering the hostility of the King's courtiers, they escaped when they could by seeking out Richard Price, minister of Newington Green Unitarian Church and instigator of the debate over the Revolution within Britain.[129] Adams corresponded with his sons John Quincy and Charles, both of whom were at Harvard, cautioning the former against the ""smell of the midnight lamp"" while admonishing the latter to devote sufficient time to study.[130] Jefferson visited Adams in 1786 while serving as Minister to France; the two toured the countryside and saw many historical sites.[131] While in London, Adams met his old friend Jonathan Sewall, but the two discovered that they had grown too far apart to renew their friendship. Adams considered Sewall one of the war's casualties, and Sewall critiqued him as an ambassador:
 While in London Adams wrote his three-volume A Defense of the Constitutions of Government of the United States of America, a response to those he had met in Europe who criticized the government systems of the American states.[133]
 Adams's tenure in Britain was complicated by both countries failing to follow their treaty obligations. The American states had been delinquent in paying debts owed to British merchants, and in response, the British refused to vacate forts in the northwest as promised. Adams's attempts to resolve this dispute failed, and he was often frustrated by a lack of news of progress from home.[134] The news he received of tumult at home, such as Shays' Rebellion, heightened his anxiety. He asked Jay to be relieved;[135] in 1788, he took his leave of George III, who promised to uphold his end of the treaty once America did the same.[136] Adams then went to The Hague to take formal leave of his ambassadorship there and to secure refinancing from the Dutch, allowing the United States to meet obligations on earlier loans.[137]
 On June 17, 1788, Adams returned to a triumphant welcome in Massachusetts. He returned to farming life in the months after. The nation's first presidential election was soon to take place. Because George Washington was widely expected to win the presidency, many felt that the vice presidency should go to a northerner. Although he made no public comments on the matter, Adams was the primary contender.[138] Each state's presidential electors gathered on February 4, 1789, to cast their two votes for the president. The person with the most votes would be president and the second would become vice president.[139] Adams received 34 electoral college votes in the election, second behind Washington, who was a unanimous choice with 69 votes. As a result, Washington became the nation's first president, and Adams became its first vice president. Adams finished well ahead of all others except Washington, but was still offended by Washington receiving more than twice as many votes.[140] In an effort to ensure that Adams did not accidentally become president and that Washington would have an overwhelming victory, Alexander Hamilton convinced at least 7 of the 69 electors not to cast their vote for Adams. After finding out about the manipulation but not Hamilton's role in it, Adams wrote to Benjamin Rush that his election was ""a curse rather than a blessing.""[140][141]
 Although his term started on March 4, 1789, Adams did not begin serving as vice president until April 21, because he did not arrive in New York in time.[142][143]
 The sole constitutionally prescribed responsibility of the vice president is to preside over the U.S. Senate, where they were empowered to cast a tie-breaking vote.[144] Early in his term, Adams became deeply involved in a lengthy Senate controversy over the official titles for the president and executive officers of the new government. Although the House agreed that the president should be addressed simply as ""George Washington, President of the United States"", the Senate debated the issue at some length. Adams favored the style of Highness (as well as the title of Protector of Their [the United States'] Liberties) for the president.[145] Some senators favored a variant of Highness or the lesser Excellency.[146] Anti-federalists in the Senate objected to the monarchical sound of them all; Jefferson described them as ""superlatively ridiculous.""[147] They argued that these ""distinctions,"" as Adams called them, violated the Constitution's prohibition on titles of nobility. Adams said that the distinctions were necessary because the highest office of the United States must be marked with ""dignity and splendor"". He was widely derided for his combative nature and stubbornness, especially as he actively debated and lectured the senators. ""For forty minutes he harangued us from the chair,"" wrote Senator William Maclay of Pennsylvania. Maclay became Adams's fiercest opponent and repeatedly expressed personal contempt for him in public and private. He likened Adams to ""a monkey just put into breeches.""[148] Ralph Izard suggested that Adams be referred to as ""His Rotundity,"" a joke which soon became popular.[149] On May 14, 1789, the Senate decided that the title of ""Mr. President"" would be used.[150] Privately, Adams conceded that his vice presidency had begun poorly and that perhaps he had been out of the country too long to know the sentiment of the people. Washington quietly expressed his displeasure with the fuss.[151]
 As vice president, Adams largely sided with the Washington administration and the emerging Federalist Party. He supported Washington's policies against opposition from anti-Federalist Republicans.[152] He cast 29 tie-breaking votes, and is one of only three vice presidents who have cast more than 20 during their tenure.[153] He voted against a bill sponsored by Maclay that would have required Senate consent for the removal of executive branch officials who had been confirmed by the Senate.[154] In 1790, Jefferson, James Madison, and Hamilton struck a bargain guaranteeing Republican support for Hamilton's debt assumption plan in exchange for the capital being temporarily moved from New York to Philadelphia, and then to a permanent site on the Potomac River to placate Southerners. In the Senate, Adams cast a tie-breaking vote against a last-minute motion to keep the capital in New York.[155]
 Adams played a minor role in politics as vice president. He attended few cabinet meetings, and the President sought his counsel infrequently.[144] While Adams brought energy and dedication to the office,[156] by mid-1789 he had already found it ""not quite adapted to my character ... too inactive, and mechanical.""[157] He wrote, ""My country has in its wisdom contrived for me the most insignificant office that ever the invention of man contrived or his imagination conceived.""[158] Adams's initial behavior in the Senate made him a target for critics of the Washington administration. Toward the end of his first term, he grew accustomed to a marginal role, and rarely intervened in debate.[159] Adams never questioned Washington's courage or patriotism, but Washington did join Franklin and others as the object of Adams's ire or envy. ""The History of our Revolution will be one continued lie,"" Adams declared. ""The essence of the whole will be that Dr. Franklin's electrical Rod smote the Earth and out sprung General Washington. That Franklin electrified him with his Rod – and henceforth these two conducted all the Policy, Negotiations, Legislatures and War.""[160] Adams won reelection with little difficulty in 1792 with 77 votes. His strongest challenger, George Clinton, had 50.[161]
 On July 14, 1789, the French Revolution began. Republicans were jubilant. Adams at first expressed cautious optimism, but soon began denouncing the revolutionaries as barbarous and tyrannical.[162] Washington eventually consulted Adams more often, but not until near the end of his administration, by which point distinguished cabinet members Hamilton and Jefferson had resigned.[163] The British had been raiding American trading vessels, and John Jay was sent to London to negotiate an end to hostilities. When he returned in 1795 with a peace treaty on terms unfavorable to the United States, Adams urged Washington to sign it to prevent war. Washington did so, igniting protests and riots. He was accused of surrendering American honor to a tyrannical monarchy and of turning his back on the French Republic.[164] John Adams predicted in a letter to Abigail that ratification would deeply divide the nation.[165]
 The 1796 election was the first contested American presidential election.[166] Twice, George Washington had been elected to office unanimously but, during his presidency, deep philosophical differences between the two leading figures in the administration – Hamilton and Jefferson – had caused a rift, leading to the founding of the Federalist and Republican parties.[167] When Washington announced that he would not stand for a third term, an intense partisan struggle for control of Congress and the presidency began.[168]
 As in the previous two presidential elections, no candidates were put forward for voters to choose between in 1796. The Constitution provided for the selection of electors who would then choose a president.[169] In seven states voters chose the presidential electors. In the remaining nine states, they were chosen by the state's legislature.[170] The clear Republican favorite was Jefferson.[171] Adams was the Federalist frontrunner.[169] The Republicans held a congressional nominating caucus and named Jefferson and Aaron Burr as their presidential choices.[172] Jefferson at first declined the nomination, but he agreed to run a few weeks later. Federalist members of Congress held an informal nominating caucus and named Adams and Thomas Pinckney as their candidates.[171][173] The campaign was mostly confined to newspaper attacks, pamphlets, and political rallies;[169] of the four contenders, only Burr actively campaigned. The practice of not campaigning for office would persist for decades.[170] Adams stated that he wanted to stay out of the ""silly and wicked game"" of electioneering.[174]
 As the campaign progressed, fears grew among Hamilton and his supporters that Adams was too vain, opinionated, unpredictable and stubborn to follow their directions.[175] Indeed, Adams did not consider himself a strong member of the Federalist Party. He had remarked that Hamilton's economic program, centered around banks, would ""swindle"" the poor and unleash the ""gangrene of avarice.""[176] Desiring ""a more pliant president than Adams,"" Hamilton maneuvered to tip the election to Pinckney. He coerced South Carolina Federalist electors, pledged to vote for ""favorite son"" Pinckney, to scatter their second votes among candidates other than Adams. Hamilton's scheme was undone when several New England state electors heard of it and agreed not to vote for Pinckney.[177] Adams wrote shortly after the election that Hamilton was a ""proud Spirited, conceited, aspiring Mortal always pretending to Morality, with as debauched Morals as old Franklin who is more his Model than any one I know.""[178] Throughout his life, Adams made highly critical statements about Hamilton. He made derogatory references to his womanizing, real or alleged, and slurred him as the ""Creole bastard.""[179]
 Adams won the presidency by a narrow margin, receiving 71 electoral votes to 68 for Jefferson, who became the vice president; Pinckney finished third with 59 votes, and Burr came fourth with 30. The balance of the votes were dispersed among nine other candidates.[180] This is the only election to date in which a president and vice president were elected from opposing tickets.[181]
 Adams was sworn into office as the nation's second president on March 4, 1797. He followed Washington's lead in using the presidency to exemplify republican values and civic virtue, and his service was free of scandal.[182] Adams spent much of his term at his Massachusetts home Peacefield, preferring the quietness of domestic life to business at the capital. He ignored the political patronage and office-seeking which other officeholders utilized.[183]
 Historians debate the wisdom of his decision to retain Washington's cabinet given its loyalty to Hamilton. The ""Hamiltonians who surround him,"" Jefferson remarked, ""are only a little less hostile to him than to me.""[184] Although aware of Hamilton's influence, Adams was convinced that their retention ensured a smoother succession.[185] Adams maintained the economic programs of Hamilton, who regularly consulted with key cabinet members, especially the powerful Treasury Secretary, Oliver Wolcott Jr.[186] Adams was in other respects quite independent of his cabinet, often making decisions despite opposition from it.[187] Hamilton had grown accustomed to being regularly consulted by Washington. Shortly after Adams was inaugurated, Hamilton sent him a detailed letter with policy suggestions. Adams dismissively ignored it.[188]
 Historian Joseph Ellis writes that ""[t]he Adams presidency was destined to be dominated by a single question of American policy to an extent seldom if ever encountered by any succeeding occupant of the office."" That question was whether to make war with France or find peace.[189] Britain and France were at war as a result of the French Revolution. Hamilton and the Federalists strongly favored the British monarchy against what they denounced as the political radicalism and anti-religious frenzy of the French Revolution. Jefferson and the Republicans, with their firm opposition to monarchy, strongly supported the French overthrowing their king.[190] The French had supported Jefferson for president in 1796 and became belligerent at his loss.[191] Adams continued Washington's policy of staying out of the war. Because of the Jay Treaty, the French saw America as Britain's junior partner and began seizing American merchant ships that were trading with the British. Most Americans were still pro-French due to France's assistance during the Revolution, the perceived humiliation of the Jay Treaty, and their desire to support a republic against the British monarchy, and would not tolerate war with France.[192]
 On May 16, 1797, Adams gave a speech to the House and Senate in which he called for increasing defense capabilities in case of war with France.[193] He announced that he would send a peace commission to France but simultaneously called for a military buildup to counter any potential French threat. The speech was well received by the Federalists. Adams was depicted as an eagle holding an olive branch in one talon and the ""emblems of defense"" in the other. The Republicans were outraged, for Adams not only had failed to express support for the cause of the French Republic but appeared to be calling for war against it.[194]
 Sentiments changed with the XYZ Affair. The peace commission that Adams appointed consisted of John Marshall, Charles Cotesworth Pinckney and Elbridge Gerry.[195] Jefferson met four times with Joseph Letombe, the French consul in Philadelphia. Letombe wrote to Paris stating that Jefferson had told him that it was in France's best interest to treat the American ministers civilly but ""then drag out the negotiations at length"" to arrive at most favorable solution. According to Letombe, Jefferson called Adams ""vain, suspicious, and stubborn.""[196] When the envoys arrived in October, they were kept waiting for several days, and then granted only a 15-minute meeting with French Foreign Minister Talleyrand. The diplomats were then met by three of Talleyrand's agents (later code-named, X, Y, and Z), who refused to conduct negotiations unless the United States paid enormous bribes to France and to Talleyrand personally.[195] Supposedly this was to make up for offenses given to France by Adams in his speech.[197] The Americans refused to negotiate on such terms.[198] Marshall and Pinckney returned home, while Gerry remained.[199]
 News of the disastrous peace mission arrived in a memorandum from Marshall on March 4, 1798. Adams, not wanting to incite violent impulses among the populace, announced that the mission had failed without providing details.[200] He also sent a message to Congress asking for a renewal of the nation's defenses. The Republicans frustrated the President's defense measures. Suspecting that he might be hiding material favorable to France, Republicans in the House, with the support of Federalists who had heard rumors of what was contained in the messages, voted overwhelmingly to demand that Adams release the papers. Once they were released, the Republicans, according to Abigail, were ""struck dumb.""[201] Benjamin Franklin Bache, editor of the Philadelphia Aurora, blamed Adams's aggression for the disaster. Among the general public however, the affair substantially weakened popular American support of France. Adams reached the height of his popularity as many in the country called for full-scale war against the French.[202]
 Despite the XYZ Affair, Republican opposition persisted. Federalists accused the French and their immigrants of provoking civil unrest. In an attempt to quell the outcry, the Federalists introduced, and the Congress passed, a series of laws collectively referred to as the Alien and Sedition Acts.[203] Passage of the Naturalization Act, the Alien Friends Act, the Alien Enemies Act and the Sedition Act all came within a period of two weeks, in what Jefferson called an ""unguarded passion."" The first three acts targeted immigrants, specifically French, by giving the president greater deportation authority and increasing citizenship requirements. The Sedition Act made it a crime to publish ""false, scandalous, and malicious writing"" against the government or its officials.[204] Adams had not promoted any of these acts, but signed them in June 1798 at the urging of his wife and cabinet.[205]
 The administration initiated fourteen or more indictments under the Sedition Act, as well as suits against five of the six most prominent Republican newspapers. The majority of the legal actions began in 1798 and 1799, and went to trial on the eve of the 1800 presidential election.[206] Vocal opponents of the Federalists were imprisoned or fined under the Sedition Act for criticizing the government.[207] Among them was Congressman Matthew Lyon of Vermont, who was sentenced to four months in jail for criticizing the President.[208] The alien acts were not stringently enforced because Adams resisted Secretary of State Timothy Pickering's attempts to deport aliens, although many left on their own, largely in response to the hostile environment.[206] Republicans were outraged. Jefferson, disgusted by the acts, wrote nothing publicly but partnered with Madison to secretly draft the Kentucky and Virginia Resolutions. Jefferson wrote for Kentucky that states had the ""natural right"" to nullify any acts they deemed unconstitutional. Writing to Madison, he speculated that as a last resort the states might have to ""sever ourselves from the union we so much value.""[209] Federalists reacted bitterly to the resolutions, and the acts energized and unified the Republican Party while doing little to unite the Federalists.[210]
 In May 1798, a French privateer captured a merchant vessel off of New York Harbor. An increase in attacks on sea marked the beginning of the undeclared naval war known as the Quasi-War.[211] Adams knew that America would be unable to win a major conflict, both because of its internal divisions and because France at the time was dominating the fight in most of Europe. He pursued a strategy whereby America harassed French ships in an effort sufficient to stem the French assaults on American interests.[212] In May, shortly after the attack in New York, Congress created a separate Navy Department. The prospect of a French invasion led for calls to build up the army. Hamilton and other ""High Federalists"" were particularly adamant that a large army be called up, in spite of a common fear, particularly among Republicans, that large standing armies were subversive to liberty. In May, a provisional army of 10,000 soldiers was authorized by Congress. In July, Congress created twelve infantry regiments and provided for six cavalry companies, exceeding Adams's requests but falling short of Hamilton's.[213]
 Federalists pressured Adams to appoint Hamilton, who had served as Washington's aide-de-camp during the Revolution, to command the army.[214] Distrustful of Hamilton and fearing a plot to subvert his administration, Adams chose Washington without consulting him. As a condition of his acceptance, Washington demanded that he be permitted to appoint his own subordinates. He wished to have Henry Knox as second-in-command, followed by Hamilton, and then Charles Pinckney.[215] On June 2, Hamilton wrote to Washington stating that he would not serve unless he was made Inspector General and second-in-command.[216] Washington conceded that Hamilton, despite holding a rank lower than Knox and Pinckney, had, by serving on his staff, more opportunity to comprehend the whole military scene, and should therefore outrank them. Adams sent Secretary of War James McHenry to Mount Vernon to convince Washington to accept the post. McHenry put forth his opinion that Washington would not serve unless permitted to choose his own officers.[217] Adams had intended to appoint Republicans Burr and Frederick Muhlenberg to make the army appear bipartisan. Washington's list consisted entirely of Federalists.[218] Adams relented and agreed to submit to the Senate the names of Hamilton, Pinckney, and Knox, in that order, although final decisions of rank would be reserved to Adams.[217] Knox refused to serve under these conditions. Adams intended to give to Hamilton the lowest possible rank, while Washington and many other Federalists insisted that the order in which the names had been submitted to the Senate must determine seniority. On September 21, Adams received a letter from McHenry relaying a statement from Washington threatening to resign if Hamilton were not made second-in-command.[219] Fearing Federalist backlash, Adams capitulated, despite bitter resentment.[220] The illness of Abigail, whom Adams feared was near death, exacerbated his suffering.[219]
 It quickly became apparent that due to Washington's advanced age, Hamilton was the army's de facto commander. He exerted effective control over the War Department, taking over supplies for the army.[221] Meanwhile, Adams built up the Navy, adding six fast, powerful frigates, most notably the USS Constitution.[222]
 The Quasi-War continued, but there was a decline in war fever beginning in the fall once news arrived of the French defeat at the Battle of the Nile, which many Americans hoped would make them more disposed to negotiate.[223] In October, Adams heard from Gerry in Paris that the French wanted to make peace and would properly receive an American delegation. That December in his address to Congress, Adams relayed these statements while expressing the need to maintain adequate defenses. The speech angered both Federalists, including Hamilton, many of whom had wanted a request for a declaration of war, and Republicans.[224] Hamilton secretly promoted a plan, already rejected by Adams, in which American and British troops would jointly seize Spanish Florida and Louisiana, ostensibly to deter a possible French invasion. Hamilton's critics, including Abigail, saw in his military buildups the signs of an aspiring military dictator.[225]
 On February 18, 1799, Adams nominated diplomat William Vans Murray for a peace mission to France without consulting either his cabinet or Abigail, who nonetheless upon hearing of it described it as a ""master stroke."" To placate Republicans, he nominated Patrick Henry and Ellsworth to accompany Murray, and the Senate immediately approved them on March 3. Henry declined the nomination and Adams chose William Richardson Davie to replace him.[226] Hamilton strongly criticized the decision, as did Adams's cabinet members, who maintained frequent communication with him. Adams again questioned their loyalty but did not remove them.[187] To the annoyance of many, Adams spent March to September 1799 in Peacefield. He returned to Trenton, where the government had set up temporary quarters due to the yellow fever epidemic, after a letter arrived from Talleyrand confirming that American ministers would be received. Adams then decided to send the commissioners to France.[227] Adams arrived in Trenton on October 10.[228] Shortly after, Hamilton, in a breach of military protocol, arrived uninvited at the city to speak with the President, urging him not to send the peace commissioners but instead to ally with Britain to restore the Bourbons. ""I heard him with perfect good humor, though never in my life did I hear a man talk more like a fool,"" Adams said. On November 15, the commissioners set sail for Paris.[229]
 To pay for the military buildup of the Quasi-War, Adams and his Federalist allies enacted the Direct Tax of 1798. Direct taxation by the federal government was widely unpopular, and the government's revenue under Washington had mostly come from excise taxes and tariffs. Though Washington had maintained a balanced budget with the help of a growing economy, increased military expenditures threatened to cause major budget deficits, and the Federalists developed a taxation plan to meet the need for increased government revenue. The Direct Tax of 1798 instituted a progressive land value tax of up to 1% of a property's value. Taxpayers in eastern Pennsylvania resisted federal tax collectors, and in March 1799 the bloodless Fries's Rebellion broke out. Led by Revolutionary War veteran John Fries, rural German-speaking farmers protested what they saw as a threat to their liberties. They intimidated tax collectors, who often found themselves unable to go about their business.[230] The disturbance was quickly ended with Hamilton leading the army to restore peace.[231]
 Fries and two other leaders were arrested, found guilty of treason, and sentenced to hang. They appealed to Adams requesting a pardon. The cabinet unanimously advised Adams to refuse, but he instead granted the pardon, arguing the men had instigated a mere riot as opposed to a rebellion.[232] In his pamphlet attacking Adams before the election, Hamilton wrote that ""it was impossible to commit a greater error.""[233]
 On May 5, 1800, Adams's frustrations with the Hamilton wing of the party exploded during a meeting with McHenry, a Hamilton loyalist who was universally regarded, even by Hamilton, as an inept Secretary of War. Adams accused him of subservience to Hamilton and declared that he would rather serve as Jefferson's vice president or minister at The Hague than be beholden to Hamilton for the presidency. McHenry offered to resign at once, and Adams accepted. On May 10, he asked Pickering to resign. Pickering refused and was summarily dismissed. Adams named John Marshall as Secretary of State and Samuel Dexter as Secretary of War.[234][235] In 1799, Napoleon took over as head of the French government in the Coup of 18 Brumaire and declared the French Revolution over.[236] News of this event increased Adams's desire to disband the provisional army, which, with Washington now dead, was commanded only by Hamilton.[237] His moves to end the army after the departures of McHenry and Pickering were met with little opposition.[238] Federalists joined with Republicans in voting to disband the army in mid-1800.[237]
 Napoleon, determining that further conflict was pointless, signaled his readiness for friendly relations. By the Convention of 1800, the two sides agreed to return any captured ships and to allow for the peaceful transfer of non-military goods to an enemy of the nation. On January 23, 1801, the Senate voted 16–14 in favor of the treaty, four votes short of the necessary two thirds. Some Federalists, including Hamilton, urged that the Senate vote in favor of the treaty with reservations. A new proposal was then drawn up demanding that the Treaty of Alliance of 1778 be superseded and that France pay for its damages to American property. On February 3, the treaty with the reservations passed 22–9 and was signed by Adams.[239][c] News of the peace treaty did not arrive in the United States until after the election, too late to sway the results.[241]
 As president, Adams proudly avoided war, but deeply split his party in the process. Historian Ron Chernow writes that ""the threat of Jacobinism"" was the one thing that united the Federalist Party, and that Adams's elimination of it unwittingly contributed to the party's demise.[242]
 Adams's leadership on naval defense has sometimes led him to be called the ""father of the American Navy.""[243][244] In July 1798, he signed into law An Act for the relief of sick and disabled seamen, which authorized the establishment of a government-operated marine hospital service.[245] In 1800, he signed the law establishing the Library of Congress.[246]
 Adams made his first official visit to the nation's new seat of government in early June 1800. Amid the ""raw and unfinished"" cityscape, the President found the public buildings ""in a much greater forwardness of completion than expected.""[247] He moved into the nearly completed President's Mansion (later known as the White House) on November 1. Abigail arrived a few weeks later. On arrival, Adams wrote to her, ""Before I end my letter, I pray Heaven to bestow the best of Blessings on this House and all that shall hereafter inhabit it. May none but honest and wise Men ever rule under this roof.""[248] The Senate of the 7th Congress met for the first time in the new Congress House (later known as the Capitol building) on November 17, 1800. On November 22, Adams delivered his fourth State of the Union Address to a joint session of Congress.[249] This would be the last annual message any president would personally deliver to Congress for the next 113 years.[250]
 With the Federalist Party deeply split over his negotiations with France, and the opposition Republican Party enraged over the Alien and Sedition Acts and the expansion of the military, Adams faced a daunting reelection campaign in 1800.[170] The Federalist congressmen caucused in the spring of 1800 and nominated Adams and Pinckney. The Republicans nominated Jefferson and Burr, their candidates in the previous election.[251]
 The campaign was bitter and characterized by malicious insults by partisan presses on both sides. Federalists claimed that the Republicans were the enemies of ""all who love order, peace, virtue, and religion."" They were said to be libertines and dangerous radicals who favored states' rights over the Union and would instigate anarchy and civil war. Jefferson's rumored affairs with slaves were used against him. Republicans accused Federalists of subverting republican principles through punitive federal laws and of favoring Britain and the other coalition countries in their war with France to promote aristocratic, anti-republican values. Jefferson was portrayed as an apostle of liberty and man of the people, while Adams was labelled a monarchist. He was accused of insanity and marital infidelity.[252] James T. Callender, a Republican propagandist secretly financed by Jefferson, degraded Adams's character and accused him of attempting to make war with France. Callender was arrested and jailed under the Sedition Act, which further inflamed Republican passions.[253]
 Opposition from the Federalist Party was at times equally intense. Some, including Pickering, accused Adams of colluding with Jefferson so that he would end up either president or vice president.[254] Hamilton was hard at work, attempting to sabotage the President's reelection. Planning an indictment of Adams's character, he requested and received private documents from both the ousted cabinet secretaries and Wolcott.[255] The letter was intended for only a few Federalist electors. Upon seeing a draft, several Federalists urged Hamilton not to send it. Wolcott wrote that ""the poor old man"" could do himself in without Hamilton's assistance. Hamilton did not heed their advice.[256] On October 24, he sent a pamphlet strongly attacking Adams's policies and character. Hamilton denounced the ""precipitate nomination"" of Murray, the pardoning of Fries, and the firing of Pickering. He vilified the President's ""disgusting egotism"" and ""ungovernable temper."" Adams, he concluded, was ""emotionally unstable, given to impulsive and irrational decisions, unable to coexist with his closest advisers, and generally unfit to be president.""[233] Strangely, it ended by saying that the electors should support Adams and Pinckney equally.[257] Thanks to Burr, who had covertly obtained a copy, the pamphlet became public knowledge and was distributed throughout the country by Republicans.[258] The pamphlet ended Hamilton's political career and helped ensure Adams's already likely defeat.[257]
 When the electoral votes were counted, Adams finished third with 65 votes, and Pinckney came in fourth with 64 votes. Jefferson and Burr tied for first with 73 votes each. Because of the tie, the election devolved upon the House of Representatives, with each state having one vote and a majority required for victory. On February 17, 1801 – on the 36th ballot – Jefferson was elected by a vote of 10 to 4 (two states abstained).[170][180] Hamilton's scheme, although it made the Federalists appear divided and therefore helped Jefferson win, failed in its overall attempt to woo Federalist electors away from Adams.[259][d]
 To compound the agony of his defeat, Adams's son Charles, a long-time alcoholic, died on November 30. Anxious to rejoin Abigail, who had already left for Massachusetts, Adams departed the White House in the predawn hours of March 4, 1801, and did not attend Jefferson's inauguration.[262][263] Including him, only five out-going presidents (having served a full term) have not attended their successors' inaugurations.[264] The complications of the 1796 and 1800 elections prompted a modification to the Electoral College through the 12th Amendment.[265]
 Adams appointed two U.S. Supreme Court associate justices during his term in office: Bushrod Washington, the nephew of George Washington, and Alfred Moore.[266] After Ellsworth's retirement due to ill health in 1800, it fell to Adams to appoint the Court's fourth Chief Justice. At the time, it was not yet certain whether Jefferson or Burr would win the election. Regardless, Adams believed that the choice should be someone ""in the full vigor of middle age"" who could counter what might be a long line of successive Republican presidents. Adams chose his Secretary of State John Marshall.[267] He, along with Stoddert, was one of Adams's few trusted cabinet members, and was among the first to greet him when he arrived at the White House.[257] Adams signed his commission on January 31 and the Senate approved it immediately.[268] Marshall's long tenure left a lasting influence on the Court. He maintained a carefully reasoned nationalistic interpretation of the Constitution and established the judicial branch as the equal of the executive and legislative branches.[269]
 After the Federalists lost control of both houses of Congress along with the White House in the election of 1800, the lame-duck session of the 6th Congress in February 1801 approved a judiciary act, commonly known as the Midnight Judges Act, which created a set of federal appeals courts between the district courts and the Supreme Court. Adams filled the vacancies created in this statute by appointing a series of judges, whom his opponents called the ""Midnight Judges"", just days before his term expired. Most of these judges lost their posts when the 7th Congress, with a solid Republican majority, approved the Judiciary Act of 1802, abolishing the newly created courts.[270]
 Adams resumed farming at Peacefield in Quincy, Massachusetts, and also began work on an autobiography. The work had numerous gaps and was eventually abandoned and left unedited.[271] Most of Adams's attention was focused on farm work,[272] although he mostly left manual labor to hired hands.[273] His frugal lifestyle and presidential salary gave him a considerable fortune by 1801. In 1803, Bird, Savage & Bird, the bank holding his cash reserves of about $13,000, collapsed.[274] John Quincy resolved the crisis by buying his properties in Weymouth and Quincy, including Peacefield, for $12,800.[272] During his first four years of retirement, Adams made little effort to contact others, but eventually resumed contact with old acquaintances such as Benjamin Waterhouse and Benjamin Rush.[275]
 Adams generally stayed quiet on public matters. He did not publicly denounce Jefferson's actions as president, believing that ""instead of opposing Systematically any Administration, running down their Characters and opposing all their Measures right or wrong, We ought to Support every Administration as far as We can in Justice.""[276][277] When a disgruntled James Callender, angry at not being appointed to an office, turned on the President by revealing the Sally Hemings affair, Adams said nothing.[278] John Quincy was elected to the Senate in 1803. Shortly thereafter, both he and his father crossed party lines to support Jefferson's Louisiana Purchase.[279] The only major political incident involving the elder Adams during the Jefferson years was a dispute with Mercy Otis Warren in 1806. Warren, an old friend, had written a history of the American Revolution attacking Adams for his ""partiality for monarchy"" and ""pride of talents and much ambition."" A tempestuous correspondence ensued between her and Adams. In time, their friendship healed.[280] Adams did privately criticize the President over his Embargo Act,[277] although John Quincy voted for it.[281] John Quincy resigned from the Senate in 1808 after the Federalist-controlled Massachusetts Senate refused to nominate him for a second term. After the Federalists denounced John Quincy as no longer being of their party, Adams wrote to him that he himself had long since ""abdicated and disclaimed the name and character and attributes of that sect.""[2]
 After Jefferson's retirement in 1809, Adams became more vocal. He published a three-year marathon of letters in the Boston Patriot newspaper, refuting line-by-line Hamilton's 1800 pamphlet. The initial piece was written shortly after his return from Peacefield and ""had gathered dust for eight years."" Adams had decided to shelve it over fears that it could negatively impact John Quincy should he ever seek office. Although Hamilton had died in 1804 in a duel with Aaron Burr, Adams felt the need to vindicate his character against his charges. With John Quincy having broken from the Federalist Party and joined the Republicans, he felt that he could safely do so without threatening his political career.[282] Adams supported the War of 1812. Having worried over the rise of sectionalism, he celebrated the growth of a ""national character"" that accompanied it.[283] Adams supported James Madison for reelection to the presidency in 1812.[284]
 Adams's daughter Abigail (""Nabby"") was married to William Stephens Smith, but she returned to her parents' home after the failure of the marriage; she died of breast cancer in 1813.[285]
 In early 1801, Adams sent Thomas Jefferson a brief note wishing him a happy and prosperous presidency. Jefferson failed to respond, and they did not speak again for nearly 12 years. In 1804, Abigail, unbeknownst to her husband, wrote to Jefferson to express her condolences upon the death of his daughter Polly, who had stayed with the Adamses in London in 1787. This initiated a brief correspondence between the two which quickly descended into political rancor. Jefferson terminated it by not replying to Abigail's fourth letter. Aside from that, by 1812 there had been no communication between Monticello, the home of Jefferson, and Peacefield since Adams left office.[286]
 In early 1812, Adams reconciled with Jefferson. The previous year had been tragic for Adams; his brother-in-law and friend Richard Cranch had died along with his widow Mary, and Nabby had been diagnosed with breast cancer. These events mellowed Adams and caused him to soften his outlook.[282] Their mutual friend Benjamin Rush, who had been corresponding with both, encouraged them to reach out to each other. On New Year's Day, Adams sent a brief, friendly note to Jefferson to accompany a two-volume collection of lectures on rhetoric by John Quincy Adams. Jefferson replied immediately with a cordial letter, and the two revived their friendship, which they sustained by mail. Their correspondence lasted the rest of their lives, and has been hailed as among their great legacies of American literature. Their letters represent an insight into both the period and the minds of the two revolutionary leaders and presidents. The missives lasted fourteen years, and consisted of 158 letters – 109 from Adams and 49 from Jefferson.[287]
 Early on, Adams repeatedly tried to turn the correspondence to a discussion of their actions in the political arena.[288] Jefferson refused to oblige him, saying that ""nothing new can be added by you or me to what has been said by others and will be said in every age.""[289] Adams made one more attempt, writing that ""You and I ought not to die before we have explained ourselves to each other.""[290] Still, Jefferson declined to engage Adams in this sort of discussion. Adams accepted this, and the correspondence turned to other matters, particularly philosophy and their daily habits.[291][e]
 As the two grew older, the letters grew fewer and farther between. There was also important information that each man kept to himself. Jefferson said nothing about his construction of a new house, domestic turmoil, slave ownership, or poor financial situation, while Adams did not mention the troublesome behavior of his son Thomas, who had failed as a lawyer and become an alcoholic, resorting afterwards to living primarily as a caretaker at Peacefield.[294]
 Abigail died of typhoid on October 28, 1818, at Peacefield.[295] 1824 was filled with excitement in America, featuring a four-way presidential contest that included John Quincy. The Marquis de Lafayette toured the country and met with Adams, who greatly enjoyed Lafayette's visit to Peacefield.[296] Adams was delighted by the election of John Quincy to the presidency. The results became official in February 1825 after a deadlock was decided in the House of Representatives. He remarked, ""No man who ever held the office of President would congratulate a friend on obtaining it.""[297]
 On July 4, 1826, the 50th anniversary of the adoption of the Declaration of Independence, Adams died of a heart attack at Peacefield at approximately 6:20 pm.[298][299] His last words included an acknowledgement of his longtime friend and rival: ""Thomas Jefferson survives."" Adams was unaware that Jefferson had died several hours before.[300][301] At 90, Adams was the longest-lived US president until Ronald Reagan surpassed him in 2001.[302]
 John and Abigail Adams's crypt at United First Parish Church in Quincy also contains the bodies of John Quincy and Louisa Adams.[303]
 During the First Continental Congress, Adams was sometimes solicited for his views on government. While recognizing its importance, Adams had privately criticized Thomas Paine's 1776 pamphlet Common Sense, which attacked all forms of monarchy, even constitutional monarchy of the sort advocated by John Locke. It supported a unicameral legislature and a weak executive elected by the legislature. According to Adams, the author had ""a better hand at pulling down than building.""[304] He believed that the views expressed in the pamphlet were ""so democratical, without any restraint or even an attempt at any equilibrium or counter poise, that it must produce confusion and every evil work.""[305] What Paine advocated was a radical democracy, incompatible with the system of checks and balances that conservatives like Adams would implement.[306] At the urging of some delegates, Adams committed his views to paper in separate letters. So impressed was Richard Henry Lee that, with Adams's consent, he had the most comprehensive letter printed. Published anonymously in April 1776, it was titled Thoughts on Government and styled as ""a Letter from a Gentleman to his Friend."" Many historians agree that none of Adams's other compositions rivaled the enduring influence of this pamphlet.[70]
 Adams advised that the form of government should be chosen to attain the desired ends – the happiness and virtue of the greatest number of people. He wrote, ""There is no good government but what is republican. That the only valuable part of the British constitution is so because the very definition of a republic is an empire of laws, and not of men."" The treatise defended bicameralism, for ""a single assembly is liable to all the vices, follies and frailties of an individual.""[307] Adams suggested that there should be a separation of powers between the executive, the judicial and the legislative branches, and further recommended that if a continental government were to be formed then it ""should sacredly be confined"" to certain enumerated powers. Thoughts on Government was referenced in every state-constitution writing hall. Adams used the letter to attack opponents of independence. He claimed that John Dickinson's fear of republicanism was responsible for his refusal to support independence, and that opposition from Southern planters was rooted in fear that their aristocratic slaveholding status would be endangered.[70]
 After returning from his first mission to France in 1779, Adams was elected to the Massachusetts Constitutional Convention with the purpose of establishing a new constitution for Massachusetts. He served on a committee of three, also including Samuel Adams and James Bowdoin, to draft the constitution. The writing fell primarily to John Adams. The resulting Constitution of Massachusetts was approved in 1780. It was the first constitution written by a special committee, then ratified by the people, and was the first to feature a bicameral legislature. Included were a distinct executive – though restrained by an executive council – with a qualified (two-thirds) veto, and an independent judicial branch. The judges were given lifetime appointments, to ""hold their offices during good behavior.""[308]
 The Constitution affirmed the ""duty"" of the individual to worship the ""Supreme Being,"" and the right to do so without molestation ""in the manner most agreeable to the dictates of his own conscience.""[309] It established free public education for three years to the children of all citizens.[310] Adams was a strong believer in education as a pillar of the Enlightenment. He believed that people ""in a State of Ignorance"" were more easily enslaved while those ""enlightened with knowledge"" would be better able to protect their liberties.[311]
 Adams's preoccupation with political and governmental affairs, which caused considerable separation from his wife and children, had a distinct familial context, which he articulated in 1780: ""I must study Politicks and War that my sons may have the liberty to study Mathematicks and Philosophy. My sons ought to study Geography, natural History, Naval Architecture, navigation, Commerce and Agriculture, in order to give their children a right to study Painting, Poetry, Musick, Architecture, Statuary, Tapestry, and Porcelaine.""[312]
 While in London, Adams learned of a convention being planned to amend the Articles of Confederation. In January 1787, he published a work entitled A Defence of the Constitutions of Government of the United States.[313] The pamphlet repudiated the views of Turgot and other European writers as to the viciousness of state government frameworks. He suggested that ""the rich, the well-born and the able"" should be set apart from other men in a senate – that would prevent them from dominating the lower house. Adams's Defence is described as an articulation of the theory of mixed government. Adams contended that social classes exist in every political society, and that a good government must accept that reality. For centuries, a mixed regime balancing monarchy, aristocracy, and democracy was required to preserve order and liberty.[314]
 Historian Gordon S. Wood maintained that Adams's political philosophy had become irrelevant by the time the Federal Constitution was ratified. By then, American political thought, transformed by more than a decade of vigorous debate as well as formative experiential pressures, had abandoned the classical perception of politics as a mirror of social estates. Americans' new understanding of popular sovereignty was that the citizenry were the sole possessors of power in the nation. Representatives in the government enjoyed mere portions of the people's power and only for a limited time. Adams was thought to have overlooked this evolution and revealed his continued attachment to the older version of politics.[315] Yet Wood was accused of ignoring Adams's peculiar definition of the term ""republic"", and his support for a constitution ratified by the people.[316]
 On separation of powers, Adams wrote that, ""Power must be opposed to power, and interest to interest.""[317] This sentiment was later echoed by James Madison's statement that, ""[a]mbition must be made to counteract ambition"", in Federalist No. 51, explaining the separation of powers established under the new Constitution.[317][318] Adams believed that humans naturally wanted to further their own ambitions, and a single democratically elected house, if left unchecked, would be subject to this error; it needed to be checked by an upper house and an executive. He wrote that a strong executive would defend the people's liberties against ""aristocrats"" attempting to take it away.[319]
 Adams first saw the new United States Constitution in late 1787. To Jefferson, he wrote that he read it ""with great satisfaction."" Adams expressed regret that the president would be unable to make appointments without Senate approval and over the absence of a Bill of Rights.[320]
 
Adams never owned a slave and declined on principle to use slave labor, saying,   Before the war, he occasionally represented slaves in suits for their freedom.[322] Adams generally tried to keep the issue out of national politics, because of the anticipated Southern response during a time when unity was needed to achieve independence. He spoke out in 1777 against a bill to emancipate slaves in Massachusetts, saying that the issue was presently too divisive so the legislation should ""sleep for a time."" He was against use of black soldiers in the Revolution due to opposition from Southerners.[323] Slavery was abolished in Massachusetts about 1780, when it was forbidden by implication in the Declaration of Rights that John Adams wrote into the Massachusetts Constitution.[324] Abigail Adams vocally opposed slavery.[325]
 Adams expressed controversial and shifting views regarding the virtues of monarchical and hereditary political institutions.[327] At times he conveyed substantial support for these approaches, suggesting for example that ""hereditary monarchy or aristocracy"" are the ""only institutions that can possibly preserve the laws and liberties of the people.""[328] At other times he distanced himself from such ideas, calling himself ""a mortal and irreconcilable enemy to Monarchy"".[147] Such denials did not assuage his critics, and Adams was often accused of being a monarchist.[329] Historian Clinton Rossiter portrays Adams as a revolutionary conservative who sought to balance republicanism with the stability of monarchy to create ""ordered liberty.""[330] His 1790 Discourses on Davila published in the Gazette of the United States warned once again of the dangers of unbridled democracy.[331]
 Many attacks on Adams were scurrilous, including suggestions that he was planning to ""crown himself king"" and ""grooming John Quincy as heir to the throne.""[329] The allegations were totally false, he told Jefferson—he never wanted an American monarchy.[332] Adams felt that the great danger was that an oligarchy of the wealthy would take hold to the detriment of equality. To counter that danger, the power of the wealthy needed to be channeled by institutions, and checked by a strong executive.[333][319]
 According to biographer David McCullough, ""Adams was both a devout Christian, and an independent thinker, and he saw no conflict in that.""[334] He believed that regular church service was beneficial to man's moral sense.[335] Adams was raised in the Congregational church. In Quincy, the Unitarian faction was dominant and included Adams and his father. It was a new force in the colonies and denied the Trinity and the divinity of Jesus Christ. It was opposed by the Calvinist faction. In 1825, the Unitarians split off as a separate denomination that included John Adams.[336][337]
 Adams' family was descended from Puritans. Strict Puritanism had profoundly shaped New England's culture, laws, and traditions, and Adams praised the historical Puritans as ""bearers of freedom, a cause that still had a holy urgency"".[338] Adams recalled that his parents ""held every Species of Libertinage in ... Contempt and horror"".[4]
 Fielding argues that Adams's beliefs synthesized Puritan, deist, and humanist concepts.[339] Frazer notes that while he shared many perspectives with deists and often used deistic terminology, ""Adams clearly was not a deist... Adams did believe in miracles, providence, and, to a certain extent, the Bible as revelation.""[340] In 1796, Adams denounced Thomas Paine's deistic criticisms of Christianity in The Age of Reason, saying, ""The Christian religion is, above all the religions that ever prevailed or existed in ancient or modern times, the religion of wisdom, virtue, equity and humanity, let the Blackguard Paine say what he will.""[341] Gordon S. Wood writes, ""Although both Jefferson and Adams denied the miracles of the Bible and the divinity of Christ, Adams always retained a respect for the religiosity of people that Jefferson never had"".[342] In his retirement years, Adams moved closer to more mainstream Enlightenment religious ideals. He blamed institutional Christianity and established churches in Britain and France for causing much suffering but insisted that religion was necessary for society.[343]
 Benjamin Franklin summarized what many thought of Adams, saying ""He means well for his country, is always an honest man, often a wise one, but sometimes, and in some things, absolutely out of his senses.""[344] Adams strongly felt that he would be forgotten and underappreciated by history. These feelings often manifested themselves through envy and verbal attacks on other Founders.[160][345] Edmund Morgan argues, ""Adams was ridiculously vain, absurdly jealous, embarrassingly hungry for compliments. But no man ever served his country more selflessly.""[346]
 Historian George C. Herring argued that Adams was the most independent minded of the Founders.[347] Though he formally aligned with the Federalists, he was somewhat a party unto himself, at times disagreeing with the Federalists as much as he did the Republicans.[348] He was often described as prickly, but his tenacity was fed by decisions made in the face of universal opposition.[347] Adams was often combative, as he admitted: ""[As President] I refused to suffer in silence. I sighed, sobbed, and groaned, and sometimes screeched and screamed. And I must confess to my shame and sorrow that I sometimes swore.""[349] Stubbornness was seen as one of his defining traits, a fact for which Adams made no apology. ""Thanks to God that he gave me stubbornness when I know I am right,"" he wrote.[350] His resolve to advance peace with France while maintaining a posture of defense reduced his popularity and contributed to his defeat for reelection.[351] Most historians applaud him for avoiding an all-out war with France during his presidency. His signing of the Alien and Sedition Acts is almost always condemned.[352]
 According to Ferling, Adams's political philosophy fell ""out of step"" with national trends. The country tended further away from Adams's emphasis on order and the rule of law and towards the Jeffersonian vision of liberty and weak central government. In the years following his retirement, as first Jeffersonianism and then Jacksonian democracy grew to dominate American politics, Adams was largely forgotten.[353] In the 1840 presidential election, Whig candidate William Henry Harrison was attacked by Democrats on the false allegation that he had been a supporter of John Adams.[354] Adams was eventually subject to criticism from states' rights advocates. Edward A. Pollard, a strong supporter of the Confederacy during the American Civil War, singled out Adams, writing:
 In the 21st century, Adams remains less well known than many of the Founders, in accordance with his predictions. McCullough argued that ""[t]he problem with Adams is that most Americans know nothing about him."" Todd Leopold of CNN wrote in 2001 that Adams is ""remembered as that guy who served a single term as president between Washington and Jefferson.""[356] He has always been seen, Ferling says, as ""honest and dedicated"", but despite his lengthy career in public service, is still overshadowed.[357] Gilbert Chinard, in his 1933 biography of Adams, described him as ""staunch, honest, stubborn and somewhat narrow.""[358] In his 1962 biography, Page Smith lauds Adams for his fight against radicals whose promised reforms portended anarchy and misery. Ferling, in his 1992 biography, writes that ""Adams was his own worst enemy.""[359] He criticizes him for his ""pettiness ... jealousy, and vanity"", and faults his frequent separations from his family. He praises Adams for his willingness to acknowledge his deficiencies and for striving to overcome them.[360]
 In 2001, McCullough published the biography John Adams, in which he lauds Adams for consistency and honesty, ""plays down or explains away"" his more controversial actions, and criticizes Jefferson. The book sold very well and was very favorably received and, along with the Ferling biography, contributed to a rapid resurgence in Adams's reputation.[361] In 2008, a miniseries was released based on the McCullough biography, featuring Paul Giamatti as Adams.[362]
 Adams is commemorated as the namesake of various counties, buildings, and other items.[246][363][364] One example is the John Adams Building of the Library of Congress, an institution whose existence Adams had signed into law.[246]
 Adams is honored on the Memorial to the 56 Signers of the Declaration of Independence in Washington D.C.[365] He does not have an individual monument dedicated to him in the city,[366] although a family Adams Memorial was authorized in 2001. According to McCullough, ""Popular symbolism has not been very generous toward Adams. There is no memorial, no statue ... in his honor in our nation's capital, and to me that is absolutely inexcusable. It's long past time when we should recognize what he did, and who he was.""[367]
"
Commander-in-chief,https://en.wikipedia.org/wiki/Commander-in-chief,"
 A commander-in-chief or supreme commander (supreme commander-in-chief) is the person who exercises supreme command and control over an armed force or a military branch. As a technical term, it refers to military competencies that reside in a country's executive leadership, a head of state, head of government, or other designated government official.
 The formal role and title of a ruler commanding the armed forces derives from Imperator of the Roman Kingdom, Roman Republic and Roman Empire, who possessed imperium (command and other regal) powers.[1]
 In English use, the term was first used during the English Civil War.[2] A nation's head of state (monarchical or republican) usually holds the position of commander-in-chief, even if effective executive power is held by a separate head of government. In a parliamentary system, the executive branch is ultimately dependent upon the will of the legislature; although the legislature does not issue orders directly to the armed forces and therefore does not control the military in any operational sense. Governors-general and colonial governors are also often appointed commander-in-chief of the military forces within their territory.
 A commander in chief is sometimes referred to as supreme commander, which is sometimes used as a specific term. The term is also used for military officers who hold such power and authority, not always through dictatorship, and as a subordinate (usually) to a head of state (see Generalissimo). The term is also used for officers who hold authority over an individual military branch, special branch or within a theatre of operations.[3]
 This includes heads of states who:
 According to the Constitution of Albania, the president of the Republic of Albania is the commander-in-chief of Albanian Armed Forces.
 Under part II, chapter III, article 99, subsections 12, 13, 14 and 15, the Constitution of Argentina states that the president of the Argentine Nation is the ""Commander-in-chief of all the armed forces of the Nation"". It also states that the president is entitled to provide military posts in the granting of the jobs or grades of senior officers of the armed forces, and by itself on the battlefield; runs with its organization and distribution according to needs of the Nation and declares war and orders reprisals with the consent and approval of the Argentine National Congress.[4]
 The Ministry of Defense is the government department that assists and serves the president in the management of the armed forces (Army, Navy and Air Force).[5]
 
Under chapter II of section 68 titled Command of the naval and military forces, the Constitution of Australia states that: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequotecite{line-height:1.5em;text-align:left;margin-top:0}@media(min-width:500px){.mw-parser-output .templatequotecite{padding-left:1.6em}} 
In practice, however, the Governor-General does not play an active part in the Australian Defence Force's command structure, and the democratically accountable Australian Cabinet (chaired by the Prime Minister) de facto controls the ADF. The Minister for Defence and several subordinate ministers exercise this control through the Australian Defence Organisation. Section 8 of the Defence Act 1903 states: According to the Constitution of Barbados, the president of Barbados is the commander-in-chief of Barbados Defense Force. Between 1966 and 2021, prior to the transition to a republican system, the monarch of Barbados, Queen Elizabeth II, was head of the Defense Force, with the Governor-General of Barbados as her viceroy. The president adopted these powers.
 First President Sheikh Mujibur Rahman was the Supreme Commander of all the Armed Forces of the Republic. In absence of him, then Vice President Syed Nazrul Islam was acting President and acting Supreme Commander of all the Armed Forces of the Republic.
 The commander-in-chief of Bangladesh Armed Forces is the president, although executive power and responsibility for national defense resides with the prime minister. This is discharged through the Ministry of Defence, headed by the minister of defence, which provides the policy framework and resources to the Armed Forces to discharge their responsibilities in the context of the defence of the country.
 The first commander-in-chief, General M. A. G. Osmani, during Bangladesh Liberation War in 1971, who was commander of Muktibahini/Bangladesh Forces, reinstated to active duty by official BD government order, which after independence was gazetted in 1972. He retired on 7 April 1972 and relinquished all authority and duties to the president of Bangladesh.[10]
 The president of Belarus is the Commander-in-Chief of the Belarusian Armed Forces (Belarusian: Галоўнакамандуючы Узброенымі Сіламі Рэспублікі Беларусь).[11] The Belarusian commander in chief has an official uniform befitting of the rank, which the president wears on official occasion and ceremonies in relation to the military. The role of commander in chief is laid out in Article 28 of the Constitution of Belarus, which states that he/she has the authority to ""appoint and dismiss the high command of the Armed Forces"".[12]
 Article 167 of the Constitution of Belgium designates the king as the commander-in-chief. In practice, the Chief of Defence is the head and commander of the Belgian Armed Forces. He reports directly to the Minister of Defence and is responsible for advising the Minister, for the implementation of defence policy and for the administration of the department.
 According to the Constitution of Bosnia and Herzegovina, the collective Presidency of Bosnia and Herzegovina is the commander-in-chief of the Armed Forces of Bosnia and Herzegovina. In peace, the commander-in-chief exercises his command through the minister of defence. In war and in cases where the minister of defence is not fulfilling orders, the commander-in-chief exercises his command directly through the Chief of Joint Staff.
 Article 142 of the Brazilian Constitution of 1988 states that the Brazilian Armed Forces is under the supreme command of the president of the Republic.[13]
 The sultan of Brunei is the commander-in-chief of the Royal Brunei Armed Forces.
 The powers of command-in-chief over the Canadian Armed Forces are vested in the Canadian monarch,[14] and are delegated to the governor general of Canada, who also uses the title Commander-in-Chief.[15] In this capacity, the governor general is entitled to the uniform of a general/flag officer, with the crest of the office and special cuff braid serving as rank insignia.
 By constitutional convention, the Crown's prerogative powers over the armed forces and constitutional powers as commander-in-chief are exercised on the advice of the prime minister and the rest of Cabinet, the governing ministry that commands the confidence of the House of Commons. According to the National Defence Act, the Minister of National Defence is responsible and accountable to the Parliament of Canada for all matters related to national defence and the Canadian Armed Forces.[16]
 According to the Croatian constitution, the president of Croatia is the commander-in-chief of the Armed Forces of the Republic of Croatia. There was originally a rank insignia and name for the position, known as ""Vrhovnik"". This was held by former President Franjo Tudjman and was abolished after his death. In peace, the commander-in-chief exercises his command through the minister of defence. In war and in cases where the minister of defence is not fulfilling orders, the commander-in-chief exercises his command directly through the chief of General Staff.
 According to the 1992 constitution, the president of the Czech Republic is the commander-in-chief of the Armed Forces according to Article 63(1)(c), and appoints and promotes generals under Article 63(1)(f). The president needs the countersignature of the prime minister for decisions concerning the above-mentioned provisions as per Articles 63(3–4), or otherwise, they are not valid. The prime minister may delegate to other ministers the right to countersign these decisions of the president. The political responsibility for the Armed Forces is borne by the Government, which in Article 67 is defined as the ""supreme body of executive power"". According to Articles 39 & 43, the Parliament must give consent to the dispatch of Czech military forces outside the territory of the Czech Republic.[17]
 The Ministry of Defence is the central authority of the state administration for the control of the Armed Forces.[18] The actual day-to-day management is vested in the chief of the general staff, the Czech chief of defence equivalent.[19]
 The position of the Danish monarch as the head of the military is deeply rooted in tradition. While the 1953 constitution does not explicitly designate the monarch as commander-in-chief; it is implicit, given the general provision in article 12 and the more specific wording of article 19 (2): ""Except for purposes of defence against an armed attack upon the Realm or Danish forces, the King shall not use military force against any foreign state without the consent of the Folketing. Any measure which the King may take in pursuance of this provision shall forthwith be submitted to the Folketing"".[20]
 However, when reading the Danish Constitution, it is important to bear in mind that the king in this context is understood by Danish jurists to be read as the government (consisting of the prime minister and other ministers). This is a logical consequence of articles 12, 13 and 14, all of which in essence stipulates that the powers vested in the monarch can only be exercised through ministers, who are responsible for all acts. Thus, the Government, in effect, holds the supreme command authority implied in articles 12 and 19(2).[21]
 The Danish Defence Law (Danish: Forsvarsloven) designates in article 9 the minister of defence as the supreme authority in Defence (Danish: højeste ansvarlige myndighed for forsvaret). Under the minister is the chief of defence, the senior-ranking professional military officer heading the Defence Command, who commands the Army, the Navy, the Air Force and other units not reporting directly to the Ministry of Defence.[22][23]
 According to the Constitution, Article 128, Section II, Title IV, the president is the head of foreign policy, the civil administration and the commander-in-chief of the Armed Forces, the National Police and all other state's security agencies.[24]
 In Egypt, the president of the Republic holds the ceremonial title of Supreme Commander of the Armed Forces. A member of the government, usually defence minister, is commander-in-chief of the Egyptian Armed Forces. The president is the only individual capable of declaring war. With the exception of Mohamed Morsi, who briefly served as president from 2012 to 2013, all Egyptian presidents have been former military officers. During the Yom Kippur War, the president played a major role at all levels of the planning of the war, and was, in a literal sense, Supreme Commander of the Armed Forces, giving direct orders to the commanders from the headquarters during the war as field marshal of the army, marshal of the air force and air defence forces and admiral of the navy.
 The king of Eswatini is the commander in chief of the Umbutfo Eswatini Defence Force.
 The president of Ethiopia is the Supreme Commander of the Ethiopian Armed Forces which is the ceremonial role that is limited to granting high military titles and awards. The prime minister of Ethiopia is the commander-in-chief of the Ethiopian National Defense Force.
 According to the Finnish constitution, the president of Finland is the commander-in-chief of all Finnish military forces. In practice, the everyday command and control is in the hands of the chief of defence and the commander of the Finnish Border Guard. The economic administration of the Finnish Defence Force is the responsibility of Ministry of Defence. The duty of the president is to decide upon[25]: §31 
 Since the constitutional reform of 2000, the minister of defence has the right to be present when the president uses his command powers, unless the matter is of immediate concern. In questions of strategic importance, the prime minister has the same right.[25]: §32 
 The president commissions and promotes officers and decides on activating reservists for extraordinary service and on the mobilisation of the Defence Forces.[25]: §40 [26][27]: § 128.2  If Parliament is not in session when a decision to mobilise is taken, it must be immediately convened.[27]: § 129  Declarations of a state of emergency (Finnish: valmiustila, literally, ""state of preparedness"") and state of war (Finnish: puolustustila, lit. ""state of defence"") are declared by a presidential decree, given after a motion by the government, which is then submitted to the Parliament for ratification.[28][29]
 The president has, in a state of emergency, the right to transfer the position of the commander-in-chief to another Finnish citizen.[27]: § 129 
 In France, the president of the Republic is designated as ""Chef des Armées"" (literally ""Chief of the Armies"") under article 15 of the Constitution; the officeholder is as such the supreme executive authority in military affairs. Article 16 provides the president with extensive emergency powers.[30]
 However, owing to the nature of the semi-presidential system, the prime minister also has key constitutional powers under article 21: ""He shall be responsible for national defence"" and has ""power to make regulations and shall make appointments to civil and military posts"".[30]
 Since the reign of Louis XIV, France has been strongly centralised. After crushing local nobles engaged in warlord-ism, the kings of France retained all authority with the help of able yet discreet Prime ministers (Mazarin, Richelieu).
 The French Revolution transferred the supreme authority to the King (in the context of the short-lived constitutional monarchy), then to the multi-member Comité de Salut Public during the Convention, as well as later to the Directoire, before being regained in the hands of Consul Napoléon Bonaparte, later Emperor Napoléon I, alone.
 The Restoration restored the authority of the King, first in an absolute monarchy, then the constitutional July Monarchy of Louis Philippe, before it was overthrown in turn by the Second Republic and later the Second Empire of Napoleon III.
 The following Third Republic was a parliamentary system, where the military authority was held by the president of the Council of Ministers, head of government, although the president, head of state, retained ceremonial powers. During World War I, the many visits to the trenches by the elder statesman Georges Clemenceau impressed the soldiers and earned him the nickname Father of Victory (French: Le Père de la Victoire).
 During World War II, Maréchal Philippe Pétain assumed power and held the supreme authority in Vichy France, while Général Charles de Gaulle, acting on behalf of the previous regime, founded the Free French Forces, upon which he held supreme authority all through the war.
 The following and short-lived Fourth Republic was a parliamentary system, which was replaced by the present Fifth Republic, a semi-presidential system.
 According to the Constitution of Ghana, the president of Ghana is the commander-in-chief of the Ghana Armed Forces. He holds the rank of Field Marshal.
 According to the Guyanese constitution, the president is commander-in-chief of the Armed Forces. There is a rank insignia for the position.
 The supreme commander of the Indian Armed Forces is the principal commanding authority of the Indian Armed Forces, a position that is vested in the head of state,[31] the president of the Republic, in accordance to Article 53 of the Constitution of India.[32]
 The president exercises supreme command with accordance to the law. As commander in chief, the president has the power to declare war however they must subject to the approval of the Parliament of India. The commander in chief also appoints the chiefs of each branch of the armed forces as well as the Chairman Chiefs of Staff Committee with the advice of the Minister of Defence.
 Whilst the constitution names the president as the de jure commander in chief, executive command authority is exercised de facto by the prime minister and their Union Council of Ministers.
 On 15 August 1947, each service was placed under its own commander-in-chief. In 1955, the three service chiefs were re-designated as the chief of the Army staff (rank of general), the chief of the naval staff (rank of vice admiral) and the chief of the air staff (rank of air marshal) with the president as the supreme commander. The chief of the air staff was raised to the rank of air chief marshal in 1965 and the chief of the naval staff raised to the rank of admiral in 1968. Starting from 1 January 2020, all the three chiefs of staff report to the newly formed chief of defence staff.
 According to article 10 of the Constitution of Indonesia, the president of Indonesia holds the supreme command of the Indonesian National Armed Forces. Day-to-day operations of the Armed Forces is handled by the commander of the Armed Forces (Indonesian: Panglima TNI), a 4-star officer whom can be a general (Army/Marine), an admiral (Navy), or an air chief marshal (Air Force). The commander of the Armed Forces is appointed by the president from active chiefs of staff (Army, Navy, or Air Force) and must get approval from the House of Representatives. The chief of staff is also appointed by the president from senior military officers. The president as commander-in-chief also has authority in senior military officer mutation and promotion in tour of duty. The minister of defense has responsibility to assist the president in defense issues and create policies about authorization use of military force, manage defense budget, etc. According to article 11 of the Constitution, For authorization use of military forces or declaration of war, the president must get approval from House of Representatives. The commander of the Armed Forces gives recommendations to the minister of defense in creating national defense policies.
 Before 1979, the shah was the commander-in-chief in Iran. After the inception of the Islamic Republic, the president of Iran was initially appointed that task, with Abolhassan Bani Sadr being the first commander-in-chief. However, Abolhassan Bani Sadr was impeached on 22 June 1981. It was after this event that the role of commander-in-chief of the Armed Forces of the Islamic Republic of Iran was given to the Supreme Leader of Iran.
 The supreme commander of the Defence Forces is the president of Ireland,[33] but in practice the minister for defence acts on the president's behalf and reports to the Government of Ireland.[34] The minister for defence is advised by the Council of Defence on the business of the Department of Defence.[35] The Defence Forces are organised under the chief of staff, a three star officer, and are organised into three service branches, the Army, Naval Service, and Air Corps.
 The Constitution of Italy, in article 87, states that the president of the Republic:
""is the commander of the armed forces and chairman of the supreme defense council constituted by law,       although effective executive power and responsibility for national defence resides with the government headed by the prime minister; the president declares war according to the decision of the parliament"".[36]
 Chapter 131 of the Constitution of Kenya identifies the president as the commander-in-chief of the Kenya Defence Forces and the chairperson of the National Security Council.[37] There is a rank for the position. The president appoints a chief of general staff, known as the Chief of the Kenya Defence Forces, who acts as the principal military adviser to the president and the National Security Council. The Chief of the Kenya Defence Forces is drawn from one of the branches of the Armed Forces, the Kenya Army, the Kenya Navy or the Kenya Air Force.
 In accordance with Article 42 of the Constitution of Latvia, the president of Latvia is Commander-in-Chief of the Latvian National Armed Forces. The president may appoint a chief military commander in times of war.
 In accordance with Article 41 of the Federal Constitution of Malaysia, the Yang di-Pertuan Agong is Supreme Commander of the Malaysian Armed Forces and holds the rank of Field Marshal. As such, he is the highest-ranking officer in the military establishment, with the power to appoint the Chief of Staff (on the advice of the Armed Forces Council). He also appoints the service heads of each of the three branches of the military.
 The Federal Constitution establishes that the office of Supreme Commander is attached to the person of the Yang di-Pertuan Agong as the Federation's head of state:
 The Federal Parliament passed the Federal Armed Forces Act to consolidate in one law all regulations that govern the three services ( Army, Navy, and Air Force ). It establishes the function and duties of the Federal Head of State in the capacity as Supreme Commander.
 Section VI of Article 89 of the Constitution states that the president of the United Mexican States shall ""Preserve national security, in accordance with the respective law, and dispose of the full permanent Armed Force, that is to say the Army, the Navy and the Air Force, for the interior security and exterior defense of the Federation"".[38]
 Both the Organic Law of the Mexican Army and Air Force and the Organic Law of the Mexican Navy clearly state the president of the Republic is ""Supreme Commander of the Armed Forces"". The President is ex officio the only five-star general of Mexico.[39][40]
 The Constitution also grants the president freedom to appoint and remove the secretary of the Navy and the secretary of national defense.
 Both the Monarch of New Zealand and their representative, the governor-general, constitutionally serve as the supreme authority in defence matters in New Zealand.[41] The position of commander-in-chief is vested in the sovereign by the constitution. In practice however, the position of the commander-in-chief is largely ceremonial, with the governor-general primarily serving as a ""patron of the New Zealand Defence Force"".[42] The governor-general exercises their authority as commander-in-chief on the advice of the minister of defence or other ministers of the New Zealand Government.[41][43]
 The Letter Patents 1983 consolidated the roles of governor-general and commander-in-chief into one office, with its compounded title being the Governor-General and Commander-in-Chief.[44] The governor-general's is statutorily defined in the Defence Act 1990.[45] Sections five and six of the Defence Act 1990 outlines the governor-general's authority to raise and maintain armed forces.[43]
 In accordance with the Nigerian Constitution, the president of Nigeria is the commander-in-chief of the Nigerian Armed Forces.
 Harald V, King of Norway, officially retains executive power. Article 25 of the constitution states: ""The King is commander-in-chief of the armed forces of the realm""
 However, following the introduction of a parliamentary system of government, the duties of the monarch have since become strictly representative and ceremonial, such as the formal appointment and dismissal of the prime minister and other ministers in the executive government. Accordingly, the Monarch is commander-in-chief of the Norwegian Armed Forces, and serves as chief diplomatic official abroad and as a symbol of unity.
 In Pakistan, before the 1973 Constitution, the head of the army, was known as the Commander-in-Chief of the Pakistan Army, heads of the navy and the air force were also titled as ""Commander-in-Chief"".[46] The head of army term was replaced to ""Chief of Army Staff"" on 20 March 1972 during military reforms : 62 [47] The chief of staff is a four-star officer whose term is 3 years, but can be extended or renewed once. After 1973 constitution The chief of Army/Air/Naval staff is chosen by the prime minister of Pakistan and appointed by the president of Pakistan as commander in chief of Pakistan Armed Forces.
So, the president of Pakistan is the commander-in-chief.
 The president of the Philippines is both head of state and head of government, and is mandated by Article VII, Section 18 of the 1987 Constitution to be commander-in-chief of the Armed Forces.[48]
 
The president of Poland is the supreme commander (Polish: najwyższy zwierzchnik) of the Polish Armed Forces according to the Constitution and  in times of peace exercises their authority through Minister of National Defence. However, the art. 134 ust. 4 of the constitution states:[49]  During the interbellum period, the General Inspector of the Armed Forces was appointed the commander-in-chief for the time of war (Supreme Commander of the Armed Forces). However, after the war this function ceased to exist—thus it is likely that if Poland formally participates in a war, the chief of the general staff of the Polish Armed Forces will be appointed supreme commander.
 The president of the Portuguese Republic is the constitutional supreme commander of the Armed Forces (in Portuguese: Comandante Supremo das Forças Armadas). However, the operational command is delegated in the chief of the general staff of the Armed Forces.
 In the Portuguese military parlance, the term ""Commander-in-Chief"" (in Portuguese: comandante-em-chefe or simply comandante-chefe) refers to the unified military commander of all the land, naval and air forces in a theater of operations.
 According to the Constitution of the Russian Federation, (Chapter 4, Article 87, Section 1) the president is the Supreme Commander-in-Chief of the Armed Forces.[51] The president approves the military doctrine and appoints the defense minister and the chief and other members of the general staff.[52]
 The Russian Armed Forces is divided into three services: the Russian Ground Forces, the Russian Navy, and the Russian Aerospace Forces. In addition there are two independent arms of service: Strategic Missile Troops, and the Russian Airborne Forces. The Air Defence Troops, the former Soviet Air Defence Forces, have been subordinated into the Air Force since 1998.
 According to the Constitution of Rwanda, The president of Rwanda is the commander-in-chief of Rwanda Defence Forces.
 Article 60 of the Basic Law of Saudi Arabia states: ""The King is the commander-in-chief of all the Military Forces. He appoints officers and puts an end to their duties in accordance with the law.""
 Article 61 further states: ""The King declares a state of emergency, general mobilization and war, and the law defines the rules for this.""
 Lastly, Article 62 states: ""If there is a danger threatening the safety of the Kingdom or its territorial integrity, or the security of its people and its interests, or which impedes the functioning of the state institutions, the King may take urgent measures in order to deal with this danger And if the King considers that these measures should continue, he may then implement the necessary regulations to this end.""
 In accordance with the law, the president of Serbia is the commander-in-chief of Armed Forces and in command of the military. He appoints, promotes and recalls officers of the Army of Serbia.[53]
 In Slovenia, the commander-in-chief is formally the president of Slovenia. In peacetime, the role of commander in chief is usually assumed by the minister of defence.
 Chapter 11, section 202(1) of the Constitution of South Africa states that the president of South Africa is the commander-in-chief of the South African National Defence Force. The constitution places conditions on when and how that power may be employed and requires regular reports to the Parliament of South Africa.[54]
 In accordance with the Constitution of the Republic of Korea, the commander-in-chief and the supreme authority on all military matters is the president of South Korea.
 As with most remaining European monarchies, the position of the Spanish monarch as the nominal head of the armed forces is deeply rooted in tradition.
 
The Spanish Constitution of 1978 authorizes the King in article 62 (h):  The king regularly chairs sessions of the National Security Council, the Joint Chiefs of staff and the individual general staffs of each branch of the Armed Forces in his capacity as supreme commander.
 All promotions to military rank and positions in the high command of the armed forces are made by Royal decree signed by the king and the minister of defense
 However, article 64 require that all official acts of the King must be countersigned, by the President of the Government or other competent minister, for them to become valid. This counter/signature is used to limit a possible abuse of power by any single individual.
 This constitutional provision can and has been made the subject of an exception in crisis situations.
 
In 1981 the king as supreme commander of the armed forces assumed direct command in order to put down a military coup attempt. All members of the government were at that time trapped/held hostage in Parliament and were unable to counter sign the kings orders. This did not however result in those orders being ruled unenforceable or unconstitutional. The coup collapsed after the king ordered all army units to leave the streets and return to their barracks. Furthermore, article 97 stipulates that;  No provision in the constitution requires the king/government to seek approval from the Cortes Generales before sending the armed forces abroad.[55]
 Since 1984, the chief of the defence staff is the professional head of the armed forces and, under the authority of the minister of defence, is responsible for military operations and military organisation.
 As head of state, the president of Sri Lanka, is nominally the commander-in-chief of the armed forces. The National Security Council, chaired by the president is the authority charged with formulating and executing defence policy for the nation. The highest level of military headquarters is the Ministry of Defence, since 1978 except for a few rare occasions the president retained the portfolio defence, thus being the minister of defence. The ministry and the armed forces have been controlled by the during these periods by either a minister of state, deputy minister for defence, and of recently the permanent secretary to the Ministry of Defence. Prior to 1978 the prime minister held the portfolio of minister of defence and external affairs, and was supported by a parliamentary secretary for defence and external affairs.
 Responsibility for the management of the forces is Ministry of Defence, while the planning and execution of combined operations is the responsibility of the Joint Operations Command (JOC). The JOC is headed by the chief of the defence staff who is the most senior officer in the Armed Forces and is an appointment that can be held by an air chief marshal, admiral, or general. The three services have their own respective professional chiefs: the commander of the Army, the commander of the Navy and the commander of the Air Force, who have much autonomy.
 In Suriname, the constitution gives the president ""supreme authority over the armed forces and all of its members"".[56]
 As stipulated in the Constitution of the Republic of China, the president is also the commander-in-chief of the ROC Armed Forces.
 The ""Head of the Thai Armed Forces"" (Thai: จอมทัพไทย; RTGS: Chom Thap Thai) is a position vested in the Thai monarch,[b] who as sovereign and head of state is the commander-in-chief of the Royal Thai Armed Forces.[57]
 The president of Turkey has the constitutional right to represent the Supreme Military Command of the Turkish Armed Forces, on behalf of the Grand National Assembly of Turkey, and to decide on the mobilization of the Turkish Armed Forces, to appoint the chief of the general staff, to call the National Security Council to meet, to preside over the National Security Council, to proclaim martial law or state of emergency, and to issue decrees having the force of law, upon a decision of the Council of Ministers meeting under his/her chairmanship. With all these issues above written in the Constitution of Turkey, the executive rights are given to the president of the Republic of Turkey to be represented as the commander-in-chief of the nation.
 While the commander-in-chief of the Armed Forces of Ukraine is the highest-ranking military officer (i.e. the chief of defence), the president of Ukraine is the constitutional Supreme Commander-in-Chief of the Armed Forces of Ukraine [uk].
 The British monarch is the ""Head of the British Armed Forces""[58] and has also been described as ""Commander-in-Chief of the British Armed Forces"".[59] The prime minister (acting with the support of the Cabinet) makes the key decisions on the use of the armed forces.[60][61] The King, however, remains the ceremonial ""ultimate authority"" of the military, with officers and personnel swearing allegiance only to the monarch.[62]
 The term is also used for the military commander-in-chief of a command (a region of military authority, sometimes combined with the civil office of Governor of a colony (now called a British Overseas Territory)), and for the naval commander-in-chief of a station of the Royal Navy, such as the North America and West Indies Station.
 According to Article II, Section 2, Clause I of the Constitution, the president of the United States is ""Commander in Chief of the Army and Navy of the United States, and of the militia of the several States, when called into the actual Service of the United States.""[63] There have been 45 presidents of the United States (counting Grover Cleveland and Donald Trump once), but there have been 47 commanders-in-chief of the United States due to the fact that Dick Cheney and Kamala Harris each temporarily held the position of acting president under the Twenty-fifth Amendment.[64] (George H. W. Bush was also temporarily acting president but later was elected president.) Since the National Security Act of 1947, the commander-in-chief provision has been understood to mean all United States Armed Forces. U.S. ranks have their roots in British military traditions, with the president possessing ultimate authority, but no rank, maintaining a civilian status.[65]
The exact degree of authority that the Constitution grants to the president as commander-in-chief has been the subject of much debate throughout history, with Congress at various times granting the president wide authority and at others attempting to restrict that authority.[66]
 In U.S. States, the governor also serves as the commander-in-chief of the National Guard, State Militia, and State Defense Forces. In the Commonwealth of Kentucky, for example, KRS 37.180[67] states:
 Similarly, Section 140 of Article 2 of the California Military and Veterans Code states:[68]
 The Uzbek president holds the constitutional position of Supreme Commander of the Armed Forces of Uzbekistan, according to the Constitution of Uzbekistan. In this capacity, the president give decisions on declaring war or martial law, the appointment of senior officials, and the development of the armed forces. In the event of an attack on the republic, the president announce a state of war and will submit within 72 hours a resolution for a plan of action to the Oliy Majlis. When the country is in a wartime situation, the minister of defense will serve in an official capacity as the deputy supreme commander-in-chief of the armed forces, essentially assisting the president in his day-to-day activities and decisions regarding national security.[69]
 According to the Venezuelan constitution, the president is the commander-in-chief of the Armed Forces. The office of the Venezuelan military supreme commander in chief has always been held by the president of Venezuela as per constitutional requirements. However, with a new law sanctioned in 2008, the ""comandante en jefe"" rank is not only a function attributed to the executive branch but a full military rank given to the president upon taking office. Upon assumption he receives a saber, epaulette, shoulder knot, shoulder board and sleeve insignia and full military uniform to be used in military events while performing the duties as president. The shoulder insignia mirrors Cuban practice but is derived from the German-styled officer rank insignia.
 The commander-in-chief of the armed forces is the president of Vietnam, through his post as chairman of National Defense and Security Council. Though this position is nominal and real power is assumed by the Central Military Commission of the Communist Party of Vietnam. The secretary of Central Military Commission  (general secretary of the Communist Party of Vietnam ex officio) is the de facto commander-in-chief.
 The minister of Defence oversees operations of the Ministry of Defence, and the Vietnam People's Army. He also oversees such agencies as the General Staff and the General Logistics Department. However, military policy is ultimately directed by the Central Military Commission of the ruling Communist Party of Vietnam.
 The prime minister of Armenia holds the title of Supreme Commander in Chief of the Armenian Armed Forces (Armenian: Հայաստանի Զինված ուժերի գերագույն հրամանատար). The hereditary title and rank of Sparapet' (Armenian: սպարապետ) was a used to describe the supreme commander of the military forces of ancient and medieval Armenia. Since its introduction in the 2nd century BC, it is often used today to describe famous and high-ranking military officials. Notable Armenians to have held the title include Garegin Nzhdeh, the supreme commander of the Republic of Mountainous Armenia.[70] and Vazgen Sargsyan, the two-time defense minister of Armenia and prime minister in the 1990s.[71]
 Article 93 of the Constitution of the People's Republic of China states the authority to direct the armed forces is invested to the Central Military Commission of the People's Republic of China. The same article also states that the chairman of the Central Military Commission assumes overall responsibility for the work of the Central Military Commission and that it is responsible to the National People's Congress and its Standing Committee.[72] There is also the Central Military Commission of the Chinese Communist Party under the authority of the Party Central Committee. In practice, both commissions have identitical membership, except for a brief period between the Party Congress and the National People's Congress, and are practically the same institutution under the system of ""one institution, two names"".
 Furthermore, Article 80 gives the president of the People's Republic of China (in addition to ceremonial head of state duties) the power to proclaim martial law, proclaim a state of war, and to issue mobilisation orders upon the decision of National People's Congress and its Standing Committee.[72]
 The state president and the CMC chairman are distinctly separate state offices and they have not always been held by the same persons. However, beginning in 1993, during the tenure of Jiang Zemin as General Secretary of the Communist Party and CMC chairman, it has been standard practice to have the offices of the CCP general secretary, president, and the CMC chairman to be normally held by the same person; although the slight differences in the start and end of terms for those respective offices means that there is some overlap between an occupant and his predecessor.
 When Hong Kong was under British authority, the civilian governor was the ex officio commander-in-chief of the British Forces Overseas Hong Kong. After the territory's handover to the People's Republic of China in 1997, the commanders of the People's Liberation Army Hong Kong Garrison are PLA personnel from mainland China and commanded by the CMC.
 The 1995 Constitution designates the prime minister of Ethiopia as ""Commander-in-Chief of the national armed forces"" in Article 74(1).[73]
 Upon the re-militarization of West Germany in 1955, when it joined NATO, the Basic Law for the Federal Republic of Germany was amended in 1956 to include constitutional provisions for the command of the armed forces.
 Placing the command authority over the armed forces directly with the responsible minister in charge of the military establishment breaks with the longstanding German constitutional tradition in both earlier monarchical and republican systems of placing it with the head of state. The rationale was that in a democratic parliamentary system the command authority should directly reside where it would be exercised and where it is subject to the parliamentary control of the Bundestag at all times. By assigning it directly to the responsible minister, instead of with the Federal Chancellor, this also meant that military affairs is but one of the many integrated responsibilities of the government; in stark contrast of earlier times when the separate division of the military establishment from the civil administration allowed the former to act as a state within a state (in contrast to the Federal Republic, the Weimar Republic began with the Ebert–Groener pact, which kept the military establishment as an autonomous force outside the control of politics; the 1925 election of Paul von Hindenburg as Reichspräsident, surrounded by his camarilla and the machinations of Kurt von Schleicher, did little to reverse the trend).[75][76]
 The legislature of the German Democratic Republic (GDR), the Volkskammer, enacted on 13 February 1960 the Law on the Formation of the National Defense Council of the GDR, which established a council consisting of a chairman and at least 12 members. This was later incorporated into the GDR Constitution in April 1968. The National Defense Council held the supreme command of the National People's Army (including the internal security forces), and the council's chairman (usually the General Secretary of the ruling Socialist Unity Party) was considered the GDR's commander-in-chief.
 The GDR joined with the Federal Republic of Germany on 3 October 1990, upon which the GDR's constitution and armed forces were abolished.
 During the Kingdom of Prussia, German Empire, Weimar Republic and the Nazi era, whoever was the head of state—the king of Prussia/German emperor (under the Constitution of the Kingdom of Prussia/Constitution of the German Empire) to 1918, the Reichspräsident (under the Weimar Constitution) to 1934, and the Führer from 1934 to 1945—was the head of the Armed Forces (German: Oberbefehlshaber: literally ""Possessor of highest command"").
 Below the level of the head of state, each military branch (German: Teilstreitkraft) had its own head who reported directly to the head of state and held the highest rank in his service; in the Reichsheer - Generalfeldmarschall, and in the Reichsmarine - Grossadmiral.
 After Chancellor Adolf Hitler assumed power as Führer[77] (after the death of President Paul von Hindenburg), he would later grant his war minister, Generalfeldmarschall Werner von Blomberg, the title of Commander-in-Chief of the Armed Forces in 1935, when conscription was reintroduced. However, in 1938 due to the Blomberg–Fritsch Affair, Hitler withdrew the commander-in-chief title, abolished the war ministerial post and assumed personal command of the Armed Forces. The war ministerial post was de facto overtaken by the Oberkommando der Wehrmacht, which was headed by Generalfeldmarschall Wilhelm Keitel until the German surrender.
 According to Article 45 of the Greek Constitution, the president is the head of the Greek Armed Forces, but their administration is exercised by the government.[78] The prime minister, the minister for national defence and the chief of the general staff are the ones who command the Armed Forces.
 In pre-war Iraq, the commander-in-chief was the head of state, i.e. the president. In the current constitution, the commander-in-chief of the Iraqi Armed Forces is the prime minister, and the president only retains a ceremonial and honorary role of awarding medals and decorations on the recommendation of the commander-in-chief.[79]
 In Israel, the applicable basic law states that the ultimate authority over the Israel Defense Forces rests with the Government of Israel (chaired by the prime minister) as a collective body. The authority of the government is exercised by the minister of defense on behalf of the Government. However, the commander-in-chief of the IDF is the chief of general staff who, despite being subordinate to the minister of defense, holds the highest level of command within the military.[80]
 In Japan, prior to the Meiji Restoration the role of the commander-in-chief was vested in the shōgun (the most militarily powerful samurai daimyō). After the dissolution of the Tokugawa shogunate, the role of the commander-in-chief resided with the Emperor of Japan. The present-day constitutional role of the emperor is that of a ceremonial figurehead (Japanese Constitution calls it symbol) without any military role.
 After Japan's move towards democracy, the position of commander-in-chief of the Japan Self-Defense Forces is held by the prime minister of Japan. Military authority runs from the prime minister to the cabinet-level minister of defense of the Japanese Ministry of Defense.[81][82][83][84]
 The Malta Armed Forces Act does not directly establish the president of Malta as the supreme commander of the Armed Forces. However, Maltese law allows the president to raise by voluntary enlistment and maintain an armed force. Likewise, the law allows the president to issue orders in order to the administrate the armed forces.
 The Armed Forces do not swear allegiance to the president of Malta, but rather to the Republic of Malta. On this basis, there is no direct link between the head of state and the armed forces. For this reason, this link is mediated by the minister responsible for defence.
 Nonetheless, the Presidential Palaces are guarded by the Armed Forces as a symbolic gesture of social cohesion.
 In Myanmar, the Commander-in-Chief of Defence Services (Tatmadaw) is the commanding officer national military, a position vested in a military officer, not the president. The commander-in-chief is, however, a member of the National Defence and Security Council and reports to the president. The commander-in-chief is assisted in his/her role by the Deputy Commander-in-Chief of Defence Services.
 The Constitution of the Netherlands states, in article 97, that ""the Government shall have supreme authority over the armed forces"". Article 42 defines the Government as the Monarch and the ministers, and that only ministers are responsible for acts of government. Article 45 further defines the ministers as constituting the Cabinet, chaired by the prime minister, with ""authority to decide upon overall government policy"".[85][86]
 Before the constitution change in 1983, the equivalent section stated that: ""The King shall have supreme authority over the armed forces"". Nevertheless, the role of the monarch as commander in chief was ceremonial as in most European constitutional monarchies.[86] As a consequence of being only part of the government, monarchs of the Netherlands do not hold a military rank. The current king of the Netherlands Willem-Alexander of the Netherlands resigned from his military ranks (equivalent to one-star general in all branches) upon becoming king in 2013. He has been provided with royal insignia to show his lasting commitment to the armed forces, but these represent no formal rank.
 The minister of defence has the primary ministerial responsibility for the armed forces, which are formally a part of the Ministry of Defence.[86] The chief of defence is the highest ranked professional military officer, and serves as an intermediary between the minister of defence and the Armed Forces, and is responsible to the minister for military-strategic planning, operations and deployment of the Armed Forces.[87]
 Article 47 of the Rules of the Workers' Party of Korea stipulates that the Korean People's Army is ""Revolutionary Armed Forces of the Workers' Party of Korea"" and ""Korean People's Army conducts all military and political activities under the leadership of the Party."" Article 30 invests commanding authority to the Party Central Military Commission, whose ex officio chair is the General Secretary of the Workers' Party of Korea.[88]
 Article 103 of the Constitution of North Korea designates the President of the State Affairs Commission, as the country's head of state and commander-in-chief of the armed forces.[89]
 Currently both offices are occupied by Kim Jong Un. Since 2018, he started issuing orders in the name of the Chairman of the Central Military Commission, instead of the Supreme Commander.
 In Sweden, with the Ordinance of Alsnö in 1280, nobles were exempted from land taxation if they provided cavalrymen to the king's service. Following the Swedish War of Liberation (1521–23) from the Kalmar Union, a Guards Regiment was formed under the King and from there the modern Swedish Army has its roots. During the age of the Swedish Empire, several kings—Gustavus Adolphus the Great, Charles X Gustav, Charles XI & Charles XII—personally led their forces into battle. Under the Instrument of Government of 1809, which was in force until the current Instrument of Government of 1974 went into force on 1 January 1975; the monarch was in §§ 14-15 explicitly designated as the commander-in-chief of the Swedish Armed Forces (Swedish: Högste befälhavare).[90]
 At present, the Government (Swedish: Regeringen) as a collective body, chaired and formed by the prime minister of Sweden, holds the highest Executive Authority, subject to the will of the Riksdag; and is thus the present day closest equivalent of a command-in-chief, although not explicitly designated as such.[91] The reason for this change was, apart from the fact that the king was since 1917 no longer expected to make political decisions without ministerial advice, that the new Instrument of Government was intended to be made as descriptive on the workings of the State as possible, and reflective on how decisions are actually made. Minister of Justice Lennart Geijer further remarked in the government bill that any continued pretensions of royal involvement in government decisions would be of a ""fictitious nature"" and ""highly unsatisfactory"".[92]
 Certain government decisions regarding the Armed Forces (Swedish: Särskilda regeringsbeslut) may be delegated to the minister for defence, under the supervision of the prime minister and to the extent laid down in ordinances.[93]
 To add to some confusion to the above, the title of the agency head of the Swedish Armed Forces and highest ranked commissioned officer on active duty, is actually the supreme commander of the Swedish Armed Forces (Swedish: Överbefälhavaren).[94]
 However, the Monarch (as of present King Carl XVI Gustaf), is still a four-star general and admiral à la suite in the Swedish Army, Navy and Air Force and is by unwritten convention regarded as the foremost head and representative of the Swedish Armed Forces.[95] The king has, as part of his court, a military staff. The military staff is headed by a senior officer (usually a general or admiral, retired from active service) and is composed of active duty military officers serving as aides to the king and his family.[96]
 Supreme authority over the military belongs to the Federal Council, which is the Swiss collegial head of state. Notwithstanding the previous sentence, under the Constitution, the Federal Council can only, in the operational sense, command a maximum of 4,000 soldiers, with a time limit of three weeks of mobilisation.[97] For it to field more service personnel, the Federal Assembly must elect a General[97] who is given four stars.[98] Thus, the General is elected by the Federal Assembly to give him the same democratic legitimacy as the Federal Council.[97]
 In peacetime, the Armed Forces are led by the Chief of the Armed Forces (Chef der Armee), who reports to the head of the Federal Department of Defence, Civil Protection and Sports and to the Federal Council as a whole. The Chief of the Armed Forces has the rank of Korpskommandant or Commandant de corps (OF-8 in NATO equivalence).
 In a time of declared war or national emergency, however, the Federal Assembly, assembled as the United Federal Assembly, specifically for the purpose of taking on the war-time responsibilities elect a General as commander-in-chief of the Armed Forces under Article 168 of the Constitution. Whilst the General acts as the highest military authority with a high degree of autonomy, he is still subordinate to the Federal Council (See Articles 58, 60, 174, 177, 180 & 185).[99] The Federal Assembly retains the sole power to dismiss the General, but the General remains subordinate to the Federal Council by the council's ability to demobilise, thereby making the position of General redundant.[97]
 Four generals were appointed in Swiss history, General Henri Dufour during the Swiss Civil War, General Hans Herzog during the Franco-Prussian War, General Ulrich Wille during the First World War, and General Henri Guisan during the Second World War (""la Mob"", ""the Mobilisation""). Although Switzerland remained neutral during the latter three conflicts, the threat of having its territory used as a battlefield by the much bigger war parties of Germany and France required mobilization of the army.
 Within NATO and the European Union, the term Chief of Defence (CHOD) is usually used as a generic term for the highest-ranked office held by a professional military officer on active duty, irrespective of their actual title or powers.[100]
"
Artemas Ward,https://en.wikipedia.org/wiki/Artemas_Ward,"
 Artemas Ward (November 26, 1727 – October 28, 1800) was an American major general in the American Revolutionary War and a Congressman from Massachusetts. He was considered an effective political leader, President John Adams describing him as ""universally esteemed, beloved, and confided in by his army and his country"".[1]
 Artemas Ward was born at Shrewsbury in the Province of Massachusetts Bay in 1727 to Nahum Ward (1684–1754) and Martha (Howe) Ward.[2]  He was the sixth of seven children.  His father had broad and successful career interests as a sea captain, merchant, land developer, farmer, lawyer and jurist.  As a child he attended the common schools and shared a tutor with his brothers and sisters.  He graduated from Harvard in 1748 and taught there briefly.[3]
 On July 31, 1750, he married Sarah Trowbridge (December 3, 1724 – December 13, 1788), the daughter of Reverend Caleb Trowbridge and Hannah Trowbridge of Groton.[4] The young couple returned to Shrewsbury where Artemas opened a general store.[5] In the next fifteen years they would have eight children: Ithamar in 1752, Nahum (1754), Sara (1756), Thomas (1758), Artemas Jr. (1762), Henry Dana (1768), Martha (1760), and Maria (1764).[6]
 In 1751, at age 23 or 24, he was named a township assessor for Worcester County, the first of many public offices he was to fill.[7] In 1752 he was elected a justice of the peace and to the first of many terms in the Massachusetts provincial assembly, or ""general court.""[5]
 Between 1755 and 1757, Ward was called to active duty at intervals that alternated with his attendance at the General Court. In 1755 the Massachusetts militia was restructured for the war; Ward was made a major in the 3rd Regiment which drew its company mainly from Worcester County.[8] The 3rd primarily served as a garrison force along the frontier in western Massachusetts. In 1757 he was promoted to  regimental colonel of the 3rd Regiment of the militias of Middlesex and Worcester counties.[9] In 1758 the regiment marched with Abercrombie's force to sortie on Fort Ticonderoga,[10] but Ward was sidelined during the campaign by an ""attack of the stone.""
 By 1762, Ward returned to Shrewsbury permanently and was named to the Court of Common Pleas.[11] In the General Court (the provincial assembly) he,  with Samuel Adams and John Hancock, was appointed to the taxation committee. On the floor, he was second only to James Otis in speaking out against the acts of parliament in London. His prominence in these debates prompted the Royal Governor Francis Bernard to revoke his military commission in 1767.[12] At the next election in 1768, Bernard voided the election results for Worcester and banned Ward from the assembly, but this didn't silence him.[13]
 In the growing sentiment favoring rebellion, the 3rd Regiment resigned en masse from British service on October 3, 1774. They then marched on Shrewsbury to inform Ward that they had unanimously elected him their leader. Later that month the governor abolished the assembly. The towns of Massachusetts responded by setting up a colony-wide Committee of Safety. One of the first actions of the committee was to name Ward as general and commander-in-chief of the colony's militia.[14]
 Following the Battle of Lexington and Concord on April 19, 1775, the rebel (colonial) forces followed the British troops back to Boston and deployed to start the Siege of Boston, cutting all land access to the city.  At first Ward directed his forces from his sickbed (in Shrewsbury), later moving his headquarters to Cambridge. Soon, both the New Hampshire and Connecticut provisional governments named him commander of their forces participating in the siege.  Most of his efforts during this time were devoted to organization and supply problems.
 Additional British forces arrived by sea in May, and in June, Ward learned of their plan to attack Bunker Hill.  He gave orders to fortify the point, setting the stage for the Battle of Bunker Hill on June 17, 1775. Command during the battle devolved upon General Israel Putnam and Colonel William Prescott.[15]
 Meanwhile, the Continental Congress was creating the Continental Army. On June 17, Congress commissioned Ward a major general, and appointed him second in command to General George Washington.[16] (Ward was one of the original four major generals in the Continental Army along with Charles Lee, Philip Schuyler and Israel Putnam.)[17] Over the next nine months he helped convert the assembled militia units into the Continental Army.
 After the British evacuation of Boston on March 17, 1776, Washington led the main body of the army to New York City. Ward took command of the Eastern Department and held that post until March 1777, when ill health forced his resignation from the army.[18][19]
 Even during his military service, Ward also served as a state court justice in 1776 and 1777. From 1777 to 1779, as President of the state's Executive Council, he effectively served as governor before the ratification of the Massachusetts Constitution in 1780. He was continuously elected to the Massachusetts House of Representatives from 1779 through 1785, leading it as Speaker in 1785.[20]
 Ward was appointed a delegate to the Continental Congress in 1780 and 1781,[21] and from 1791 to 1795 was elected twice to the United States House of Representatives[22] after being an unsuccessful candidate in 1788[23]. He was one of nine representatives to vote against final passage of the Eleventh Amendment to the United States Constitution.[24]
 Ward died at his home in Shrewsbury on October 28, 1800, and was buried with Sarah in the town's Mountain View Cemetery.[25] (His great-grandson Artemas Ward wrote The Grocer's Encyclopedia, published in 1911.)[26]
 Ward, Massachusetts was incorporated in 1778 in honor of Artemas Ward. In 1837, the town was renamed to Auburn, Massachusetts after complaints from the U.S. postal service that the name Ward was too similar to the nearby town of Ware.[27]
 Wards's lifelong home had been built by his father, Nahum, about the time Artemas was born. The home is now known as the Artemas Ward House and is a museum preserved by Harvard University. Located at 786 Main Street in Shrewsbury, Massachusetts it is open to the public for limited hours during the summer months.[28]
 Ward Circle is a traffic circle at the intersection of Nebraska and Massachusetts Avenues in Northwest Washington, D.C. The land on three sides of Ward Circle is owned by American University. The circle contains a statue of Ward.[29]
 The great-grandson of Ward gave over four million dollars to Harvard University on the condition that they erect a statue in honor of Ward, and maintain his home in Shrewsbury.[30] Harvard's initial offer in 1927 of $50,000 toward the statue was enough for a statue, but inadequate to provide the general with a horse.[31]
 The statue was unveiled on November 3, 1938[32] by Maj. Gen. Ward's great-great-great-granddaughter, Mrs. Lewis Wesley Feick.[33] Although there are no crosswalks for pedestrian access to the circle, the base of the statue bears this inscription:[34]
 American University named the Ward Circle Building, home of the American University School of Public Affairs, in honor of Artemas Ward.[35][36]
  Media related to Artemas Ward at Wikimedia Commons
"
Horatio Gates,https://en.wikipedia.org/wiki/Horatio_Gates,"
 Horatio Lloyd Gates (July 26, 1727 – April 10, 1806) was a British-born American army officer who served as a general in the Continental Army during the early years of the Revolutionary War. He took credit for the American victory in the Battles of Saratoga (1777) – a matter of contemporary and historical controversy – and was blamed for the defeat at the Battle of Camden in 1780. Gates has been described as ""one of the Revolution's most controversial military figures"" because of his role in the Conway Cabal, which attempted to discredit and replace General George Washington; the battle at Saratoga; and his actions during and after his defeat at Camden.[1][2]
 Born in the town of Maldon in Essex, Gates served in the British Army during the War of the Austrian Succession and the French and Indian War. Frustrated by his inability to advance in the army, Gates sold his commission and established a small plantation in Virginia. On Washington's recommendation, the Continental Congress made Gates the Adjutant General of the Continental Army in 1775. He was assigned command of Fort Ticonderoga in 1776 and command of the Northern Department in 1777. Shortly after Gates took charge of the Northern Department, the Continental Army defeated the British at the crucial Battles of Saratoga. After the battles, some members of Congress considered replacing Washington with Gates, but Washington ultimately retained his position as commander-in-chief of the Continental Army.
 Gates took command of the Southern Department in 1780, but was removed from command later that year after the disastrous Battle of Camden. Gates's military reputation was destroyed by the battle and he did not hold another command for the remainder of the war. Gates retired to his Virginian estate after the war, but eventually decided to free his slaves and move to New York. He was elected to a single term in the New York State Legislature and died in 1806.
 Horatio Gates was born on July 26, 1727, in Maldon, in the English county of Essex. His parents (of record) were Robert and Dorothea Gates. Evidence suggests that Dorothea was the granddaughter of John Hubbock Sr. (died 1692) postmaster at Fulham, and the daughter of John Hubbock Jr., listed in 1687 sources as a vintner. She had a prior marriage, to Thomas Reeve, whose family was well situated in the royal Customs service. Dorothea Reeve was housekeeper for the second Duke of Leeds, Peregrine Osborne (died  June 25, 1729), which in the social context of England at the time was a patronage plum. Marriage into the Reeve family opened the way for Robert Gates to get into and then up through the Customs service. So too, Dorothea Gates's appointment circa 1729 to housekeeper for the third Duke of Bolton provided Horatio Gates with otherwise off-bounds opportunities for education and social advancement. Through Dorothea Gates's associations and energetic networking, young Horace Walpole was enlisted as Horatio's godfather and namesake.[1]
 In 1745, Horatio Gates obtained a military commission with financial help from his parents, and political support from the Duke of Bolton. Gates served with the 20th Foot in Germany during the War of the Austrian Succession. He arrived in Halifax, Nova Scotia, under Edward Cornwallis and later was promoted to captain in the 45th Foot, under the command of Hugh Warburton, the following year.[3] He participated in several engagements against the Mi'kmaq and Acadians, particularly the Battle at Chignecto.  He married the daughter of Erasmus James Philipps, Elizabeth, at St. Paul's Church (Halifax) in 1754. Leaving Nova Scotia, he sold his commission in 1754 and purchased a captaincy in one of the New York Independent Companies.  One of his mentors in his early years was Edward Cornwallis, the uncle of Charles Cornwallis, against whom the Americans would later fight.  Gates served under Cornwallis when the latter was governor of Nova Scotia, and also developed a friendship with the lieutenant governor, Robert Monckton.[4]
 During the French and Indian War, Gates served General Edward Braddock in America.  In 1755 he accompanied the ill-fated Braddock Expedition in its attempt to control access to the Ohio Valley. This force included other future Revolutionary War leaders such as Thomas Gage, Charles Lee, Daniel Morgan, and George Washington. Gates didn't see significant combat, since he was severely injured early in the action. His experience in the early years of the war was limited to commanding small companies, but he apparently became quite good at military administration. In 1759 he was made brigade major to Brigadier General John Stanwix, a position he continued when General Robert Monckton took over Stanwix's command in 1760.[5] Gates served under Monckton in the capture of Martinique in 1762, although he saw little combat. Monckton bestowed on him the honor of bringing news of the success to England, which brought him a promotion to major. The end of the war also brought an end to Gates' prospects for advancement, as the army was demobilized and he did not have the financial wherewithal to purchase commissions for higher ranks.[5]
 In November 1755, Gates married Elizabeth Phillips and had a son, Robert, in 1758. Gates' military career stalled, as advancement in the British army required money or influence.  Frustrated by the British class hierarchy, he sold his major's commission in 1769, and came to North America. In 1772 he reestablished contact with George Washington, and purchased a modest plantation in Virginia the following year.
 When the word reached Gates of the outbreak of war in late May 1775, he rushed to Mount Vernon and offered his services to Washington. In June, the Continental Congress began organizing the Continental Army. In accepting command, Washington urged the appointment of Gates as adjutant of the army. On June 17, 1775, Congress commissioned Gates as a brigadier general and adjutant general of the Continental Army. He is considered to be the first Adjutant General of the United States Army.[6]
 Gates's previous wartime service in administrative posts was invaluable to the fledgling army, as he, Washington and Charles Lee were the only men with significant experience in the British regular army. As adjutant, Horatio Gates created the army's system of records and orders and helped standardize regiments from the various colonies. During the siege of Boston, he was a voice of caution, speaking in war councils against what he saw as overly risky actions.[7]
 Although his administrative skills were valuable, Gates longed for a field command. By June 1776, he had been promoted to major general and given command of the Canadian Department to replace John Sullivan.  This unit of the army was then in disorganized retreat from Quebec, following the arrival of British reinforcements at Quebec City.  Furthermore, disease, especially smallpox, had taken a significant toll on the ranks, which also suffered from poor morale and dissension over pay and conditions.  The retreat from Quebec to Fort Ticonderoga also brought Gates into conflict with the authority of Major General Philip Schuyler, commander of the army's Northern Department, which retained jurisdiction over Ticonderoga. During the summer of 1776, this struggle was resolved, with Schuyler given command of the department as a whole and Gates command of Ticonderoga and the defense of Lake Champlain.[8]
 Gates spent the summer of 1776 overseeing the enlargement of the American fleet that would be needed to prevent the British from taking control of Lake Champlain.  Much of this work eventually fell to Benedict Arnold, who had been with the army during its retreat and was also an experienced seaman.  Gates rewarded Arnold's initiative by giving him command of the fleet when it sailed to meet the British.  The American fleet was defeated at the Battle of Valcour Island in October 1776, although the defense of the lake was sufficient to delay a British advance against Ticonderoga until 1777.[9]
 When it was clear that the British were not going to make an attempt on Ticonderoga in 1776, Gates marched some of the army south to join Washington's army in Pennsylvania, where it had retreated after the fall of New York City.  Though his troops were with Washington at the Battle of Trenton, Gates was not.  Always an advocate of defensive action, Gates argued that Washington should retreat further rather than attack.  When Washington dismissed this advice, Gates claimed illness as an excuse not to join the nighttime attack and instead traveled on to Baltimore, where the Continental Congress was meeting.  Gates had always maintained that he, not Washington, should have commanded the Continental Army. This opinion was supported by several wealthy and prominent New England delegates to the Continental Congress.  Although Gates actively lobbied Congress for the appointment, Washington's stunning successes at Trenton and Princeton subsequently left no doubt as to who should be commander-in-chief. Gates was then sent back north with orders to assist Schuyler in the Northern Department.
 But in 1777, Congress blamed Schuyler and St. Clair for the loss of Fort Ticonderoga, though Gates had exercised a lengthy command in the region. Congress finally gave Gates command of the Northern Department on August 4.
 Gates assumed command of the Northern Department on August 19 and led the army during the defeat of British General Burgoyne's invasion in the Battles of Saratoga. While Gates and his supporters took credit for the victory, military action was directed by a cohort of field commanders led by Benedict Arnold, Enoch Poor, Benjamin Lincoln, and Daniel Morgan.  Arnold in particular took the field against Gates' orders and rallied the troops in a furious attack on the British lines, suffering serious injuries to his leg.  John Stark's defeat of a sizable British raiding force at the Battle of Bennington–Stark's forces killed or captured over 900 British soldiers–was also a substantial factor in the outcome at Saratoga.
 Gates stands front and center in John Trumbull's painting of the Surrender of General Burgoyne at Saratoga,[10][11] which hangs in the U.S. Capitol Rotunda. By Congressional resolution, a gold medal was presented to Gates to commemorate his victories over the British in the Battles of Bennington, Fort Stanwix and Saratoga. Gold and bronze replicas of that medal are still awarded by the Adjutant General's Corps Regimental Association in recognition of outstanding service.[12]
 Gates followed up the victory at Saratoga with a proposal to invade Quebec, but his suggestion was rejected by Washington.[13]
 Gates attempted to maximize his political return on the victory, particularly as George Washington was having no present successes with the main army.  In fact, Gates insulted Washington by sending reports directly to Congress instead of to Washington, his commanding officer.  At the behest of Gates's friends and the delegates from New England, Congress named Gates to president of the Board of War, a post he filled while retaining his field command—an unprecedented conflict of interest. The post technically made Gates Washington's civilian superior, conflicting with his lower military rank. At this time, some members of Congress briefly considered replacing Washington with Gates as commander-in-chief, supported by military officers also in disagreement with Washington's leadership.
 Washington learned of the campaign against him by Gates's adjutant, James Wilkinson. Following a drunken party, Wilkinson repeated the remarks of General Thomas Conway to Gates, which were critical of Washington, to aides of General William Alexander, who passed them on to Washington.[14] Gates (then unaware of Wilkinson's involvement) accused persons unknown of copying his mail and forwarded Conway's letter to the president of Congress, Henry Laurens.  Washington's supporters in Congress and the army rallied to his side, ending the ""Conway Cabal"".[15][16] Gates then apologized to Washington for his role in the affair, resigned from the Board of War, and took an assignment as commander of the Eastern Department in November 1778.
 In May 1780, news of the fall of Charleston, South Carolina, and the capture of General Benjamin Lincoln's southern army reached Congress.[17] It voted to place Gates in command of the Southern Department.[18] He learned of his new command at his home near Shepherdstown, Virginia (now West Virginia), and headed south to assume command of the remaining Continental forces near the Deep River in North Carolina on July 25, 1780.[19]
 Gates led Continental forces and militia south and prepared to face the British forces of Charles Cornwallis, who had advanced to Camden, South Carolina. In the Battle of Camden on August 16, Gates's army was routed, with nearly 1,000 men captured, along with the army's baggage train and artillery. Analysis of the debacle suggests that Gates greatly overestimated the capabilities of his inexperienced militia, an error magnified when he lined those forces against the British right, traditional position of the strongest troops. He also failed to make proper arrangements for an organized retreat. Gates's principal accomplishment in the unsuccessful campaign was to cover 170 miles (270 km) in three days on horseback, heading north in retreat. His disappointment was compounded by news of his son Robert's death in combat in October. Nathanael Greene replaced Gates as commander on December 3, and Gates returned home to Virginia. Gates's devastating defeat at Camden not only ruined his new American army, but it also ruined his military reputation.
 Because of the debacle at Camden, Congress passed a resolution calling for a board of inquiry, the prelude to a court-martial, to look into Gates's conduct. Always one to support a court-martial of other officers, particularly those with whom he was in competition for advancement, such as Benedict Arnold, Gates vehemently opposed the inquiry into his own conduct. Although he never was again placed in field command, Gates's New England supporters in Congress came to his aid in 1782, repealing the call for an inquiry. Gates then rejoined Washington's staff at Newburgh, New York. Rumors implicated some of his aides in the Newburgh Conspiracy of 1783. Gates may have agreed to involve himself, though this remains unclear.[20]
 Gates' wife Elizabeth died in the summer of 1783. He retired in 1784 and again returned to his estate, Traveller's Rest, in Virginia (near present-day Kearneysville, Jefferson County, West Virginia). Gates served as vice president of the Society of the Cincinnati, the organization of former Continental Army officers, and president of its Virginia chapter, and worked to rebuild his life. He proposed marriage to Janet Montgomery, the widow of General Richard Montgomery, but she refused.[21]
 In 1786, Gates married Mary Valens, a wealthy Liverpudlian who had come to the colonies in 1773 with her sister and Rev. Bartholomew Booth, who ran a boys' boarding school in Maryland.[22]  Booth had been the curate for the ""Chapel in the Woods,"" later to become Saint John's Church at Hagerstown, Maryland. Many have suggested that Gates freed his slaves at the urging of his friend John Adams along with the sale of Traveller's Rest in 1790.[23]  This narrative was popularized in 1837 by the Anti-Slavery Record, an abolitionist publication.[24] The paper produced an account of the event in which Gates supposedly “summoned his numerous family and slaves about him, and amidst their tears of affection and gratitude, gave them their freedom.”[24] In fact, the terms of the deed of sale for Traveller's Rest indicate that Gates sold his slaves for £800 together with the plantation.[25]  The deed did not immediately free any of Gates's slaves, rather it stipulated that five would be free after five years; the remaining eleven would have to wait until they reached the age of twenty-eight.[25]  Nevertheless, even this limited gesture toward emancipation surpassed the other major generals of the revolutionary era; for none but Gates made any efforts to emancipate their slaves during their lifetimes.[25]
 The couple thereupon moved to an estate at Rose Hill in present-day midtown Manhattan, where the local authorities received him warmly.[26] His later support for Jefferson's presidential candidacy ended his friendship with Adams. Gates and his wife remained active in New York City society, and he was elected to a single term in the New York State Legislature in 1800.[27] He died in his Rose Hill home on April 10, 1806, and was buried in the Trinity Church graveyard at the foot of Wall Street, though the exact location of his grave is unknown.[28]
 
"
Charles Lee (general),https://en.wikipedia.org/wiki/Charles_Lee_(general),"
 Charles Lee (6 February 1732 [O.S. 26 January 1731] – 2 October 1782) was a British-born American military officer who served as a general of the Continental Army during the American Revolutionary War. He also served earlier in the British Army during the Seven Years War. He sold his commission after the Seven Years War and served for a time in the Polish army of King Stanislaus II Augustus.
 Lee moved to North America in 1773 and bought an estate in western Virginia. When the fighting broke out in the American Revolutionary War in 1775, he volunteered to serve with rebel forces. Lee's ambitions to become Commander in Chief of the Continental Army were thwarted by the appointment of George Washington to that post.
 In 1776, forces under his command repulsed a British attempt to capture Charleston, which boosted his standing with the army and Congress. Later that year, he was captured by British cavalry under Banastre Tarleton; he was held by the British as a prisoner until exchanged in 1778. During the Battle of Monmouth later that year, Lee led an assault on the British that miscarried. He was subsequently court-martialed and his military service brought to an end. He died in Philadelphia in 1782.
 Lee was born on 6 February 1732 [O.S. 26 January 1731][1][2] in Darnhall, Cheshire, England, Great Britain, the son of Major General John Lee [a][3] and his wife Isabella Bunbury (daughter of Sir Henry Bunbury, 3rd Baronet).[1][3][4][5] His mother's family were landed gentry with national stature—his maternal grandfather had been an MP for Cheshire, and a cousin, Sir Thomas Charles Bunbury, was a MP for Suffolk. Five of Lee's six older siblings had died – only his sister Sidney Lee, four years older, survived to adulthood. Sidney never married.[6]
 Like his mother, with whom he did not get along well, Lee would have a temperamental personality and poor physical health (suffering rheumatism and chronic attacks of gout), which caused him to travel often to medicinal spas.[7] He received a private education from tutors, then was sent to a grammar school near Chester and a private academy in Switzerland before being sent to King Edward VI School, Bury St Edmunds, a free grammar school near the home of his uncle, Rev. William Bunbury.[8] Lee became proficient in several languages, including Latin, Greek, and French.[1][2][3][4] His father was colonel of the 55th Foot (later renumbered the 44th) when he purchased a commission on 9 April 1747, for Charles as an ensign in the same regiment.[1][3]
 Despite inheriting money upon his mother's death, Lee became known for a peripatetic and extravagant lifestyle, which led to financial difficulties several times in his life, including after liquidating land grants in East Florida and St. John's Island in the Gulf of St. Laurence in the late 1760s (which he received because of his service in the French and Indian War).[9] By 1770, Lee had acquired the services of Giuseppe Minghini, who would remain his servant until the end of his life and received a bequest.[10] Lee owned at least six slaves shortly before his death,[11] and his will divided ownership of all his slaves (three mentioned by name) between Minghini and Elizabeth Dunne, Lee's housekeeper.[12] After paying his debts and a number of specific bequests, some involving horses and others money (usually to purchase mourning rings), Lee directed his executors (future congressman Alexander White and former Rev. Charles Mynn Thurston),  to pay the remainder of his estate (worth about $700 according to the filed inventory) to his sister Sidney.[13][14]
 After completing his schooling, Lee reported for duty with his regiment in Ireland.[1] Shortly after his father's death, on 2 May 1751, he received[4] (or purchased[1]) a lieutenant's commission in the 44th. He was sent with the regiment to North America in 1754 for service in the French and Indian War[1] under Major General Edward Braddock, in what was a front for the Seven Years War between Britain and France. He was with Braddock at his defeat at the Battle of the Monongahela in 1755.[1][3][4] During this time in America, Lee married the daughter of a Mohawk 
chief.[1][2][15] His wife (name unknown) gave birth to twins.[1][2] Lee was known to the Mohawk, who were allies of the English, as Ounewaterika or ""Boiling Water"".[1][2][3][4][15]
 On 11 June 1756, Lee purchased a Captain's commission in the 44th[1] for the sum of £900.[3][4] The following year he took part in an expedition against the French fortress of Louisbourg, and on 1 July 1758, he was wounded in a failed assault on Fort Ticonderoga.[1][3][4] He was sent to Long Island to recuperate. A surgeon whom he had earlier rebuked and thrashed attacked him.[1][3][4] After recovering, Lee took part in the capture of Fort Niagara in 1759[1][3][4] and Montreal in 1760.[1][3][4] This brought the war in the North American theater to an end by completing the Conquest of Canada.[3][4]
 Lee went back to Europe, transferred to the 103rd Foot as a major,[1][3][4] and served as a lieutenant colonel in the Portuguese army. He fought against the Spanish during their unsuccessful invasion of the country, and distinguished himself under John Burgoyne at the Battle of Vila Velha.[1][3][4]
 Lee returned to England in 1763 following the Peace of Paris, which ended the Seven Years' War.[3][4] His regiment was disbanded and he was retired on half pay as a major.[1][3][4] On 26 May 1772, although still inactive, he was promoted to lieutenant colonel.[16][1][3][4]
 In 1765, Lee served as an aide-de-camp under Stanislaus II, King of Poland.[1][3][4] After many adventures he came home to England.[3][4] Unable to secure promotion in the British Army, in 1769 he returned to Poland and then saw action in the Russo-Turkish War. In a duel in Italy, he lost two fingers, but in a second duel with the same Italian officer, he killed his opponent.[1][3][4]
 Returning to England again, he found that he was sympathetic to the North American colonists in their quarrel with Great Britain.[1][3][4] He moved to the colonies in 1773 and in 1775 purchased an estate worth £3,000 in Berkeley County, near the home of his friend Horatio Gates, with whom he had served in the French and Indian War and who had moved back to the colony in 1772.[17] This area is now part of West Virginia.[1][3][4] He spent ten months travelling through the colonies and acquainting himself with patriots.[1][3][4]
 Although Lee was generally acknowledged at the Second Continental Congress to be the most capable candidate for the command of the Continental Army, the role was given to George Washington. Lee recognized the sense of giving the position to a native-born North American, but expected to be given the role of second-in-command. He was disappointed when that role went to Artemas Ward, whom Lee considered too inexperienced for the job. Lee was appointed major-general and third in line, but succeeded to second-in-command in 1776 when Ward resigned due to ill health.[18]
 Lee also received various other titles: in 1776, he was named commander of the so-called Canadian Department, although he never got to serve in this capacity.[1][3][4] He was appointed as the first commander of the Southern Department.[1][3][4] He served in this post for six months, until he was recalled to the main army. During his time in the South, the British sent an expedition under Henry Clinton to recover Charleston, South Carolina.[3][4] Lee oversaw the fortification of the city.[1] Fort Sullivan was a fortification built out of palmetto logs, later named for commander Col. William Moultrie.[3][4] Lee ordered the army to evacuate the fort because as he said it would only last thirty minutes and all soldiers would be killed.[19] Governor John Rutledge forbade Moultrie to evacuate and the fort held.[3][4] The spongy palmetto logs repelled the cannonball from the British ships.[20] The assault on Sullivan's Island was driven off, and Clinton abandoned his attempts to capture the city. Lee was acclaimed as the ""hero of Charleston"", although according to some American accounts the credit for the defense was not his.[3][4]
 The British capture of Fort Washington and its near 3,000-strong garrison on 16 November 1776, prompted Lee's first overt criticism of Washington. Believing the commander-in-chief's hesitation to evacuate the fort to be responsible for the loss, Lee wrote to Joseph Reed lamenting Washington's indecision, a criticism Washington read when he opened the letter believing it to be official business.[21] As Washington retreated across New Jersey after the defeat at New York, he urged Lee, whose troops were north of New York, to join him. Although Lee's orders were at first discretionary, and although there were good tactical reasons for delaying, his slow progress has been characterized as insubordinate. On 12 December 1776, Lee was captured by British troops at White's Tavern in Basking Ridge, New Jersey, while writing a letter to General Horatio Gates complaining about Washington's deficiency.[22][23]
 Lee was released on parole as part of a prisoner exchange in early April 1778 and, while on his way to York, Pennsylvania, was greeted enthusiastically by Washington at Valley Forge. Lee was ignorant of the changes that had occurred during his sixteen-month captivity; he was not aware of what Washington believed to be a conspiracy to install Gates as commander-in-chief or of the reformation of the Continental Army under the tutelage of Baron von Steuben.[24] According to Elias Boudinot, the commissary who had negotiated the prisoner exchange, Lee claimed that ""he found the Army in a worse situation than he expected and that General Washington was not fit to command a sergeant's guard."" While in York, Lee lobbied Congress for promotion to lieutenant general, and went above Washington's head to submit to it a plan for reorganizing the army in a way that was markedly different from that which Washington had worked long to implement.[25]
 Lee's suggestion was for a militia army that avoided competing with a professional enemy in a pitched battle and relied instead on a defensive strategy which would wear down an opposing army with harassing, small-unit actions.[26] After completing his parole, Lee returned to duty with the Continental Army as Washington's second-in-command on 21 May.[27] In June, as the British evacuated Philadelphia and marched through New Jersey en route to New York, Washington twice convened war councils to discuss the best course of action. In both, his generals largely agreed that Washington should avoid a major battle, Lee arguing that such a battle would be criminal, though a minority favored a limited engagement. At the second council, Lee argued the Continental Army was no match for the British Army, and favored allowing the British to proceed unimpeded and waiting until French military intervention following the Franco-American alliance could shift the balance in favor of the Americans.[28]
 Washington agreed with the minority of his generals who favored an aggressive but limited action. He allocated some 4,500 troops, approximately a third of his army, to a vanguard that could land a heavy blow on the British without risking his army in a general engagement. The main body would follow and provide support if circumstances warranted.[29] He offered Lee command of the vanguard, but Lee turned the job down on the basis that the force was too small for a man of his rank and position.[30][31] Washington gave the position to Major General the Marquis de Lafayette. In his haste to catch the British, Lafayette pushed the vanguard to exhaustion and outran his supplies, prompting Washington to send Lee, who had in the meantime changed his mind, to replace him.[32]
 Lee took over on 27 June at Englishtown.[33] The British were at Monmouth Courthouse (modern-day Freehold), six miles (ten kilometres) from Englishtown. Washington was with the main body of just over 7,800 troops and the bulk of the artillery at Manalapan Bridge, four miles (six kilometres) behind Lee.[34] Believing action to be imminent, Washington conferred with the vanguard's senior officers at Englishtown that afternoon but did not offer a battle plan. Lee believed he had full discretion on whether and how to attack and called his own war council after Washington left. He intended to advance as soon as he knew the British were on the move, in the hope of catching their rearguard when it was most vulnerable. In the absence of any intelligence about British intentions or the terrain, Lee believed it would be useless to form a precise plan of his own.[35]
 When news arrived at 05:00 on 28 June that the British were moving, Lee led the vanguard towards Monmouth Court House, where he discovered the British rearguard, which he estimated at 2,000 troops. He ordered Brigadier General Anthony Wayne with some 550 men to fix the rearguard in place while he led the remainder of the vanguard on a left hook with the intention of outflanking the British, but he neglected to inform his subordinates, Brigadier General Charles Scott and Brigadier General William Maxwell, of his plan. Lee's confidence crept into reports back to Washington that implied ""the certainty of success.""[36]
 As soon as the British commander, General Sir Henry Clinton, received news that his rearguard was being probed, he ordered his main combat division to march back towards Monmouth Court House.[37] Lee became concerned that his right flank would be vulnerable and moved with Lafayette's detachment to secure it.[38] To his left, Scott and Maxwell were not in communication with Lee and not privy to his plan. They became concerned that the arriving British troops would isolate them, and decided to withdraw. To their left, Wayne's isolated troops, having witnessed the British marching back, were also withdrawing.[39][40] Lee witnessed one of Lafayette's units pulling back after a failed attempt to silence some British artillery around the same time as one of his staff officers returned with the news that Scott had withdrawn. With his troops withdrawing without orders, it became clear to Lee that he was losing control of the vanguard, and with his immediate command now only 2,500 strong, he realized his plan to envelop the British rearguard was finished. His priority became the safety of his troops in the face of superior numbers, and he ordered a general retreat.[41]
 Lee had significant difficulties communicating with his subordinates and could exercise only limited command and control of the vanguard, but at unit level, the retreat was generally conducted with a discipline that did credit to Steuben's training, and the Americans suffered few casualties. Lee believed he had conducted a model ""retrograde manoeuver in the face and under fire of an enemy"" and claimed his troops moved with ""order and precision."" He had remained calm during the retreat but began to unravel at Ker's house. When two of General Washington's aides informed Lee that the main body was still some two miles (three kilometres) away and asked him what to report back, Lee replied ""that he really did not know what to say.""[42] Crucially, he failed to keep Washington informed of the retreat.[43]
 Without any recent news from Lee, Washington had no reason to be concerned as he approached the battlefield with the main body shortly after midday. In the space of some ten minutes, his confidence gave way to alarm as he encountered a straggler bearing the first news of Lee's retreat and then whole units in retreat. None of the officers Washington met could tell him where they were supposed to be going or what they were supposed to be doing. As the commander-in-chief rode on ahead, he saw the vanguard in full retreat but no sign of the British. At around 12:45, Washington found Lee marshalling the last of his command across the middle morass, marshy ground southeast of a bridge over the Spotswood Middle Brook.[44]
 Expecting praise for a retreat he believed had been generally conducted in good order, Lee was uncharacteristically lost for words when Washington asked without pleasantries, ""I desire to know, sir, what is the reason – whence arises this disorder and confusion?""[45] When he regained his composure, Lee attempted to explain his actions. He blamed faulty intelligence and his officers, especially Scott, for pulling back without orders, leaving him no choice but to retreat in the face of a superior force, and reminded Washington that he had opposed the attack in the first place.[45][46] Washington was not convinced; ""All this may be very true, sir,"" he replied, ""but you ought not to have undertaken it unless you intended to go through with it.""[45] Washington made it clear he was disappointed with Lee and rode off to organize the battle he felt his subordinate should have given. Lee followed at a distance, bewildered and believing he had been relieved of command.[47][b]
 With the main body still arriving and the British no more than one-half mile (one kilometre) away, Washington began to rally the vanguard to set up the very defenses Lee had been attempting to organize. He then offered Lee a choice: remain and command the rearguard, or fall back across the bridge and organize the main defenses on Perrine's Hill. Lee opted for the former while Washington departed to take care of the latter.[52][49] Lee fought the counter-attacking British in a rearguard action that lasted no more than thirty minutes, enough time for Washington to complete the deployment of the main body, and at 13:30, he was one of the last American officers to withdraw across the bridge.[53] When Lee reached Perrine's Hill, Washington sent him with part of the former vanguard to form a reserve at Englishtown. At 15:00, Steuben arrived at Englishtown and relieved Lee of command.[54]
 General Lee regarded John Skey Eustace as his adopted son and declared him as his heir,[55][56][57] but the handsome Eustace decided to desert the unpredictable Lee.[58][56]
 Even before the day was out, Lee was cast in the role of villain, and his vilification became an integral part of after-battle reports written by Washington's officers.[59] Lee continued in his post as second-in-command immediately after the battle, and it is likely that the issue would have simply subsided if he had let it go. On 30 June, after protesting his innocence to all who would listen, Lee wrote an insolent letter to Washington in which he blamed ""dirty earwigs"" for turning Washington against him, claimed his decision to retreat had saved the day, and pronounced Washington to be ""guilty of an act of cruel injustice"" towards him. Instead of the apology Lee was tactlessly seeking, Washington replied that the tone of Lee's letter was ""highly improper"" and that he would initiate an official inquiry into Lee's conduct. Lee's response demanding a court martial was again insolent, and Washington ordered his arrest and set about obliging him.[60][61][62]
 The court convened on 4 July 1778, and three charges were laid before Lee: disobeying orders in not attacking on the morning of the battle, contrary to ""repeated instructions""; conducting an ""unnecessary, disorderly, and shameful retreat""; and disrespect towards the commander-in-chief. The trial concluded on 12 August 1778, and the accusations and counter-accusations continued to fly until the verdict was confirmed by Congress on 5 December 1778.[63] Lee's defense was articulate but fatally flawed by his efforts to turn it into a personal contest between himself and Washington. He denigrated the commander-in-chief's role in the battle, calling Washington's official account ""from beginning to end a most abominable damn'd lie"", and disingenuously cast his own decision to retreat as a ""masterful manoeuvre"" designed to lure the British onto the main body.[64] Washington remained aloof from the controversy, but his allies portrayed Lee as a traitor who had allowed the British to escape and linked him to the previous winter's alleged conspiracy against Washington.[65]
 Although the first two charges proved to be dubious,[c] Lee was undeniably guilty of disrespect, and Washington was too powerful to cross.[69] As the historian John Shy noted, ""Under the circumstances, an acquittal on the first two charges would have been a vote of no-confidence in Washington.""[70] Lee was found guilty on all three counts, but the court deleted ""shameful"" from the second and noted the retreat was ""disorderly"" only ""in some few instances."" Lee was suspended from the army for a year, a sentence so lenient that some interpreted it as a vindication of all but the charge of disrespect.[71] Lee continued to argue his case and rage against Washington to anyone who would listen, prompting both Lieutenant Colonel John Laurens, one of Washington's aides, and Steuben to challenge him to a duel.[72] Only the duel with Laurens actually transpired on December 23, 1778, during which Lee was wounded in the side. Laurens, believing the wound was more serious than it seemed, went to help the general. However, Lee said it was fine and proposed to shoot a second time. The men's seconds, Alexander Hamilton and Evan Edwards, opposed this idea and had the duel end there, despite Lee's protests to fire again and Laurens's agreeance. In 1780, Lee sent such a poorly received letter to Congress that it terminated his service with the army.[73][74][75]
 Lee retired to his Prato Rio property, where he bred horses and dogs.[3][4] However, debts had again accumulated, and his advisors recommended liquidating the property. By the spring of 1780, in addition to more frequent gout attacks, Lee had acquired a chronic cough which with other symptoms might have indicated tuberculosis. He made a final tour of Baltimore, Williamsburg, and Fredericksburg, Virginia; Frederick, Maryland; and western Pennsylvania.[76] While visiting Philadelphia to complete the property's sale (that to Maryland buyers having fallen through), he was stricken with fever and died[1] in an inn on 2 October 1782.[3][4] Despite a provision of his will that denounced organized religion and specifically forbade burial near a church or religious meeting house, his remains were taken to the City Tavern for friends and dignitaries to pay their respects, then a military escort took his remains to Christ Church, where after a brief Anglican service, Lee was buried in the churchyard in an unmarked grave.[1][3][4][77] Lee left his property to his sister, Sidney Lee, who died unmarried in 1788.[3][78]
 Lee considered Native Americans as fitting the model of the noble savage, as did others of his time, including his friend Sir William Johnson, 1st Baronet. Describing them as ""hospitable, friendly and civil to an immense degree"", Lee wrote letters home to his family and friends, urging them to educate themselves on the truth against the media's false reports on the Natives: ""I can assure you that they are a much better sort of people than commonly represented"".[79]
 Lee's last home, Prato Rio, still exists, and is listed on the National Register of Historic Places. A historical marker indicates General Lee's service. Much of the adjoining property, which has many natural springs, has been federally owned since 1931, and is currently operated by the U.S. Geological Survey as the Leetown Science Center (formerly the National Fish Hatchery and Research Station), as well as the federal agency's eastern regional office.[80]
 Fort Lee, New Jersey, on the west side of the Hudson River (across the water from Fort Washington, New York), was named for him during his life. Lee, Massachusetts; Lee, New Hampshire; and Leetown, West Virginia[81] were also named for him.
 Lee's place in history was further tarnished in the 1850s when George H. Moore, the librarian at the New-York Historical Society, discovered a manuscript dated 29 March 1777, written by Lee while he was a British prisoner of war. It was addressed to the ""Royal Commissioners"", i.e., Richard Howe, later 1st Earl Howe, and Richard's brother, Sir William Howe, later 5th Viscount Howe, respectively the British naval and army commanders in North America at the time, and detailed a plan by which the British might defeat the rebellion. Moore's discovery, presented in a paper titled ""The Treason of Charles Lee"" in 1858, influenced perceptions of Lee for decades.[82] Lee's infamy became orthodoxy in such 19th-century works as Washington Irving's Life of George Washington (1855–1859), George Washington Parke Custis's Recollections and Private Memoirs of Washington (1861) and George Bancroft's History of the United States of America, from the Discovery of the American Continent (1854–1878).[83] Although most modern scholars reject the idea that Lee was guilty of treason, it is given credence in some accounts, examples being Willard Sterne Randall's account of the Battle of Monmouth in George Washington: A Life (1997), and Dominick Mazzagetti's Charles Lee: Self Before Country (2013).[84][85][86]
"
Philip Schuyler,https://en.wikipedia.org/wiki/Philip_Schuyler,"Philip John Schuyler (/ˈskaɪlər/; November 20, 1733 - November 18, 1804) was an American general in the Revolutionary War and a United States Senator from New York.[2] He is usually known as Philip Schuyler, while his son is usually known as Philip J. Schuyler.
 Born in Albany, Province of New York, into the prosperous Schuyler family, Schuyler fought in the French and Indian War. He won election to the New York General Assembly in 1768 and to the Continental Congress in 1775. He planned the Continental Army's 1775 Invasion of Quebec, but poor health forced him to delegate command of the invasion to Richard Montgomery. He prepared the Continental Army's defense of the 1777 Saratoga campaign, but was replaced by Major General Horatio Gates as the commander of Continental forces in the theater. Schuyler resigned from the Continental Army in 1779.
 Schuyler served in the New York State Senate for most of the 1780s and supported the ratification of the United States Constitution. He represented New York in the 1st United States Congress but lost his state's 1791 Senate election to Aaron Burr. After a period in the state senate, he won election to the United States Senate again in 1797, affiliating with the Federalist Party. He resigned due to poor health the following year. He was the father of Elizabeth Schuyler Hamilton and the father-in-law of Secretary of the Treasury Alexander Hamilton.
 Philip John Schuyler was born on November 20 [O.S. November 9] 1733[3] in Albany, New York, to Cornelia Van Cortlandt (1698–1762) and Johannes (""John"") Schuyler Jr. (1697–1741), the third generation of the Dutch Schuyler family in America. His maternal grandfather was Stephanus Van Cortlandt, the 17th Mayor of New York City.[4]
 Before his father died on the eve of his eighth birthday, Schuyler attended the public school in Albany.[5] Afterward, he was educated by tutors at the Van Cortlandt family estate at New Rochelle. Fluent in both Dutch and English from childhood,[6] in 1748 he began to study with Reverend Peter Strouppe at the New Rochelle French Protestant Church, where he learned French and mathematics.[5] While he was at New Rochelle he also joined numerous trade expeditions where he met Iroquois leaders and learned to speak Mohawk.[6]
 Schuyler joined the British forces in 1755 during the French and Indian War, raised a provincial company, and was commissioned as its captain by his cousin, Lieutenant Governor James Delancey.[5] In 1756, he accompanied British officer Colonel John Bradstreet to Oswego, where he gained experience as a quartermaster, which ended when the outpost fell to the French.[5] Schuyler took part in the battles of Lake George, Oswego River, Carillon and Fort Frontenac.[5]
 After the war, Bradstreet sent Schuyler to England to settle Bradstreet's reimbursement claims for expenses he incurred during the war effort, and he remained in England from 1760 to 1763.[7] After returning to British America he took over management of several farms and business enterprises in upstate New York, including a lumber venture in Saratoga.[8] In addition, Schuyler was responsible for constructing the first flax mill in the American colonies.[9] Schuyler became colonel and commander of a militia district regiment in 1767.[10] In 1768, he served as a member of the New York Assembly.[11]
 Schuyler was elected to the Continental Congress in 1775 and served until he was appointed a major general of the Continental Army in June. General Schuyler took command of the Northern Department and planned the Invasion of Quebec. His poor health required him to place Richard Montgomery in command of the invasion.[12] In 1777, he again served in the Continental Congress.
 After returning to the command of the Northern Department in 1777, Schuyler was active in preparing a defense against the Saratoga Campaign, part of a British ""Three Pronged Attack"" strategy to cut the American Colonies in two by invading and occupying New York. During his preparation efforts, Schuyler complained to Major General William Heath about the quality of the reinforcements sent to him, writing that ""one third of the few that have been sent are boys, aged men and negroes, who disgrace our arms... Is it consistent with the Sons of Freedom to trust their all to be defended by slaves?""[13]
 In the summer of 1777, John Burgoyne marched his army south from Quebec and through the valleys of Lakes Champlain and George. On the way he invested the small American garrison occupying Fort Ticonderoga at the nexus of the two lakes. When General Arthur St. Clair abandoned Fort Ticonderoga in July, Congress replaced Schuyler with General Horatio Gates, who had accused Schuyler of dereliction of duty. In 1778, Schuyler and Arthur St. Clair faced a court of inquiry over the loss of Ticonderoga, and both were acquitted.[14][15]
 The British offensive was eventually stopped by Continental Army then under the command of conflict and Benedict Arnold in the Battles of Saratoga. That victory, the first complete victory over a large British force, marked a turning point in the revolution, for it convinced France to enter the war on the American side. When Schuyler demanded a court martial to answer Gates' charges, he was vindicated but resigned from the Continental Army on April 19, 1779. He then served in two more sessions of the Continental Congress in 1779 and 1780.
 As a prominent politician and Patriot leader in New York, Schuyler was the subject of an unsuccessful kidnapping attempt, which was plotted and led by John Walden Meyers on August 7, 1781. Schuyler was able to vacate his Albany mansion before the kidnappers arrived.[16] Schuyler was an original member of the New York Society of the Cincinnati.
 After the war, he expanded his Saratoga estate to tens of thousands of acres, adding slaves, tenant farmers, a store, and mills for flour, flax, and lumber. He built several schooners on the Hudson River, and named the first Saratoga. According to the Schuyler Mansion Historic Society, there were around 40 slaves between the Albany and Saratoga estates.[17]
 He was a member of the New York State Senate from 1780 to 1784, and at the same time New York State Surveyor General from 1781 to 1784.[18] Afterwards he returned to the State Senate from 1786 to 1790, where he actively supported the adoption of the United States Constitution.[19]
 In 1789, he was elected a U.S. Senator from New York to the First United States Congress, serving from July 27, 1789, to March 3, 1791. After losing his bid for re-election in 1791 to Aaron Burr, he returned to the State Senate from 1792 to 1797. In 1797, he was selected again to the U.S. Senate and served in the 5th United States Congress from March 4, 1797, until his resignation because of ill health on January 3, 1798.[20]
 According to the Schuyler Family's Bible, on September 7, 1755, he married Catherine Van Rensselaer (1734–1803) at Albany. In the Bible entry, he was called ""Philip Johannes Schuyler"" and she was called ""Catherina Van Rensselaer"".  She was the daughter of Johannes Van Rensselaer (1707/08–1783) and his first wife, Engeltje Livingston (1698–1746/47). Johannes was the grandson of Hendrick van Rensselaer (1667–1740). Engeltje was the daughter of Robert Livingston the Younger. Philip and Catherine had 15 children together, eight of whom survived to adulthood, including:
 Schuyler's country home had been destroyed by General John Burgoyne's forces in October 1777. Later that year, he began rebuilding on the same site, now located in southern Schuylerville, New York. This later home is maintained by the National Park Service as part of the Saratoga National Historical Park, and is open to the public.
 Schuyler died at the Schuyler Mansion in Albany on November 18, 1804, four months after his son-in-law, Alexander Hamilton, was killed in a duel and 2 days before his 71st birthday. He is buried at Albany Rural Cemetery in Menands, New York.
 Geographic locations and buildings named in Schuyler's honor include:
 Schuyler was depicted by John Trumbull in his 1821 painting Surrender of General Burgoyne, which hangs in the United States Capitol rotunda in Washington, D.C.
 Major General Philip Schuyler, a bronze statue by sculptor J. Massey Rhind, was erected outside Albany City Hall in 1925. In June 2020, Albany mayor Kathy Sheehan signed an executive order for the statue to be removed and given to a ""museum or other institution for future display with appropriate historical context"", due to Schuyler's ownership of slaves.[30] The statue was requested the next day by the mayor of Schuylerville, New York, who suggested that it be relocated to Schuyler House.[31] In the early morning of June 10, 2023, the statue was removed from its pedestal to a trailer and transported to an undisclosed storage location.[32] After the statue was removed on June 10, 2023, a time capsule was discovered underneath in a sealed metal box.
 The non-speaking role of Philip Schuyler was originated by ensemble member Sydney James Harcourt in the 2015 Broadway musical Hamilton, in which Schuyler's son-in-law Alexander Hamilton is the title character.[33]
"
Nathanael Greene,https://en.wikipedia.org/wiki/Nathanael_Greene,"
 Major-General Nathanael Greene (August 7, 1742 – June 19, 1786) was an American military officer and planter who served in the Continental Army during the Revolutionary War. He emerged from the war with a reputation as one of George Washington's most talented and dependable officers and is known for his successful command in the Southern theater of the conflict.
 Born into a prosperous Quaker family in Warwick, Rhode Island, Greene became active in the colonial opposition to British revenue policies in the early 1770s and helped establish the Kentish Guards, a state militia unit. After the April 1775 Battles of Lexington and Concord, the legislature of Rhode Island established an army and appointed Greene to command it. Later in the year, Greene became a general in the newly established Continental Army. Greene served under George Washington in the Boston campaign, the New York and New Jersey campaign, and the Philadelphia campaign before being appointed quartermaster general of the Continental Army in 1778.
 In October 1780, Washington appointed Greene as the commander of the Continental Army in the southern theater, where he was involved in several engagements, primarily in Virginia, Georgia, and South Carolina. After taking command, Greene engaged in a successful campaign of guerrilla warfare against a numerically superior British force led by Charles Cornwallis. He gained several strategic victories at Guilford Court House, Hobkirk's Hill, and Eutaw Springs, eroding British control over the American South.
 Major fighting on land came to an end following the surrender of Cornwallis at the siege of Yorktown in October 1781, but Greene continued to serve in the Continental Army until late 1783. After the war, he settled down to a career as a plantation owner in Georgia, but his rice crops were mostly a failure. He died in 1786 at the Mulberry Grove Plantation in Chatham County, Georgia. Numerous locations in the United States are named for him.
 Greene was born on Forge Farm at Potowomut in the township of Warwick, Rhode Island, which was then part of the Colony of Rhode Island and Providence Plantations in British America. He was the second son of Mary Mott and Nathanael Greene Sr., a prosperous Quaker merchant and farmer.[1] Greene was descended from John Greene and Samuel Gorton, both of whom were founding settlers of Warwick.[2] Greene had two older half-brothers from his father's first marriage and was one of six children born to Nathanael and Mary. Due to religious beliefs, Greene's father discouraged book learning, as well as dancing and other activities.[3] Nonetheless, Greene convinced his father to hire a tutor, and he studied mathematics, the classics, law, and various works of the Age of Enlightenment.[4] At some point during his childhood, Greene gained a slight limp that would remain with him for the rest of his life.[5]
 In 1770, Greene moved to Coventry, Rhode Island, to take charge of the family-owned foundry, and he built a house in Coventry called Spell Hall. Later in the year, Greene and his brothers inherited the family business after their father's death. Greene began to assemble a large library that included military histories by authors like Julius Caesar, Frederick the Great, and Maurice de Saxe.[6]
 In July 1774, Greene married the nineteen-year-old Catharine Littlefield, a niece by marriage of his distant cousin, William Greene, an influential political leader in Rhode Island.[7] The same year, one of Greene's younger brothers married a daughter of Samuel Ward, a prominent Rhode Island politician who became an important political ally until his death in 1776.[8] Greene and Catherine's first child was born in 1776, and they had six more children between 1777 and 1786.[9]
 After the French and Indian War (1754–1763), the British parliament began imposing new policies designed to raise revenue from British America for a war which colonists had played a pivotal role in instigating.[10][11] After British official William Dudington seized a vessel owned by Greene and his brothers, Greene filed an ultimately successful lawsuit against Dudington for damages. While the lawsuit was pending, Dudington's vessel was torched by a Rhode Island mob in what became known as the Gaspee Affair. In the aftermath of the Gaspee Affair, Greene became increasingly alienated from the British.[12] At the same time, Greene drifted away from his father's Quaker faith, and he was suspended from Quaker meetings in July 1773.[13] In 1774, after the passage of measures that colonials derided as the ""Intolerable Acts,"" Greene helped organize a state militia unit known as the Kentish Guards.[14] Because of his limp, Greene was not selected as an officer in the militia.[15]
 The American Revolutionary War broke out with the April 1775 Battles of Lexington and Concord. In early May, the legislature of Rhode Island established the Rhode Island Army of Observation and appointed Greene to command it. Greene's army marched to Boston, where other colonial forces were laying siege to a British garrison.[16] He missed the June 1775 Battle of Bunker Hill because he was visiting Rhode Island at the time, but he returned almost immediately after the battle and was impressed by the performance of colonial forces.[17] That same month, the Second Continental Congress established the Continental Army and appointed George Washington to command all colonial forces. In addition to Washington, Congress appointed sixteen generals, and Greene was appointed as a brigadier general in the Continental Army. Washington took command of the Siege of Boston in July 1775, bringing with him generals such as Charles Lee, Horatio Gates, and Thomas Mifflin.[18] Washington organized the Continental Army into three divisions, each consisting of regiments from different colonies, and Greene was given command of a brigade consisting of seven regiments.[19] The Siege of Boston continued until March 1776, when British forces evacuated from the city. After the end of the siege, Greene briefly served as the commander of military forces in Boston, but he rejoined Washington's army in April 1776.[20]
 Washington established his headquarters in Manhattan, and Greene was tasked with preparing for the invasion of nearby Long Island.[21] While he focused on building up fortifications in Brooklyn, Greene befriended General Henry Knox and struck up a correspondence with John Adams. He was also, along with several other individuals, promoted to major general by an act of Congress.[22] Because of a severe fever, he did not take part in the Battle of Long Island, which ended with an American retreat from Long Island.[23] After the battle, Greene urged Washington to raze Manhattan so that it would not fall into the hands of the British, but Congress forbade Washington from doing so. Unable to raze Manhattan, Washington initially wanted to fortify the city, but Greene joined with several officers in convincing Washington that the city was indefensible. During the withdrawal from Manhattan, Greene saw combat for the first time in the Battle of Harlem Heights, a minor British defeat that nonetheless represented one of the first American victories in the war.[24]
 After the Battle of Harlem Heights, Washington placed Greene in command of both Fort Constitution (later known as Fort Lee), which was on New Jersey side of the Hudson River, and Fort Washington, which was across the river from Fort Constitution.[25] While in command of Fort Lee, Greene established supply depots in New Jersey along a potential line of retreat; these would later prove to be valuable resources for the Continental Army.[26] Washington suggested to Greene that he remove the garrison from Fort Washington due to its vulnerability to a British attack, but he ultimately deferred to Greene's decision to continue to station soldiers there. In the subsequent Battle of Fort Washington, fought in November 1776, the British captured the Fort Washington and its 3,000-man garrison. Greene was subjected to heavy criticism in the aftermath of the battle, but Washington declined to relieve Greene from command.[27] Shortly after the Battle of Fort Washington, a British force under General Cornwallis captured Fort Lee, and the Continental Army began a retreat across New Jersey and into Pennsylvania.[28] Greene commanded part of Washington's army in the December 1776 Battle of Trenton and the January 1777 Battle of Princeton, both of which were victories for the Continental Army.[29]
 Along with the rest of Washington's army, Greene was stationed in New Jersey throughout the first half of 1777.[30] In July 1777, he publicly threatened to resign over the appointment of a French officer to the Continental Army, but he ultimately retained his commission.[31] Meanwhile, the British began a campaign to capture Philadelphia, the seat of Congress. At the Battle of the Brandywine, Greene commanded a division at the center of the American line, but the British launched a flanking maneuver. Greene's division helped prevent the envelopment of American forces and allowed for a safe retreat.[32] The British captured Philadelphia shortly after the Battle of the Brandywine, but Washington launched a surprise attack on a British force at the October 1777 Battle of Germantown.[33] Greene's detachment arrived late to the battle, which ended in another American defeat.[34] In December, Greene joined with the rest of Washington's army in establishing a camp at Valley Forge, located twenty-five miles northwest of Philadelphia.[35] Over the winter of 1777–1778, he clashed with Thomas Mifflin and other members of the Conway Cabal, a group that frequently criticized Washington and sought to install Horatio Gates as commander-in-chief of the Continental Army.[36]
 In March 1778, Greene reluctantly accepted the position of quartermaster general, making him responsible for procuring supplies for the Continental Army.[37] Along with his top two assistants, Charles Pettit and John Cox, Greene reorganized his 3,000-person department, establishing supply depots in strategic places across the United States.[38] As quartermaster general, Greene continued to attend Washington's councils-of-war, an unusual arrangement for a staff officer.[39] After France joined the war in early 1778, the British army in Philadelphia was ordered to New York.[39] Along with Anthony Wayne and the Marquis de Lafayette, Greene recommended an attack on the British force while it retreated across New Jersey to New York. Greene commanded a division in the subsequent Battle of Monmouth, which, after hours of fighting, ended indecisively.[40]
 In July 1778, Washington granted Greene temporary leave as quartermaster general so that he could take part in an attack on British forces stationed in his home state of Rhode Island.[41] The offensive was designed as a combined Franco-American operation under the command of General John Sullivan and French admiral d'Estaing, but the French fleet withdrew due to bad weather conditions.[42] Greene fought in the subsequent Battle of Rhode Island, an inconclusive battle that ended with a British retreat from the American position. After the battle, the American force under Sullivan left Rhode Island, while Greene returned to his duties as quartermaster general.[43]
 After mid-1778, the Northern theater of the war became a stalemate, as the main British force remained in New York City and Washington's force was stationed nearby on the Hudson River. The British turned their attention to the Southern theater of the war, launching an ultimately successful expedition to capture Savannah.[44] Though he desired a battlefield command, Greene continued to serve as the Continental Army's quartermaster general.[45] As Congress was increasingly powerless to furnish funds for supplies, Greene became an advocate of a stronger national government.[46] In June 1780, while Washington's main force continued to guard the Hudson River, Greene led a detachment to block the advance of a British contingent through New Jersey. Despite being vastly outnumbered in the Battle of Springfield, Greene forced the withdrawal of the British force on the field.[47] Shortly after the battle, Greene resigned as quartermaster general in a letter that strongly criticized Congress; although some members of Congress were so outraged by the letter that they sought to relieve Greene of his officer's commission, Washington's intervention ensured that Greene retained a position in the Continental Army.[48] After Benedict Arnold defected to the British, Greene briefly served as the commandant of West Point and presided over the execution of John André, Arnold's contact in the British army.[49]
 By October 1780, the Continental Army had suffered several devastating defeats in the South under the command of Benjamin Lincoln and Horatio Gates, leaving the United States at a major disadvantage in the Southern theater of the war.[50] On October 14, 1780, Washington, acting on the authorization of Congress, appointed Greene as the commander of the Southern Department of the Continental Army.[51] By the time he took command, the British were in control of key portions of Georgia and South Carolina, and the governments of the Southern states were unable to provide much support to the Continental Army. Greene would face a 6,000-man British army led by General Cornwallis and cavalry commander Banastre Tarleton, as well as numerous Loyalist militias that worked with the British. Outnumbered and under-supplied, Greene settled on a strategy of guerrilla warfare rather than pitched battles in order to prevent the advance of the British into North Carolina and Virginia.[52] His strategy would heavily depend on riverboats and cavalry to outmaneuver and harass British forces.[53] Among Greene's key subordinates in the Southern campaign were his second-in-command, Friedrich Wilhelm von Steuben, cavalry commander Henry Lee, the Marquis de Lafayette, Daniel Morgan, and Francis Marion.[54]
 While en route to the Southern theater, Greene learned of the October 1780 American victory at the Battle of Kings Mountain, which postponed Cornwallis's planned advance into North Carolina.[55] Upon arriving in Charlotte, North Carolina, in December 1780, Greene went against conventional military strategy by dividing his forces; he would lead the main American force southeast, while Morgan would lead a smaller detachment to the southwest.[56] Cornwallis responded by dividing his own forces, marching the main detachment against Greene while Tarleton led a force against Morgan. In the January 1781 Battle of Cowpens, Morgan led Continental troops to a major victory that resulted in the near-total destruction of Tarleton's force.[57] After the battle, Cornwallis set off in pursuit of Morgan, burning some of his own supplies in order to speed up his army's movement. Greene linked up with Morgan and retreated into North Carolina, purposely forcing Cornwallis away from British supply lines.[58] On February 9, in consultation with Morgan[a] and other top officers, Greene decided to continue the retreat north, heading toward the Dan River at the North Carolina-Virginia border.[60]
 With the British in close pursuit, Greene divided his forces, leading the main contingent north while sending a smaller group under Colonel Otho Williams to harass British forces. Greene's force outpaced the British and crossed the Dan River on February 14. Greene's contemporaries were impressed by the speed and efficiency of the retreat through difficult territory; Alexander Hamilton wrote that it was a ""masterpiece of military skill and exertion."" Unwilling to travel even farther from his supply lines, General Cornwallis led his army south to Hillsborough, North Carolina. On February 22, Greene's force crossed back over the Dan River to challenge Cornwallis in North Carolina.[61]
 After crossing back into North Carolina, Greene harassed Cornwallis's army. In early March, he received reinforcements from North Carolina and Virginia, doubling the size of his force to approximately 4,000 men. On March 14, he led his army to Guilford Courthouse and began preparing for an attack by Cornwallis, using a strategy based on Morgan's plan at the Battle of Cowpens. Greene established three defensive lines, with the North Carolina militia making up the first line, the Virginia militia making up the second line, and the Continental Army regulars, positioned on a hill behind a small stream, making up the third line.[62] After skirmishes on the morning of March 15, the main British force launched a full attack in the afternoon, beginning the Battle of Guilford Court House. The first American line fired volleys and then fled, either to the next line or away from the battlefield. The second line held up for longer and continued to resist the British advance while Cornwallis ordered an unsuccessful assault against the third line. The British re-formed and launched an assault on the left flank of the third line, but were overwhelmed by Henry Lee's cavalry. In response, Cornwallis ordered his artillery to fire grapeshot into the fray, hitting British and American soldiers alike. With his army's left flank collapsing, Greene ordered a retreat, bringing the battle to an end. Although the Battle of Guilford Court House ended with an American defeat, the British suffered substantially greater losses.[63]
 After the Battle of Guilford Court House, Cornwallis's force headed south to Wilmington, North Carolina. Greene initially gave chase but declined to press for an attack after much of the militia returned home. To Greene's surprise, in late April Cornwallis's force began a march north to Yorktown, Virginia.[64] Rather than follow Cornwallis, Greene headed South, where he challenged British commander Francis Rawdon for control of South Carolina and Georgia.[65] On April 20, he began a siege of Camden, South Carolina, and established a camp at a nearby ridge known as Hobkirk's Hill. On the 25th, Rawdon launched a surprise attack on Greene's position, beginning the Battle of Hobkirk's Hill. Despite having been taken by surprise, Greene's force nearly achieved victory, but the left flank collapsed and the cavalry failed to arrive. Facing total defeat, Greene ordered a retreat, bringing an end to the battle. Although the American and British forces suffered a similar number of losses in the Battle of Hobkirk's Hill, Greene was deeply disappointed by the result of the battle.[66]
 On May 10, Rawdon's force left Camden for Charleston, South Carolina, effectively conceding control of much of interior South Carolina to the Continental Army. In a series of small actions known as the ""war of the posts,"" Greene and his subordinates further eroded British control of interior South Carolina by capturing several British forts.[67] On June 18, after undertaking the month-long siege of Ninety Six, Greene launched an unsuccessful attack on the British fort at Ninety Six, South Carolina. Although the assault failed, Rawdon ordered the fort abandoned shortly thereafter. Meanwhile, Greene's subordinates further expanded Continental control, capturing Augusta, Georgia, on June 5. By the end of June, the British controlled little more than a thin strip of coastal land from Charleston to Savannah.[68] After resting through much of July and August, the Continental Army resumed operations and engaged a British force on September 8 at the Battle of Eutaw Springs.[69] The battle ended with a Continental retreat, but the British suffered more substantial losses. After the battle, the British force returned to Charleston, leaving interior South Carolina in full control of Continental forces. Congress issued Greene a gold medal and passed a resolution congratulating him for his victory at Eutaw Springs.[70]
 While Greene campaigned in South Carolina and Georgia, Lafayette led Continental resistance to Cornwallis's army in Virginia. Although Greene's command gave him leadership of Continental operations in Virginia, he was unable to closely control events in Virginia from South Carolina. Lafayette heeded Greene's advice to avoid combat, but his force only narrowly escaped destruction at the July 1781 Battle of Green Spring. In August, Washington and French general Rochambeau left New York for Yorktown, intent on inflicting a decisive defeat against Cornwallis.[71] Washington laid siege to Cornwallis at Yorktown, and Cornwallis surrendered on October 19.[72]
 Yorktown was widely regarded as a disastrous defeat for the British, and many considered the war to have effectively ended in late 1781.[73] The governments of North Carolina, South Carolina, and Georgia each voted Greene liberal grants of lands and money, including an estate called ""Boone's Barony"" in Bamberg County, South Carolina, and Mulberry Grove Plantation near Savannah.[74] Nonetheless, the British still controlled New York, Savannah, and Charleston, and Greene still contended with Loyalist militias who sought to destabilize Continental control. With American finances in a disastrous state, Greene also struggled to clothe and feed his troops. In late 1781, he declined appointment to the newly created position of secretary of war, which was charged with overseeing the Continental Army.[75] He also corresponded with Robert Morris, the superintendent of finance of the United States, who shared Greene's view on the need for a stronger national government than the one that had been established in the Articles of Confederation.[76] No major military action occurred in 1782, and the British evacuated Savannah and Charleston before the end of that year.[77] Congress officially declared the end of the war in April 1783, and Greene resigned his commission in late 1783.[78]
 After resigning his commission, Greene returned to Newport. Facing a large amount of debt, he relocated to the South to focus on the slave plantations he had been awarded during the war, and he made his home at the Mulberry Grove Plantation outside of Savannah.[79] In 1784, Greene declined appointment to a commission tasked with negotiating treaties with Native Americans, but he agreed to attend the first meeting of the Society of the Cincinnati.[80] He then became an original member with the Rhode Island Society of the Cincinnati.[81]
 Greene fell ill on June 12, 1786, and he died at Mulberry Grove on June 19, 1786, at the age of 43.[82] The official cause of death was sunstroke. For over a century, his remains were interred at the Graham Vault in Colonial Park Cemetery in Savannah, alongside John Maitland, his arch-rival in the conflict.[83] On November 14, 1902, through the efforts of Rhode Island Society of the Cincinnati President Asa Bird Gardiner, his remains were moved to a monument in Johnson Square in Savannah.[84] Greene Square, about a third of a mile southeast of Johnson Square, was named for him upon its platting in 1799.[85]
 As noted above, Greene was in debt. In 1782 and 1783, Greene had difficulty supplying his troops in Charleston with clothing and provisions. He contracted with Banks & Co to furnish supplies but was compelled to put his name to the bond for the supplies. An order was given by Greene to Robert Morris for payment of the amount; this was paid by the Government of the United States to the contractor, who did not use it to pay the debt and left the bond unpaid. Greene paid the debt himself, and in 1791, his executrix petitioned Congress for relief. Greene had obtained some security from a partner of Banks & Co named Ferrie on a mortgage or lien on a tract of land, but the land was liable to a prior mortgage of £1,000 sterling to an Englishman named Murray. In 1788, the mortgagor in England filed a bill to foreclose on the mortgage, while Greene's family instituted proceedings against Ferrie, who was entitled to a reversionary interest in the land. The court ordered the land be sold and the sale proceeds to be first used to extinguish the mortgage, with the balance to go to representatives of General Greene. The land was sold, and after the £1,000 mortgage had been paid off, the residue of £2,400 was to go Greene's representatives. However, the purchaser never took title and never paid the money on the grounds that the title was in dispute. In 1792, a Relief Act was passed by Congress for General Greene which was based upon the decree of the land sale; the sum of which he was entitled to (£2,400) was exempted out of the indemnity allowed him at that time, not one cent of which his heirs received except $2,000 (~$60,380 in 2023). In 1830, the administrators of Murray filed a bill of Chancery against the land; however, his agent who had bought the land had not taken title to it, on the grounds that there was a dispute about the land. The claim to the title was not resolved, and the money was never paid. Meanwhile, from 1789 to 1840, the plantation had gone to ruin; under the original decree, the land, instead of bringing the sum it had first bought, was sold for only $13,000. This left Greene's representatives only about $2,000 instead of £2,400. In 1840, they applied to Congress for the difference between the two sums. In 1854, the case was put to Congress for the relief of Phineas Nightingale, who was the administrator of the deceased General Greene.[86]
 Defense analyst Robert Killebrew writes that Greene was ""regarded by peers and historians as the second-best American general"" in the Revolutionary War, after Washington.[87] The historian Russell Weigley believed that ""Greene's outstanding characteristic as a strategist was his ability to weave the maraudings of partisan raiders into a coherent pattern, coordinating them with the maneuvers of a field army otherwise too weak to accomplish much, and making the combination a deadly one.... [He] remains alone as an American master developing a strategy of unconventional war.""[87] Historian Curtis F. Morgan Jr. describes Greene as Washington's ""most trusted military subordinate.""[88] According to Golway, ""on at least two occasions, fellow officers and politicians described Greene... as the man Washington had designated to succeed him if he were killed or captured.""[89] He was also respected by his opponents; Cornwallis wrote that Greene was ""as dangerous as Washington. He is vigilant, enterprising, and full of resources–there is but little hope of gaining an advantage over him.""[90] Alexander Hamilton wrote that Greene's death deprived the country of a ""universal and pervading genius which qualified him not less for the Senate than for the field.""[91] Killebrew argues that Greene was the ""most underrated general"" in American history.[87]
 His statue, along with that of Roger Williams, represents the state of Rhode Island in the National Statuary Hall Collection in the United States Capitol. Washington, D.C., also hosts a bronze equestrian statue of Greene in Stanton Park. A large oil portrait of Nathanael Greene hangs in the State Room in the Rhode Island State House, and a statue stands outside the building's south facade. A cenotaph to him stands in the Old Forge Burial Ground in Warwick.[92] Greene is also memorialized by statues in or near Philadelphia, Valley Forge National Historical Park, Greensboro, North Carolina,[93] Greensburg, Pennsylvania, and Greenville, South Carolina. The Nathanael Greene Monument in Savannah, Georgia, serves as his burial place.
 Numerous places and things have been named after Greene across the United States. Fourteen counties are named for Greene, the most populous of which is Greene County, Missouri. Municipalities named for Greene include Greensboro, North Carolina; Greensboro, Georgia; Greensburg, Pennsylvania; Greenville, North Carolina; Greenville, South Carolina; Greeneville, Tennessee; and Greensboro, Pennsylvania. Other things named for Greene include Fort Greene Park in Brooklyn and several schools. Several ships have been named for Greene, including the USRC General Green, the USS General Greene, the USS Nathanael Greene, and the USAV MGen Nathanael Greene.
 The Nathanael Greene Homestead in Coventry, Rhode Island, features Spell Hall, which was General Greene's home, built in 1774. Greene commissioned cabinetmaker Thomas Spencer to build a desk and bookcase, likely to be put in this new home. The desk and bookcase is now at the High Museum of Art in Atlanta, Georgia. It was built in East Greenwich, Rhode Island, in the Chippendale Style. An inscription written in graphite on an interior drawer says that the desk originally belonged to Nathanael Greene.[94]
"
